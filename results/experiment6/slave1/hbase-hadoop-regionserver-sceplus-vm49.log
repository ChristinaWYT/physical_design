Tue Jul  1 07:31:36 PDT 2014 Starting regionserver on sceplus-vm49
core file size          (blocks, -c) 0
data seg size           (kbytes, -d) unlimited
scheduling priority             (-e) 0
file size               (blocks, -f) unlimited
pending signals                 (-i) 128203
max locked memory       (kbytes, -l) 64
max memory size         (kbytes, -m) unlimited
open files                      (-n) 32768
pipe size            (512 bytes, -p) 8
POSIX message queues     (bytes, -q) 819200
real-time priority              (-r) 0
stack size              (kbytes, -s) 8192
cpu time               (seconds, -t) unlimited
max user processes              (-u) 128203
virtual memory          (kbytes, -v) unlimited
file locks                      (-x) unlimited
2014-07-01 07:31:37,304 INFO  [main] util.VersionInfo: HBase 0.98.3-hadoop1
2014-07-01 07:31:37,305 INFO  [main] util.VersionInfo: Subversion git://acer/usr/src/Hadoop/hbase -r d5e65a9144e315bb0a964e7730871af32f5018d5
2014-07-01 07:31:37,305 INFO  [main] util.VersionInfo: Compiled by apurtell on Sat May 31 19:34:57 PDT 2014
2014-07-01 07:31:37,524 INFO  [main] util.ServerCommandLine: env:JAVA_HOME=/usr/lib/jvm/java-1.6.0-openjdk-amd64/
2014-07-01 07:31:37,524 INFO  [main] util.ServerCommandLine: env:SHLVL=3
2014-07-01 07:31:37,524 INFO  [main] util.ServerCommandLine: env:HBASE_LOG_DIR=/home/hadoop/hbase/bin/../logs
2014-07-01 07:31:37,524 INFO  [main] util.ServerCommandLine: env:HBASE_HOME=/home/hadoop/hbase/bin/..
2014-07-01 07:31:37,524 INFO  [main] util.ServerCommandLine: env:HBASE_OPTS=-XX:+UseConcMarkSweepGC   -Dhbase.log.dir=/home/hadoop/hbase/bin/../logs -Dhbase.log.file=hbase-hadoop-regionserver-sceplus-vm49.log -Dhbase.home.dir=/home/hadoop/hbase/bin/.. -Dhbase.id.str=hadoop -Dhbase.root.logger=INFO,RFA -Djava.library.path=/home/hadoop/hbase/bin/../lib/native/Linux-amd64-64 -Dhbase.security.logger=INFO,RFAS
2014-07-01 07:31:37,524 INFO  [main] util.ServerCommandLine: env:HBASE_ENV_INIT=true
2014-07-01 07:31:37,524 INFO  [main] util.ServerCommandLine: env:SSH_CLIENT=9.1.143.58 47865 22
2014-07-01 07:31:37,524 INFO  [main] util.ServerCommandLine: env:HBASE_HEAPSIZE=10240
2014-07-01 07:31:37,525 INFO  [main] util.ServerCommandLine: env:MAIL=/var/mail/hadoop
2014-07-01 07:31:37,525 INFO  [main] util.ServerCommandLine: env:HBASE_ZNODE_FILE=/var/hadoop/pids/hbase-hadoop-regionserver.znode
2014-07-01 07:31:37,525 INFO  [main] util.ServerCommandLine: env:PWD=/home/hadoop/hbase
2014-07-01 07:31:37,525 INFO  [main] util.ServerCommandLine: env:LOGNAME=hadoop
2014-07-01 07:31:37,525 INFO  [main] util.ServerCommandLine: env:HBASE_MANAGES_ZK=true
2014-07-01 07:31:37,525 INFO  [main] util.ServerCommandLine: env:HBASE_NICENESS=0
2014-07-01 07:31:37,525 INFO  [main] util.ServerCommandLine: env:HBASE_REST_OPTS=
2014-07-01 07:31:37,525 INFO  [main] util.ServerCommandLine: env:LD_LIBRARY_PATH=/usr/lib/jvm/java-6-openjdk-amd64/jre/lib/amd64/server:/usr/lib/jvm/java-6-openjdk-amd64/jre/lib/amd64:/usr/lib/jvm/java-6-openjdk-amd64/jre/../lib/amd64::/home/hadoop/hbase/bin/../lib/native/Linux-amd64-64
2014-07-01 07:31:37,525 INFO  [main] util.ServerCommandLine: env:MALLOC_ARENA_MAX=4
2014-07-01 07:31:37,525 INFO  [main] util.ServerCommandLine: env:SSH_CONNECTION=9.1.143.58 47865 9.1.143.59 22
2014-07-01 07:31:37,525 INFO  [main] util.ServerCommandLine: env:SHELL=/bin/bash
2014-07-01 07:31:37,526 INFO  [main] util.ServerCommandLine: env:HBASE_PID_DIR=/var/hadoop/pids
2014-07-01 07:31:37,526 INFO  [main] util.ServerCommandLine: env:HBASE_ROOT_LOGGER=INFO,RFA
2014-07-01 07:31:37,528 INFO  [main] util.ServerCommandLine: env:CLASSPATH=/home/hadoop/hbase/conf:/usr/lib/jvm/java-1.6.0-openjdk-amd64//lib/tools.jar:/home/hadoop/hbase/bin/..:/home/hadoop/hbase/bin/../lib/activation-1.1.jar:/home/hadoop/hbase/bin/../lib/asm-3.1.jar:/home/hadoop/hbase/bin/../lib/commons-beanutils-1.7.0.jar:/home/hadoop/hbase/bin/../lib/commons-beanutils-core-1.8.0.jar:/home/hadoop/hbase/bin/../lib/commons-cli-1.2.jar:/home/hadoop/hbase/bin/../lib/commons-codec-1.7.jar:/home/hadoop/hbase/bin/../lib/commons-collections-3.2.1.jar:/home/hadoop/hbase/bin/../lib/commons-configuration-1.6.jar:/home/hadoop/hbase/bin/../lib/commons-digester-1.8.jar:/home/hadoop/hbase/bin/../lib/commons-el-1.0.jar:/home/hadoop/hbase/bin/../lib/commons-httpclient-3.1.jar:/home/hadoop/hbase/bin/../lib/commons-io-2.4.jar:/home/hadoop/hbase/bin/../lib/commons-lang-2.6.jar:/home/hadoop/hbase/bin/../lib/commons-logging-1.1.1.jar:/home/hadoop/hbase/bin/../lib/commons-math-2.1.jar:/home/hadoop/hbase/bin/../lib/commons-net-1.4.1.jar:/home/hadoop/hbase/bin/../lib/findbugs-annotations-1.3.9-1.jar:/home/hadoop/hbase/bin/../lib/guava-12.0.1.jar:/home/hadoop/hbase/bin/../lib/hadoop-core-1.2.1.jar:/home/hadoop/hbase/bin/../lib/hamcrest-core-1.3.jar:/home/hadoop/hbase/bin/../lib/hbase-client-0.98.3-hadoop1.jar:/home/hadoop/hbase/bin/../lib/hbase-common-0.98.3-hadoop1.jar:/home/hadoop/hbase/bin/../lib/hbase-common-0.98.3-hadoop1-tests.jar:/home/hadoop/hbase/bin/../lib/hbase-examples-0.98.3-hadoop1.jar:/home/hadoop/hbase/bin/../lib/hbase-hadoop1-compat-0.98.3-hadoop1.jar:/home/hadoop/hbase/bin/../lib/hbase-hadoop-compat-0.98.3-hadoop1.jar:/home/hadoop/hbase/bin/../lib/hbase-it-0.98.3-hadoop1.jar:/home/hadoop/hbase/bin/../lib/hbase-it-0.98.3-hadoop1-tests.jar:/home/hadoop/hbase/bin/../lib/hbase-prefix-tree-0.98.3-hadoop1.jar:/home/hadoop/hbase/bin/../lib/hbase-protocol-0.98.3-hadoop1.jar:/home/hadoop/hbase/bin/../lib/hbase-server-0.98.3-hadoop1.jar:/home/hadoop/hbase/bin/../lib/hbase-server-0.98.3-hadoop1-tests.jar:/home/hadoop/hbase/bin/../lib/hbase-shell-0.98.3-hadoop1.jar:/home/hadoop/hbase/bin/../lib/hbase-testing-util-0.98.3-hadoop1.jar:/home/hadoop/hbase/bin/../lib/hbase-thrift-0.98.3-hadoop1.jar:/home/hadoop/hbase/bin/../lib/high-scale-lib-1.1.1.jar:/home/hadoop/hbase/bin/../lib/htrace-core-2.04.jar:/home/hadoop/hbase/bin/../lib/httpclient-4.1.3.jar:/home/hadoop/hbase/bin/../lib/httpcore-4.1.3.jar:/home/hadoop/hbase/bin/../lib/jackson-core-asl-1.8.8.jar:/home/hadoop/hbase/bin/../lib/jackson-jaxrs-1.8.8.jar:/home/hadoop/hbase/bin/../lib/jackson-mapper-asl-1.8.8.jar:/home/hadoop/hbase/bin/../lib/jackson-xc-1.8.8.jar:/home/hadoop/hbase/bin/../lib/jamon-runtime-2.3.1.jar:/home/hadoop/hbase/bin/../lib/jasper-compiler-5.5.23.jar:/home/hadoop/hbase/bin/../lib/jasper-runtime-5.5.23.jar:/home/hadoop/hbase/bin/../lib/jaxb-api-2.2.2.jar:/home/hadoop/hbase/bin/../lib/jaxb-impl-2.2.3-1.jar:/home/hadoop/hbase/bin/../lib/jersey-core-1.8.jar:/home/hadoop/hbase/bin/../lib/jersey-json-1.8.jar:/home/hadoop/hbase/bin/../lib/jersey-server-1.8.jar:/home/hadoop/hbase/bin/../lib/jettison-1.3.1.jar:/home/hadoop/hbase/bin/../lib/jetty-6.1.26.jar:/home/hadoop/hbase/bin/../lib/jetty-sslengine-6.1.26.jar:/home/hadoop/hbase/bin/../lib/jetty-util-6.1.26.jar:/home/hadoop/hbase/bin/../lib/jruby-complete-1.6.8.jar:/home/hadoop/hbase/bin/../lib/jsp-2.1-6.1.14.jar:/home/hadoop/hbase/bin/../lib/jsp-api-2.1-6.1.14.jar:/home/hadoop/hbase/bin/../lib/jsr305-1.3.9.jar:/home/hadoop/hbase/bin/../lib/junit-4.11.jar:/home/hadoop/hbase/bin/../lib/libthrift-0.9.0.jar:/home/hadoop/hbase/bin/../lib/log4j-1.2.17.jar:/home/hadoop/hbase/bin/../lib/metrics-core-2.1.2.jar:/home/hadoop/hbase/bin/../lib/netty-3.6.6.Final.jar:/home/hadoop/hbase/bin/../lib/protobuf-java-2.5.0.jar:/home/hadoop/hbase/bin/../lib/servlet-api-2.5-6.1.14.jar:/home/hadoop/hbase/bin/../lib/slf4j-api-1.6.4.jar:/home/hadoop/hbase/bin/../lib/slf4j-log4j12-1.6.4.jar:/home/hadoop/hbase/bin/../lib/xmlenc-0.52.jar:/home/hadoop/hbase/bin/../lib/zookeeper-3.4.6.jar:
2014-07-01 07:31:37,528 INFO  [main] util.ServerCommandLine: env:HBASE_THRIFT_OPTS=
2014-07-01 07:31:37,528 INFO  [main] util.ServerCommandLine: env:PATH=/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/usr/games:/usr/local/games
2014-07-01 07:31:37,528 INFO  [main] util.ServerCommandLine: env:USER=hadoop
2014-07-01 07:31:37,528 INFO  [main] util.ServerCommandLine: env:HBASE_SECURITY_LOGGER=INFO,RFAS
2014-07-01 07:31:37,528 INFO  [main] util.ServerCommandLine: env:HOME=/home/hadoop
2014-07-01 07:31:37,529 INFO  [main] util.ServerCommandLine: env:HBASE_LIBRARY_PATH=/home/hadoop/hbase/lib/native/Linux-amd64-64
2014-07-01 07:31:37,529 INFO  [main] util.ServerCommandLine: env:HBASE_START_FILE=/var/hadoop/pids/hbase-hadoop-regionserver.autorestart
2014-07-01 07:31:37,529 INFO  [main] util.ServerCommandLine: env:XDG_SESSION_ID=46
2014-07-01 07:31:37,529 INFO  [main] util.ServerCommandLine: env:HBASE_LOGFILE=hbase-hadoop-regionserver-sceplus-vm49.log
2014-07-01 07:31:37,529 INFO  [main] util.ServerCommandLine: env:XDG_RUNTIME_DIR=/run/user/1001
2014-07-01 07:31:37,529 INFO  [main] util.ServerCommandLine: env:LANG=en_US.UTF-8
2014-07-01 07:31:37,529 INFO  [main] util.ServerCommandLine: env:HBASE_LOG_PREFIX=hbase-hadoop-regionserver-sceplus-vm49
2014-07-01 07:31:37,529 INFO  [main] util.ServerCommandLine: env:HBASE_IDENT_STRING=hadoop
2014-07-01 07:31:37,531 INFO  [main] util.ServerCommandLine: vmName=OpenJDK 64-Bit Server VM, vmVendor=Sun Microsystems Inc., vmVersion=23.25-b01
2014-07-01 07:31:37,531 INFO  [main] util.ServerCommandLine: vmInputArguments=[-Dproc_regionserver, -XX:OnOutOfMemoryError=kill -9 %p, -Xmx10240m, -XX:+UseConcMarkSweepGC, -Dhbase.log.dir=/home/hadoop/hbase/bin/../logs, -Dhbase.log.file=hbase-hadoop-regionserver-sceplus-vm49.log, -Dhbase.home.dir=/home/hadoop/hbase/bin/.., -Dhbase.id.str=hadoop, -Dhbase.root.logger=INFO,RFA, -Djava.library.path=/home/hadoop/hbase/bin/../lib/native/Linux-amd64-64, -Dhbase.security.logger=INFO,RFAS]
2014-07-01 07:31:37,743 DEBUG [main] regionserver.HRegionServer: regionserver/sceplus-vm49.almaden.ibm.com/9.1.143.59:60020 HConnection server-to-server retries=350
2014-07-01 07:31:38,070 INFO  [main] ipc.RpcServer: regionserver/sceplus-vm49.almaden.ibm.com/9.1.143.59:60020: started 10 reader(s).
2014-07-01 07:31:38,145 INFO  [main] impl.MetricsConfig: loaded properties from hadoop-metrics2-hbase.properties
2014-07-01 07:31:38,157 INFO  [main] impl.MetricsSinkAdapter: Sink file-all started
2014-07-01 07:31:38,216 INFO  [main] impl.MetricsSourceAdapter: MBean for source MetricsSystem,sub=Stats registered.
2014-07-01 07:31:38,217 INFO  [main] impl.MetricsSystemImpl: Scheduled snapshot period at 10 second(s).
2014-07-01 07:31:38,217 INFO  [main] impl.MetricsSystemImpl: HBase metrics system started
2014-07-01 07:31:38,222 INFO  [main] impl.MetricsSourceAdapter: MBean for source jvm registered.
2014-07-01 07:31:38,226 INFO  [main] impl.MetricsSourceAdapter: MBean for source IPC,sub=IPC registered.
2014-07-01 07:31:38,303 INFO  [main] impl.MetricsSourceAdapter: MBean for source ugi registered.
2014-07-01 07:31:38,303 WARN  [main] impl.MetricsSystemImpl: Source name ugi already exists!
2014-07-01 07:31:38,307 DEBUG [main] util.DirectMemoryUtils: Failed to retrieve nio.BufferPool direct MemoryUsed attribute.
javax.management.InstanceNotFoundException: java.nio:type=BufferPool,name=direct
	at com.sun.jmx.interceptor.DefaultMBeanServerInterceptor.getMBean(DefaultMBeanServerInterceptor.java:1117)
	at com.sun.jmx.interceptor.DefaultMBeanServerInterceptor.getAttribute(DefaultMBeanServerInterceptor.java:678)
	at com.sun.jmx.mbeanserver.JmxMBeanServer.getAttribute(JmxMBeanServer.java:682)
	at org.apache.hadoop.hbase.util.DirectMemoryUtils.<clinit>(DirectMemoryUtils.java:72)
	at org.apache.hadoop.hbase.io.hfile.CacheConfig.instantiateBlockCache(CacheConfig.java:396)
	at org.apache.hadoop.hbase.io.hfile.CacheConfig.<init>(CacheConfig.java:179)
	at org.apache.hadoop.hbase.regionserver.HRegionServer.<init>(HRegionServer.java:621)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:57)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:534)
	at org.apache.hadoop.hbase.regionserver.HRegionServer.constructRegionServer(HRegionServer.java:2393)
	at org.apache.hadoop.hbase.regionserver.HRegionServerCommandLine.start(HRegionServerCommandLine.java:61)
	at org.apache.hadoop.hbase.regionserver.HRegionServerCommandLine.run(HRegionServerCommandLine.java:85)
	at org.apache.hadoop.util.ToolRunner.run(ToolRunner.java:65)
	at org.apache.hadoop.hbase.util.ServerCommandLine.doMain(ServerCommandLine.java:126)
	at org.apache.hadoop.hbase.regionserver.HRegionServer.main(HRegionServer.java:2410)
2014-07-01 07:31:38,309 INFO  [main] hfile.CacheConfig: Allocating LruBlockCache with maximum size 4.0g
2014-07-01 07:31:38,372 INFO  [main] mortbay.log: Logging to org.slf4j.impl.Log4jLoggerAdapter(org.mortbay.log) via org.mortbay.log.Slf4jLog
2014-07-01 07:31:38,423 INFO  [main] http.HttpServer: Added global filtersafety (class=org.apache.hadoop.http.HttpServer$QuotingInputFilter)
2014-07-01 07:31:38,431 INFO  [main] http.HttpServer: Port returned by webServer.getConnectors()[0].getLocalPort() before open() is -1. Opening the listener on 60030
2014-07-01 07:31:38,433 INFO  [main] http.HttpServer: listener.getLocalPort() returned 60030 webServer.getConnectors()[0].getLocalPort() returned 60030
2014-07-01 07:31:38,433 INFO  [main] http.HttpServer: Jetty bound to port 60030
2014-07-01 07:31:38,433 INFO  [main] mortbay.log: jetty-6.1.26
2014-07-01 07:31:38,717 INFO  [main] mortbay.log: Started SelectChannelConnector@0.0.0.0:60030
2014-07-01 07:31:38,765 INFO  [regionserver60020] zookeeper.ZooKeeper: Client environment:zookeeper.version=3.4.6-1569965, built on 02/20/2014 09:09 GMT
2014-07-01 07:31:38,765 INFO  [regionserver60020] zookeeper.ZooKeeper: Client environment:host.name=sceplus-vm49.almaden.ibm.com
2014-07-01 07:31:38,765 INFO  [main] regionserver.ShutdownHook: Installed shutdown hook thread: Shutdownhook:regionserver60020
2014-07-01 07:31:38,765 INFO  [regionserver60020] zookeeper.ZooKeeper: Client environment:java.version=1.6.0_31
2014-07-01 07:31:38,765 INFO  [regionserver60020] zookeeper.ZooKeeper: Client environment:java.vendor=Sun Microsystems Inc.
2014-07-01 07:31:38,765 INFO  [regionserver60020] zookeeper.ZooKeeper: Client environment:java.home=/usr/lib/jvm/java-6-openjdk-amd64/jre
2014-07-01 07:31:38,765 INFO  [regionserver60020] zookeeper.ZooKeeper: Client environment:java.class.path=/home/hadoop/hbase/conf:/usr/lib/jvm/java-1.6.0-openjdk-amd64//lib/tools.jar:/home/hadoop/hbase/bin/..:/home/hadoop/hbase/bin/../lib/activation-1.1.jar:/home/hadoop/hbase/bin/../lib/asm-3.1.jar:/home/hadoop/hbase/bin/../lib/commons-beanutils-1.7.0.jar:/home/hadoop/hbase/bin/../lib/commons-beanutils-core-1.8.0.jar:/home/hadoop/hbase/bin/../lib/commons-cli-1.2.jar:/home/hadoop/hbase/bin/../lib/commons-codec-1.7.jar:/home/hadoop/hbase/bin/../lib/commons-collections-3.2.1.jar:/home/hadoop/hbase/bin/../lib/commons-configuration-1.6.jar:/home/hadoop/hbase/bin/../lib/commons-digester-1.8.jar:/home/hadoop/hbase/bin/../lib/commons-el-1.0.jar:/home/hadoop/hbase/bin/../lib/commons-httpclient-3.1.jar:/home/hadoop/hbase/bin/../lib/commons-io-2.4.jar:/home/hadoop/hbase/bin/../lib/commons-lang-2.6.jar:/home/hadoop/hbase/bin/../lib/commons-logging-1.1.1.jar:/home/hadoop/hbase/bin/../lib/commons-math-2.1.jar:/home/hadoop/hbase/bin/../lib/commons-net-1.4.1.jar:/home/hadoop/hbase/bin/../lib/findbugs-annotations-1.3.9-1.jar:/home/hadoop/hbase/bin/../lib/guava-12.0.1.jar:/home/hadoop/hbase/bin/../lib/hadoop-core-1.2.1.jar:/home/hadoop/hbase/bin/../lib/hamcrest-core-1.3.jar:/home/hadoop/hbase/bin/../lib/hbase-client-0.98.3-hadoop1.jar:/home/hadoop/hbase/bin/../lib/hbase-common-0.98.3-hadoop1.jar:/home/hadoop/hbase/bin/../lib/hbase-common-0.98.3-hadoop1-tests.jar:/home/hadoop/hbase/bin/../lib/hbase-examples-0.98.3-hadoop1.jar:/home/hadoop/hbase/bin/../lib/hbase-hadoop1-compat-0.98.3-hadoop1.jar:/home/hadoop/hbase/bin/../lib/hbase-hadoop-compat-0.98.3-hadoop1.jar:/home/hadoop/hbase/bin/../lib/hbase-it-0.98.3-hadoop1.jar:/home/hadoop/hbase/bin/../lib/hbase-it-0.98.3-hadoop1-tests.jar:/home/hadoop/hbase/bin/../lib/hbase-prefix-tree-0.98.3-hadoop1.jar:/home/hadoop/hbase/bin/../lib/hbase-protocol-0.98.3-hadoop1.jar:/home/hadoop/hbase/bin/../lib/hbase-server-0.98.3-hadoop1.jar:/home/hadoop/hbase/bin/../lib/hbase-server-0.98.3-hadoop1-tests.jar:/home/hadoop/hbase/bin/../lib/hbase-shell-0.98.3-hadoop1.jar:/home/hadoop/hbase/bin/../lib/hbase-testing-util-0.98.3-hadoop1.jar:/home/hadoop/hbase/bin/../lib/hbase-thrift-0.98.3-hadoop1.jar:/home/hadoop/hbase/bin/../lib/high-scale-lib-1.1.1.jar:/home/hadoop/hbase/bin/../lib/htrace-core-2.04.jar:/home/hadoop/hbase/bin/../lib/httpclient-4.1.3.jar:/home/hadoop/hbase/bin/../lib/httpcore-4.1.3.jar:/home/hadoop/hbase/bin/../lib/jackson-core-asl-1.8.8.jar:/home/hadoop/hbase/bin/../lib/jackson-jaxrs-1.8.8.jar:/home/hadoop/hbase/bin/../lib/jackson-mapper-asl-1.8.8.jar:/home/hadoop/hbase/bin/../lib/jackson-xc-1.8.8.jar:/home/hadoop/hbase/bin/../lib/jamon-runtime-2.3.1.jar:/home/hadoop/hbase/bin/../lib/jasper-compiler-5.5.23.jar:/home/hadoop/hbase/bin/../lib/jasper-runtime-5.5.23.jar:/home/hadoop/hbase/bin/../lib/jaxb-api-2.2.2.jar:/home/hadoop/hbase/bin/../lib/jaxb-impl-2.2.3-1.jar:/home/hadoop/hbase/bin/../lib/jersey-core-1.8.jar:/home/hadoop/hbase/bin/../lib/jersey-json-1.8.jar:/home/hadoop/hbase/bin/../lib/jersey-server-1.8.jar:/home/hadoop/hbase/bin/../lib/jettison-1.3.1.jar:/home/hadoop/hbase/bin/../lib/jetty-6.1.26.jar:/home/hadoop/hbase/bin/../lib/jetty-sslengine-6.1.26.jar:/home/hadoop/hbase/bin/../lib/jetty-util-6.1.26.jar:/home/hadoop/hbase/bin/../lib/jruby-complete-1.6.8.jar:/home/hadoop/hbase/bin/../lib/jsp-2.1-6.1.14.jar:/home/hadoop/hbase/bin/../lib/jsp-api-2.1-6.1.14.jar:/home/hadoop/hbase/bin/../lib/jsr305-1.3.9.jar:/home/hadoop/hbase/bin/../lib/junit-4.11.jar:/home/hadoop/hbase/bin/../lib/libthrift-0.9.0.jar:/home/hadoop/hbase/bin/../lib/log4j-1.2.17.jar:/home/hadoop/hbase/bin/../lib/metrics-core-2.1.2.jar:/home/hadoop/hbase/bin/../lib/netty-3.6.6.Final.jar:/home/hadoop/hbase/bin/../lib/protobuf-java-2.5.0.jar:/home/hadoop/hbase/bin/../lib/servlet-api-2.5-6.1.14.jar:/home/hadoop/hbase/bin/../lib/slf4j-api-1.6.4.jar:/home/hadoop/hbase/bin/../lib/slf4j-log4j12-1.6.4.jar:/home/hadoop/hbase/bin/../lib/xmlenc-0.52.jar:/home/hadoop/hbase/bin/../lib/zookeeper-3.4.6.jar:
2014-07-01 07:31:38,765 INFO  [regionserver60020] zookeeper.ZooKeeper: Client environment:java.library.path=/home/hadoop/hbase/bin/../lib/native/Linux-amd64-64
2014-07-01 07:31:38,765 INFO  [regionserver60020] zookeeper.ZooKeeper: Client environment:java.io.tmpdir=/tmp
2014-07-01 07:31:38,765 INFO  [regionserver60020] zookeeper.ZooKeeper: Client environment:java.compiler=<NA>
2014-07-01 07:31:38,765 INFO  [regionserver60020] zookeeper.ZooKeeper: Client environment:os.name=Linux
2014-07-01 07:31:38,765 INFO  [regionserver60020] zookeeper.ZooKeeper: Client environment:os.arch=amd64
2014-07-01 07:31:38,765 INFO  [regionserver60020] zookeeper.ZooKeeper: Client environment:os.version=3.13.0-24-generic
2014-07-01 07:31:38,765 INFO  [regionserver60020] zookeeper.ZooKeeper: Client environment:user.name=hadoop
2014-07-01 07:31:38,765 INFO  [regionserver60020] zookeeper.ZooKeeper: Client environment:user.home=/home/hadoop
2014-07-01 07:31:38,765 INFO  [regionserver60020] zookeeper.ZooKeeper: Client environment:user.dir=/home/hadoop/hbase-0.98.3-hadoop1
2014-07-01 07:31:38,766 INFO  [regionserver60020] zookeeper.ZooKeeper: Initiating client connection, connectString=sceplus-vm49.almaden.ibm.com:2181,sceplus-vm48.almaden.ibm.com:2181 sessionTimeout=90000 watcher=regionserver:60020, quorum=sceplus-vm49.almaden.ibm.com:2181,sceplus-vm48.almaden.ibm.com:2181, baseZNode=/hbase
2014-07-01 07:31:38,786 INFO  [regionserver60020] zookeeper.RecoverableZooKeeper: Process identifier=regionserver:60020 connecting to ZooKeeper ensemble=sceplus-vm49.almaden.ibm.com:2181,sceplus-vm48.almaden.ibm.com:2181
2014-07-01 07:31:38,789 INFO  [regionserver60020-SendThread(master:2181)] zookeeper.ClientCnxn: Opening socket connection to server master/9.1.143.58:2181. Will not attempt to authenticate using SASL (unknown error)
2014-07-01 07:31:38,792 INFO  [regionserver60020-SendThread(master:2181)] zookeeper.ClientCnxn: Socket connection established to master/9.1.143.58:2181, initiating session
2014-07-01 07:31:38,803 INFO  [regionserver60020-SendThread(master:2181)] zookeeper.ClientCnxn: Session establishment complete on server master/9.1.143.58:2181, sessionid = 0x46f25690f50001, negotiated timeout = 90000
2014-07-01 07:31:48,392 INFO  [regionserver60020] zookeeper.ZooKeeper: Initiating client connection, connectString=sceplus-vm49.almaden.ibm.com:2181,sceplus-vm48.almaden.ibm.com:2181 sessionTimeout=90000 watcher=hconnection-0x307d2e98, quorum=sceplus-vm49.almaden.ibm.com:2181,sceplus-vm48.almaden.ibm.com:2181, baseZNode=/hbase
2014-07-01 07:31:48,393 INFO  [regionserver60020] zookeeper.RecoverableZooKeeper: Process identifier=hconnection-0x307d2e98 connecting to ZooKeeper ensemble=sceplus-vm49.almaden.ibm.com:2181,sceplus-vm48.almaden.ibm.com:2181
2014-07-01 07:31:48,393 INFO  [regionserver60020-SendThread(master:2181)] zookeeper.ClientCnxn: Opening socket connection to server master/9.1.143.58:2181. Will not attempt to authenticate using SASL (unknown error)
2014-07-01 07:31:48,395 INFO  [regionserver60020-SendThread(master:2181)] zookeeper.ClientCnxn: Socket connection established to master/9.1.143.58:2181, initiating session
2014-07-01 07:31:48,398 INFO  [regionserver60020-SendThread(master:2181)] zookeeper.ClientCnxn: Session establishment complete on server master/9.1.143.58:2181, sessionid = 0x46f25690f50003, negotiated timeout = 90000
2014-07-01 07:31:48,637 DEBUG [regionserver60020] catalog.CatalogTracker: Starting catalog tracker org.apache.hadoop.hbase.catalog.CatalogTracker@1a6bfc15
2014-07-01 07:31:48,642 INFO  [regionserver60020] regionserver.HRegionServer: ClusterId : 3b61b992-e8ee-43f8-b0c6-14cd23a8afbe
2014-07-01 07:31:48,647 INFO  [regionserver60020] procedure.RegionServerProcedureManagerHost: Procedure online-snapshot is initializing
2014-07-01 07:31:48,661 INFO  [regionserver60020] zookeeper.RecoverableZooKeeper: Node /hbase/online-snapshot/acquired already exists and this is not a retry
2014-07-01 07:31:48,688 INFO  [regionserver60020] procedure.RegionServerProcedureManagerHost: Procedure online-snapshot is initialized
2014-07-01 07:31:48,692 INFO  [regionserver60020] regionserver.MemStoreFlusher: globalMemStoreLimit=4.0g, globalMemStoreLimitLowMark=3.8g, maxHeap=9.9g
2014-07-01 07:31:48,696 INFO  [regionserver60020] regionserver.HRegionServer: CompactionChecker runs every 10sec
2014-07-01 07:31:48,710 INFO  [regionserver60020] regionserver.HRegionServer: reportForDuty to master=sceplus-vm48.almaden.ibm.com,60000,1404225098696 with port=60020, startcode=1404225098235
2014-07-01 07:31:49,039 DEBUG [regionserver60020] regionserver.HRegionServer: Config from master: hbase.rootdir=hdfs://master:54310/hbase
2014-07-01 07:31:49,039 DEBUG [regionserver60020] regionserver.HRegionServer: Config from master: fs.default.name=hdfs://master:54310
2014-07-01 07:31:49,039 INFO  [regionserver60020] regionserver.HRegionServer: Master passed us a different hostname to use; was=sceplus-vm49.almaden.ibm.com, but now=slave1
2014-07-01 07:31:49,070 INFO  [regionserver60020] fs.HFileSystem: Added intercepting call to namenode#getBlockLocations so can do block reordering using class class org.apache.hadoop.hbase.fs.HFileSystem$ReorderWALBlocks
2014-07-01 07:31:49,078 DEBUG [regionserver60020] regionserver.HRegionServer: logdir=hdfs://master:54310/hbase/WALs/slave1,60020,1404225098235
2014-07-01 07:31:49,119 DEBUG [regionserver60020] regionserver.Replication: ReplicationStatisticsThread 300
2014-07-01 07:31:49,132 INFO  [regionserver60020] wal.FSHLog: WAL/HLog configuration: blocksize=64 MB, rollsize=60.8 MB, enabled=true
2014-07-01 07:31:49,230 INFO  [regionserver60020] wal.FSHLog: New WAL /hbase/WALs/slave1,60020,1404225098235/slave1%2C60020%2C1404225098235.1404225109145
2014-07-01 07:31:49,247 INFO  [regionserver60020] impl.MetricsSourceAdapter: MBean for source RegionServer,sub=WAL registered.
2014-07-01 07:31:49,252 INFO  [regionserver60020] regionserver.MetricsRegionServerWrapperImpl: Computing regionserver metrics every 5000 milliseconds
2014-07-01 07:31:49,255 INFO  [regionserver60020] impl.MetricsSourceAdapter: MBean for source RegionServer,sub=Server registered.
2014-07-01 07:31:49,259 INFO  [regionserver60020] trace.SpanReceiverHost: SpanReceiver org.cloudera.htrace.impl.LocalFileSpanReceiver was loaded successfully.
2014-07-01 07:31:49,262 DEBUG [regionserver60020] executor.ExecutorService: Starting executor service name=RS_OPEN_REGION-slave1:60020, corePoolSize=3, maxPoolSize=3
2014-07-01 07:31:49,262 DEBUG [regionserver60020] executor.ExecutorService: Starting executor service name=RS_OPEN_META-slave1:60020, corePoolSize=1, maxPoolSize=1
2014-07-01 07:31:49,262 DEBUG [regionserver60020] executor.ExecutorService: Starting executor service name=RS_CLOSE_REGION-slave1:60020, corePoolSize=3, maxPoolSize=3
2014-07-01 07:31:49,262 DEBUG [regionserver60020] executor.ExecutorService: Starting executor service name=RS_CLOSE_META-slave1:60020, corePoolSize=1, maxPoolSize=1
2014-07-01 07:31:49,262 DEBUG [regionserver60020] executor.ExecutorService: Starting executor service name=RS_LOG_REPLAY_OPS-slave1:60020, corePoolSize=2, maxPoolSize=2
2014-07-01 07:31:49,271 INFO  [regionserver60020] regionserver.ReplicationSourceManager: Current list of replicators: [slave1,60020,1404225098235, sceplus-vm48.almaden.ibm.com,60020,1404225100003] other RSs: [slave1,60020,1404225098235, sceplus-vm48.almaden.ibm.com,60020,1404225100003]
2014-07-01 07:31:49,294 INFO  [regionserver60020] impl.MetricsSourceAdapter: MBean for source RegionServer,sub=Replication registered.
2014-07-01 07:31:49,296 INFO  [regionserver60020] zookeeper.ZooKeeper: Initiating client connection, connectString=sceplus-vm49.almaden.ibm.com:2181,sceplus-vm48.almaden.ibm.com:2181 sessionTimeout=90000 watcher=hconnection-0x618daf35, quorum=sceplus-vm49.almaden.ibm.com:2181,sceplus-vm48.almaden.ibm.com:2181, baseZNode=/hbase
2014-07-01 07:31:49,297 INFO  [regionserver60020] zookeeper.RecoverableZooKeeper: Process identifier=hconnection-0x618daf35 connecting to ZooKeeper ensemble=sceplus-vm49.almaden.ibm.com:2181,sceplus-vm48.almaden.ibm.com:2181
2014-07-01 07:31:49,298 INFO  [regionserver60020-SendThread(master:2181)] zookeeper.ClientCnxn: Opening socket connection to server master/9.1.143.58:2181. Will not attempt to authenticate using SASL (unknown error)
2014-07-01 07:31:49,299 INFO  [regionserver60020-SendThread(master:2181)] zookeeper.ClientCnxn: Socket connection established to master/9.1.143.58:2181, initiating session
2014-07-01 07:31:49,303 INFO  [regionserver60020-SendThread(master:2181)] zookeeper.ClientCnxn: Session establishment complete on server master/9.1.143.58:2181, sessionid = 0x46f25690f50006, negotiated timeout = 90000
2014-07-01 07:31:49,311 INFO  [RpcServer.responder] ipc.RpcServer: RpcServer.responder: starting
2014-07-01 07:31:49,311 INFO  [RpcServer.listener,port=60020] ipc.RpcServer: RpcServer.listener,port=60020: starting
2014-07-01 07:31:49,354 INFO  [regionserver60020] regionserver.HRegionServer: Serving as slave1,60020,1404225098235, RpcServer on sceplus-vm49.almaden.ibm.com/9.1.143.59:60020, sessionid=0x46f25690f50001
2014-07-01 07:31:49,354 INFO  [SplitLogWorker-slave1,60020,1404225098235] regionserver.SplitLogWorker: SplitLogWorker slave1,60020,1404225098235 starting
2014-07-01 07:31:49,354 INFO  [regionserver60020] procedure.RegionServerProcedureManagerHost: Procedure online-snapshot is starting
2014-07-01 07:31:49,355 DEBUG [regionserver60020] snapshot.RegionServerSnapshotManager: Start Snapshot Manager slave1,60020,1404225098235
2014-07-01 07:31:49,355 DEBUG [regionserver60020] procedure.ZKProcedureMemberRpcs: Starting procedure member 'slave1,60020,1404225098235'
2014-07-01 07:31:49,355 DEBUG [regionserver60020] procedure.ZKProcedureMemberRpcs: Checking for aborted procedures on node: '/hbase/online-snapshot/abort'
2014-07-01 07:31:49,356 DEBUG [regionserver60020] procedure.ZKProcedureMemberRpcs: Looking for new procedures under znode:'/hbase/online-snapshot/acquired'
2014-07-01 07:31:49,357 INFO  [regionserver60020] procedure.RegionServerProcedureManagerHost: Procedure online-snapshot is started
2014-07-01 07:36:38,318 DEBUG [LruStats #0] hfile.LruBlockCache: Total=4.16 MB, free=3.95 GB, max=3.96 GB, blocks=0, accesses=0, hits=0, hitRatio=0, cachingAccesses=0, cachingHits=0, cachingHitsRatio=0,evictions=0, evicted=0, evictedPerRun=NaN
2014-07-01 07:37:01,885 INFO  [Priority.RpcServer.handler=0,port=60020] regionserver.HRegionServer: Open usertable,user2,1404225423342.07064f253c865a7cc0c08893a4f56af9.
2014-07-01 07:37:02,005 INFO  [Priority.RpcServer.handler=0,port=60020] regionserver.HRegionServer: Open usertable,user6,1404225423342.89cb00f0096b305c3317b7753d66adf1.
2014-07-01 07:37:02,006 DEBUG [RS_OPEN_REGION-slave1:60020-0] zookeeper.ZKAssign: regionserver:60020-0x46f25690f50001, quorum=sceplus-vm49.almaden.ibm.com:2181,sceplus-vm48.almaden.ibm.com:2181, baseZNode=/hbase Transitioning 07064f253c865a7cc0c08893a4f56af9 from M_ZK_REGION_OFFLINE to RS_ZK_REGION_OPENING
2014-07-01 07:37:02,006 INFO  [Priority.RpcServer.handler=0,port=60020] regionserver.HRegionServer: Open usertable,,1404225423341.387a171caad6315f0b61271a30313d1a.
2014-07-01 07:37:02,007 DEBUG [RS_OPEN_REGION-slave1:60020-1] zookeeper.ZKAssign: regionserver:60020-0x46f25690f50001, quorum=sceplus-vm49.almaden.ibm.com:2181,sceplus-vm48.almaden.ibm.com:2181, baseZNode=/hbase Transitioning 89cb00f0096b305c3317b7753d66adf1 from M_ZK_REGION_OFFLINE to RS_ZK_REGION_OPENING
2014-07-01 07:37:02,008 INFO  [Priority.RpcServer.handler=0,port=60020] regionserver.HRegionServer: Open usertable,user5,1404225423342.c7604442b843793f8451582c14f6bf59.
2014-07-01 07:37:02,009 DEBUG [RS_OPEN_REGION-slave1:60020-2] zookeeper.ZKAssign: regionserver:60020-0x46f25690f50001, quorum=sceplus-vm49.almaden.ibm.com:2181,sceplus-vm48.almaden.ibm.com:2181, baseZNode=/hbase Transitioning 387a171caad6315f0b61271a30313d1a from M_ZK_REGION_OFFLINE to RS_ZK_REGION_OPENING
2014-07-01 07:37:02,031 DEBUG [RS_OPEN_REGION-slave1:60020-1] zookeeper.ZKAssign: regionserver:60020-0x46f25690f50001, quorum=sceplus-vm49.almaden.ibm.com:2181,sceplus-vm48.almaden.ibm.com:2181, baseZNode=/hbase Transitioned node 89cb00f0096b305c3317b7753d66adf1 from M_ZK_REGION_OFFLINE to RS_ZK_REGION_OPENING
2014-07-01 07:37:02,031 DEBUG [RS_OPEN_REGION-slave1:60020-0] zookeeper.ZKAssign: regionserver:60020-0x46f25690f50001, quorum=sceplus-vm49.almaden.ibm.com:2181,sceplus-vm48.almaden.ibm.com:2181, baseZNode=/hbase Transitioned node 07064f253c865a7cc0c08893a4f56af9 from M_ZK_REGION_OFFLINE to RS_ZK_REGION_OPENING
2014-07-01 07:37:02,032 DEBUG [RS_OPEN_REGION-slave1:60020-2] zookeeper.ZKAssign: regionserver:60020-0x46f25690f50001, quorum=sceplus-vm49.almaden.ibm.com:2181,sceplus-vm48.almaden.ibm.com:2181, baseZNode=/hbase Transitioned node 387a171caad6315f0b61271a30313d1a from M_ZK_REGION_OFFLINE to RS_ZK_REGION_OPENING
2014-07-01 07:37:02,046 DEBUG [RS_OPEN_REGION-slave1:60020-1] regionserver.HRegion: Opening region: {ENCODED => 89cb00f0096b305c3317b7753d66adf1, NAME => 'usertable,user6,1404225423342.89cb00f0096b305c3317b7753d66adf1.', STARTKEY => 'user6', ENDKEY => 'user7'}
2014-07-01 07:37:02,046 DEBUG [RS_OPEN_REGION-slave1:60020-2] regionserver.HRegion: Opening region: {ENCODED => 387a171caad6315f0b61271a30313d1a, NAME => 'usertable,,1404225423341.387a171caad6315f0b61271a30313d1a.', STARTKEY => '', ENDKEY => 'user2'}
2014-07-01 07:37:02,046 DEBUG [RS_OPEN_REGION-slave1:60020-0] regionserver.HRegion: Opening region: {ENCODED => 07064f253c865a7cc0c08893a4f56af9, NAME => 'usertable,user2,1404225423342.07064f253c865a7cc0c08893a4f56af9.', STARTKEY => 'user2', ENDKEY => 'user3'}
2014-07-01 07:37:02,070 INFO  [RS_OPEN_REGION-slave1:60020-2] impl.MetricsSourceAdapter: MBean for source RegionServer,sub=Regions registered.
2014-07-01 07:37:02,070 DEBUG [RS_OPEN_REGION-slave1:60020-2] regionserver.MetricsRegionSourceImpl: Creating new MetricsRegionSourceImpl for table usertable 387a171caad6315f0b61271a30313d1a
2014-07-01 07:37:02,070 DEBUG [RS_OPEN_REGION-slave1:60020-0] regionserver.MetricsRegionSourceImpl: Creating new MetricsRegionSourceImpl for table usertable 07064f253c865a7cc0c08893a4f56af9
2014-07-01 07:37:02,070 DEBUG [RS_OPEN_REGION-slave1:60020-1] regionserver.MetricsRegionSourceImpl: Creating new MetricsRegionSourceImpl for table usertable 89cb00f0096b305c3317b7753d66adf1
2014-07-01 07:37:02,070 DEBUG [RS_OPEN_REGION-slave1:60020-2] regionserver.HRegion: Instantiated usertable,,1404225423341.387a171caad6315f0b61271a30313d1a.
2014-07-01 07:37:02,070 DEBUG [RS_OPEN_REGION-slave1:60020-1] regionserver.HRegion: Instantiated usertable,user6,1404225423342.89cb00f0096b305c3317b7753d66adf1.
2014-07-01 07:37:02,070 DEBUG [RS_OPEN_REGION-slave1:60020-0] regionserver.HRegion: Instantiated usertable,user2,1404225423342.07064f253c865a7cc0c08893a4f56af9.
2014-07-01 07:37:02,078 INFO  [RS_OPEN_REGION-slave1:60020-2] util.NativeCodeLoader: Loaded the native-hadoop library
2014-07-01 07:37:02,080 INFO  [RS_OPEN_REGION-slave1:60020-2] zlib.ZlibFactory: Successfully loaded & initialized native-zlib library
2014-07-01 07:37:02,081 INFO  [RS_OPEN_REGION-slave1:60020-2] compress.CodecPool: Got brand-new compressor
2014-07-01 07:37:02,082 INFO  [RS_OPEN_REGION-slave1:60020-0] compress.CodecPool: Got brand-new compressor
2014-07-01 07:37:02,082 INFO  [RS_OPEN_REGION-slave1:60020-1] compress.CodecPool: Got brand-new compressor
2014-07-01 07:37:02,197 INFO  [StoreOpener-07064f253c865a7cc0c08893a4f56af9-1] compactions.CompactionConfiguration: size [268435456, 9223372036854775807); files [3, 10); ratio 1.200000; off-peak ratio 5.000000; throttle point 5368709120; delete expired; major period 604800000, major jitter 0.500000
2014-07-01 07:37:02,197 INFO  [StoreOpener-89cb00f0096b305c3317b7753d66adf1-1] compactions.CompactionConfiguration: size [268435456, 9223372036854775807); files [3, 10); ratio 1.200000; off-peak ratio 5.000000; throttle point 5368709120; delete expired; major period 604800000, major jitter 0.500000
2014-07-01 07:37:02,202 INFO  [StoreOpener-387a171caad6315f0b61271a30313d1a-1] compactions.CompactionConfiguration: size [268435456, 9223372036854775807); files [3, 10); ratio 1.200000; off-peak ratio 5.000000; throttle point 5368709120; delete expired; major period 604800000, major jitter 0.500000
2014-07-01 07:37:02,217 INFO  [StoreOpener-07064f253c865a7cc0c08893a4f56af9-1] util.ChecksumType: Checksum using org.apache.hadoop.util.PureJavaCrc32
2014-07-01 07:37:02,229 DEBUG [RS_OPEN_REGION-slave1:60020-0] regionserver.HRegion: Found 0 recovered edits file(s) under hdfs://master:54310/hbase/data/default/usertable/07064f253c865a7cc0c08893a4f56af9
2014-07-01 07:37:02,230 DEBUG [RS_OPEN_REGION-slave1:60020-1] regionserver.HRegion: Found 0 recovered edits file(s) under hdfs://master:54310/hbase/data/default/usertable/89cb00f0096b305c3317b7753d66adf1
2014-07-01 07:37:02,231 DEBUG [RS_OPEN_REGION-slave1:60020-2] regionserver.HRegion: Found 0 recovered edits file(s) under hdfs://master:54310/hbase/data/default/usertable/387a171caad6315f0b61271a30313d1a
2014-07-01 07:37:02,235 INFO  [RS_OPEN_REGION-slave1:60020-0] regionserver.HRegion: Onlined 07064f253c865a7cc0c08893a4f56af9; next sequenceid=1
2014-07-01 07:37:02,235 INFO  [RS_OPEN_REGION-slave1:60020-1] regionserver.HRegion: Onlined 89cb00f0096b305c3317b7753d66adf1; next sequenceid=1
2014-07-01 07:37:02,235 DEBUG [RS_OPEN_REGION-slave1:60020-0] zookeeper.ZKAssign: regionserver:60020-0x46f25690f50001, quorum=sceplus-vm49.almaden.ibm.com:2181,sceplus-vm48.almaden.ibm.com:2181, baseZNode=/hbase Attempting to retransition opening state of node 07064f253c865a7cc0c08893a4f56af9
2014-07-01 07:37:02,235 DEBUG [RS_OPEN_REGION-slave1:60020-1] zookeeper.ZKAssign: regionserver:60020-0x46f25690f50001, quorum=sceplus-vm49.almaden.ibm.com:2181,sceplus-vm48.almaden.ibm.com:2181, baseZNode=/hbase Attempting to retransition opening state of node 89cb00f0096b305c3317b7753d66adf1
2014-07-01 07:37:02,239 INFO  [PostOpenDeployTasks:07064f253c865a7cc0c08893a4f56af9] regionserver.HRegionServer: Post open deploy tasks for region=usertable,user2,1404225423342.07064f253c865a7cc0c08893a4f56af9.
2014-07-01 07:37:02,239 INFO  [PostOpenDeployTasks:89cb00f0096b305c3317b7753d66adf1] regionserver.HRegionServer: Post open deploy tasks for region=usertable,user6,1404225423342.89cb00f0096b305c3317b7753d66adf1.
2014-07-01 07:37:02,271 INFO  [RS_OPEN_REGION-slave1:60020-2] regionserver.HRegion: Onlined 387a171caad6315f0b61271a30313d1a; next sequenceid=1
2014-07-01 07:37:02,271 DEBUG [RS_OPEN_REGION-slave1:60020-2] zookeeper.ZKAssign: regionserver:60020-0x46f25690f50001, quorum=sceplus-vm49.almaden.ibm.com:2181,sceplus-vm48.almaden.ibm.com:2181, baseZNode=/hbase Attempting to retransition opening state of node 387a171caad6315f0b61271a30313d1a
2014-07-01 07:37:02,274 INFO  [PostOpenDeployTasks:387a171caad6315f0b61271a30313d1a] regionserver.HRegionServer: Post open deploy tasks for region=usertable,,1404225423341.387a171caad6315f0b61271a30313d1a.
2014-07-01 07:37:02,360 INFO  [PostOpenDeployTasks:07064f253c865a7cc0c08893a4f56af9] catalog.MetaEditor: Updated row usertable,user2,1404225423342.07064f253c865a7cc0c08893a4f56af9. with server=slave1,60020,1404225098235
2014-07-01 07:37:02,361 INFO  [PostOpenDeployTasks:07064f253c865a7cc0c08893a4f56af9] regionserver.HRegionServer: Finished post open deploy task for usertable,user2,1404225423342.07064f253c865a7cc0c08893a4f56af9.
2014-07-01 07:37:02,360 INFO  [PostOpenDeployTasks:89cb00f0096b305c3317b7753d66adf1] catalog.MetaEditor: Updated row usertable,user6,1404225423342.89cb00f0096b305c3317b7753d66adf1. with server=slave1,60020,1404225098235
2014-07-01 07:37:02,360 INFO  [PostOpenDeployTasks:387a171caad6315f0b61271a30313d1a] catalog.MetaEditor: Updated row usertable,,1404225423341.387a171caad6315f0b61271a30313d1a. with server=slave1,60020,1404225098235
2014-07-01 07:37:02,361 INFO  [PostOpenDeployTasks:89cb00f0096b305c3317b7753d66adf1] regionserver.HRegionServer: Finished post open deploy task for usertable,user6,1404225423342.89cb00f0096b305c3317b7753d66adf1.
2014-07-01 07:37:02,361 INFO  [PostOpenDeployTasks:387a171caad6315f0b61271a30313d1a] regionserver.HRegionServer: Finished post open deploy task for usertable,,1404225423341.387a171caad6315f0b61271a30313d1a.
2014-07-01 07:37:02,361 DEBUG [RS_OPEN_REGION-slave1:60020-0] zookeeper.ZKAssign: regionserver:60020-0x46f25690f50001, quorum=sceplus-vm49.almaden.ibm.com:2181,sceplus-vm48.almaden.ibm.com:2181, baseZNode=/hbase Transitioning 07064f253c865a7cc0c08893a4f56af9 from RS_ZK_REGION_OPENING to RS_ZK_REGION_OPENED
2014-07-01 07:37:02,361 DEBUG [RS_OPEN_REGION-slave1:60020-1] zookeeper.ZKAssign: regionserver:60020-0x46f25690f50001, quorum=sceplus-vm49.almaden.ibm.com:2181,sceplus-vm48.almaden.ibm.com:2181, baseZNode=/hbase Transitioning 89cb00f0096b305c3317b7753d66adf1 from RS_ZK_REGION_OPENING to RS_ZK_REGION_OPENED
2014-07-01 07:37:02,362 DEBUG [RS_OPEN_REGION-slave1:60020-2] zookeeper.ZKAssign: regionserver:60020-0x46f25690f50001, quorum=sceplus-vm49.almaden.ibm.com:2181,sceplus-vm48.almaden.ibm.com:2181, baseZNode=/hbase Transitioning 387a171caad6315f0b61271a30313d1a from RS_ZK_REGION_OPENING to RS_ZK_REGION_OPENED
2014-07-01 07:37:02,368 DEBUG [RS_OPEN_REGION-slave1:60020-0] zookeeper.ZKAssign: regionserver:60020-0x46f25690f50001, quorum=sceplus-vm49.almaden.ibm.com:2181,sceplus-vm48.almaden.ibm.com:2181, baseZNode=/hbase Transitioned node 07064f253c865a7cc0c08893a4f56af9 from RS_ZK_REGION_OPENING to RS_ZK_REGION_OPENED
2014-07-01 07:37:02,368 DEBUG [RS_OPEN_REGION-slave1:60020-0] handler.OpenRegionHandler: Transitioned 07064f253c865a7cc0c08893a4f56af9 to OPENED in zk on slave1,60020,1404225098235
2014-07-01 07:37:02,368 DEBUG [RS_OPEN_REGION-slave1:60020-0] handler.OpenRegionHandler: Opened usertable,user2,1404225423342.07064f253c865a7cc0c08893a4f56af9. on slave1,60020,1404225098235
2014-07-01 07:37:02,369 DEBUG [RS_OPEN_REGION-slave1:60020-0] zookeeper.ZKAssign: regionserver:60020-0x46f25690f50001, quorum=sceplus-vm49.almaden.ibm.com:2181,sceplus-vm48.almaden.ibm.com:2181, baseZNode=/hbase Transitioning c7604442b843793f8451582c14f6bf59 from M_ZK_REGION_OFFLINE to RS_ZK_REGION_OPENING
2014-07-01 07:37:02,370 DEBUG [RS_OPEN_REGION-slave1:60020-2] zookeeper.ZKAssign: regionserver:60020-0x46f25690f50001, quorum=sceplus-vm49.almaden.ibm.com:2181,sceplus-vm48.almaden.ibm.com:2181, baseZNode=/hbase Transitioned node 387a171caad6315f0b61271a30313d1a from RS_ZK_REGION_OPENING to RS_ZK_REGION_OPENED
2014-07-01 07:37:02,370 DEBUG [RS_OPEN_REGION-slave1:60020-2] handler.OpenRegionHandler: Transitioned 387a171caad6315f0b61271a30313d1a to OPENED in zk on slave1,60020,1404225098235
2014-07-01 07:37:02,371 DEBUG [RS_OPEN_REGION-slave1:60020-2] handler.OpenRegionHandler: Opened usertable,,1404225423341.387a171caad6315f0b61271a30313d1a. on slave1,60020,1404225098235
2014-07-01 07:37:02,371 DEBUG [RS_OPEN_REGION-slave1:60020-1] zookeeper.ZKAssign: regionserver:60020-0x46f25690f50001, quorum=sceplus-vm49.almaden.ibm.com:2181,sceplus-vm48.almaden.ibm.com:2181, baseZNode=/hbase Transitioned node 89cb00f0096b305c3317b7753d66adf1 from RS_ZK_REGION_OPENING to RS_ZK_REGION_OPENED
2014-07-01 07:37:02,371 DEBUG [RS_OPEN_REGION-slave1:60020-1] handler.OpenRegionHandler: Transitioned 89cb00f0096b305c3317b7753d66adf1 to OPENED in zk on slave1,60020,1404225098235
2014-07-01 07:37:02,371 DEBUG [RS_OPEN_REGION-slave1:60020-1] handler.OpenRegionHandler: Opened usertable,user6,1404225423342.89cb00f0096b305c3317b7753d66adf1. on slave1,60020,1404225098235
2014-07-01 07:37:02,375 DEBUG [RS_OPEN_REGION-slave1:60020-0] zookeeper.ZKAssign: regionserver:60020-0x46f25690f50001, quorum=sceplus-vm49.almaden.ibm.com:2181,sceplus-vm48.almaden.ibm.com:2181, baseZNode=/hbase Transitioned node c7604442b843793f8451582c14f6bf59 from M_ZK_REGION_OFFLINE to RS_ZK_REGION_OPENING
2014-07-01 07:37:02,375 DEBUG [RS_OPEN_REGION-slave1:60020-0] regionserver.HRegion: Opening region: {ENCODED => c7604442b843793f8451582c14f6bf59, NAME => 'usertable,user5,1404225423342.c7604442b843793f8451582c14f6bf59.', STARTKEY => 'user5', ENDKEY => 'user6'}
2014-07-01 07:37:02,376 DEBUG [RS_OPEN_REGION-slave1:60020-0] regionserver.MetricsRegionSourceImpl: Creating new MetricsRegionSourceImpl for table usertable c7604442b843793f8451582c14f6bf59
2014-07-01 07:37:02,376 DEBUG [RS_OPEN_REGION-slave1:60020-0] regionserver.HRegion: Instantiated usertable,user5,1404225423342.c7604442b843793f8451582c14f6bf59.
2014-07-01 07:37:02,386 INFO  [StoreOpener-c7604442b843793f8451582c14f6bf59-1] compactions.CompactionConfiguration: size [268435456, 9223372036854775807); files [3, 10); ratio 1.200000; off-peak ratio 5.000000; throttle point 5368709120; delete expired; major period 604800000, major jitter 0.500000
2014-07-01 07:37:02,393 DEBUG [RS_OPEN_REGION-slave1:60020-0] regionserver.HRegion: Found 0 recovered edits file(s) under hdfs://master:54310/hbase/data/default/usertable/c7604442b843793f8451582c14f6bf59
2014-07-01 07:37:02,399 INFO  [RS_OPEN_REGION-slave1:60020-0] regionserver.HRegion: Onlined c7604442b843793f8451582c14f6bf59; next sequenceid=1
2014-07-01 07:37:02,399 DEBUG [RS_OPEN_REGION-slave1:60020-0] zookeeper.ZKAssign: regionserver:60020-0x46f25690f50001, quorum=sceplus-vm49.almaden.ibm.com:2181,sceplus-vm48.almaden.ibm.com:2181, baseZNode=/hbase Attempting to retransition opening state of node c7604442b843793f8451582c14f6bf59
2014-07-01 07:37:02,403 INFO  [PostOpenDeployTasks:c7604442b843793f8451582c14f6bf59] regionserver.HRegionServer: Post open deploy tasks for region=usertable,user5,1404225423342.c7604442b843793f8451582c14f6bf59.
2014-07-01 07:37:02,414 INFO  [PostOpenDeployTasks:c7604442b843793f8451582c14f6bf59] catalog.MetaEditor: Updated row usertable,user5,1404225423342.c7604442b843793f8451582c14f6bf59. with server=slave1,60020,1404225098235
2014-07-01 07:37:02,414 INFO  [PostOpenDeployTasks:c7604442b843793f8451582c14f6bf59] regionserver.HRegionServer: Finished post open deploy task for usertable,user5,1404225423342.c7604442b843793f8451582c14f6bf59.
2014-07-01 07:37:02,414 DEBUG [RS_OPEN_REGION-slave1:60020-0] zookeeper.ZKAssign: regionserver:60020-0x46f25690f50001, quorum=sceplus-vm49.almaden.ibm.com:2181,sceplus-vm48.almaden.ibm.com:2181, baseZNode=/hbase Transitioning c7604442b843793f8451582c14f6bf59 from RS_ZK_REGION_OPENING to RS_ZK_REGION_OPENED
2014-07-01 07:37:02,422 DEBUG [RS_OPEN_REGION-slave1:60020-0] zookeeper.ZKAssign: regionserver:60020-0x46f25690f50001, quorum=sceplus-vm49.almaden.ibm.com:2181,sceplus-vm48.almaden.ibm.com:2181, baseZNode=/hbase Transitioned node c7604442b843793f8451582c14f6bf59 from RS_ZK_REGION_OPENING to RS_ZK_REGION_OPENED
2014-07-01 07:37:02,422 DEBUG [RS_OPEN_REGION-slave1:60020-0] handler.OpenRegionHandler: Transitioned c7604442b843793f8451582c14f6bf59 to OPENED in zk on slave1,60020,1404225098235
2014-07-01 07:37:02,422 DEBUG [RS_OPEN_REGION-slave1:60020-0] handler.OpenRegionHandler: Opened usertable,user5,1404225423342.c7604442b843793f8451582c14f6bf59. on slave1,60020,1404225098235
2014-07-01 07:37:20,539 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-01 07:37:20,795 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 117 synced till here 93
2014-07-01 07:37:21,374 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1404225098235/slave1%2C60020%2C1404225098235.1404225109145 with entries=117, filesize=92.8m; new WAL /hbase/WALs/slave1,60020,1404225098235/slave1%2C60020%2C1404225098235.1404225440540
2014-07-01 07:37:23,435 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-01 07:37:23,528 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 216 synced till here 200
2014-07-01 07:37:23,993 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1404225098235/slave1%2C60020%2C1404225098235.1404225440540 with entries=99, filesize=84.0m; new WAL /hbase/WALs/slave1,60020,1404225098235/slave1%2C60020%2C1404225098235.1404225443435
2014-07-01 07:37:26,859 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-01 07:37:27,192 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 353 synced till here 316
2014-07-01 07:37:27,909 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1404225098235/slave1%2C60020%2C1404225098235.1404225443435 with entries=137, filesize=108.9m; new WAL /hbase/WALs/slave1,60020,1404225098235/slave1%2C60020%2C1404225098235.1404225446859
2014-07-01 07:37:30,380 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-01 07:37:30,935 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 493 synced till here 484
2014-07-01 07:37:31,113 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1404225098235/slave1%2C60020%2C1404225098235.1404225446859 with entries=140, filesize=109.4m; new WAL /hbase/WALs/slave1,60020,1404225098235/slave1%2C60020%2C1404225098235.1404225450510
2014-07-01 07:37:33,085 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-01 07:37:33,295 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 592 synced till here 570
2014-07-01 07:37:33,606 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1404225098235/slave1%2C60020%2C1404225098235.1404225450510 with entries=99, filesize=77.7m; new WAL /hbase/WALs/slave1,60020,1404225098235/slave1%2C60020%2C1404225098235.1404225453085
2014-07-01 07:38:00,910 INFO  [regionserver60020-SendThread(master:2181)] zookeeper.ClientCnxn: Unable to read additional data from server sessionid 0x46f25690f50001, likely server has closed socket, closing socket connection and attempting reconnect
2014-07-01 07:38:00,910 INFO  [regionserver60020-SendThread(master:2181)] zookeeper.ClientCnxn: Unable to read additional data from server sessionid 0x46f25690f50003, likely server has closed socket, closing socket connection and attempting reconnect
2014-07-01 07:38:00,943 INFO  [regionserver60020-SendThread(master:2181)] zookeeper.ClientCnxn: Unable to read additional data from server sessionid 0x46f25690f50006, likely server has closed socket, closing socket connection and attempting reconnect
2014-07-01 07:38:01,388 INFO  [regionserver60020-SendThread(sceplus-vm49.almaden.ibm.com:2181)] zookeeper.ClientCnxn: Opening socket connection to server sceplus-vm49.almaden.ibm.com/9.1.143.59:2181. Will not attempt to authenticate using SASL (unknown error)
2014-07-01 07:38:01,389 INFO  [regionserver60020-SendThread(sceplus-vm49.almaden.ibm.com:2181)] zookeeper.ClientCnxn: Socket connection established to sceplus-vm49.almaden.ibm.com/9.1.143.59:2181, initiating session
2014-07-01 07:38:01,390 INFO  [regionserver60020-SendThread(sceplus-vm49.almaden.ibm.com:2181)] zookeeper.ClientCnxn: Unable to read additional data from server sessionid 0x46f25690f50003, likely server has closed socket, closing socket connection and attempting reconnect
2014-07-01 07:38:01,493 INFO  [regionserver60020-SendThread(sceplus-vm49.almaden.ibm.com:2181)] zookeeper.ClientCnxn: Opening socket connection to server sceplus-vm49.almaden.ibm.com/9.1.143.59:2181. Will not attempt to authenticate using SASL (unknown error)
2014-07-01 07:38:01,493 INFO  [regionserver60020-SendThread(sceplus-vm49.almaden.ibm.com:2181)] zookeeper.ClientCnxn: Socket connection established to sceplus-vm49.almaden.ibm.com/9.1.143.59:2181, initiating session
2014-07-01 07:38:01,496 INFO  [regionserver60020-SendThread(sceplus-vm49.almaden.ibm.com:2181)] zookeeper.ClientCnxn: Unable to read additional data from server sessionid 0x46f25690f50001, likely server has closed socket, closing socket connection and attempting reconnect
2014-07-01 07:38:01,652 INFO  [regionserver60020-SendThread(sceplus-vm49.almaden.ibm.com:2181)] zookeeper.ClientCnxn: Opening socket connection to server sceplus-vm49.almaden.ibm.com/9.1.143.59:2181. Will not attempt to authenticate using SASL (unknown error)
2014-07-01 07:38:01,652 INFO  [regionserver60020-SendThread(sceplus-vm49.almaden.ibm.com:2181)] zookeeper.ClientCnxn: Socket connection established to sceplus-vm49.almaden.ibm.com/9.1.143.59:2181, initiating session
2014-07-01 07:38:01,653 INFO  [regionserver60020-SendThread(sceplus-vm49.almaden.ibm.com:2181)] zookeeper.ClientCnxn: Unable to read additional data from server sessionid 0x46f25690f50006, likely server has closed socket, closing socket connection and attempting reconnect
2014-07-01 07:38:02,728 INFO  [regionserver60020-SendThread(master:2181)] zookeeper.ClientCnxn: Opening socket connection to server master/9.1.143.58:2181. Will not attempt to authenticate using SASL (unknown error)
2014-07-01 07:38:02,826 INFO  [regionserver60020-SendThread(master:2181)] zookeeper.ClientCnxn: Socket connection established to master/9.1.143.58:2181, initiating session
2014-07-01 07:38:03,307 INFO  [regionserver60020-SendThread(master:2181)] zookeeper.ClientCnxn: Unable to read additional data from server sessionid 0x46f25690f50003, likely server has closed socket, closing socket connection and attempting reconnect
2014-07-01 07:38:03,415 INFO  [regionserver60020-SendThread(master:2181)] zookeeper.ClientCnxn: Opening socket connection to server master/9.1.143.58:2181. Will not attempt to authenticate using SASL (unknown error)
2014-07-01 07:38:03,425 INFO  [regionserver60020-SendThread(master:2181)] zookeeper.ClientCnxn: Socket connection established to master/9.1.143.58:2181, initiating session
2014-07-01 07:38:03,442 INFO  [regionserver60020-SendThread(master:2181)] zookeeper.ClientCnxn: Unable to read additional data from server sessionid 0x46f25690f50006, likely server has closed socket, closing socket connection and attempting reconnect
2014-07-01 07:38:03,466 INFO  [regionserver60020-SendThread(master:2181)] zookeeper.ClientCnxn: Opening socket connection to server master/9.1.143.58:2181. Will not attempt to authenticate using SASL (unknown error)
2014-07-01 07:38:03,479 INFO  [regionserver60020-SendThread(master:2181)] zookeeper.ClientCnxn: Socket connection established to master/9.1.143.58:2181, initiating session
2014-07-01 07:38:03,502 INFO  [regionserver60020-SendThread(master:2181)] zookeeper.ClientCnxn: Unable to read additional data from server sessionid 0x46f25690f50001, likely server has closed socket, closing socket connection and attempting reconnect
2014-07-01 07:38:03,587 INFO  [regionserver60020-SendThread(sceplus-vm49.almaden.ibm.com:2181)] zookeeper.ClientCnxn: Opening socket connection to server sceplus-vm49.almaden.ibm.com/9.1.143.59:2181. Will not attempt to authenticate using SASL (unknown error)
2014-07-01 07:38:03,587 INFO  [regionserver60020-SendThread(sceplus-vm49.almaden.ibm.com:2181)] zookeeper.ClientCnxn: Socket connection established to sceplus-vm49.almaden.ibm.com/9.1.143.59:2181, initiating session
2014-07-01 07:38:03,588 INFO  [regionserver60020-SendThread(sceplus-vm49.almaden.ibm.com:2181)] zookeeper.ClientCnxn: Unable to read additional data from server sessionid 0x46f25690f50003, likely server has closed socket, closing socket connection and attempting reconnect
2014-07-01 07:38:03,917 INFO  [regionserver60020-SendThread(sceplus-vm49.almaden.ibm.com:2181)] zookeeper.ClientCnxn: Opening socket connection to server sceplus-vm49.almaden.ibm.com/9.1.143.59:2181. Will not attempt to authenticate using SASL (unknown error)
2014-07-01 07:38:03,917 INFO  [regionserver60020-SendThread(sceplus-vm49.almaden.ibm.com:2181)] zookeeper.ClientCnxn: Socket connection established to sceplus-vm49.almaden.ibm.com/9.1.143.59:2181, initiating session
2014-07-01 07:38:03,918 INFO  [regionserver60020-SendThread(sceplus-vm49.almaden.ibm.com:2181)] zookeeper.ClientCnxn: Unable to read additional data from server sessionid 0x46f25690f50001, likely server has closed socket, closing socket connection and attempting reconnect
2014-07-01 07:38:04,045 INFO  [regionserver60020-SendThread(sceplus-vm49.almaden.ibm.com:2181)] zookeeper.ClientCnxn: Opening socket connection to server sceplus-vm49.almaden.ibm.com/9.1.143.59:2181. Will not attempt to authenticate using SASL (unknown error)
2014-07-01 07:38:04,046 INFO  [regionserver60020-SendThread(sceplus-vm49.almaden.ibm.com:2181)] zookeeper.ClientCnxn: Socket connection established to sceplus-vm49.almaden.ibm.com/9.1.143.59:2181, initiating session
2014-07-01 07:38:04,046 INFO  [regionserver60020-SendThread(sceplus-vm49.almaden.ibm.com:2181)] zookeeper.ClientCnxn: Unable to read additional data from server sessionid 0x46f25690f50006, likely server has closed socket, closing socket connection and attempting reconnect
2014-07-01 07:38:05,096 INFO  [regionserver60020-SendThread(master:2181)] zookeeper.ClientCnxn: Opening socket connection to server master/9.1.143.58:2181. Will not attempt to authenticate using SASL (unknown error)
2014-07-01 07:38:05,108 INFO  [regionserver60020-SendThread(master:2181)] zookeeper.ClientCnxn: Socket connection established to master/9.1.143.58:2181, initiating session
2014-07-01 07:38:05,211 INFO  [regionserver60020-SendThread(master:2181)] zookeeper.ClientCnxn: Session establishment complete on server master/9.1.143.58:2181, sessionid = 0x46f25690f50003, negotiated timeout = 90000
2014-07-01 07:38:05,232 INFO  [regionserver60020-SendThread(master:2181)] zookeeper.ClientCnxn: Opening socket connection to server master/9.1.143.58:2181. Will not attempt to authenticate using SASL (unknown error)
2014-07-01 07:38:05,246 INFO  [regionserver60020-SendThread(master:2181)] zookeeper.ClientCnxn: Socket connection established to master/9.1.143.58:2181, initiating session
2014-07-01 07:38:05,300 INFO  [regionserver60020-SendThread(master:2181)] zookeeper.ClientCnxn: Opening socket connection to server master/9.1.143.58:2181. Will not attempt to authenticate using SASL (unknown error)
2014-07-01 07:38:05,364 INFO  [regionserver60020-SendThread(master:2181)] zookeeper.ClientCnxn: Socket connection established to master/9.1.143.58:2181, initiating session
2014-07-01 07:38:05,367 INFO  [regionserver60020-SendThread(master:2181)] zookeeper.ClientCnxn: Session establishment complete on server master/9.1.143.58:2181, sessionid = 0x46f25690f50006, negotiated timeout = 90000
2014-07-01 07:38:05,368 INFO  [regionserver60020-SendThread(master:2181)] zookeeper.ClientCnxn: Session establishment complete on server master/9.1.143.58:2181, sessionid = 0x46f25690f50001, negotiated timeout = 90000
2014-07-01 07:38:16,367 WARN  [RpcServer.handler=33,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":45341,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.58:33337","starttimems":1404225450828,"queuetimems":2934,"class":"HRegionServer","responsesize":15509,"method":"Multi"}
2014-07-01 07:38:16,367 WARN  [RpcServer.handler=37,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":45478,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.58:33337","starttimems":1404225450691,"queuetimems":2962,"class":"HRegionServer","responsesize":15629,"method":"Multi"}
2014-07-01 07:38:16,367 WARN  [RpcServer.handler=40,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":44758,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.58:33337","starttimems":1404225451411,"queuetimems":3348,"class":"HRegionServer","responsesize":10406,"method":"Multi"}
2014-07-01 07:38:16,367 WARN  [RpcServer.handler=34,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":45486,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.58:33337","starttimems":1404225450683,"queuetimems":2965,"class":"HRegionServer","responsesize":15693,"method":"Multi"}
2014-07-01 07:38:16,371 WARN  [RpcServer.handler=11,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":45341,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.58:33337","starttimems":1404225450828,"queuetimems":2944,"class":"HRegionServer","responsesize":15557,"method":"Multi"}
2014-07-01 07:38:19,696 DEBUG [RpcServer.handler=1,port=60020] regionserver.HRegion: Flush requested on usertable,user2,1404225423342.07064f253c865a7cc0c08893a4f56af9.
2014-07-01 07:38:19,704 DEBUG [MemStoreFlusher.1] regionserver.HRegion: Started memstore flush for usertable,user2,1404225423342.07064f253c865a7cc0c08893a4f56af9., current region memstore size 258.4m
2014-07-01 07:38:19,705 WARN  [RpcServer.handler=38,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":46080,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.58:33337","starttimems":1404225453625,"queuetimems":4122,"class":"HRegionServer","responsesize":753,"method":"Multi"}
2014-07-01 07:38:19,712 WARN  [RpcServer.handler=41,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":49328,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.58:33337","starttimems":1404225450383,"queuetimems":2991,"class":"HRegionServer","responsesize":15890,"method":"Multi"}
2014-07-01 07:38:19,813 WARN  [RpcServer.handler=46,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":49417,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.58:33337","starttimems":1404225450396,"queuetimems":2707,"class":"HRegionServer","responsesize":15833,"method":"Multi"}
2014-07-01 07:38:23,882 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-01 07:38:23,883 DEBUG [RpcServer.handler=44,port=60020] regionserver.HRegion: Flush requested on usertable,user5,1404225423342.c7604442b843793f8451582c14f6bf59.
2014-07-01 07:38:23,884 WARN  [RpcServer.handler=45,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":50796,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.58:33337","starttimems":1404225453086,"queuetimems":5016,"class":"HRegionServer","responsesize":10176,"method":"Multi"}
2014-07-01 07:38:23,884 DEBUG [MemStoreFlusher.0] regionserver.HRegion: Started memstore flush for usertable,user5,1404225423342.c7604442b843793f8451582c14f6bf59., current region memstore size 256.3m
2014-07-01 07:38:23,887 WARN  [RpcServer.handler=13,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":50795,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.58:33337","starttimems":1404225453087,"queuetimems":5011,"class":"HRegionServer","responsesize":10359,"method":"Multi"}
2014-07-01 07:38:24,317 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 733 synced till here 694
2014-07-01 07:38:24,517 DEBUG [MemStoreFlusher.1] util.FSUtils: DFS Client does not support most favored nodes create; using default create
2014-07-01 07:38:24,769 WARN  [RpcServer.handler=27,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":51354,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.58:33337","starttimems":1404225453415,"queuetimems":4520,"class":"HRegionServer","responsesize":8355,"method":"Multi"}
2014-07-01 07:38:24,770 WARN  [RpcServer.handler=26,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":51674,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.58:33337","starttimems":1404225453096,"queuetimems":4884,"class":"HRegionServer","responsesize":10495,"method":"Multi"}
2014-07-01 07:38:24,770 WARN  [RpcServer.handler=7,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":51683,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.58:33337","starttimems":1404225453087,"queuetimems":4891,"class":"HRegionServer","responsesize":11629,"method":"Multi"}
2014-07-01 07:38:24,772 WARN  [RpcServer.handler=24,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":51684,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.58:33337","starttimems":1404225453087,"queuetimems":5003,"class":"HRegionServer","responsesize":11902,"method":"Multi"}
2014-07-01 07:38:24,772 WARN  [RpcServer.handler=28,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":51676,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.58:33337","starttimems":1404225453095,"queuetimems":4890,"class":"HRegionServer","responsesize":13248,"method":"Multi"}
2014-07-01 07:38:24,772 WARN  [RpcServer.handler=48,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":51675,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.58:33337","starttimems":1404225453096,"queuetimems":4873,"class":"HRegionServer","responsesize":9437,"method":"Multi"}
2014-07-01 07:38:25,251 DEBUG [MemStoreFlusher.0] util.FSUtils: DFS Client does not support most favored nodes create; using default create
2014-07-01 07:38:25,262 INFO  [MemStoreFlusher.0] compress.CodecPool: Got brand-new compressor
2014-07-01 07:38:25,513 WARN  [RpcServer.handler=44,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":52097,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.58:33337","starttimems":1404225453415,"queuetimems":4446,"class":"HRegionServer","responsesize":9161,"method":"Multi"}
2014-07-01 07:38:25,524 WARN  [RpcServer.handler=1,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":52092,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.58:33337","starttimems":1404225453427,"queuetimems":4357,"class":"HRegionServer","responsesize":7840,"method":"Multi"}
2014-07-01 07:38:25,530 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1404225098235/slave1%2C60020%2C1404225098235.1404225453085 with entries=141, filesize=107.4m; new WAL /hbase/WALs/slave1,60020,1404225098235/slave1%2C60020%2C1404225098235.1404225503887
2014-07-01 07:38:25,781 DEBUG [RpcServer.handler=5,port=60020] regionserver.HRegion: Flush requested on usertable,user6,1404225423342.89cb00f0096b305c3317b7753d66adf1.
2014-07-01 07:38:25,781 WARN  [RpcServer.handler=20,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":52175,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.58:33337","starttimems":1404225453605,"queuetimems":4504,"class":"HRegionServer","responsesize":10211,"method":"Multi"}
2014-07-01 07:38:25,781 WARN  [RpcServer.handler=12,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":52160,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.58:33337","starttimems":1404225453621,"queuetimems":4147,"class":"HRegionServer","responsesize":9029,"method":"Multi"}
2014-07-01 07:38:25,781 WARN  [RpcServer.handler=5,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":52177,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.58:33337","starttimems":1404225453604,"queuetimems":4527,"class":"HRegionServer","responsesize":9179,"method":"Multi"}
2014-07-01 07:38:25,781 WARN  [RpcServer.handler=30,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":52153,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.58:33337","starttimems":1404225453627,"queuetimems":3982,"class":"HRegionServer","responsesize":6905,"method":"Multi"}
2014-07-01 07:38:25,782 WARN  [RpcServer.handler=22,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":52366,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.58:33337","starttimems":1404225453415,"queuetimems":4470,"class":"HRegionServer","responsesize":11651,"method":"Multi"}
2014-07-01 07:38:28,693 WARN  [RpcServer.handler=8,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":55078,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.58:33337","starttimems":1404225453615,"queuetimems":4244,"class":"HRegionServer","responsesize":11967,"method":"Multi"}
2014-07-01 07:38:28,693 WARN  [RpcServer.handler=18,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":55073,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.58:33337","starttimems":1404225453620,"queuetimems":4164,"class":"HRegionServer","responsesize":11691,"method":"Multi"}
2014-07-01 07:38:28,694 WARN  [RpcServer.handler=49,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":55089,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.58:33337","starttimems":1404225453605,"queuetimems":4365,"class":"HRegionServer","responsesize":11900,"method":"Multi"}
2014-07-01 07:38:28,693 WARN  [RpcServer.handler=25,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":55088,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.58:33337","starttimems":1404225453605,"queuetimems":4490,"class":"HRegionServer","responsesize":10436,"method":"Multi"}
2014-07-01 07:38:28,700 WARN  [RpcServer.handler=15,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":55095,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.58:33337","starttimems":1404225453605,"queuetimems":4375,"class":"HRegionServer","responsesize":12086,"method":"Multi"}
2014-07-01 07:38:28,797 WARN  [RpcServer.handler=31,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":55389,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.58:33337","starttimems":1404225453407,"queuetimems":5096,"class":"HRegionServer","responsesize":15738,"method":"Multi"}
2014-07-01 07:38:28,797 WARN  [RpcServer.handler=32,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":55382,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.58:33337","starttimems":1404225453415,"queuetimems":4858,"class":"HRegionServer","responsesize":15875,"method":"Multi"}
2014-07-01 07:38:28,799 WARN  [RpcServer.handler=35,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":55386,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.58:33337","starttimems":1404225453412,"queuetimems":5072,"class":"HRegionServer","responsesize":15498,"method":"Multi"}
2014-07-01 07:38:28,812 WARN  [RpcServer.handler=36,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":55185,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.58:33337","starttimems":1404225453627,"queuetimems":4115,"class":"HRegionServer","responsesize":8058,"method":"Multi"}
2014-07-01 07:38:28,823 WARN  [RpcServer.handler=3,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":55208,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.58:33337","starttimems":1404225453615,"queuetimems":4255,"class":"HRegionServer","responsesize":11543,"method":"Multi"}
2014-07-01 07:38:28,823 WARN  [RpcServer.handler=9,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":55201,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.58:33337","starttimems":1404225453622,"queuetimems":4141,"class":"HRegionServer","responsesize":7037,"method":"Multi"}
2014-07-01 07:38:28,824 WARN  [RpcServer.handler=17,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":55199,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.58:33337","starttimems":1404225453625,"queuetimems":4129,"class":"HRegionServer","responsesize":6148,"method":"Multi"}
2014-07-01 07:38:28,825 WARN  [RpcServer.handler=42,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":55410,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.58:33337","starttimems":1404225453414,"queuetimems":4878,"class":"HRegionServer","responsesize":16189,"method":"Multi"}
2014-07-01 07:38:28,825 WARN  [RpcServer.handler=43,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":55200,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.58:33337","starttimems":1404225453625,"queuetimems":4123,"class":"HRegionServer","responsesize":5462,"method":"Multi"}
2014-07-01 07:38:28,825 WARN  [RpcServer.handler=6,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":55194,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.58:33337","starttimems":1404225453631,"queuetimems":3784,"class":"HRegionServer","responsesize":15629,"method":"Multi"}
2014-07-01 07:38:28,825 WARN  [RpcServer.handler=10,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":55205,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.58:33337","starttimems":1404225453620,"queuetimems":4154,"class":"HRegionServer","responsesize":11761,"method":"Multi"}
2014-07-01 07:38:28,826 WARN  [RpcServer.handler=21,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":55202,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.58:33337","starttimems":1404225453624,"queuetimems":4134,"class":"HRegionServer","responsesize":7855,"method":"Multi"}
2014-07-01 07:38:28,830 WARN  [RpcServer.handler=29,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":55417,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.58:33337","starttimems":1404225453412,"queuetimems":5085,"class":"HRegionServer","responsesize":15689,"method":"Multi"}
2014-07-01 07:38:28,825 WARN  [RpcServer.handler=0,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":55219,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.58:33337","starttimems":1404225453606,"queuetimems":4353,"class":"HRegionServer","responsesize":11820,"method":"Multi"}
2014-07-01 07:38:28,833 WARN  [RpcServer.handler=14,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":55200,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.58:33337","starttimems":1404225453633,"queuetimems":3710,"class":"HRegionServer","responsesize":15807,"method":"Multi"}
2014-07-01 07:38:28,830 WARN  [RpcServer.handler=2,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":55194,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.58:33337","starttimems":1404225453636,"queuetimems":3694,"class":"HRegionServer","responsesize":15890,"method":"Multi"}
2014-07-01 07:38:28,830 WARN  [RpcServer.handler=23,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":55416,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.58:33337","starttimems":1404225453414,"queuetimems":4868,"class":"HRegionServer","responsesize":15682,"method":"Multi"}
2014-07-01 07:38:28,826 WARN  [RpcServer.handler=19,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":55207,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.58:33337","starttimems":1404225453619,"queuetimems":4236,"class":"HRegionServer","responsesize":11740,"method":"Multi"}
2014-07-01 07:38:29,832 INFO  [MemStoreFlusher.1] regionserver.DefaultStoreFlusher: Flushed, sequenceid=201, memsize=76.4m, hasBloomFilter=true, into tmp file hdfs://master:54310/hbase/data/default/usertable/07064f253c865a7cc0c08893a4f56af9/.tmp/7a635f0ec6ec42c4ac52c602545f5ea2
2014-07-01 07:38:29,936 INFO  [MemStoreFlusher.1] compress.CodecPool: Got brand-new decompressor
2014-07-01 07:38:29,949 INFO  [MemStoreFlusher.0] regionserver.DefaultStoreFlusher: Flushed, sequenceid=198, memsize=76.3m, hasBloomFilter=true, into tmp file hdfs://master:54310/hbase/data/default/usertable/c7604442b843793f8451582c14f6bf59/.tmp/fb647c51cb654ed29d46bd4320090bd4
2014-07-01 07:38:30,064 DEBUG [MemStoreFlusher.1] regionserver.HRegionFileSystem: Committing store file hdfs://master:54310/hbase/data/default/usertable/07064f253c865a7cc0c08893a4f56af9/.tmp/7a635f0ec6ec42c4ac52c602545f5ea2 as hdfs://master:54310/hbase/data/default/usertable/07064f253c865a7cc0c08893a4f56af9/family/7a635f0ec6ec42c4ac52c602545f5ea2
2014-07-01 07:38:30,180 DEBUG [MemStoreFlusher.0] regionserver.HRegionFileSystem: Committing store file hdfs://master:54310/hbase/data/default/usertable/c7604442b843793f8451582c14f6bf59/.tmp/fb647c51cb654ed29d46bd4320090bd4 as hdfs://master:54310/hbase/data/default/usertable/c7604442b843793f8451582c14f6bf59/family/fb647c51cb654ed29d46bd4320090bd4
2014-07-01 07:38:30,191 INFO  [MemStoreFlusher.1] regionserver.HStore: Added hdfs://master:54310/hbase/data/default/usertable/07064f253c865a7cc0c08893a4f56af9/family/7a635f0ec6ec42c4ac52c602545f5ea2, entries=278040, sequenceid=201, filesize=19.8m
2014-07-01 07:38:30,191 INFO  [MemStoreFlusher.1] regionserver.HRegion: Finished memstore flush of ~286.9m/300830480, currentsize=35.9m/37626560 for region usertable,user2,1404225423342.07064f253c865a7cc0c08893a4f56af9. in 10487ms, sequenceid=201, compaction requested=false
2014-07-01 07:38:30,193 DEBUG [MemStoreFlusher.1] regionserver.HRegion: Started memstore flush for usertable,user6,1404225423342.89cb00f0096b305c3317b7753d66adf1., current region memstore size 295.8m
2014-07-01 07:38:30,246 INFO  [MemStoreFlusher.0] regionserver.HStore: Added hdfs://master:54310/hbase/data/default/usertable/c7604442b843793f8451582c14f6bf59/family/fb647c51cb654ed29d46bd4320090bd4, entries=277710, sequenceid=198, filesize=19.8m
2014-07-01 07:38:30,247 INFO  [MemStoreFlusher.0] regionserver.HRegion: Finished memstore flush of ~280.9m/294568320, currentsize=34.5m/36179680 for region usertable,user5,1404225423342.c7604442b843793f8451582c14f6bf59. in 6363ms, sequenceid=198, compaction requested=false
2014-07-01 07:38:31,795 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-01 07:38:31,797 WARN  [RpcServer.handler=47,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":58383,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.58:33337","starttimems":1404225453413,"queuetimems":5045,"class":"HRegionServer","responsesize":16060,"method":"Multi"}
2014-07-01 07:38:31,797 WARN  [RpcServer.handler=33,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":15428,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.58:33337","starttimems":1404225496368,"queuetimems":46383,"class":"HRegionServer","responsesize":15833,"method":"Multi"}
2014-07-01 07:38:31,798 WARN  [RpcServer.handler=39,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":58166,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.58:33337","starttimems":1404225453631,"queuetimems":3964,"class":"HRegionServer","responsesize":15557,"method":"Multi"}
2014-07-01 07:38:31,806 WARN  [RpcServer.handler=4,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":58393,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.58:33337","starttimems":1404225453413,"queuetimems":4886,"class":"HRegionServer","responsesize":15704,"method":"Multi"}
2014-07-01 07:38:31,809 WARN  [RpcServer.handler=37,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":15441,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.58:33337","starttimems":1404225496368,"queuetimems":46400,"class":"HRegionServer","responsesize":15693,"method":"Multi"}
2014-07-01 07:38:31,809 WARN  [RpcServer.handler=16,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":58396,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.58:33337","starttimems":1404225453413,"queuetimems":5061,"class":"HRegionServer","responsesize":15925,"method":"Multi"}
2014-07-01 07:38:31,810 WARN  [RpcServer.handler=40,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":15441,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.58:33337","starttimems":1404225496368,"queuetimems":46235,"class":"HRegionServer","responsesize":15726,"method":"Multi"}
2014-07-01 07:38:31,811 WARN  [RpcServer.handler=34,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":15439,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.58:33337","starttimems":1404225496372,"queuetimems":46219,"class":"HRegionServer","responsesize":15991,"method":"Multi"}
2014-07-01 07:38:31,811 WARN  [RpcServer.handler=11,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":15439,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.58:33337","starttimems":1404225496372,"queuetimems":46198,"class":"HRegionServer","responsesize":15509,"method":"Multi"}
2014-07-01 07:38:31,929 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 830 synced till here 827
2014-07-01 07:38:31,949 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1404225098235/slave1%2C60020%2C1404225098235.1404225503887 with entries=97, filesize=77.7m; new WAL /hbase/WALs/slave1,60020,1404225098235/slave1%2C60020%2C1404225098235.1404225511795
2014-07-01 07:38:32,377 DEBUG [MemStoreFlusher.1] util.FSUtils: DFS Client does not support most favored nodes create; using default create
2014-07-01 07:38:36,284 INFO  [MemStoreFlusher.1] regionserver.DefaultStoreFlusher: Flushed, sequenceid=224, memsize=76.2m, hasBloomFilter=true, into tmp file hdfs://master:54310/hbase/data/default/usertable/89cb00f0096b305c3317b7753d66adf1/.tmp/d00692167bd84170be8548fef003d734
2014-07-01 07:38:36,490 DEBUG [MemStoreFlusher.1] regionserver.HRegionFileSystem: Committing store file hdfs://master:54310/hbase/data/default/usertable/89cb00f0096b305c3317b7753d66adf1/.tmp/d00692167bd84170be8548fef003d734 as hdfs://master:54310/hbase/data/default/usertable/89cb00f0096b305c3317b7753d66adf1/family/d00692167bd84170be8548fef003d734
2014-07-01 07:38:36,594 INFO  [MemStoreFlusher.1] regionserver.HStore: Added hdfs://master:54310/hbase/data/default/usertable/89cb00f0096b305c3317b7753d66adf1/family/d00692167bd84170be8548fef003d734, entries=277540, sequenceid=224, filesize=19.8m
2014-07-01 07:38:36,594 INFO  [MemStoreFlusher.1] regionserver.HRegion: Finished memstore flush of ~315.0m/330293200, currentsize=10.9m/11393200 for region usertable,user6,1404225423342.89cb00f0096b305c3317b7753d66adf1. in 6401ms, sequenceid=224, compaction requested=false
2014-07-01 07:38:38,187 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-01 07:38:38,360 WARN  [RpcServer.handler=20,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":12577,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.58:33337","starttimems":1404225505782,"queuetimems":54021,"class":"HRegionServer","responsesize":15738,"method":"Multi"}
2014-07-01 07:38:38,360 WARN  [RpcServer.handler=12,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":12577,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.58:33337","starttimems":1404225505782,"queuetimems":53796,"class":"HRegionServer","responsesize":15925,"method":"Multi"}
2014-07-01 07:38:38,360 WARN  [RpcServer.handler=27,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":12578,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.58:33337","starttimems":1404225505781,"queuetimems":54033,"class":"HRegionServer","responsesize":15704,"method":"Multi"}
2014-07-01 07:38:38,371 WARN  [RpcServer.handler=22,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":12589,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.58:33337","starttimems":1404225505782,"queuetimems":53619,"class":"HRegionServer","responsesize":15498,"method":"Multi"}
2014-07-01 07:38:38,371 WARN  [RpcServer.handler=5,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":12589,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.58:33337","starttimems":1404225505782,"queuetimems":53824,"class":"HRegionServer","responsesize":15875,"method":"Multi"}
2014-07-01 07:38:38,372 WARN  [RpcServer.handler=30,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":12589,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.58:33337","starttimems":1404225505782,"queuetimems":53638,"class":"HRegionServer","responsesize":16189,"method":"Multi"}
2014-07-01 07:38:38,372 WARN  [RpcServer.handler=44,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":12858,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.58:33337","starttimems":1404225505513,"queuetimems":54013,"class":"HRegionServer","responsesize":15682,"method":"Multi"}
2014-07-01 07:38:38,384 WARN  [RpcServer.handler=1,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":12859,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.58:33337","starttimems":1404225505524,"queuetimems":53795,"class":"HRegionServer","responsesize":16060,"method":"Multi"}
2014-07-01 07:38:38,505 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 948 synced till here 930
2014-07-01 07:38:38,921 WARN  [RpcServer.handler=8,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":10227,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.58:33337","starttimems":1404225508694,"queuetimems":56485,"class":"HRegionServer","responsesize":15689,"method":"Multi"}
2014-07-01 07:38:39,583 WARN  [RpcServer.handler=19,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":10604,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.58:33337","starttimems":1404225508979,"queuetimems":55106,"class":"HRegionServer","responsesize":15509,"method":"Multi"}
2014-07-01 07:38:39,610 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1404225098235/slave1%2C60020%2C1404225098235.1404225511795 with entries=118, filesize=92.8m; new WAL /hbase/WALs/slave1,60020,1404225098235/slave1%2C60020%2C1404225098235.1404225518187
2014-07-01 07:38:41,879 WARN  [RpcServer.handler=11,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":10067,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.58:33337","starttimems":1404225511812,"queuetimems":20567,"class":"HRegionServer","responsesize":15629,"method":"Multi"}
2014-07-01 07:38:42,409 WARN  [RpcServer.handler=45,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":10597,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.58:33337","starttimems":1404225511812,"queuetimems":17363,"class":"HRegionServer","responsesize":15890,"method":"Multi"}
2014-07-01 07:38:43,014 WARN  [RpcServer.handler=24,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":10560,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.58:33337","starttimems":1404225512454,"queuetimems":17060,"class":"HRegionServer","responsesize":15833,"method":"Multi"}
2014-07-01 07:38:43,014 WARN  [RpcServer.handler=48,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":11079,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.58:33337","starttimems":1404225511935,"queuetimems":16624,"class":"HRegionServer","responsesize":15557,"method":"Multi"}
2014-07-01 07:38:43,014 WARN  [RpcServer.handler=28,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":10554,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.58:33337","starttimems":1404225512460,"queuetimems":16826,"class":"HRegionServer","responsesize":15629,"method":"Multi"}
2014-07-01 07:38:43,015 WARN  [RpcServer.handler=7,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":10561,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.58:33337","starttimems":1404225512454,"queuetimems":17116,"class":"HRegionServer","responsesize":15875,"method":"Multi"}
2014-07-01 07:38:43,015 WARN  [RpcServer.handler=26,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":11083,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.58:33337","starttimems":1404225511932,"queuetimems":16643,"class":"HRegionServer","responsesize":16060,"method":"Multi"}
2014-07-01 07:38:43,707 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-01 07:38:43,888 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 1065 synced till here 1049
2014-07-01 07:38:44,120 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1404225098235/slave1%2C60020%2C1404225098235.1404225518187 with entries=117, filesize=91.2m; new WAL /hbase/WALs/slave1,60020,1404225098235/slave1%2C60020%2C1404225098235.1404225523707
2014-07-01 07:38:45,147 WARN  [RpcServer.handler=47,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":10307,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.58:33337","starttimems":1404225514840,"queuetimems":19123,"class":"HRegionServer","responsesize":15807,"method":"Multi"}
2014-07-01 07:38:46,075 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-01 07:38:46,295 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 1171 synced till here 1154
2014-07-01 07:38:46,556 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1404225098235/slave1%2C60020%2C1404225098235.1404225523707 with entries=106, filesize=79.5m; new WAL /hbase/WALs/slave1,60020,1404225098235/slave1%2C60020%2C1404225098235.1404225526075
2014-07-01 07:38:47,389 DEBUG [RpcServer.handler=34,port=60020] regionserver.HRegion: Flush requested on usertable,,1404225423341.387a171caad6315f0b61271a30313d1a.
2014-07-01 07:38:47,390 DEBUG [MemStoreFlusher.0] regionserver.HRegion: Started memstore flush for usertable,,1404225423341.387a171caad6315f0b61271a30313d1a., current region memstore size 260.4m
2014-07-01 07:38:47,885 DEBUG [MemStoreFlusher.0] util.FSUtils: DFS Client does not support most favored nodes create; using default create
2014-07-01 07:38:48,684 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-01 07:38:49,136 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 1289 synced till here 1275
2014-07-01 07:38:49,628 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1404225098235/slave1%2C60020%2C1404225098235.1404225526075 with entries=118, filesize=88.0m; new WAL /hbase/WALs/slave1,60020,1404225098235/slave1%2C60020%2C1404225098235.1404225528685
2014-07-01 07:38:50,363 DEBUG [RpcServer.handler=40,port=60020] regionserver.HRegion: Flush requested on usertable,user2,1404225423342.07064f253c865a7cc0c08893a4f56af9.
2014-07-01 07:38:50,363 DEBUG [MemStoreFlusher.1] regionserver.HRegion: Started memstore flush for usertable,user2,1404225423342.07064f253c865a7cc0c08893a4f56af9., current region memstore size 256.7m
2014-07-01 07:38:50,864 INFO  [MemStoreFlusher.0] regionserver.DefaultStoreFlusher: Flushed, sequenceid=176, memsize=38.8m, hasBloomFilter=true, into tmp file hdfs://master:54310/hbase/data/default/usertable/387a171caad6315f0b61271a30313d1a/.tmp/a65780cad89a42c28d8e2bd69979ed56
2014-07-01 07:38:50,896 DEBUG [MemStoreFlusher.0] regionserver.HRegionFileSystem: Committing store file hdfs://master:54310/hbase/data/default/usertable/387a171caad6315f0b61271a30313d1a/.tmp/a65780cad89a42c28d8e2bd69979ed56 as hdfs://master:54310/hbase/data/default/usertable/387a171caad6315f0b61271a30313d1a/family/a65780cad89a42c28d8e2bd69979ed56
2014-07-01 07:38:50,950 INFO  [MemStoreFlusher.0] regionserver.HStore: Added hdfs://master:54310/hbase/data/default/usertable/387a171caad6315f0b61271a30313d1a/family/a65780cad89a42c28d8e2bd69979ed56, entries=141160, sequenceid=176, filesize=10.1m
2014-07-01 07:38:50,950 INFO  [MemStoreFlusher.0] regionserver.HRegion: Finished memstore flush of ~260.4m/273046480, currentsize=11.6m/12199360 for region usertable,,1404225423341.387a171caad6315f0b61271a30313d1a. in 3560ms, sequenceid=176, compaction requested=false
2014-07-01 07:38:51,451 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-01 07:38:51,456 DEBUG [RpcServer.handler=12,port=60020] regionserver.HRegion: Flush requested on usertable,user5,1404225423342.c7604442b843793f8451582c14f6bf59.
2014-07-01 07:38:51,543 DEBUG [MemStoreFlusher.0] regionserver.HRegion: Started memstore flush for usertable,user5,1404225423342.c7604442b843793f8451582c14f6bf59., current region memstore size 276.8m
2014-07-01 07:38:51,671 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 1393 synced till here 1388
2014-07-01 07:38:51,743 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1404225098235/slave1%2C60020%2C1404225098235.1404225528685 with entries=104, filesize=78.9m; new WAL /hbase/WALs/slave1,60020,1404225098235/slave1%2C60020%2C1404225098235.1404225531451
2014-07-01 07:38:51,743 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1404225098235/slave1%2C60020%2C1404225098235.1404225109145
2014-07-01 07:38:51,743 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1404225098235/slave1%2C60020%2C1404225098235.1404225440540
2014-07-01 07:38:51,743 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1404225098235/slave1%2C60020%2C1404225098235.1404225443435
2014-07-01 07:38:51,744 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1404225098235/slave1%2C60020%2C1404225098235.1404225446859
2014-07-01 07:38:51,744 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1404225098235/slave1%2C60020%2C1404225098235.1404225450510
2014-07-01 07:38:52,054 DEBUG [MemStoreFlusher.1] util.FSUtils: DFS Client does not support most favored nodes create; using default create
2014-07-01 07:38:52,453 DEBUG [MemStoreFlusher.0] util.FSUtils: DFS Client does not support most favored nodes create; using default create
2014-07-01 07:38:53,348 DEBUG [RpcServer.handler=14,port=60020] regionserver.HRegion: Flush requested on usertable,user6,1404225423342.89cb00f0096b305c3317b7753d66adf1.
2014-07-01 07:38:55,302 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-01 07:38:55,593 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 1514 synced till here 1492
2014-07-01 07:38:55,980 INFO  [MemStoreFlusher.1] regionserver.DefaultStoreFlusher: Flushed, sequenceid=414, memsize=70.3m, hasBloomFilter=true, into tmp file hdfs://master:54310/hbase/data/default/usertable/07064f253c865a7cc0c08893a4f56af9/.tmp/2eef1e7e35f74b42af169c821224976c
2014-07-01 07:38:56,041 DEBUG [MemStoreFlusher.1] regionserver.HRegionFileSystem: Committing store file hdfs://master:54310/hbase/data/default/usertable/07064f253c865a7cc0c08893a4f56af9/.tmp/2eef1e7e35f74b42af169c821224976c as hdfs://master:54310/hbase/data/default/usertable/07064f253c865a7cc0c08893a4f56af9/family/2eef1e7e35f74b42af169c821224976c
2014-07-01 07:38:56,077 INFO  [MemStoreFlusher.1] regionserver.HStore: Added hdfs://master:54310/hbase/data/default/usertable/07064f253c865a7cc0c08893a4f56af9/family/2eef1e7e35f74b42af169c821224976c, entries=255960, sequenceid=414, filesize=18.2m
2014-07-01 07:38:56,078 INFO  [MemStoreFlusher.1] regionserver.HRegion: Finished memstore flush of ~292.2m/306373360, currentsize=58.3m/61152720 for region usertable,user2,1404225423342.07064f253c865a7cc0c08893a4f56af9. in 5715ms, sequenceid=414, compaction requested=false
2014-07-01 07:38:56,079 DEBUG [MemStoreFlusher.1] regionserver.HRegion: Started memstore flush for usertable,user6,1404225423342.89cb00f0096b305c3317b7753d66adf1., current region memstore size 269.1m
2014-07-01 07:38:56,266 INFO  [MemStoreFlusher.0] regionserver.DefaultStoreFlusher: Flushed, sequenceid=414, memsize=69.9m, hasBloomFilter=true, into tmp file hdfs://master:54310/hbase/data/default/usertable/c7604442b843793f8451582c14f6bf59/.tmp/f2cce55c95e5461ea8942a2ed282b7f5
2014-07-01 07:38:56,293 DEBUG [MemStoreFlusher.0] regionserver.HRegionFileSystem: Committing store file hdfs://master:54310/hbase/data/default/usertable/c7604442b843793f8451582c14f6bf59/.tmp/f2cce55c95e5461ea8942a2ed282b7f5 as hdfs://master:54310/hbase/data/default/usertable/c7604442b843793f8451582c14f6bf59/family/f2cce55c95e5461ea8942a2ed282b7f5
2014-07-01 07:38:56,328 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1404225098235/slave1%2C60020%2C1404225098235.1404225531451 with entries=121, filesize=94.2m; new WAL /hbase/WALs/slave1,60020,1404225098235/slave1%2C60020%2C1404225098235.1404225535303
2014-07-01 07:38:56,334 INFO  [MemStoreFlusher.0] regionserver.HStore: Added hdfs://master:54310/hbase/data/default/usertable/c7604442b843793f8451582c14f6bf59/family/f2cce55c95e5461ea8942a2ed282b7f5, entries=254460, sequenceid=414, filesize=18.1m
2014-07-01 07:38:56,334 INFO  [MemStoreFlusher.0] regionserver.HRegion: Finished memstore flush of ~295.4m/309780720, currentsize=26.2m/27522800 for region usertable,user5,1404225423342.c7604442b843793f8451582c14f6bf59. in 4791ms, sequenceid=414, compaction requested=false
2014-07-01 07:38:56,496 DEBUG [MemStoreFlusher.1] util.FSUtils: DFS Client does not support most favored nodes create; using default create
2014-07-01 07:38:58,324 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-01 07:38:58,432 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 1622 synced till here 1606
2014-07-01 07:38:58,735 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1404225098235/slave1%2C60020%2C1404225098235.1404225535303 with entries=108, filesize=84.8m; new WAL /hbase/WALs/slave1,60020,1404225098235/slave1%2C60020%2C1404225098235.1404225538325
2014-07-01 07:38:58,735 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1404225098235/slave1%2C60020%2C1404225098235.1404225453085
2014-07-01 07:38:58,735 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1404225098235/slave1%2C60020%2C1404225098235.1404225503887
2014-07-01 07:39:00,090 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-01 07:39:00,186 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 1705 synced till here 1696
2014-07-01 07:39:00,879 INFO  [MemStoreFlusher.1] regionserver.DefaultStoreFlusher: Flushed, sequenceid=429, memsize=69.9m, hasBloomFilter=true, into tmp file hdfs://master:54310/hbase/data/default/usertable/89cb00f0096b305c3317b7753d66adf1/.tmp/e41790abfd37484abb4e97a2b2c8865e
2014-07-01 07:39:00,895 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1404225098235/slave1%2C60020%2C1404225098235.1404225538325 with entries=83, filesize=66.7m; new WAL /hbase/WALs/slave1,60020,1404225098235/slave1%2C60020%2C1404225098235.1404225540090
2014-07-01 07:39:00,908 DEBUG [MemStoreFlusher.1] regionserver.HRegionFileSystem: Committing store file hdfs://master:54310/hbase/data/default/usertable/89cb00f0096b305c3317b7753d66adf1/.tmp/e41790abfd37484abb4e97a2b2c8865e as hdfs://master:54310/hbase/data/default/usertable/89cb00f0096b305c3317b7753d66adf1/family/e41790abfd37484abb4e97a2b2c8865e
2014-07-01 07:39:00,990 INFO  [MemStoreFlusher.1] regionserver.HStore: Added hdfs://master:54310/hbase/data/default/usertable/89cb00f0096b305c3317b7753d66adf1/family/e41790abfd37484abb4e97a2b2c8865e, entries=254400, sequenceid=429, filesize=18.1m
2014-07-01 07:39:00,990 INFO  [MemStoreFlusher.1] regionserver.HRegion: Finished memstore flush of ~279.7m/293314720, currentsize=91.6m/96032000 for region usertable,user6,1404225423342.89cb00f0096b305c3317b7753d66adf1. in 4911ms, sequenceid=429, compaction requested=false
2014-07-01 07:39:02,680 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-01 07:39:02,767 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 1791 synced till here 1789
2014-07-01 07:39:02,970 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1404225098235/slave1%2C60020%2C1404225098235.1404225540090 with entries=86, filesize=64.0m; new WAL /hbase/WALs/slave1,60020,1404225098235/slave1%2C60020%2C1404225098235.1404225542681
2014-07-01 07:39:02,970 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1404225098235/slave1%2C60020%2C1404225098235.1404225511795
2014-07-01 07:39:02,970 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1404225098235/slave1%2C60020%2C1404225098235.1404225518187
2014-07-01 07:39:02,970 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1404225098235/slave1%2C60020%2C1404225098235.1404225523707
2014-07-01 07:39:05,631 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-01 07:39:05,674 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 1879 synced till here 1877
2014-07-01 07:39:05,811 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1404225098235/slave1%2C60020%2C1404225098235.1404225542681 with entries=88, filesize=64.2m; new WAL /hbase/WALs/slave1,60020,1404225098235/slave1%2C60020%2C1404225098235.1404225545632
2014-07-01 07:39:09,863 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-01 07:39:10,050 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 1975 synced till here 1950
2014-07-01 07:39:10,488 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1404225098235/slave1%2C60020%2C1404225098235.1404225545632 with entries=96, filesize=81.2m; new WAL /hbase/WALs/slave1,60020,1404225098235/slave1%2C60020%2C1404225098235.1404225549864
2014-07-01 07:39:14,242 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-01 07:39:14,405 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 2075 synced till here 2069
2014-07-01 07:39:14,549 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1404225098235/slave1%2C60020%2C1404225098235.1404225549864 with entries=100, filesize=73.2m; new WAL /hbase/WALs/slave1,60020,1404225098235/slave1%2C60020%2C1404225098235.1404225554242
2014-07-01 07:39:15,151 DEBUG [RpcServer.handler=12,port=60020] regionserver.HRegion: Flush requested on usertable,user2,1404225423342.07064f253c865a7cc0c08893a4f56af9.
2014-07-01 07:39:15,152 DEBUG [MemStoreFlusher.0] regionserver.HRegion: Started memstore flush for usertable,user2,1404225423342.07064f253c865a7cc0c08893a4f56af9., current region memstore size 258.6m
2014-07-01 07:39:15,735 DEBUG [MemStoreFlusher.0] util.FSUtils: DFS Client does not support most favored nodes create; using default create
2014-07-01 07:39:15,875 DEBUG [RpcServer.handler=48,port=60020] regionserver.HRegion: Flush requested on usertable,user5,1404225423342.c7604442b843793f8451582c14f6bf59.
2014-07-01 07:39:15,880 DEBUG [MemStoreFlusher.1] regionserver.HRegion: Started memstore flush for usertable,user5,1404225423342.c7604442b843793f8451582c14f6bf59., current region memstore size 260.7m
2014-07-01 07:39:18,001 DEBUG [MemStoreFlusher.1] util.FSUtils: DFS Client does not support most favored nodes create; using default create
2014-07-01 07:39:19,272 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-01 07:39:19,273 DEBUG [RpcServer.handler=35,port=60020] regionserver.HRegion: Flush requested on usertable,user6,1404225423342.89cb00f0096b305c3317b7753d66adf1.
2014-07-01 07:39:19,319 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 2178 synced till here 2155
2014-07-01 07:39:21,473 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1404225098235/slave1%2C60020%2C1404225098235.1404225554242 with entries=103, filesize=81.1m; new WAL /hbase/WALs/slave1,60020,1404225098235/slave1%2C60020%2C1404225098235.1404225559272
2014-07-01 07:39:21,744 INFO  [MemStoreFlusher.0] regionserver.DefaultStoreFlusher: Flushed, sequenceid=597, memsize=113.5m, hasBloomFilter=true, into tmp file hdfs://master:54310/hbase/data/default/usertable/07064f253c865a7cc0c08893a4f56af9/.tmp/a1e49ce455894f0ba4a51b0141e0c2f2
2014-07-01 07:39:21,774 DEBUG [MemStoreFlusher.0] regionserver.HRegionFileSystem: Committing store file hdfs://master:54310/hbase/data/default/usertable/07064f253c865a7cc0c08893a4f56af9/.tmp/a1e49ce455894f0ba4a51b0141e0c2f2 as hdfs://master:54310/hbase/data/default/usertable/07064f253c865a7cc0c08893a4f56af9/family/a1e49ce455894f0ba4a51b0141e0c2f2
2014-07-01 07:39:21,804 INFO  [MemStoreFlusher.0] regionserver.HStore: Added hdfs://master:54310/hbase/data/default/usertable/07064f253c865a7cc0c08893a4f56af9/family/a1e49ce455894f0ba4a51b0141e0c2f2, entries=413130, sequenceid=597, filesize=29.5m
2014-07-01 07:39:21,804 INFO  [MemStoreFlusher.0] regionserver.HRegion: Finished memstore flush of ~267.7m/280674800, currentsize=39.9m/41801840 for region usertable,user2,1404225423342.07064f253c865a7cc0c08893a4f56af9. in 6652ms, sequenceid=597, compaction requested=true
2014-07-01 07:39:21,807 DEBUG [MemStoreFlusher.0] regionserver.CompactSplitThread: Small Compaction requested: system; Because: MemStoreFlusher.0; compaction_queue=(0:0), split_queue=0, merge_queue=0
2014-07-01 07:39:21,807 DEBUG [MemStoreFlusher.0] regionserver.HRegion: Started memstore flush for usertable,user6,1404225423342.89cb00f0096b305c3317b7753d66adf1., current region memstore size 260.2m
2014-07-01 07:39:21,808 DEBUG [regionserver60020-smallCompactions-1404225561807] compactions.RatioBasedCompactionPolicy: Selecting compaction from 3 store files, 0 compacting, 3 eligible, 20 blocking
2014-07-01 07:39:21,810 DEBUG [regionserver60020-smallCompactions-1404225561807] compactions.ExploringCompactionPolicy: Exploring compaction algorithm has selected 3 files of size 70767386 starting at candidate #0 after considering 1 permutations with 1 in ratio
2014-07-01 07:39:21,813 DEBUG [regionserver60020-smallCompactions-1404225561807] regionserver.HStore: 07064f253c865a7cc0c08893a4f56af9 - family: Initiating major compaction
2014-07-01 07:39:21,813 INFO  [regionserver60020-smallCompactions-1404225561807] regionserver.HRegion: Starting compaction on family in region usertable,user2,1404225423342.07064f253c865a7cc0c08893a4f56af9.
2014-07-01 07:39:21,814 INFO  [regionserver60020-smallCompactions-1404225561807] regionserver.HStore: Starting compaction of 3 file(s) in family of usertable,user2,1404225423342.07064f253c865a7cc0c08893a4f56af9. into tmpdir=hdfs://master:54310/hbase/data/default/usertable/07064f253c865a7cc0c08893a4f56af9/.tmp, totalSize=67.5m
2014-07-01 07:39:21,814 DEBUG [regionserver60020-smallCompactions-1404225561807] compactions.Compactor: Compacting hdfs://master:54310/hbase/data/default/usertable/07064f253c865a7cc0c08893a4f56af9/family/7a635f0ec6ec42c4ac52c602545f5ea2, keycount=27804, bloomtype=ROW, size=19.8m, encoding=NONE, seqNum=201, earliestPutTs=1404225440261
2014-07-01 07:39:21,815 DEBUG [regionserver60020-smallCompactions-1404225561807] compactions.Compactor: Compacting hdfs://master:54310/hbase/data/default/usertable/07064f253c865a7cc0c08893a4f56af9/family/2eef1e7e35f74b42af169c821224976c, keycount=25596, bloomtype=ROW, size=18.2m, encoding=NONE, seqNum=414, earliestPutTs=1404225525304
2014-07-01 07:39:21,815 DEBUG [regionserver60020-smallCompactions-1404225561807] compactions.Compactor: Compacting hdfs://master:54310/hbase/data/default/usertable/07064f253c865a7cc0c08893a4f56af9/family/a1e49ce455894f0ba4a51b0141e0c2f2, keycount=41313, bloomtype=ROW, size=29.5m, encoding=NONE, seqNum=597, earliestPutTs=1404225540770
2014-07-01 07:39:21,841 DEBUG [regionserver60020-smallCompactions-1404225561807] util.FSUtils: DFS Client does not support most favored nodes create; using default create
2014-07-01 07:39:23,376 DEBUG [MemStoreFlusher.0] util.FSUtils: DFS Client does not support most favored nodes create; using default create
2014-07-01 07:39:23,388 INFO  [MemStoreFlusher.0] compress.CodecPool: Got brand-new compressor
2014-07-01 07:39:23,388 INFO  [MemStoreFlusher.0] compress.CodecPool: Got brand-new compressor
2014-07-01 07:39:24,352 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-01 07:39:24,542 INFO  [MemStoreFlusher.1] regionserver.DefaultStoreFlusher: Flushed, sequenceid=599, memsize=113.3m, hasBloomFilter=true, into tmp file hdfs://master:54310/hbase/data/default/usertable/c7604442b843793f8451582c14f6bf59/.tmp/6540eb6dbdd34ab2ba6bacde26613bd9
2014-07-01 07:39:24,546 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 2276 synced till here 2266
2014-07-01 07:39:24,576 DEBUG [MemStoreFlusher.1] regionserver.HRegionFileSystem: Committing store file hdfs://master:54310/hbase/data/default/usertable/c7604442b843793f8451582c14f6bf59/.tmp/6540eb6dbdd34ab2ba6bacde26613bd9 as hdfs://master:54310/hbase/data/default/usertable/c7604442b843793f8451582c14f6bf59/family/6540eb6dbdd34ab2ba6bacde26613bd9
2014-07-01 07:39:24,602 INFO  [MemStoreFlusher.1] regionserver.HStore: Added hdfs://master:54310/hbase/data/default/usertable/c7604442b843793f8451582c14f6bf59/family/6540eb6dbdd34ab2ba6bacde26613bd9, entries=412360, sequenceid=599, filesize=29.4m
2014-07-01 07:39:24,603 INFO  [MemStoreFlusher.1] regionserver.HRegion: Finished memstore flush of ~269.9m/283038960, currentsize=69.8m/73144160 for region usertable,user5,1404225423342.c7604442b843793f8451582c14f6bf59. in 8723ms, sequenceid=599, compaction requested=true
2014-07-01 07:39:24,605 DEBUG [MemStoreFlusher.1] regionserver.CompactSplitThread: Small Compaction requested: system; Because: MemStoreFlusher.1; compaction_queue=(0:1), split_queue=0, merge_queue=0
2014-07-01 07:39:24,764 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1404225098235/slave1%2C60020%2C1404225098235.1404225559272 with entries=98, filesize=77.3m; new WAL /hbase/WALs/slave1,60020,1404225098235/slave1%2C60020%2C1404225098235.1404225564352
2014-07-01 07:39:26,619 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-01 07:39:26,687 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 2351 synced till here 2350
2014-07-01 07:39:26,710 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1404225098235/slave1%2C60020%2C1404225098235.1404225564352 with entries=75, filesize=62.8m; new WAL /hbase/WALs/slave1,60020,1404225098235/slave1%2C60020%2C1404225098235.1404225566620
2014-07-01 07:39:28,832 INFO  [MemStoreFlusher.0] regionserver.DefaultStoreFlusher: Flushed, sequenceid=616, memsize=119.3m, hasBloomFilter=true, into tmp file hdfs://master:54310/hbase/data/default/usertable/89cb00f0096b305c3317b7753d66adf1/.tmp/0b6d370f5853456c80fb9b04b428b36a
2014-07-01 07:39:28,855 INFO  [regionserver60020-smallCompactions-1404225561807] compress.CodecPool: Got brand-new decompressor
2014-07-01 07:39:28,859 DEBUG [MemStoreFlusher.0] regionserver.HRegionFileSystem: Committing store file hdfs://master:54310/hbase/data/default/usertable/89cb00f0096b305c3317b7753d66adf1/.tmp/0b6d370f5853456c80fb9b04b428b36a as hdfs://master:54310/hbase/data/default/usertable/89cb00f0096b305c3317b7753d66adf1/family/0b6d370f5853456c80fb9b04b428b36a
2014-07-01 07:39:28,883 INFO  [MemStoreFlusher.0] regionserver.HStore: Added hdfs://master:54310/hbase/data/default/usertable/89cb00f0096b305c3317b7753d66adf1/family/0b6d370f5853456c80fb9b04b428b36a, entries=434490, sequenceid=616, filesize=31.0m
2014-07-01 07:39:28,884 INFO  [MemStoreFlusher.0] regionserver.HRegion: Finished memstore flush of ~273.3m/286567280, currentsize=92.5m/96959120 for region usertable,user6,1404225423342.89cb00f0096b305c3317b7753d66adf1. in 7077ms, sequenceid=616, compaction requested=true
2014-07-01 07:39:28,884 DEBUG [MemStoreFlusher.0] regionserver.CompactSplitThread: Small Compaction requested: system; Because: MemStoreFlusher.0; compaction_queue=(0:2), split_queue=0, merge_queue=0
2014-07-01 07:39:30,228 DEBUG [regionserver60020-smallCompactions-1404225561807] regionserver.HRegionFileSystem: Committing store file hdfs://master:54310/hbase/data/default/usertable/07064f253c865a7cc0c08893a4f56af9/.tmp/7ba6631180da4242a995895cc6aa53fc as hdfs://master:54310/hbase/data/default/usertable/07064f253c865a7cc0c08893a4f56af9/family/7ba6631180da4242a995895cc6aa53fc
2014-07-01 07:39:30,860 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-01 07:39:31,259 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 2431 synced till here 2428
2014-07-01 07:39:31,545 DEBUG [regionserver60020-smallCompactions-1404225561807] regionserver.HStore: Removing store files after compaction...
2014-07-01 07:39:31,563 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1404225098235/slave1%2C60020%2C1404225098235.1404225566620 with entries=80, filesize=64.1m; new WAL /hbase/WALs/slave1,60020,1404225098235/slave1%2C60020%2C1404225098235.1404225570861
2014-07-01 07:39:31,583 DEBUG [regionserver60020-smallCompactions-1404225561807] backup.HFileArchiver: Finished archiving from class org.apache.hadoop.hbase.backup.HFileArchiver$FileableStoreFile, file:hdfs://master:54310/hbase/data/default/usertable/07064f253c865a7cc0c08893a4f56af9/family/7a635f0ec6ec42c4ac52c602545f5ea2, to hdfs://master:54310/hbase/archive/data/default/usertable/07064f253c865a7cc0c08893a4f56af9/family/7a635f0ec6ec42c4ac52c602545f5ea2
2014-07-01 07:39:31,620 DEBUG [regionserver60020-smallCompactions-1404225561807] backup.HFileArchiver: Finished archiving from class org.apache.hadoop.hbase.backup.HFileArchiver$FileableStoreFile, file:hdfs://master:54310/hbase/data/default/usertable/07064f253c865a7cc0c08893a4f56af9/family/2eef1e7e35f74b42af169c821224976c, to hdfs://master:54310/hbase/archive/data/default/usertable/07064f253c865a7cc0c08893a4f56af9/family/2eef1e7e35f74b42af169c821224976c
2014-07-01 07:39:31,640 DEBUG [regionserver60020-smallCompactions-1404225561807] backup.HFileArchiver: Finished archiving from class org.apache.hadoop.hbase.backup.HFileArchiver$FileableStoreFile, file:hdfs://master:54310/hbase/data/default/usertable/07064f253c865a7cc0c08893a4f56af9/family/a1e49ce455894f0ba4a51b0141e0c2f2, to hdfs://master:54310/hbase/archive/data/default/usertable/07064f253c865a7cc0c08893a4f56af9/family/a1e49ce455894f0ba4a51b0141e0c2f2
2014-07-01 07:39:31,640 INFO  [regionserver60020-smallCompactions-1404225561807] regionserver.HStore: Completed major compaction of 3 file(s) in family of usertable,user2,1404225423342.07064f253c865a7cc0c08893a4f56af9. into 7ba6631180da4242a995895cc6aa53fc(size=31.0m), total size for store is 31.0m. This selection was in queue for 0sec, and took 9sec to execute.
2014-07-01 07:39:31,642 INFO  [regionserver60020-smallCompactions-1404225561807] regionserver.CompactSplitThread: Completed compaction: Request = regionName=usertable,user2,1404225423342.07064f253c865a7cc0c08893a4f56af9., storeName=family, fileCount=3, fileSize=67.5m, priority=17, time=45673202115157; duration=9sec
2014-07-01 07:39:31,642 DEBUG [regionserver60020-smallCompactions-1404225561807] regionserver.CompactSplitThread: CompactSplitThread Status: compaction_queue=(0:2), split_queue=0, merge_queue=0
2014-07-01 07:39:31,643 DEBUG [regionserver60020-smallCompactions-1404225561807] compactions.RatioBasedCompactionPolicy: Selecting compaction from 3 store files, 0 compacting, 3 eligible, 20 blocking
2014-07-01 07:39:31,643 DEBUG [regionserver60020-smallCompactions-1404225561807] compactions.ExploringCompactionPolicy: Exploring compaction algorithm has selected 3 files of size 70591017 starting at candidate #0 after considering 1 permutations with 1 in ratio
2014-07-01 07:39:31,643 DEBUG [regionserver60020-smallCompactions-1404225561807] regionserver.HStore: c7604442b843793f8451582c14f6bf59 - family: Initiating major compaction
2014-07-01 07:39:31,643 INFO  [regionserver60020-smallCompactions-1404225561807] regionserver.HRegion: Starting compaction on family in region usertable,user5,1404225423342.c7604442b843793f8451582c14f6bf59.
2014-07-01 07:39:31,644 INFO  [regionserver60020-smallCompactions-1404225561807] regionserver.HStore: Starting compaction of 3 file(s) in family of usertable,user5,1404225423342.c7604442b843793f8451582c14f6bf59. into tmpdir=hdfs://master:54310/hbase/data/default/usertable/c7604442b843793f8451582c14f6bf59/.tmp, totalSize=67.3m
2014-07-01 07:39:31,644 DEBUG [regionserver60020-smallCompactions-1404225561807] compactions.Compactor: Compacting hdfs://master:54310/hbase/data/default/usertable/c7604442b843793f8451582c14f6bf59/family/fb647c51cb654ed29d46bd4320090bd4, keycount=27771, bloomtype=ROW, size=19.8m, encoding=NONE, seqNum=198, earliestPutTs=1404225440608
2014-07-01 07:39:31,644 DEBUG [regionserver60020-smallCompactions-1404225561807] compactions.Compactor: Compacting hdfs://master:54310/hbase/data/default/usertable/c7604442b843793f8451582c14f6bf59/family/f2cce55c95e5461ea8942a2ed282b7f5, keycount=25446, bloomtype=ROW, size=18.1m, encoding=NONE, seqNum=414, earliestPutTs=1404225526466
2014-07-01 07:39:31,644 DEBUG [regionserver60020-smallCompactions-1404225561807] compactions.Compactor: Compacting hdfs://master:54310/hbase/data/default/usertable/c7604442b843793f8451582c14f6bf59/family/6540eb6dbdd34ab2ba6bacde26613bd9, keycount=41236, bloomtype=ROW, size=29.4m, encoding=NONE, seqNum=599, earliestPutTs=1404225540977
2014-07-01 07:39:31,665 DEBUG [regionserver60020-smallCompactions-1404225561807] util.FSUtils: DFS Client does not support most favored nodes create; using default create
2014-07-01 07:39:33,276 DEBUG [RpcServer.handler=20,port=60020] regionserver.HRegion: Flush requested on usertable,,1404225423341.387a171caad6315f0b61271a30313d1a.
2014-07-01 07:39:33,277 DEBUG [MemStoreFlusher.1] regionserver.HRegion: Started memstore flush for usertable,,1404225423341.387a171caad6315f0b61271a30313d1a., current region memstore size 256.1m
2014-07-01 07:39:33,722 DEBUG [MemStoreFlusher.1] util.FSUtils: DFS Client does not support most favored nodes create; using default create
2014-07-01 07:39:38,557 DEBUG [regionserver60020-smallCompactions-1404225561807] regionserver.HRegionFileSystem: Committing store file hdfs://master:54310/hbase/data/default/usertable/c7604442b843793f8451582c14f6bf59/.tmp/d8b5a24bf10c49bb869641dd093f02a2 as hdfs://master:54310/hbase/data/default/usertable/c7604442b843793f8451582c14f6bf59/family/d8b5a24bf10c49bb869641dd093f02a2
2014-07-01 07:39:38,618 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-01 07:39:38,800 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 2525 synced till here 2522
2014-07-01 07:39:38,906 DEBUG [regionserver60020-smallCompactions-1404225561807] regionserver.HStore: Removing store files after compaction...
2014-07-01 07:39:38,949 DEBUG [regionserver60020-smallCompactions-1404225561807] backup.HFileArchiver: Finished archiving from class org.apache.hadoop.hbase.backup.HFileArchiver$FileableStoreFile, file:hdfs://master:54310/hbase/data/default/usertable/c7604442b843793f8451582c14f6bf59/family/fb647c51cb654ed29d46bd4320090bd4, to hdfs://master:54310/hbase/archive/data/default/usertable/c7604442b843793f8451582c14f6bf59/family/fb647c51cb654ed29d46bd4320090bd4
2014-07-01 07:39:38,957 DEBUG [regionserver60020-smallCompactions-1404225561807] backup.HFileArchiver: Finished archiving from class org.apache.hadoop.hbase.backup.HFileArchiver$FileableStoreFile, file:hdfs://master:54310/hbase/data/default/usertable/c7604442b843793f8451582c14f6bf59/family/f2cce55c95e5461ea8942a2ed282b7f5, to hdfs://master:54310/hbase/archive/data/default/usertable/c7604442b843793f8451582c14f6bf59/family/f2cce55c95e5461ea8942a2ed282b7f5
2014-07-01 07:39:38,962 DEBUG [regionserver60020-smallCompactions-1404225561807] backup.HFileArchiver: Finished archiving from class org.apache.hadoop.hbase.backup.HFileArchiver$FileableStoreFile, file:hdfs://master:54310/hbase/data/default/usertable/c7604442b843793f8451582c14f6bf59/family/6540eb6dbdd34ab2ba6bacde26613bd9, to hdfs://master:54310/hbase/archive/data/default/usertable/c7604442b843793f8451582c14f6bf59/family/6540eb6dbdd34ab2ba6bacde26613bd9
2014-07-01 07:39:38,962 INFO  [regionserver60020-smallCompactions-1404225561807] regionserver.HStore: Completed major compaction of 3 file(s) in family of usertable,user5,1404225423342.c7604442b843793f8451582c14f6bf59. into d8b5a24bf10c49bb869641dd093f02a2(size=31.1m), total size for store is 31.1m. This selection was in queue for 0sec, and took 7sec to execute.
2014-07-01 07:39:38,963 INFO  [regionserver60020-smallCompactions-1404225561807] regionserver.CompactSplitThread: Completed compaction: Request = regionName=usertable,user5,1404225423342.c7604442b843793f8451582c14f6bf59., storeName=family, fileCount=3, fileSize=67.3m, priority=17, time=45683032082538; duration=7sec
2014-07-01 07:39:38,963 DEBUG [regionserver60020-smallCompactions-1404225561807] regionserver.CompactSplitThread: CompactSplitThread Status: compaction_queue=(0:1), split_queue=0, merge_queue=0
2014-07-01 07:39:38,963 DEBUG [regionserver60020-smallCompactions-1404225561807] compactions.RatioBasedCompactionPolicy: Selecting compaction from 3 store files, 0 compacting, 3 eligible, 20 blocking
2014-07-01 07:39:38,963 DEBUG [regionserver60020-smallCompactions-1404225561807] compactions.ExploringCompactionPolicy: Exploring compaction algorithm has selected 3 files of size 72206480 starting at candidate #0 after considering 1 permutations with 1 in ratio
2014-07-01 07:39:38,964 DEBUG [regionserver60020-smallCompactions-1404225561807] regionserver.HStore: 89cb00f0096b305c3317b7753d66adf1 - family: Initiating major compaction
2014-07-01 07:39:38,964 INFO  [regionserver60020-smallCompactions-1404225561807] regionserver.HRegion: Starting compaction on family in region usertable,user6,1404225423342.89cb00f0096b305c3317b7753d66adf1.
2014-07-01 07:39:38,965 INFO  [regionserver60020-smallCompactions-1404225561807] regionserver.HStore: Starting compaction of 3 file(s) in family of usertable,user6,1404225423342.89cb00f0096b305c3317b7753d66adf1. into tmpdir=hdfs://master:54310/hbase/data/default/usertable/89cb00f0096b305c3317b7753d66adf1/.tmp, totalSize=68.9m
2014-07-01 07:39:38,965 DEBUG [regionserver60020-smallCompactions-1404225561807] compactions.Compactor: Compacting hdfs://master:54310/hbase/data/default/usertable/89cb00f0096b305c3317b7753d66adf1/family/d00692167bd84170be8548fef003d734, keycount=27754, bloomtype=ROW, size=19.8m, encoding=NONE, seqNum=224, earliestPutTs=1404225441356
2014-07-01 07:39:38,965 DEBUG [regionserver60020-smallCompactions-1404225561807] compactions.Compactor: Compacting hdfs://master:54310/hbase/data/default/usertable/89cb00f0096b305c3317b7753d66adf1/family/e41790abfd37484abb4e97a2b2c8865e, keycount=25440, bloomtype=ROW, size=18.1m, encoding=NONE, seqNum=429, earliestPutTs=1404225528877
2014-07-01 07:39:38,965 DEBUG [regionserver60020-smallCompactions-1404225561807] compactions.Compactor: Compacting hdfs://master:54310/hbase/data/default/usertable/89cb00f0096b305c3317b7753d66adf1/family/0b6d370f5853456c80fb9b04b428b36a, keycount=43449, bloomtype=ROW, size=31.0m, encoding=NONE, seqNum=616, earliestPutTs=1404225541401
2014-07-01 07:39:38,987 DEBUG [regionserver60020-smallCompactions-1404225561807] util.FSUtils: DFS Client does not support most favored nodes create; using default create
2014-07-01 07:39:38,987 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1404225098235/slave1%2C60020%2C1404225098235.1404225570861 with entries=94, filesize=63.8m; new WAL /hbase/WALs/slave1,60020,1404225098235/slave1%2C60020%2C1404225098235.1404225578618
2014-07-01 07:39:39,208 INFO  [MemStoreFlusher.1] regionserver.DefaultStoreFlusher: Flushed, sequenceid=408, memsize=147.2m, hasBloomFilter=true, into tmp file hdfs://master:54310/hbase/data/default/usertable/387a171caad6315f0b61271a30313d1a/.tmp/6ecfc2e886e0416f84ea90379e9b2384
2014-07-01 07:39:39,263 DEBUG [MemStoreFlusher.1] regionserver.HRegionFileSystem: Committing store file hdfs://master:54310/hbase/data/default/usertable/387a171caad6315f0b61271a30313d1a/.tmp/6ecfc2e886e0416f84ea90379e9b2384 as hdfs://master:54310/hbase/data/default/usertable/387a171caad6315f0b61271a30313d1a/family/6ecfc2e886e0416f84ea90379e9b2384
2014-07-01 07:39:39,296 INFO  [MemStoreFlusher.1] regionserver.HStore: Added hdfs://master:54310/hbase/data/default/usertable/387a171caad6315f0b61271a30313d1a/family/6ecfc2e886e0416f84ea90379e9b2384, entries=536030, sequenceid=408, filesize=38.2m
2014-07-01 07:39:39,296 INFO  [MemStoreFlusher.1] regionserver.HRegion: Finished memstore flush of ~259.2m/271826080, currentsize=32.3m/33876240 for region usertable,,1404225423341.387a171caad6315f0b61271a30313d1a. in 6019ms, sequenceid=408, compaction requested=false
2014-07-01 07:39:43,604 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-01 07:39:43,648 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 2622 synced till here 2607
2014-07-01 07:39:43,886 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1404225098235/slave1%2C60020%2C1404225098235.1404225578618 with entries=97, filesize=71.0m; new WAL /hbase/WALs/slave1,60020,1404225098235/slave1%2C60020%2C1404225098235.1404225583604
2014-07-01 07:39:43,987 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1404225098235/slave1%2C60020%2C1404225098235.1404225526075
2014-07-01 07:39:43,987 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1404225098235/slave1%2C60020%2C1404225098235.1404225528685
2014-07-01 07:39:43,987 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1404225098235/slave1%2C60020%2C1404225098235.1404225531451
2014-07-01 07:39:43,987 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1404225098235/slave1%2C60020%2C1404225098235.1404225535303
2014-07-01 07:39:43,987 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1404225098235/slave1%2C60020%2C1404225098235.1404225538325
2014-07-01 07:39:43,987 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1404225098235/slave1%2C60020%2C1404225098235.1404225540090
2014-07-01 07:39:43,987 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1404225098235/slave1%2C60020%2C1404225098235.1404225542681
2014-07-01 07:39:43,987 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1404225098235/slave1%2C60020%2C1404225098235.1404225545632
2014-07-01 07:39:43,988 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1404225098235/slave1%2C60020%2C1404225098235.1404225549864
2014-07-01 07:39:45,911 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-01 07:39:46,049 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 2712 synced till here 2703
2014-07-01 07:39:46,252 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1404225098235/slave1%2C60020%2C1404225098235.1404225583604 with entries=90, filesize=66.4m; new WAL /hbase/WALs/slave1,60020,1404225098235/slave1%2C60020%2C1404225098235.1404225585912
2014-07-01 07:39:47,698 DEBUG [regionserver60020-smallCompactions-1404225561807] regionserver.HRegionFileSystem: Committing store file hdfs://master:54310/hbase/data/default/usertable/89cb00f0096b305c3317b7753d66adf1/.tmp/aaa762fa710f487890b087b543a2e26f as hdfs://master:54310/hbase/data/default/usertable/89cb00f0096b305c3317b7753d66adf1/family/aaa762fa710f487890b087b543a2e26f
2014-07-01 07:39:47,934 DEBUG [regionserver60020-smallCompactions-1404225561807] regionserver.HStore: Removing store files after compaction...
2014-07-01 07:39:47,966 DEBUG [regionserver60020-smallCompactions-1404225561807] backup.HFileArchiver: Finished archiving from class org.apache.hadoop.hbase.backup.HFileArchiver$FileableStoreFile, file:hdfs://master:54310/hbase/data/default/usertable/89cb00f0096b305c3317b7753d66adf1/family/d00692167bd84170be8548fef003d734, to hdfs://master:54310/hbase/archive/data/default/usertable/89cb00f0096b305c3317b7753d66adf1/family/d00692167bd84170be8548fef003d734
2014-07-01 07:39:47,973 DEBUG [regionserver60020-smallCompactions-1404225561807] backup.HFileArchiver: Finished archiving from class org.apache.hadoop.hbase.backup.HFileArchiver$FileableStoreFile, file:hdfs://master:54310/hbase/data/default/usertable/89cb00f0096b305c3317b7753d66adf1/family/e41790abfd37484abb4e97a2b2c8865e, to hdfs://master:54310/hbase/archive/data/default/usertable/89cb00f0096b305c3317b7753d66adf1/family/e41790abfd37484abb4e97a2b2c8865e
2014-07-01 07:39:48,112 DEBUG [regionserver60020-smallCompactions-1404225561807] backup.HFileArchiver: Finished archiving from class org.apache.hadoop.hbase.backup.HFileArchiver$FileableStoreFile, file:hdfs://master:54310/hbase/data/default/usertable/89cb00f0096b305c3317b7753d66adf1/family/0b6d370f5853456c80fb9b04b428b36a, to hdfs://master:54310/hbase/archive/data/default/usertable/89cb00f0096b305c3317b7753d66adf1/family/0b6d370f5853456c80fb9b04b428b36a
2014-07-01 07:39:48,112 INFO  [regionserver60020-smallCompactions-1404225561807] regionserver.HStore: Completed major compaction of 3 file(s) in family of usertable,user6,1404225423342.89cb00f0096b305c3317b7753d66adf1. into aaa762fa710f487890b087b543a2e26f(size=32.6m), total size for store is 32.6m. This selection was in queue for 0sec, and took 9sec to execute.
2014-07-01 07:39:48,112 INFO  [regionserver60020-smallCompactions-1404225561807] regionserver.CompactSplitThread: Completed compaction: Request = regionName=usertable,user6,1404225423342.89cb00f0096b305c3317b7753d66adf1., storeName=family, fileCount=3, fileSize=68.9m, priority=17, time=45690352933746; duration=9sec
2014-07-01 07:39:48,113 DEBUG [regionserver60020-smallCompactions-1404225561807] regionserver.CompactSplitThread: CompactSplitThread Status: compaction_queue=(0:0), split_queue=0, merge_queue=0
2014-07-01 07:39:48,482 DEBUG [RpcServer.handler=18,port=60020] regionserver.HRegion: Flush requested on usertable,user2,1404225423342.07064f253c865a7cc0c08893a4f56af9.
2014-07-01 07:39:48,483 DEBUG [MemStoreFlusher.0] regionserver.HRegion: Started memstore flush for usertable,user2,1404225423342.07064f253c865a7cc0c08893a4f56af9., current region memstore size 259.4m
2014-07-01 07:39:48,710 DEBUG [MemStoreFlusher.0] util.FSUtils: DFS Client does not support most favored nodes create; using default create
2014-07-01 07:39:48,738 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-01 07:39:48,758 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 2824 synced till here 2818
2014-07-01 07:39:48,789 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1404225098235/slave1%2C60020%2C1404225098235.1404225585912 with entries=112, filesize=68.3m; new WAL /hbase/WALs/slave1,60020,1404225098235/slave1%2C60020%2C1404225098235.1404225588738
2014-07-01 07:39:49,076 DEBUG [RpcServer.handler=14,port=60020] regionserver.HRegion: Flush requested on usertable,user5,1404225423342.c7604442b843793f8451582c14f6bf59.
2014-07-01 07:39:49,076 DEBUG [MemStoreFlusher.1] regionserver.HRegion: Started memstore flush for usertable,user5,1404225423342.c7604442b843793f8451582c14f6bf59., current region memstore size 257.7m
2014-07-01 07:39:49,353 DEBUG [MemStoreFlusher.1] util.FSUtils: DFS Client does not support most favored nodes create; using default create
2014-07-01 07:39:53,938 INFO  [MemStoreFlusher.0] regionserver.DefaultStoreFlusher: Flushed, sequenceid=762, memsize=151.8m, hasBloomFilter=true, into tmp file hdfs://master:54310/hbase/data/default/usertable/07064f253c865a7cc0c08893a4f56af9/.tmp/d222f4744f3d4abfb109179e0644f471
2014-07-01 07:39:54,093 DEBUG [MemStoreFlusher.0] regionserver.HRegionFileSystem: Committing store file hdfs://master:54310/hbase/data/default/usertable/07064f253c865a7cc0c08893a4f56af9/.tmp/d222f4744f3d4abfb109179e0644f471 as hdfs://master:54310/hbase/data/default/usertable/07064f253c865a7cc0c08893a4f56af9/family/d222f4744f3d4abfb109179e0644f471
2014-07-01 07:39:54,246 INFO  [MemStoreFlusher.0] regionserver.HStore: Added hdfs://master:54310/hbase/data/default/usertable/07064f253c865a7cc0c08893a4f56af9/family/d222f4744f3d4abfb109179e0644f471, entries=552800, sequenceid=762, filesize=39.5m
2014-07-01 07:39:54,247 INFO  [MemStoreFlusher.0] regionserver.HRegion: Finished memstore flush of ~259.4m/272005040, currentsize=5.5m/5800240 for region usertable,user2,1404225423342.07064f253c865a7cc0c08893a4f56af9. in 5764ms, sequenceid=762, compaction requested=false
2014-07-01 07:39:54,728 INFO  [MemStoreFlusher.1] regionserver.DefaultStoreFlusher: Flushed, sequenceid=764, memsize=155.2m, hasBloomFilter=true, into tmp file hdfs://master:54310/hbase/data/default/usertable/c7604442b843793f8451582c14f6bf59/.tmp/1da9d56206ec4d8b93d5e57a06f6f5fb
2014-07-01 07:39:54,926 DEBUG [MemStoreFlusher.1] regionserver.HRegionFileSystem: Committing store file hdfs://master:54310/hbase/data/default/usertable/c7604442b843793f8451582c14f6bf59/.tmp/1da9d56206ec4d8b93d5e57a06f6f5fb as hdfs://master:54310/hbase/data/default/usertable/c7604442b843793f8451582c14f6bf59/family/1da9d56206ec4d8b93d5e57a06f6f5fb
2014-07-01 07:39:55,119 INFO  [MemStoreFlusher.1] regionserver.HStore: Added hdfs://master:54310/hbase/data/default/usertable/c7604442b843793f8451582c14f6bf59/family/1da9d56206ec4d8b93d5e57a06f6f5fb, entries=564970, sequenceid=764, filesize=40.3m
2014-07-01 07:39:55,120 INFO  [MemStoreFlusher.1] regionserver.HRegion: Finished memstore flush of ~259.4m/272051440, currentsize=1.6m/1690560 for region usertable,user5,1404225423342.c7604442b843793f8451582c14f6bf59. in 6044ms, sequenceid=764, compaction requested=false
2014-07-01 07:39:59,989 DEBUG [RpcServer.handler=19,port=60020] regionserver.HRegion: Flush requested on usertable,user6,1404225423342.89cb00f0096b305c3317b7753d66adf1.
2014-07-01 07:39:59,990 DEBUG [MemStoreFlusher.0] regionserver.HRegion: Started memstore flush for usertable,user6,1404225423342.89cb00f0096b305c3317b7753d66adf1., current region memstore size 257.6m
2014-07-01 07:40:00,219 DEBUG [MemStoreFlusher.0] util.FSUtils: DFS Client does not support most favored nodes create; using default create
2014-07-01 07:40:00,538 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-01 07:40:00,611 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 2923 synced till here 2920
2014-07-01 07:40:00,676 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1404225098235/slave1%2C60020%2C1404225098235.1404225588738 with entries=99, filesize=63.4m; new WAL /hbase/WALs/slave1,60020,1404225098235/slave1%2C60020%2C1404225098235.1404225600538
2014-07-01 07:40:00,676 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1404225098235/slave1%2C60020%2C1404225098235.1404225554242
2014-07-01 07:40:06,879 INFO  [MemStoreFlusher.0] regionserver.DefaultStoreFlusher: Flushed, sequenceid=778, memsize=166.2m, hasBloomFilter=true, into tmp file hdfs://master:54310/hbase/data/default/usertable/89cb00f0096b305c3317b7753d66adf1/.tmp/79f30d7a60d941c18054ddab01f26c6c
2014-07-01 07:40:06,928 DEBUG [MemStoreFlusher.0] regionserver.HRegionFileSystem: Committing store file hdfs://master:54310/hbase/data/default/usertable/89cb00f0096b305c3317b7753d66adf1/.tmp/79f30d7a60d941c18054ddab01f26c6c as hdfs://master:54310/hbase/data/default/usertable/89cb00f0096b305c3317b7753d66adf1/family/79f30d7a60d941c18054ddab01f26c6c
2014-07-01 07:40:06,929 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-01 07:40:06,975 INFO  [MemStoreFlusher.0] regionserver.HStore: Added hdfs://master:54310/hbase/data/default/usertable/89cb00f0096b305c3317b7753d66adf1/family/79f30d7a60d941c18054ddab01f26c6c, entries=605080, sequenceid=778, filesize=43.2m
2014-07-01 07:40:06,975 INFO  [MemStoreFlusher.0] regionserver.HRegion: Finished memstore flush of ~257.6m/270115120, currentsize=34.9m/36630000 for region usertable,user6,1404225423342.89cb00f0096b305c3317b7753d66adf1. in 6985ms, sequenceid=778, compaction requested=false
2014-07-01 07:40:06,980 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 3022 synced till here 3016
2014-07-01 07:40:07,147 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1404225098235/slave1%2C60020%2C1404225098235.1404225600538 with entries=99, filesize=73.5m; new WAL /hbase/WALs/slave1,60020,1404225098235/slave1%2C60020%2C1404225098235.1404225606929
2014-07-01 07:40:07,147 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1404225098235/slave1%2C60020%2C1404225098235.1404225559272
2014-07-01 07:40:07,147 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1404225098235/slave1%2C60020%2C1404225098235.1404225564352
2014-07-01 07:40:07,147 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1404225098235/slave1%2C60020%2C1404225098235.1404225566620
2014-07-01 07:40:18,615 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-01 07:40:18,720 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 3129 synced till here 3124
2014-07-01 07:40:18,947 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1404225098235/slave1%2C60020%2C1404225098235.1404225606929 with entries=107, filesize=66.2m; new WAL /hbase/WALs/slave1,60020,1404225098235/slave1%2C60020%2C1404225098235.1404225618615
2014-07-01 07:40:22,283 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-01 07:40:22,347 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 3233 synced till here 3232
2014-07-01 07:40:22,363 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1404225098235/slave1%2C60020%2C1404225098235.1404225618615 with entries=104, filesize=63.1m; new WAL /hbase/WALs/slave1,60020,1404225098235/slave1%2C60020%2C1404225098235.1404225622283
2014-07-01 07:40:28,543 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-01 07:40:28,561 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 3339 synced till here 3334
2014-07-01 07:40:28,658 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1404225098235/slave1%2C60020%2C1404225098235.1404225622283 with entries=106, filesize=67.6m; new WAL /hbase/WALs/slave1,60020,1404225098235/slave1%2C60020%2C1404225098235.1404225628543
2014-07-01 07:40:32,469 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-01 07:40:32,818 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 3432 synced till here 3430
2014-07-01 07:40:33,045 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1404225098235/slave1%2C60020%2C1404225098235.1404225628543 with entries=93, filesize=65.4m; new WAL /hbase/WALs/slave1,60020,1404225098235/slave1%2C60020%2C1404225098235.1404225632469
2014-07-01 07:40:33,345 DEBUG [RpcServer.handler=4,port=60020] regionserver.HRegion: Flush requested on usertable,,1404225423341.387a171caad6315f0b61271a30313d1a.
2014-07-01 07:40:33,346 DEBUG [MemStoreFlusher.1] regionserver.HRegion: Started memstore flush for usertable,,1404225423341.387a171caad6315f0b61271a30313d1a., current region memstore size 258.8m
2014-07-01 07:40:33,619 DEBUG [MemStoreFlusher.1] util.FSUtils: DFS Client does not support most favored nodes create; using default create
2014-07-01 07:40:35,234 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-01 07:40:35,336 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1404225098235/slave1%2C60020%2C1404225098235.1404225632469 with entries=105, filesize=62.7m; new WAL /hbase/WALs/slave1,60020,1404225098235/slave1%2C60020%2C1404225098235.1404225635235
2014-07-01 07:40:42,711 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-01 07:40:42,966 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 3645 synced till here 3641
2014-07-01 07:40:42,984 DEBUG [RpcServer.handler=42,port=60020] regionserver.HRegion: Flush requested on usertable,user2,1404225423342.07064f253c865a7cc0c08893a4f56af9.
2014-07-01 07:40:42,985 DEBUG [MemStoreFlusher.0] regionserver.HRegion: Started memstore flush for usertable,user2,1404225423342.07064f253c865a7cc0c08893a4f56af9., current region memstore size 257.2m
2014-07-01 07:40:43,060 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1404225098235/slave1%2C60020%2C1404225098235.1404225635235 with entries=108, filesize=67.1m; new WAL /hbase/WALs/slave1,60020,1404225098235/slave1%2C60020%2C1404225098235.1404225642711
2014-07-01 07:40:43,098 INFO  [MemStoreFlusher.1] regionserver.DefaultStoreFlusher: Flushed, sequenceid=836, memsize=264.0m, hasBloomFilter=true, into tmp file hdfs://master:54310/hbase/data/default/usertable/387a171caad6315f0b61271a30313d1a/.tmp/02e1b259284f4ce487467740287b78e6
2014-07-01 07:40:43,174 DEBUG [MemStoreFlusher.1] regionserver.HRegionFileSystem: Committing store file hdfs://master:54310/hbase/data/default/usertable/387a171caad6315f0b61271a30313d1a/.tmp/02e1b259284f4ce487467740287b78e6 as hdfs://master:54310/hbase/data/default/usertable/387a171caad6315f0b61271a30313d1a/family/02e1b259284f4ce487467740287b78e6
2014-07-01 07:40:43,193 INFO  [MemStoreFlusher.1] regionserver.HStore: Added hdfs://master:54310/hbase/data/default/usertable/387a171caad6315f0b61271a30313d1a/family/02e1b259284f4ce487467740287b78e6, entries=961170, sequenceid=836, filesize=68.5m
2014-07-01 07:40:43,193 INFO  [MemStoreFlusher.1] regionserver.HRegion: Finished memstore flush of ~264.0m/276813120, currentsize=41.7m/43696960 for region usertable,,1404225423341.387a171caad6315f0b61271a30313d1a. in 9847ms, sequenceid=836, compaction requested=true
2014-07-01 07:40:43,194 DEBUG [MemStoreFlusher.1] regionserver.CompactSplitThread: Small Compaction requested: system; Because: MemStoreFlusher.1; compaction_queue=(0:0), split_queue=0, merge_queue=0
2014-07-01 07:40:43,194 DEBUG [regionserver60020-smallCompactions-1404225561807] compactions.RatioBasedCompactionPolicy: Selecting compaction from 3 store files, 0 compacting, 3 eligible, 20 blocking
2014-07-01 07:40:43,195 DEBUG [regionserver60020-smallCompactions-1404225561807] compactions.ExploringCompactionPolicy: Exploring compaction algorithm has selected 3 files of size 122453616 starting at candidate #0 after considering 1 permutations with 1 in ratio
2014-07-01 07:40:43,195 DEBUG [regionserver60020-smallCompactions-1404225561807] regionserver.HStore: 387a171caad6315f0b61271a30313d1a - family: Initiating major compaction
2014-07-01 07:40:43,195 INFO  [regionserver60020-smallCompactions-1404225561807] regionserver.HRegion: Starting compaction on family in region usertable,,1404225423341.387a171caad6315f0b61271a30313d1a.
2014-07-01 07:40:43,195 INFO  [regionserver60020-smallCompactions-1404225561807] regionserver.HStore: Starting compaction of 3 file(s) in family of usertable,,1404225423341.387a171caad6315f0b61271a30313d1a. into tmpdir=hdfs://master:54310/hbase/data/default/usertable/387a171caad6315f0b61271a30313d1a/.tmp, totalSize=116.8m
2014-07-01 07:40:43,196 DEBUG [regionserver60020-smallCompactions-1404225561807] compactions.Compactor: Compacting hdfs://master:54310/hbase/data/default/usertable/387a171caad6315f0b61271a30313d1a/family/a65780cad89a42c28d8e2bd69979ed56, keycount=14116, bloomtype=ROW, size=10.1m, encoding=NONE, seqNum=176, earliestPutTs=1404225440622
2014-07-01 07:40:43,196 DEBUG [regionserver60020-smallCompactions-1404225561807] compactions.Compactor: Compacting hdfs://master:54310/hbase/data/default/usertable/387a171caad6315f0b61271a30313d1a/family/6ecfc2e886e0416f84ea90379e9b2384, keycount=53603, bloomtype=ROW, size=38.2m, encoding=NONE, seqNum=408, earliestPutTs=1404225541012
2014-07-01 07:40:43,196 DEBUG [regionserver60020-smallCompactions-1404225561807] compactions.Compactor: Compacting hdfs://master:54310/hbase/data/default/usertable/387a171caad6315f0b61271a30313d1a/family/02e1b259284f4ce487467740287b78e6, keycount=96117, bloomtype=ROW, size=68.5m, encoding=NONE, seqNum=836, earliestPutTs=1404225573930
2014-07-01 07:40:43,221 DEBUG [regionserver60020-smallCompactions-1404225561807] util.FSUtils: DFS Client does not support most favored nodes create; using default create
2014-07-01 07:40:43,559 DEBUG [RpcServer.handler=23,port=60020] regionserver.HRegion: Flush requested on usertable,user5,1404225423342.c7604442b843793f8451582c14f6bf59.
2014-07-01 07:40:43,560 DEBUG [MemStoreFlusher.1] regionserver.HRegion: Started memstore flush for usertable,user5,1404225423342.c7604442b843793f8451582c14f6bf59., current region memstore size 259.2m
2014-07-01 07:40:43,623 DEBUG [MemStoreFlusher.0] util.FSUtils: DFS Client does not support most favored nodes create; using default create
2014-07-01 07:40:43,755 DEBUG [MemStoreFlusher.1] util.FSUtils: DFS Client does not support most favored nodes create; using default create
2014-07-01 07:40:49,895 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-01 07:40:49,965 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 3757 synced till here 3751
2014-07-01 07:40:50,078 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1404225098235/slave1%2C60020%2C1404225098235.1404225642711 with entries=112, filesize=66.4m; new WAL /hbase/WALs/slave1,60020,1404225098235/slave1%2C60020%2C1404225098235.1404225649895
2014-07-01 07:40:50,078 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1404225098235/slave1%2C60020%2C1404225098235.1404225570861
2014-07-01 07:40:50,078 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1404225098235/slave1%2C60020%2C1404225098235.1404225578618
2014-07-01 07:40:50,078 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1404225098235/slave1%2C60020%2C1404225098235.1404225583604
2014-07-01 07:40:50,078 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1404225098235/slave1%2C60020%2C1404225098235.1404225585912
2014-07-01 07:40:50,173 DEBUG [RpcServer.handler=30,port=60020] regionserver.HRegion: Flush requested on usertable,user6,1404225423342.89cb00f0096b305c3317b7753d66adf1.
2014-07-01 07:40:52,687 INFO  [MemStoreFlusher.1] regionserver.DefaultStoreFlusher: Flushed, sequenceid=916, memsize=229.0m, hasBloomFilter=true, into tmp file hdfs://master:54310/hbase/data/default/usertable/c7604442b843793f8451582c14f6bf59/.tmp/5aaffbb600f846738f0f4be1e95ddad5
2014-07-01 07:40:52,691 INFO  [MemStoreFlusher.0] regionserver.DefaultStoreFlusher: Flushed, sequenceid=916, memsize=233.0m, hasBloomFilter=true, into tmp file hdfs://master:54310/hbase/data/default/usertable/07064f253c865a7cc0c08893a4f56af9/.tmp/6a285363987443a38e099f52ebb41ff0
2014-07-01 07:40:52,703 DEBUG [MemStoreFlusher.1] regionserver.HRegionFileSystem: Committing store file hdfs://master:54310/hbase/data/default/usertable/c7604442b843793f8451582c14f6bf59/.tmp/5aaffbb600f846738f0f4be1e95ddad5 as hdfs://master:54310/hbase/data/default/usertable/c7604442b843793f8451582c14f6bf59/family/5aaffbb600f846738f0f4be1e95ddad5
2014-07-01 07:40:52,706 DEBUG [MemStoreFlusher.0] regionserver.HRegionFileSystem: Committing store file hdfs://master:54310/hbase/data/default/usertable/07064f253c865a7cc0c08893a4f56af9/.tmp/6a285363987443a38e099f52ebb41ff0 as hdfs://master:54310/hbase/data/default/usertable/07064f253c865a7cc0c08893a4f56af9/family/6a285363987443a38e099f52ebb41ff0
2014-07-01 07:40:52,717 INFO  [MemStoreFlusher.1] regionserver.HStore: Added hdfs://master:54310/hbase/data/default/usertable/c7604442b843793f8451582c14f6bf59/family/5aaffbb600f846738f0f4be1e95ddad5, entries=833740, sequenceid=916, filesize=59.4m
2014-07-01 07:40:52,717 INFO  [MemStoreFlusher.1] regionserver.HRegion: Finished memstore flush of ~259.2m/271834000, currentsize=27.8m/29127360 for region usertable,user5,1404225423342.c7604442b843793f8451582c14f6bf59. in 9157ms, sequenceid=916, compaction requested=true
2014-07-01 07:40:52,717 DEBUG [MemStoreFlusher.1] regionserver.CompactSplitThread: Small Compaction requested: system; Because: MemStoreFlusher.1; compaction_queue=(0:1), split_queue=0, merge_queue=0
2014-07-01 07:40:52,718 DEBUG [MemStoreFlusher.1] regionserver.HRegion: Started memstore flush for usertable,user6,1404225423342.89cb00f0096b305c3317b7753d66adf1., current region memstore size 268.0m
2014-07-01 07:40:52,719 INFO  [MemStoreFlusher.0] regionserver.HStore: Added hdfs://master:54310/hbase/data/default/usertable/07064f253c865a7cc0c08893a4f56af9/family/6a285363987443a38e099f52ebb41ff0, entries=848360, sequenceid=916, filesize=60.5m
2014-07-01 07:40:52,719 INFO  [MemStoreFlusher.0] regionserver.HRegion: Finished memstore flush of ~264.3m/277179440, currentsize=27.8m/29199600 for region usertable,user2,1404225423342.07064f253c865a7cc0c08893a4f56af9. in 9734ms, sequenceid=916, compaction requested=true
2014-07-01 07:40:52,719 DEBUG [MemStoreFlusher.0] regionserver.CompactSplitThread: Small Compaction requested: system; Because: MemStoreFlusher.0; compaction_queue=(0:2), split_queue=0, merge_queue=0
2014-07-01 07:40:52,881 DEBUG [MemStoreFlusher.1] util.FSUtils: DFS Client does not support most favored nodes create; using default create
2014-07-01 07:40:55,936 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-01 07:40:56,046 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 3868 synced till here 3860
2014-07-01 07:40:56,272 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1404225098235/slave1%2C60020%2C1404225098235.1404225649895 with entries=111, filesize=67.0m; new WAL /hbase/WALs/slave1,60020,1404225098235/slave1%2C60020%2C1404225098235.1404225656019
2014-07-01 07:40:59,163 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-01 07:40:59,230 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 3981 synced till here 3971
2014-07-01 07:40:59,441 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1404225098235/slave1%2C60020%2C1404225098235.1404225656019 with entries=113, filesize=69.6m; new WAL /hbase/WALs/slave1,60020,1404225098235/slave1%2C60020%2C1404225098235.1404225659164
2014-07-01 07:41:02,945 DEBUG [regionserver60020-smallCompactions-1404225561807] regionserver.HRegionFileSystem: Committing store file hdfs://master:54310/hbase/data/default/usertable/387a171caad6315f0b61271a30313d1a/.tmp/53c3c3a764ba4f3ca0067f01c0ef8dd2 as hdfs://master:54310/hbase/data/default/usertable/387a171caad6315f0b61271a30313d1a/family/53c3c3a764ba4f3ca0067f01c0ef8dd2
2014-07-01 07:41:03,039 DEBUG [regionserver60020-smallCompactions-1404225561807] regionserver.HStore: Removing store files after compaction...
2014-07-01 07:41:03,050 DEBUG [regionserver60020-smallCompactions-1404225561807] backup.HFileArchiver: Finished archiving from class org.apache.hadoop.hbase.backup.HFileArchiver$FileableStoreFile, file:hdfs://master:54310/hbase/data/default/usertable/387a171caad6315f0b61271a30313d1a/family/a65780cad89a42c28d8e2bd69979ed56, to hdfs://master:54310/hbase/archive/data/default/usertable/387a171caad6315f0b61271a30313d1a/family/a65780cad89a42c28d8e2bd69979ed56
2014-07-01 07:41:03,068 DEBUG [regionserver60020-smallCompactions-1404225561807] backup.HFileArchiver: Finished archiving from class org.apache.hadoop.hbase.backup.HFileArchiver$FileableStoreFile, file:hdfs://master:54310/hbase/data/default/usertable/387a171caad6315f0b61271a30313d1a/family/6ecfc2e886e0416f84ea90379e9b2384, to hdfs://master:54310/hbase/archive/data/default/usertable/387a171caad6315f0b61271a30313d1a/family/6ecfc2e886e0416f84ea90379e9b2384
2014-07-01 07:41:03,083 DEBUG [regionserver60020-smallCompactions-1404225561807] backup.HFileArchiver: Finished archiving from class org.apache.hadoop.hbase.backup.HFileArchiver$FileableStoreFile, file:hdfs://master:54310/hbase/data/default/usertable/387a171caad6315f0b61271a30313d1a/family/02e1b259284f4ce487467740287b78e6, to hdfs://master:54310/hbase/archive/data/default/usertable/387a171caad6315f0b61271a30313d1a/family/02e1b259284f4ce487467740287b78e6
2014-07-01 07:41:03,084 INFO  [regionserver60020-smallCompactions-1404225561807] regionserver.HStore: Completed major compaction of 3 file(s) in family of usertable,,1404225423341.387a171caad6315f0b61271a30313d1a. into 53c3c3a764ba4f3ca0067f01c0ef8dd2(size=107.5m), total size for store is 107.5m. This selection was in queue for 0sec, and took 19sec to execute.
2014-07-01 07:41:03,084 INFO  [regionserver60020-smallCompactions-1404225561807] regionserver.CompactSplitThread: Completed compaction: Request = regionName=usertable,,1404225423341.387a171caad6315f0b61271a30313d1a., storeName=family, fileCount=3, fileSize=116.8m, priority=17, time=45754584109780; duration=19sec
2014-07-01 07:41:03,084 DEBUG [regionserver60020-smallCompactions-1404225561807] regionserver.CompactSplitThread: CompactSplitThread Status: compaction_queue=(0:2), split_queue=0, merge_queue=0
2014-07-01 07:41:03,084 DEBUG [regionserver60020-smallCompactions-1404225561807] compactions.RatioBasedCompactionPolicy: Selecting compaction from 3 store files, 0 compacting, 3 eligible, 20 blocking
2014-07-01 07:41:03,084 DEBUG [regionserver60020-smallCompactions-1404225561807] compactions.ExploringCompactionPolicy: Exploring compaction algorithm has selected 3 files of size 137162413 starting at candidate #0 after considering 1 permutations with 1 in ratio
2014-07-01 07:41:03,084 DEBUG [regionserver60020-smallCompactions-1404225561807] regionserver.HStore: c7604442b843793f8451582c14f6bf59 - family: Initiating major compaction
2014-07-01 07:41:03,085 INFO  [regionserver60020-smallCompactions-1404225561807] regionserver.HRegion: Starting compaction on family in region usertable,user5,1404225423342.c7604442b843793f8451582c14f6bf59.
2014-07-01 07:41:03,085 INFO  [regionserver60020-smallCompactions-1404225561807] regionserver.HStore: Starting compaction of 3 file(s) in family of usertable,user5,1404225423342.c7604442b843793f8451582c14f6bf59. into tmpdir=hdfs://master:54310/hbase/data/default/usertable/c7604442b843793f8451582c14f6bf59/.tmp, totalSize=130.8m
2014-07-01 07:41:03,085 DEBUG [regionserver60020-smallCompactions-1404225561807] compactions.Compactor: Compacting hdfs://master:54310/hbase/data/default/usertable/c7604442b843793f8451582c14f6bf59/family/d8b5a24bf10c49bb869641dd093f02a2, keycount=43561, bloomtype=ROW, size=31.1m, encoding=NONE, seqNum=599, earliestPutTs=1404225440608
2014-07-01 07:41:03,085 DEBUG [regionserver60020-smallCompactions-1404225561807] compactions.Compactor: Compacting hdfs://master:54310/hbase/data/default/usertable/c7604442b843793f8451582c14f6bf59/family/1da9d56206ec4d8b93d5e57a06f6f5fb, keycount=56497, bloomtype=ROW, size=40.3m, encoding=NONE, seqNum=764, earliestPutTs=1404225558038
2014-07-01 07:41:03,085 DEBUG [regionserver60020-smallCompactions-1404225561807] compactions.Compactor: Compacting hdfs://master:54310/hbase/data/default/usertable/c7604442b843793f8451582c14f6bf59/family/5aaffbb600f846738f0f4be1e95ddad5, keycount=83374, bloomtype=ROW, size=59.4m, encoding=NONE, seqNum=916, earliestPutTs=1404225595809
2014-07-01 07:41:03,105 DEBUG [regionserver60020-smallCompactions-1404225561807] util.FSUtils: DFS Client does not support most favored nodes create; using default create
2014-07-01 07:41:03,301 INFO  [MemStoreFlusher.1] regionserver.DefaultStoreFlusher: Flushed, sequenceid=935, memsize=239.1m, hasBloomFilter=true, into tmp file hdfs://master:54310/hbase/data/default/usertable/89cb00f0096b305c3317b7753d66adf1/.tmp/d342f65c4ae34d8ca1a93b4dae5733a5
2014-07-01 07:41:03,325 DEBUG [MemStoreFlusher.1] regionserver.HRegionFileSystem: Committing store file hdfs://master:54310/hbase/data/default/usertable/89cb00f0096b305c3317b7753d66adf1/.tmp/d342f65c4ae34d8ca1a93b4dae5733a5 as hdfs://master:54310/hbase/data/default/usertable/89cb00f0096b305c3317b7753d66adf1/family/d342f65c4ae34d8ca1a93b4dae5733a5
2014-07-01 07:41:03,380 INFO  [MemStoreFlusher.1] regionserver.HStore: Added hdfs://master:54310/hbase/data/default/usertable/89cb00f0096b305c3317b7753d66adf1/family/d342f65c4ae34d8ca1a93b4dae5733a5, entries=870420, sequenceid=935, filesize=62.0m
2014-07-01 07:41:03,381 INFO  [MemStoreFlusher.1] regionserver.HRegion: Finished memstore flush of ~268.0m/281018160, currentsize=62.0m/64973920 for region usertable,user6,1404225423342.89cb00f0096b305c3317b7753d66adf1. in 10662ms, sequenceid=935, compaction requested=true
2014-07-01 07:41:03,381 DEBUG [MemStoreFlusher.1] regionserver.CompactSplitThread: Small Compaction requested: system; Because: MemStoreFlusher.1; compaction_queue=(0:2), split_queue=0, merge_queue=0
2014-07-01 07:41:09,199 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-01 07:41:09,269 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 4090 synced till here 4089
2014-07-01 07:41:09,372 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1404225098235/slave1%2C60020%2C1404225098235.1404225659164 with entries=109, filesize=64.0m; new WAL /hbase/WALs/slave1,60020,1404225098235/slave1%2C60020%2C1404225098235.1404225669200
2014-07-01 07:41:09,373 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1404225098235/slave1%2C60020%2C1404225098235.1404225588738
2014-07-01 07:41:09,373 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1404225098235/slave1%2C60020%2C1404225098235.1404225600538
2014-07-01 07:41:09,373 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1404225098235/slave1%2C60020%2C1404225098235.1404225606929
2014-07-01 07:41:09,373 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1404225098235/slave1%2C60020%2C1404225098235.1404225618615
2014-07-01 07:41:09,373 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1404225098235/slave1%2C60020%2C1404225098235.1404225622283
2014-07-01 07:41:09,373 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1404225098235/slave1%2C60020%2C1404225098235.1404225628543
2014-07-01 07:41:17,414 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-01 07:41:17,565 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 4201 synced till here 4194
2014-07-01 07:41:17,639 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1404225098235/slave1%2C60020%2C1404225098235.1404225669200 with entries=111, filesize=68.3m; new WAL /hbase/WALs/slave1,60020,1404225098235/slave1%2C60020%2C1404225098235.1404225677414
2014-07-01 07:41:20,365 DEBUG [regionserver60020-smallCompactions-1404225561807] regionserver.HRegionFileSystem: Committing store file hdfs://master:54310/hbase/data/default/usertable/c7604442b843793f8451582c14f6bf59/.tmp/ac870761a8ce41579543339b2d6e78f1 as hdfs://master:54310/hbase/data/default/usertable/c7604442b843793f8451582c14f6bf59/family/ac870761a8ce41579543339b2d6e78f1
2014-07-01 07:41:20,477 DEBUG [regionserver60020-smallCompactions-1404225561807] regionserver.HStore: Removing store files after compaction...
2014-07-01 07:41:20,522 DEBUG [regionserver60020-smallCompactions-1404225561807] backup.HFileArchiver: Finished archiving from class org.apache.hadoop.hbase.backup.HFileArchiver$FileableStoreFile, file:hdfs://master:54310/hbase/data/default/usertable/c7604442b843793f8451582c14f6bf59/family/d8b5a24bf10c49bb869641dd093f02a2, to hdfs://master:54310/hbase/archive/data/default/usertable/c7604442b843793f8451582c14f6bf59/family/d8b5a24bf10c49bb869641dd093f02a2
2014-07-01 07:41:20,530 DEBUG [regionserver60020-smallCompactions-1404225561807] backup.HFileArchiver: Finished archiving from class org.apache.hadoop.hbase.backup.HFileArchiver$FileableStoreFile, file:hdfs://master:54310/hbase/data/default/usertable/c7604442b843793f8451582c14f6bf59/family/1da9d56206ec4d8b93d5e57a06f6f5fb, to hdfs://master:54310/hbase/archive/data/default/usertable/c7604442b843793f8451582c14f6bf59/family/1da9d56206ec4d8b93d5e57a06f6f5fb
2014-07-01 07:41:20,533 DEBUG [regionserver60020-smallCompactions-1404225561807] backup.HFileArchiver: Finished archiving from class org.apache.hadoop.hbase.backup.HFileArchiver$FileableStoreFile, file:hdfs://master:54310/hbase/data/default/usertable/c7604442b843793f8451582c14f6bf59/family/5aaffbb600f846738f0f4be1e95ddad5, to hdfs://master:54310/hbase/archive/data/default/usertable/c7604442b843793f8451582c14f6bf59/family/5aaffbb600f846738f0f4be1e95ddad5
2014-07-01 07:41:20,533 INFO  [regionserver60020-smallCompactions-1404225561807] regionserver.HStore: Completed major compaction of 3 file(s) in family of usertable,user5,1404225423342.c7604442b843793f8451582c14f6bf59. into ac870761a8ce41579543339b2d6e78f1(size=113.8m), total size for store is 113.8m. This selection was in queue for 0sec, and took 17sec to execute.
2014-07-01 07:41:20,534 INFO  [regionserver60020-smallCompactions-1404225561807] regionserver.CompactSplitThread: Completed compaction: Request = regionName=usertable,user5,1404225423342.c7604442b843793f8451582c14f6bf59., storeName=family, fileCount=3, fileSize=130.8m, priority=17, time=45774473620074; duration=17sec
2014-07-01 07:41:20,534 DEBUG [regionserver60020-smallCompactions-1404225561807] regionserver.CompactSplitThread: CompactSplitThread Status: compaction_queue=(0:2), split_queue=0, merge_queue=0
2014-07-01 07:41:20,534 DEBUG [regionserver60020-smallCompactions-1404225561807] compactions.RatioBasedCompactionPolicy: Selecting compaction from 3 store files, 0 compacting, 3 eligible, 20 blocking
2014-07-01 07:41:20,534 DEBUG [regionserver60020-smallCompactions-1404225561807] compactions.ExploringCompactionPolicy: Exploring compaction algorithm has selected 3 files of size 137312727 starting at candidate #0 after considering 1 permutations with 1 in ratio
2014-07-01 07:41:20,534 DEBUG [regionserver60020-smallCompactions-1404225561807] regionserver.HStore: 07064f253c865a7cc0c08893a4f56af9 - family: Initiating major compaction
2014-07-01 07:41:20,534 INFO  [regionserver60020-smallCompactions-1404225561807] regionserver.HRegion: Starting compaction on family in region usertable,user2,1404225423342.07064f253c865a7cc0c08893a4f56af9.
2014-07-01 07:41:20,534 INFO  [regionserver60020-smallCompactions-1404225561807] regionserver.HStore: Starting compaction of 3 file(s) in family of usertable,user2,1404225423342.07064f253c865a7cc0c08893a4f56af9. into tmpdir=hdfs://master:54310/hbase/data/default/usertable/07064f253c865a7cc0c08893a4f56af9/.tmp, totalSize=131.0m
2014-07-01 07:41:20,535 DEBUG [regionserver60020-smallCompactions-1404225561807] compactions.Compactor: Compacting hdfs://master:54310/hbase/data/default/usertable/07064f253c865a7cc0c08893a4f56af9/family/7ba6631180da4242a995895cc6aa53fc, keycount=43521, bloomtype=ROW, size=31.0m, encoding=NONE, seqNum=597, earliestPutTs=1404225440261
2014-07-01 07:41:20,535 DEBUG [regionserver60020-smallCompactions-1404225561807] compactions.Compactor: Compacting hdfs://master:54310/hbase/data/default/usertable/07064f253c865a7cc0c08893a4f56af9/family/d222f4744f3d4abfb109179e0644f471, keycount=55280, bloomtype=ROW, size=39.5m, encoding=NONE, seqNum=762, earliestPutTs=1404225557846
2014-07-01 07:41:20,535 DEBUG [regionserver60020-smallCompactions-1404225561807] compactions.Compactor: Compacting hdfs://master:54310/hbase/data/default/usertable/07064f253c865a7cc0c08893a4f56af9/family/6a285363987443a38e099f52ebb41ff0, keycount=84836, bloomtype=ROW, size=60.5m, encoding=NONE, seqNum=916, earliestPutTs=1404225588854
2014-07-01 07:41:20,576 DEBUG [regionserver60020-smallCompactions-1404225561807] util.FSUtils: DFS Client does not support most favored nodes create; using default create
2014-07-01 07:41:25,291 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-01 07:41:25,381 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1404225098235/slave1%2C60020%2C1404225098235.1404225677414 with entries=122, filesize=62.8m; new WAL /hbase/WALs/slave1,60020,1404225098235/slave1%2C60020%2C1404225098235.1404225685291
2014-07-01 07:41:33,183 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-01 07:41:33,237 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 4423 synced till here 4418
2014-07-01 07:41:33,596 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1404225098235/slave1%2C60020%2C1404225098235.1404225685291 with entries=100, filesize=67.9m; new WAL /hbase/WALs/slave1,60020,1404225098235/slave1%2C60020%2C1404225098235.1404225693184
2014-07-01 07:41:34,327 DEBUG [RpcServer.handler=47,port=60020] regionserver.HRegion: Flush requested on usertable,,1404225423341.387a171caad6315f0b61271a30313d1a.
2014-07-01 07:41:34,327 DEBUG [MemStoreFlusher.0] regionserver.HRegion: Started memstore flush for usertable,,1404225423341.387a171caad6315f0b61271a30313d1a., current region memstore size 260.3m
2014-07-01 07:41:34,822 DEBUG [MemStoreFlusher.0] util.FSUtils: DFS Client does not support most favored nodes create; using default create
2014-07-01 07:41:35,833 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-01 07:41:36,048 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 4551 synced till here 4542
2014-07-01 07:41:36,261 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1404225098235/slave1%2C60020%2C1404225098235.1404225693184 with entries=128, filesize=85.8m; new WAL /hbase/WALs/slave1,60020,1404225098235/slave1%2C60020%2C1404225098235.1404225695833
2014-07-01 07:41:36,830 DEBUG [RpcServer.handler=44,port=60020] regionserver.HRegion: Flush requested on usertable,user2,1404225423342.07064f253c865a7cc0c08893a4f56af9.
2014-07-01 07:41:36,831 DEBUG [MemStoreFlusher.1] regionserver.HRegion: Started memstore flush for usertable,user2,1404225423342.07064f253c865a7cc0c08893a4f56af9., current region memstore size 257.8m
2014-07-01 07:41:37,069 DEBUG [RpcServer.handler=44,port=60020] regionserver.HRegion: Flush requested on usertable,user5,1404225423342.c7604442b843793f8451582c14f6bf59.
2014-07-01 07:41:37,307 DEBUG [MemStoreFlusher.1] util.FSUtils: DFS Client does not support most favored nodes create; using default create
2014-07-01 07:41:38,204 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-01 07:41:38,316 DEBUG [LruStats #0] hfile.LruBlockCache: Total=4.16 MB, free=3.95 GB, max=3.96 GB, blocks=0, accesses=18958, hits=0, hitRatio=0, cachingAccesses=0, cachingHits=0, cachingHitsRatio=0,evictions=0, evicted=0, evictedPerRun=NaN
2014-07-01 07:41:38,357 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 4645 synced till here 4621
2014-07-01 07:41:39,208 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1404225098235/slave1%2C60020%2C1404225098235.1404225695833 with entries=94, filesize=78.1m; new WAL /hbase/WALs/slave1,60020,1404225098235/slave1%2C60020%2C1404225098235.1404225698204
2014-07-01 07:41:39,589 DEBUG [RpcServer.handler=12,port=60020] regionserver.HRegion: Flush requested on usertable,user6,1404225423342.89cb00f0096b305c3317b7753d66adf1.
2014-07-01 07:41:40,143 DEBUG [regionserver60020-smallCompactions-1404225561807] regionserver.HRegionFileSystem: Committing store file hdfs://master:54310/hbase/data/default/usertable/07064f253c865a7cc0c08893a4f56af9/.tmp/c96e13177c2649c584bf5f7d42cc24dd as hdfs://master:54310/hbase/data/default/usertable/07064f253c865a7cc0c08893a4f56af9/family/c96e13177c2649c584bf5f7d42cc24dd
2014-07-01 07:41:40,232 DEBUG [regionserver60020-smallCompactions-1404225561807] regionserver.HStore: Removing store files after compaction...
2014-07-01 07:41:40,349 DEBUG [regionserver60020-smallCompactions-1404225561807] backup.HFileArchiver: Finished archiving from class org.apache.hadoop.hbase.backup.HFileArchiver$FileableStoreFile, file:hdfs://master:54310/hbase/data/default/usertable/07064f253c865a7cc0c08893a4f56af9/family/7ba6631180da4242a995895cc6aa53fc, to hdfs://master:54310/hbase/archive/data/default/usertable/07064f253c865a7cc0c08893a4f56af9/family/7ba6631180da4242a995895cc6aa53fc
2014-07-01 07:41:40,352 DEBUG [regionserver60020-smallCompactions-1404225561807] backup.HFileArchiver: Finished archiving from class org.apache.hadoop.hbase.backup.HFileArchiver$FileableStoreFile, file:hdfs://master:54310/hbase/data/default/usertable/07064f253c865a7cc0c08893a4f56af9/family/d222f4744f3d4abfb109179e0644f471, to hdfs://master:54310/hbase/archive/data/default/usertable/07064f253c865a7cc0c08893a4f56af9/family/d222f4744f3d4abfb109179e0644f471
2014-07-01 07:41:40,357 DEBUG [regionserver60020-smallCompactions-1404225561807] backup.HFileArchiver: Finished archiving from class org.apache.hadoop.hbase.backup.HFileArchiver$FileableStoreFile, file:hdfs://master:54310/hbase/data/default/usertable/07064f253c865a7cc0c08893a4f56af9/family/6a285363987443a38e099f52ebb41ff0, to hdfs://master:54310/hbase/archive/data/default/usertable/07064f253c865a7cc0c08893a4f56af9/family/6a285363987443a38e099f52ebb41ff0
2014-07-01 07:41:40,357 INFO  [regionserver60020-smallCompactions-1404225561807] regionserver.HStore: Completed major compaction of 3 file(s) in family of usertable,user2,1404225423342.07064f253c865a7cc0c08893a4f56af9. into c96e13177c2649c584bf5f7d42cc24dd(size=113.9m), total size for store is 113.9m. This selection was in queue for 0sec, and took 19sec to execute.
2014-07-01 07:41:40,357 INFO  [regionserver60020-smallCompactions-1404225561807] regionserver.CompactSplitThread: Completed compaction: Request = regionName=usertable,user2,1404225423342.07064f253c865a7cc0c08893a4f56af9., storeName=family, fileCount=3, fileSize=131.0m, priority=17, time=45791923335750; duration=19sec
2014-07-01 07:41:40,357 DEBUG [regionserver60020-smallCompactions-1404225561807] regionserver.CompactSplitThread: CompactSplitThread Status: compaction_queue=(0:1), split_queue=0, merge_queue=0
2014-07-01 07:41:40,357 DEBUG [regionserver60020-smallCompactions-1404225561807] compactions.RatioBasedCompactionPolicy: Selecting compaction from 3 store files, 0 compacting, 3 eligible, 20 blocking
2014-07-01 07:41:40,357 DEBUG [regionserver60020-smallCompactions-1404225561807] compactions.ExploringCompactionPolicy: Exploring compaction algorithm has selected 3 files of size 144549799 starting at candidate #0 after considering 1 permutations with 1 in ratio
2014-07-01 07:41:40,358 DEBUG [regionserver60020-smallCompactions-1404225561807] regionserver.HStore: 89cb00f0096b305c3317b7753d66adf1 - family: Initiating major compaction
2014-07-01 07:41:40,358 INFO  [regionserver60020-smallCompactions-1404225561807] regionserver.HRegion: Starting compaction on family in region usertable,user6,1404225423342.89cb00f0096b305c3317b7753d66adf1.
2014-07-01 07:41:40,358 INFO  [regionserver60020-smallCompactions-1404225561807] regionserver.HStore: Starting compaction of 3 file(s) in family of usertable,user6,1404225423342.89cb00f0096b305c3317b7753d66adf1. into tmpdir=hdfs://master:54310/hbase/data/default/usertable/89cb00f0096b305c3317b7753d66adf1/.tmp, totalSize=137.9m
2014-07-01 07:41:40,358 DEBUG [regionserver60020-smallCompactions-1404225561807] compactions.Compactor: Compacting hdfs://master:54310/hbase/data/default/usertable/89cb00f0096b305c3317b7753d66adf1/family/aaa762fa710f487890b087b543a2e26f, keycount=45763, bloomtype=ROW, size=32.6m, encoding=NONE, seqNum=616, earliestPutTs=1404225441356
2014-07-01 07:41:40,358 DEBUG [regionserver60020-smallCompactions-1404225561807] compactions.Compactor: Compacting hdfs://master:54310/hbase/data/default/usertable/89cb00f0096b305c3317b7753d66adf1/family/79f30d7a60d941c18054ddab01f26c6c, keycount=60508, bloomtype=ROW, size=43.2m, encoding=NONE, seqNum=778, earliestPutTs=1404225565943
2014-07-01 07:41:40,358 DEBUG [regionserver60020-smallCompactions-1404225561807] compactions.Compactor: Compacting hdfs://master:54310/hbase/data/default/usertable/89cb00f0096b305c3317b7753d66adf1/family/d342f65c4ae34d8ca1a93b4dae5733a5, keycount=87042, bloomtype=ROW, size=62.0m, encoding=NONE, seqNum=935, earliestPutTs=1404225600143
2014-07-01 07:41:40,393 DEBUG [regionserver60020-smallCompactions-1404225561807] util.FSUtils: DFS Client does not support most favored nodes create; using default create
2014-07-01 07:41:42,044 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-01 07:41:42,515 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 4755 synced till here 4726
2014-07-01 07:41:42,765 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1404225098235/slave1%2C60020%2C1404225098235.1404225698204 with entries=110, filesize=82.5m; new WAL /hbase/WALs/slave1,60020,1404225098235/slave1%2C60020%2C1404225098235.1404225702044
2014-07-01 07:41:45,154 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-01 07:41:45,421 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 4873 synced till here 4851
2014-07-01 07:41:45,758 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1404225098235/slave1%2C60020%2C1404225098235.1404225702044 with entries=118, filesize=82.9m; new WAL /hbase/WALs/slave1,60020,1404225098235/slave1%2C60020%2C1404225098235.1404225705156
2014-07-01 07:41:48,740 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-01 07:41:48,880 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 5002 synced till here 4969
2014-07-01 07:41:49,637 INFO  [MemStoreFlusher.1] regionserver.DefaultStoreFlusher: Flushed, sequenceid=1064, memsize=229.1m, hasBloomFilter=true, into tmp file hdfs://master:54310/hbase/data/default/usertable/07064f253c865a7cc0c08893a4f56af9/.tmp/82115b3ce7df419eaabd7193f445c540
2014-07-01 07:41:49,777 DEBUG [MemStoreFlusher.1] regionserver.HRegionFileSystem: Committing store file hdfs://master:54310/hbase/data/default/usertable/07064f253c865a7cc0c08893a4f56af9/.tmp/82115b3ce7df419eaabd7193f445c540 as hdfs://master:54310/hbase/data/default/usertable/07064f253c865a7cc0c08893a4f56af9/family/82115b3ce7df419eaabd7193f445c540
2014-07-01 07:41:49,922 INFO  [MemStoreFlusher.0] regionserver.DefaultStoreFlusher: Flushed, sequenceid=1366, memsize=264.9m, hasBloomFilter=true, into tmp file hdfs://master:54310/hbase/data/default/usertable/387a171caad6315f0b61271a30313d1a/.tmp/f8209a93d60641ba9eb02aca2a965e9c
2014-07-01 07:41:49,934 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1404225098235/slave1%2C60020%2C1404225098235.1404225705156 with entries=129, filesize=95.5m; new WAL /hbase/WALs/slave1,60020,1404225098235/slave1%2C60020%2C1404225098235.1404225708741
2014-07-01 07:41:49,943 INFO  [MemStoreFlusher.1] regionserver.HStore: Added hdfs://master:54310/hbase/data/default/usertable/07064f253c865a7cc0c08893a4f56af9/family/82115b3ce7df419eaabd7193f445c540, entries=834230, sequenceid=1064, filesize=59.5m
2014-07-01 07:41:49,943 INFO  [MemStoreFlusher.1] regionserver.HRegion: Finished memstore flush of ~259.6m/272195280, currentsize=173.2m/181663600 for region usertable,user2,1404225423342.07064f253c865a7cc0c08893a4f56af9. in 13113ms, sequenceid=1064, compaction requested=false
2014-07-01 07:41:49,944 DEBUG [MemStoreFlusher.1] regionserver.HRegion: Started memstore flush for usertable,user5,1404225423342.c7604442b843793f8451582c14f6bf59., current region memstore size 413.0m
2014-07-01 07:41:50,027 DEBUG [MemStoreFlusher.0] regionserver.HRegionFileSystem: Committing store file hdfs://master:54310/hbase/data/default/usertable/387a171caad6315f0b61271a30313d1a/.tmp/f8209a93d60641ba9eb02aca2a965e9c as hdfs://master:54310/hbase/data/default/usertable/387a171caad6315f0b61271a30313d1a/family/f8209a93d60641ba9eb02aca2a965e9c
2014-07-01 07:41:50,056 INFO  [MemStoreFlusher.0] regionserver.HStore: Added hdfs://master:54310/hbase/data/default/usertable/387a171caad6315f0b61271a30313d1a/family/f8209a93d60641ba9eb02aca2a965e9c, entries=964560, sequenceid=1366, filesize=68.7m
2014-07-01 07:41:50,056 INFO  [MemStoreFlusher.0] regionserver.HRegion: Finished memstore flush of ~272.9m/286106240, currentsize=114.2m/119772480 for region usertable,,1404225423341.387a171caad6315f0b61271a30313d1a. in 15729ms, sequenceid=1366, compaction requested=false
2014-07-01 07:41:50,057 DEBUG [MemStoreFlusher.0] regionserver.HRegion: Started memstore flush for usertable,user6,1404225423342.89cb00f0096b305c3317b7753d66adf1., current region memstore size 363.8m
2014-07-01 07:41:51,859 DEBUG [MemStoreFlusher.1] util.FSUtils: DFS Client does not support most favored nodes create; using default create
2014-07-01 07:41:51,878 DEBUG [MemStoreFlusher.0] util.FSUtils: DFS Client does not support most favored nodes create; using default create
2014-07-01 07:41:52,735 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-01 07:41:53,049 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 5117 synced till here 5099
2014-07-01 07:41:53,586 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1404225098235/slave1%2C60020%2C1404225098235.1404225708741 with entries=115, filesize=102.0m; new WAL /hbase/WALs/slave1,60020,1404225098235/slave1%2C60020%2C1404225098235.1404225712735
2014-07-01 07:41:53,586 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1404225098235/slave1%2C60020%2C1404225098235.1404225632469
2014-07-01 07:41:53,586 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1404225098235/slave1%2C60020%2C1404225098235.1404225635235
2014-07-01 07:41:54,087 DEBUG [RpcServer.handler=27,port=60020] regionserver.HRegion: Flush requested on usertable,user2,1404225423342.07064f253c865a7cc0c08893a4f56af9.
2014-07-01 07:41:54,698 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-01 07:41:54,733 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 5209 synced till here 5180
2014-07-01 07:41:55,160 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1404225098235/slave1%2C60020%2C1404225098235.1404225712735 with entries=92, filesize=88.8m; new WAL /hbase/WALs/slave1,60020,1404225098235/slave1%2C60020%2C1404225098235.1404225714699
2014-07-01 07:41:57,398 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-01 07:41:57,739 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 5302 synced till here 5278
2014-07-01 07:41:58,593 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1404225098235/slave1%2C60020%2C1404225098235.1404225714699 with entries=93, filesize=83.4m; new WAL /hbase/WALs/slave1,60020,1404225098235/slave1%2C60020%2C1404225098235.1404225717399
2014-07-01 07:42:02,984 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-01 07:42:03,149 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 5415 synced till here 5394
2014-07-01 07:42:03,887 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1404225098235/slave1%2C60020%2C1404225098235.1404225717399 with entries=113, filesize=102.3m; new WAL /hbase/WALs/slave1,60020,1404225098235/slave1%2C60020%2C1404225098235.1404225722985
2014-07-01 07:42:07,007 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-01 07:42:07,167 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 5531 synced till here 5495
2014-07-01 07:42:08,613 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1404225098235/slave1%2C60020%2C1404225098235.1404225722985 with entries=116, filesize=100.4m; new WAL /hbase/WALs/slave1,60020,1404225098235/slave1%2C60020%2C1404225098235.1404225727007
2014-07-01 07:42:10,802 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-01 07:42:10,832 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 5636 synced till here 5606
2014-07-01 07:42:11,338 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1404225098235/slave1%2C60020%2C1404225098235.1404225727007 with entries=105, filesize=94.5m; new WAL /hbase/WALs/slave1,60020,1404225098235/slave1%2C60020%2C1404225098235.1404225730802
2014-07-01 07:42:22,607 INFO  [JvmPauseMonitor] util.JvmPauseMonitor: Detected pause in JVM or host machine (eg GC): pause of approximately 9734ms
GC pool 'ParNew' had collection(s): count=2 time=107ms
2014-07-01 07:42:26,309 WARN  [RpcServer.handler=44,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":17926,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.58:33337","starttimems":1404225728358,"queuetimems":5153,"class":"HRegionServer","responsesize":13237,"method":"Multi"}
2014-07-01 07:42:26,311 WARN  [RpcServer.handler=30,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":14948,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.58:33337","starttimems":1404225731336,"queuetimems":6564,"class":"HRegionServer","responsesize":4061,"method":"Multi"}
2014-07-01 07:42:26,315 WARN  [RpcServer.handler=47,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":19275,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.58:33337","starttimems":1404225727009,"queuetimems":4209,"class":"HRegionServer","responsesize":13543,"method":"Multi"}
2014-07-01 07:42:26,321 WARN  [RpcServer.handler=36,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":17089,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.58:33337","starttimems":1404225729227,"queuetimems":5887,"class":"HRegionServer","responsesize":9219,"method":"Multi"}
2014-07-01 07:42:26,323 WARN  [RpcServer.handler=41,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":17935,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.58:33337","starttimems":1404225728349,"queuetimems":5170,"class":"HRegionServer","responsesize":8030,"method":"Multi"}
2014-07-01 07:42:26,309 WARN  [RpcServer.handler=34,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":19271,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.58:33337","starttimems":1404225727013,"queuetimems":4151,"class":"HRegionServer","responsesize":13204,"method":"Multi"}
2014-07-01 07:42:26,329 WARN  [RpcServer.handler=14,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":19294,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.58:33337","starttimems":1404225727013,"queuetimems":4170,"class":"HRegionServer","responsesize":13589,"method":"Multi"}
2014-07-01 07:42:26,330 WARN  [RpcServer.handler=47,port=60020] ipc.RpcServer: RpcServer.respondercallId: 3480 service: ClientService methodName: Multi size: 2.4m connection: 9.1.143.58:33337: output error
2014-07-01 07:42:26,309 WARN  [RpcServer.handler=11,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":19174,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.58:33337","starttimems":1404225727110,"queuetimems":4103,"class":"HRegionServer","responsesize":13550,"method":"Multi"}
2014-07-01 07:42:26,386 WARN  [RpcServer.handler=33,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":19274,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.58:33337","starttimems":1404225727109,"queuetimems":4110,"class":"HRegionServer","responsesize":13005,"method":"Multi"}
2014-07-01 07:42:26,379 WARN  [RpcServer.handler=19,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":16024,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.58:33337","starttimems":1404225730354,"queuetimems":6419,"class":"HRegionServer","responsesize":6175,"method":"Multi"}
2014-07-01 07:42:26,520 WARN  [RpcServer.handler=9,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":19323,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.58:33337","starttimems":1404225727009,"queuetimems":4195,"class":"HRegionServer","responsesize":13514,"method":"Multi"}
2014-07-01 07:42:26,522 WARN  [RpcServer.handler=11,port=60020] ipc.RpcServer: RpcServer.respondercallId: 3468 service: ClientService methodName: Multi size: 2.4m connection: 9.1.143.58:33337: output error
2014-07-01 07:42:26,522 WARN  [RpcServer.handler=11,port=60020] ipc.RpcServer: RpcServer.handler=11,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-07-01 07:42:26,522 WARN  [RpcServer.handler=33,port=60020] ipc.RpcServer: RpcServer.respondercallId: 3469 service: ClientService methodName: Multi size: 2.3m connection: 9.1.143.58:33337: output error
2014-07-01 07:42:26,523 WARN  [RpcServer.handler=33,port=60020] ipc.RpcServer: RpcServer.handler=33,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-07-01 07:42:26,523 WARN  [RpcServer.handler=19,port=60020] ipc.RpcServer: RpcServer.respondercallId: 3441 service: ClientService methodName: Multi size: 1.1m connection: 9.1.143.58:33337: output error
2014-07-01 07:42:26,523 WARN  [RpcServer.handler=19,port=60020] ipc.RpcServer: RpcServer.handler=19,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-07-01 07:42:26,523 WARN  [RpcServer.handler=44,port=60020] ipc.RpcServer: RpcServer.respondercallId: 3456 service: ClientService methodName: Multi size: 2.3m connection: 9.1.143.58:33337: output error
2014-07-01 07:42:26,523 WARN  [RpcServer.handler=44,port=60020] ipc.RpcServer: RpcServer.handler=44,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-07-01 07:42:26,523 WARN  [RpcServer.handler=14,port=60020] ipc.RpcServer: RpcServer.respondercallId: 3477 service: ClientService methodName: Multi size: 2.4m connection: 9.1.143.58:33337: output error
2014-07-01 07:42:26,523 WARN  [RpcServer.handler=14,port=60020] ipc.RpcServer: RpcServer.handler=14,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-07-01 07:42:26,526 WARN  [RpcServer.handler=34,port=60020] ipc.RpcServer: RpcServer.respondercallId: 3476 service: ClientService methodName: Multi size: 2.3m connection: 9.1.143.58:33337: output error
2014-07-01 07:42:26,527 WARN  [RpcServer.handler=34,port=60020] ipc.RpcServer: RpcServer.handler=34,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-07-01 07:42:26,558 WARN  [RpcServer.handler=41,port=60020] ipc.RpcServer: RpcServer.respondercallId: 3461 service: ClientService methodName: Multi size: 1.4m connection: 9.1.143.58:33337: output error
2014-07-01 07:42:26,559 WARN  [RpcServer.handler=41,port=60020] ipc.RpcServer: RpcServer.handler=41,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-07-01 07:42:26,562 WARN  [RpcServer.handler=36,port=60020] ipc.RpcServer: RpcServer.respondercallId: 3454 service: ClientService methodName: Multi size: 1.6m connection: 9.1.143.58:33337: output error
2014-07-01 07:42:26,562 WARN  [RpcServer.handler=36,port=60020] ipc.RpcServer: RpcServer.handler=36,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-07-01 07:42:26,563 WARN  [RpcServer.handler=9,port=60020] ipc.RpcServer: RpcServer.respondercallId: 3479 service: ClientService methodName: Multi size: 2.4m connection: 9.1.143.58:33337: output error
2014-07-01 07:42:26,563 WARN  [RpcServer.handler=9,port=60020] ipc.RpcServer: RpcServer.handler=9,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-07-01 07:42:26,565 WARN  [RpcServer.handler=47,port=60020] ipc.RpcServer: RpcServer.handler=47,port=60020: caught: java.io.IOException: Connection reset by peer
	at sun.nio.ch.FileDispatcher.writev0(Native Method)
	at sun.nio.ch.SocketDispatcher.writev(SocketDispatcher.java:51)
	at sun.nio.ch.IOUtil.write(IOUtil.java:182)
	at sun.nio.ch.SocketChannelImpl.write0(SocketChannelImpl.java:383)
	at sun.nio.ch.SocketChannelImpl.write(SocketChannelImpl.java:406)
	at org.apache.hadoop.hbase.ipc.BufferChain.write(BufferChain.java:106)
	at org.apache.hadoop.hbase.ipc.RpcServer.channelWrite(RpcServer.java:2209)
	at org.apache.hadoop.hbase.ipc.RpcServer$Responder.processResponse(RpcServer.java:1004)
	at org.apache.hadoop.hbase.ipc.RpcServer$Responder.doRespond(RpcServer.java:1081)
	at org.apache.hadoop.hbase.ipc.RpcServer$Call.sendResponseIfReady(RpcServer.java:496)
	at org.apache.hadoop.hbase.ipc.CallRunner.run(CallRunner.java:121)
	at org.apache.hadoop.hbase.ipc.SimpleRpcScheduler.consumerLoop(SimpleRpcScheduler.java:168)
	at org.apache.hadoop.hbase.ipc.SimpleRpcScheduler.access$000(SimpleRpcScheduler.java:39)
	at org.apache.hadoop.hbase.ipc.SimpleRpcScheduler$1.run(SimpleRpcScheduler.java:111)
	at java.lang.Thread.run(Thread.java:701)

2014-07-01 07:42:28,551 WARN  [RpcServer.handler=16,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":19096,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.58:33337","starttimems":1404225729452,"queuetimems":5564,"class":"HRegionServer","responsesize":12867,"method":"Multi"}
2014-07-01 07:42:28,552 WARN  [RpcServer.handler=16,port=60020] ipc.RpcServer: RpcServer.respondercallId: 3443 service: ClientService methodName: Multi size: 2.2m connection: 9.1.143.58:33337: output error
2014-07-01 07:42:28,552 WARN  [RpcServer.handler=16,port=60020] ipc.RpcServer: RpcServer.handler=16,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-07-01 07:42:31,657 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-01 07:42:31,661 WARN  [RpcServer.handler=8,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":22201,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.58:33337","starttimems":1404225729459,"queuetimems":5564,"class":"HRegionServer","responsesize":11651,"method":"Multi"}
2014-07-01 07:42:31,661 WARN  [RpcServer.handler=5,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":22429,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.58:33337","starttimems":1404225729231,"queuetimems":5850,"class":"HRegionServer","responsesize":13489,"method":"Multi"}
2014-07-01 07:42:31,663 WARN  [RpcServer.handler=29,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":22419,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.58:33337","starttimems":1404225729243,"queuetimems":5691,"class":"HRegionServer","responsesize":13727,"method":"Multi"}
2014-07-01 07:42:31,662 WARN  [RpcServer.handler=0,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":22422,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.58:33337","starttimems":1404225729240,"queuetimems":5841,"class":"HRegionServer","responsesize":11784,"method":"Multi"}
2014-07-01 07:42:31,663 WARN  [RpcServer.handler=8,port=60020] ipc.RpcServer: RpcServer.respondercallId: 3442 service: ClientService methodName: Multi size: 2.0m connection: 9.1.143.58:33337: output error
2014-07-01 07:42:31,663 WARN  [RpcServer.handler=8,port=60020] ipc.RpcServer: RpcServer.handler=8,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-07-01 07:42:31,664 WARN  [RpcServer.handler=5,port=60020] ipc.RpcServer: RpcServer.respondercallId: 3451 service: ClientService methodName: Multi size: 2.4m connection: 9.1.143.58:33337: output error
2014-07-01 07:42:31,664 WARN  [RpcServer.handler=5,port=60020] ipc.RpcServer: RpcServer.handler=5,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-07-01 07:42:31,664 WARN  [RpcServer.handler=2,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":22433,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.58:33337","starttimems":1404225729229,"queuetimems":5865,"class":"HRegionServer","responsesize":13178,"method":"Multi"}
2014-07-01 07:42:31,665 WARN  [RpcServer.handler=42,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":20859,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.58:33337","starttimems":1404225730804,"queuetimems":6733,"class":"HRegionServer","responsesize":13694,"method":"Multi"}
2014-07-01 07:42:31,665 WARN  [RpcServer.handler=2,port=60020] ipc.RpcServer: RpcServer.respondercallId: 3452 service: ClientService methodName: Multi size: 2.3m connection: 9.1.143.58:33337: output error
2014-07-01 07:42:31,665 WARN  [RpcServer.handler=2,port=60020] ipc.RpcServer: RpcServer.handler=2,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-07-01 07:42:31,665 WARN  [RpcServer.handler=42,port=60020] ipc.RpcServer: RpcServer.respondercallId: 3516 service: ClientService methodName: Multi size: 2.4m connection: 9.1.143.58:33337: output error
2014-07-01 07:42:31,665 WARN  [RpcServer.handler=42,port=60020] ipc.RpcServer: RpcServer.handler=42,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-07-01 07:42:31,665 WARN  [RpcServer.handler=46,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":22433,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.58:33337","starttimems":1404225729229,"queuetimems":5877,"class":"HRegionServer","responsesize":12977,"method":"Multi"}
2014-07-01 07:42:31,762 WARN  [RpcServer.handler=0,port=60020] ipc.RpcServer: RpcServer.respondercallId: 3450 service: ClientService methodName: Multi size: 2.1m connection: 9.1.143.58:33337: output error
2014-07-01 07:42:31,763 WARN  [RpcServer.handler=0,port=60020] ipc.RpcServer: RpcServer.handler=0,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-07-01 07:42:31,764 WARN  [RpcServer.handler=21,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":22420,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.58:33337","starttimems":1404225729244,"queuetimems":5683,"class":"HRegionServer","responsesize":11940,"method":"Multi"}
2014-07-01 07:42:31,764 WARN  [RpcServer.handler=29,port=60020] ipc.RpcServer: RpcServer.respondercallId: 3449 service: ClientService methodName: Multi size: 2.4m connection: 9.1.143.58:33337: output error
2014-07-01 07:42:31,764 WARN  [RpcServer.handler=29,port=60020] ipc.RpcServer: RpcServer.handler=29,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-07-01 07:42:31,764 WARN  [RpcServer.handler=21,port=60020] ipc.RpcServer: RpcServer.respondercallId: 3448 service: ClientService methodName: Multi size: 2.1m connection: 9.1.143.58:33337: output error
2014-07-01 07:42:31,764 WARN  [RpcServer.handler=21,port=60020] ipc.RpcServer: RpcServer.handler=21,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-07-01 07:42:31,766 WARN  [RpcServer.handler=12,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":22517,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.58:33337","starttimems":1404225729248,"queuetimems":5671,"class":"HRegionServer","responsesize":13271,"method":"Multi"}
2014-07-01 07:42:31,766 WARN  [RpcServer.handler=12,port=60020] ipc.RpcServer: RpcServer.respondercallId: 3447 service: ClientService methodName: Multi size: 2.3m connection: 9.1.143.58:33337: output error
2014-07-01 07:42:31,766 WARN  [RpcServer.handler=12,port=60020] ipc.RpcServer: RpcServer.handler=12,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-07-01 07:42:31,767 WARN  [RpcServer.handler=46,port=60020] ipc.RpcServer: RpcServer.respondercallId: 3453 service: ClientService methodName: Multi size: 2.3m connection: 9.1.143.58:33337: output error
2014-07-01 07:42:31,767 WARN  [RpcServer.handler=46,port=60020] ipc.RpcServer: RpcServer.handler=46,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-07-01 07:42:31,767 WARN  [RpcServer.handler=26,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":22510,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.58:33337","starttimems":1404225729256,"queuetimems":5514,"class":"HRegionServer","responsesize":12848,"method":"Multi"}
2014-07-01 07:42:31,767 WARN  [RpcServer.handler=26,port=60020] ipc.RpcServer: RpcServer.respondercallId: 3446 service: ClientService methodName: Multi size: 2.2m connection: 9.1.143.58:33337: output error
2014-07-01 07:42:31,775 WARN  [RpcServer.handler=26,port=60020] ipc.RpcServer: RpcServer.handler=26,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-07-01 07:42:31,775 WARN  [RpcServer.handler=39,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":21300,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.58:33337","starttimems":1404225730467,"queuetimems":6411,"class":"HRegionServer","responsesize":13752,"method":"Multi"}
2014-07-01 07:42:31,777 WARN  [RpcServer.handler=39,port=60020] ipc.RpcServer: RpcServer.respondercallId: 3517 service: ClientService methodName: Multi size: 2.4m connection: 9.1.143.58:33337: output error
2014-07-01 07:42:31,777 WARN  [RpcServer.handler=39,port=60020] ipc.RpcServer: RpcServer.handler=39,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-07-01 07:42:31,821 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 5744 synced till here 5713
2014-07-01 07:42:35,301 INFO  [JvmPauseMonitor] util.JvmPauseMonitor: Detected pause in JVM or host machine (eg GC): pause of approximately 2972ms
GC pool 'ParNew' had collection(s): count=2 time=3164ms
2014-07-01 07:42:36,655 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1404225098235/slave1%2C60020%2C1404225098235.1404225730802 with entries=108, filesize=96.3m; new WAL /hbase/WALs/slave1,60020,1404225098235/slave1%2C60020%2C1404225098235.1404225751659
2014-07-01 07:42:38,993 WARN  [RpcServer.handler=25,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":27657,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.58:33337","starttimems":1404225731335,"queuetimems":6701,"class":"HRegionServer","responsesize":10169,"method":"Multi"}
2014-07-01 07:42:38,993 WARN  [RpcServer.handler=25,port=60020] ipc.RpcServer: RpcServer.respondercallId: 3496 service: ClientService methodName: Multi size: 1.8m connection: 9.1.143.58:33337: output error
2014-07-01 07:42:38,993 WARN  [RpcServer.handler=25,port=60020] ipc.RpcServer: RpcServer.handler=25,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-07-01 07:42:38,993 WARN  [RpcServer.handler=40,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":27661,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.58:33337","starttimems":1404225731332,"queuetimems":6945,"class":"HRegionServer","responsesize":9005,"method":"Multi"}
2014-07-01 07:42:38,995 WARN  [RpcServer.handler=40,port=60020] ipc.RpcServer: RpcServer.respondercallId: 3505 service: ClientService methodName: Multi size: 1.6m connection: 9.1.143.58:33337: output error
2014-07-01 07:42:38,995 WARN  [RpcServer.handler=40,port=60020] ipc.RpcServer: RpcServer.handler=40,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-07-01 07:42:39,007 WARN  [RpcServer.handler=31,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":27781,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.58:33337","starttimems":1404225731226,"queuetimems":7140,"class":"HRegionServer","responsesize":12977,"method":"Multi"}
2014-07-01 07:42:39,008 WARN  [RpcServer.handler=31,port=60020] ipc.RpcServer: RpcServer.respondercallId: 3515 service: ClientService methodName: Multi size: 2.3m connection: 9.1.143.58:33337: output error
2014-07-01 07:42:39,008 WARN  [RpcServer.handler=31,port=60020] ipc.RpcServer: RpcServer.handler=31,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-07-01 07:42:39,020 WARN  [RpcServer.handler=48,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":27793,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.58:33337","starttimems":1404225731226,"queuetimems":7016,"class":"HRegionServer","responsesize":13510,"method":"Multi"}
2014-07-01 07:42:39,020 WARN  [RpcServer.handler=48,port=60020] ipc.RpcServer: RpcServer.respondercallId: 3514 service: ClientService methodName: Multi size: 2.4m connection: 9.1.143.58:33337: output error
2014-07-01 07:42:39,020 WARN  [RpcServer.handler=48,port=60020] ipc.RpcServer: RpcServer.handler=48,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-07-01 07:42:39,027 WARN  [RpcServer.handler=38,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":27693,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.58:33337","starttimems":1404225731328,"queuetimems":7080,"class":"HRegionServer","responsesize":13394,"method":"Multi"}
2014-07-01 07:42:39,047 WARN  [RpcServer.handler=38,port=60020] ipc.RpcServer: RpcServer.respondercallId: 3510 service: ClientService methodName: Multi size: 2.3m connection: 9.1.143.58:33337: output error
2014-07-01 07:42:39,051 WARN  [RpcServer.handler=38,port=60020] ipc.RpcServer: RpcServer.handler=38,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-07-01 07:42:39,059 WARN  [RpcServer.handler=32,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":27721,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.58:33337","starttimems":1404225731335,"queuetimems":6732,"class":"HRegionServer","responsesize":13337,"method":"Multi"}
2014-07-01 07:42:39,059 WARN  [RpcServer.handler=32,port=60020] ipc.RpcServer: RpcServer.respondercallId: 3500 service: ClientService methodName: Multi size: 2.3m connection: 9.1.143.58:33337: output error
2014-07-01 07:42:39,059 WARN  [RpcServer.handler=32,port=60020] ipc.RpcServer: RpcServer.handler=32,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-07-01 07:42:39,066 WARN  [RpcServer.handler=27,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":27733,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.58:33337","starttimems":1404225731332,"queuetimems":6929,"class":"HRegionServer","responsesize":13674,"method":"Multi"}
2014-07-01 07:42:39,066 WARN  [RpcServer.handler=27,port=60020] ipc.RpcServer: RpcServer.respondercallId: 3504 service: ClientService methodName: Multi size: 2.4m connection: 9.1.143.58:33337: output error
2014-07-01 07:42:39,066 WARN  [RpcServer.handler=27,port=60020] ipc.RpcServer: RpcServer.handler=27,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-07-01 07:42:40,711 WARN  [RpcServer.handler=3,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":29361,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.58:33337","starttimems":1404225731350,"queuetimems":6327,"class":"HRegionServer","responsesize":6175,"method":"Multi"}
2014-07-01 07:42:40,711 WARN  [RpcServer.handler=49,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":29376,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.58:33337","starttimems":1404225731335,"queuetimems":6716,"class":"HRegionServer","responsesize":10477,"method":"Multi"}
2014-07-01 07:42:40,712 WARN  [RpcServer.handler=3,port=60020] ipc.RpcServer: RpcServer.respondercallId: 3534 service: ClientService methodName: Multi size: 1.1m connection: 9.1.143.58:33337: output error
2014-07-01 07:42:40,713 WARN  [RpcServer.handler=3,port=60020] ipc.RpcServer: RpcServer.handler=3,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-07-01 07:42:40,713 WARN  [RpcServer.handler=49,port=60020] ipc.RpcServer: RpcServer.respondercallId: 3497 service: ClientService methodName: Multi size: 1.8m connection: 9.1.143.58:33337: output error
2014-07-01 07:42:40,713 WARN  [RpcServer.handler=49,port=60020] ipc.RpcServer: RpcServer.handler=49,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-07-01 07:42:40,824 WARN  [RpcServer.handler=1,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":29475,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.58:33337","starttimems":1404225731349,"queuetimems":6347,"class":"HRegionServer","responsesize":13535,"method":"Multi"}
2014-07-01 07:42:40,825 WARN  [RpcServer.handler=1,port=60020] ipc.RpcServer: RpcServer.respondercallId: 3536 service: ClientService methodName: Multi size: 2.4m connection: 9.1.143.58:33337: output error
2014-07-01 07:42:40,825 WARN  [RpcServer.handler=1,port=60020] ipc.RpcServer: RpcServer.handler=1,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-07-01 07:42:40,827 WARN  [RpcServer.handler=28,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":29476,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.58:33337","starttimems":1404225731350,"queuetimems":6364,"class":"HRegionServer","responsesize":13631,"method":"Multi"}
2014-07-01 07:42:40,827 WARN  [RpcServer.handler=28,port=60020] ipc.RpcServer: RpcServer.respondercallId: 3537 service: ClientService methodName: Multi size: 2.4m connection: 9.1.143.58:33337: output error
2014-07-01 07:42:40,827 WARN  [RpcServer.handler=28,port=60020] ipc.RpcServer: RpcServer.handler=28,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-07-01 07:42:40,828 WARN  [RpcServer.handler=10,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":28928,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.58:33337","starttimems":1404225731900,"queuetimems":6718,"class":"HRegionServer","responsesize":13633,"method":"Multi"}
2014-07-01 07:42:40,828 WARN  [RpcServer.handler=17,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":29479,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.58:33337","starttimems":1404225731349,"queuetimems":6333,"class":"HRegionServer","responsesize":12848,"method":"Multi"}
2014-07-01 07:42:40,829 WARN  [RpcServer.handler=10,port=60020] ipc.RpcServer: RpcServer.respondercallId: 3530 service: ClientService methodName: Multi size: 2.4m connection: 9.1.143.58:33337: output error
2014-07-01 07:42:40,829 WARN  [RpcServer.handler=10,port=60020] ipc.RpcServer: RpcServer.handler=10,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-07-01 07:42:40,829 WARN  [RpcServer.handler=17,port=60020] ipc.RpcServer: RpcServer.respondercallId: 3535 service: ClientService methodName: Multi size: 2.2m connection: 9.1.143.58:33337: output error
2014-07-01 07:42:40,829 WARN  [RpcServer.handler=17,port=60020] ipc.RpcServer: RpcServer.handler=17,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-07-01 07:42:40,830 WARN  [RpcServer.handler=22,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":29498,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.58:33337","starttimems":1404225731332,"queuetimems":6752,"class":"HRegionServer","responsesize":13294,"method":"Multi"}
2014-07-01 07:42:40,831 WARN  [RpcServer.handler=22,port=60020] ipc.RpcServer: RpcServer.respondercallId: 3503 service: ClientService methodName: Multi size: 2.3m connection: 9.1.143.58:33337: output error
2014-07-01 07:42:40,831 WARN  [RpcServer.handler=22,port=60020] ipc.RpcServer: RpcServer.handler=22,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-07-01 07:42:40,836 WARN  [RpcServer.handler=4,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":28539,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.58:33337","starttimems":1404225732297,"queuetimems":6949,"class":"HRegionServer","responsesize":13695,"method":"Multi"}
2014-07-01 07:42:40,837 WARN  [RpcServer.handler=4,port=60020] ipc.RpcServer: RpcServer.respondercallId: 3527 service: ClientService methodName: Multi size: 2.4m connection: 9.1.143.58:33337: output error
2014-07-01 07:42:40,837 WARN  [RpcServer.handler=4,port=60020] ipc.RpcServer: RpcServer.handler=4,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-07-01 07:42:40,838 WARN  [RpcServer.handler=20,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":29509,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.58:33337","starttimems":1404225731328,"queuetimems":6951,"class":"HRegionServer","responsesize":9219,"method":"Multi"}
2014-07-01 07:42:40,838 WARN  [RpcServer.handler=20,port=60020] ipc.RpcServer: RpcServer.respondercallId: 3506 service: ClientService methodName: Multi size: 1.6m connection: 9.1.143.58:33337: output error
2014-07-01 07:42:40,838 WARN  [RpcServer.handler=20,port=60020] ipc.RpcServer: RpcServer.handler=20,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-07-01 07:42:40,845 WARN  [RpcServer.handler=7,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":29142,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.58:33337","starttimems":1404225731702,"queuetimems":6554,"class":"HRegionServer","responsesize":13271,"method":"Multi"}
2014-07-01 07:42:40,845 WARN  [RpcServer.handler=7,port=60020] ipc.RpcServer: RpcServer.respondercallId: 3533 service: ClientService methodName: Multi size: 2.3m connection: 9.1.143.58:33337: output error
2014-07-01 07:42:40,845 WARN  [RpcServer.handler=7,port=60020] ipc.RpcServer: RpcServer.handler=7,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-07-01 07:42:40,848 WARN  [RpcServer.handler=6,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":28937,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.58:33337","starttimems":1404225731911,"queuetimems":6716,"class":"HRegionServer","responsesize":13421,"method":"Multi"}
2014-07-01 07:42:40,849 WARN  [RpcServer.handler=6,port=60020] ipc.RpcServer: RpcServer.respondercallId: 3529 service: ClientService methodName: Multi size: 2.3m connection: 9.1.143.58:33337: output error
2014-07-01 07:42:40,849 WARN  [RpcServer.handler=6,port=60020] ipc.RpcServer: RpcServer.handler=6,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-07-01 07:42:40,852 WARN  [RpcServer.handler=24,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":29524,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.58:33337","starttimems":1404225731327,"queuetimems":7097,"class":"HRegionServer","responsesize":13494,"method":"Multi"}
2014-07-01 07:42:40,852 WARN  [RpcServer.handler=24,port=60020] ipc.RpcServer: RpcServer.respondercallId: 3511 service: ClientService methodName: Multi size: 2.4m connection: 9.1.143.58:33337: output error
2014-07-01 07:42:40,852 WARN  [RpcServer.handler=24,port=60020] ipc.RpcServer: RpcServer.handler=24,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-07-01 07:42:40,856 WARN  [RpcServer.handler=13,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":29505,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.58:33337","starttimems":1404225731350,"queuetimems":6985,"class":"HRegionServer","responsesize":13275,"method":"Multi"}
2014-07-01 07:42:40,856 WARN  [RpcServer.handler=13,port=60020] ipc.RpcServer: RpcServer.respondercallId: 3509 service: ClientService methodName: Multi size: 2.3m connection: 9.1.143.58:33337: output error
2014-07-01 07:42:40,856 WARN  [RpcServer.handler=13,port=60020] ipc.RpcServer: RpcServer.handler=13,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-07-01 07:42:40,860 WARN  [RpcServer.handler=15,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":29517,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.58:33337","starttimems":1404225731342,"queuetimems":6548,"class":"HRegionServer","responsesize":13665,"method":"Multi"}
2014-07-01 07:42:40,860 WARN  [RpcServer.handler=45,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":28960,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.58:33337","starttimems":1404225731900,"queuetimems":6731,"class":"HRegionServer","responsesize":13203,"method":"Multi"}
2014-07-01 07:42:40,861 WARN  [RpcServer.handler=35,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":29159,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.58:33337","starttimems":1404225731702,"queuetimems":6546,"class":"HRegionServer","responsesize":11651,"method":"Multi"}
2014-07-01 07:42:40,862 WARN  [RpcServer.handler=37,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":29517,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.58:33337","starttimems":1404225731344,"queuetimems":6527,"class":"HRegionServer","responsesize":13540,"method":"Multi"}
2014-07-01 07:42:40,836 WARN  [RpcServer.handler=23,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":28725,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.58:33337","starttimems":1404225732111,"queuetimems":6901,"class":"HRegionServer","responsesize":13577,"method":"Multi"}
2014-07-01 07:42:40,860 WARN  [RpcServer.handler=15,port=60020] ipc.RpcServer: RpcServer.respondercallId: 3488 service: ClientService methodName: Multi size: 2.4m connection: 9.1.143.58:33337: output error
2014-07-01 07:42:40,860 WARN  [RpcServer.handler=43,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":29512,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.58:33337","starttimems":1404225731347,"queuetimems":6508,"class":"HRegionServer","responsesize":13397,"method":"Multi"}
2014-07-01 07:42:40,860 WARN  [RpcServer.handler=18,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":29512,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.58:33337","starttimems":1404225731347,"queuetimems":6520,"class":"HRegionServer","responsesize":13382,"method":"Multi"}
2014-07-01 07:42:40,868 WARN  [RpcServer.handler=37,port=60020] ipc.RpcServer: RpcServer.respondercallId: 3484 service: ClientService methodName: Multi size: 2.4m connection: 9.1.143.58:33337: output error
2014-07-01 07:42:40,868 WARN  [RpcServer.handler=37,port=60020] ipc.RpcServer: RpcServer.handler=37,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-07-01 07:42:40,868 WARN  [RpcServer.handler=35,port=60020] ipc.RpcServer: RpcServer.respondercallId: 3532 service: ClientService methodName: Multi size: 2.0m connection: 9.1.143.58:33337: output error
2014-07-01 07:42:40,868 WARN  [RpcServer.handler=35,port=60020] ipc.RpcServer: RpcServer.handler=35,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-07-01 07:42:40,868 WARN  [RpcServer.handler=15,port=60020] ipc.RpcServer: RpcServer.handler=15,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-07-01 07:42:40,891 WARN  [RpcServer.handler=45,port=60020] ipc.RpcServer: RpcServer.respondercallId: 3531 service: ClientService methodName: Multi size: 2.3m connection: 9.1.143.58:33337: output error
2014-07-01 07:42:40,891 WARN  [RpcServer.handler=45,port=60020] ipc.RpcServer: RpcServer.handler=45,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-07-01 07:42:40,891 WARN  [RpcServer.handler=43,port=60020] ipc.RpcServer: RpcServer.respondercallId: 3538 service: ClientService methodName: Multi size: 2.3m connection: 9.1.143.58:33337: output error
2014-07-01 07:42:40,891 WARN  [RpcServer.handler=43,port=60020] ipc.RpcServer: RpcServer.handler=43,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-07-01 07:42:40,891 WARN  [RpcServer.handler=18,port=60020] ipc.RpcServer: RpcServer.respondercallId: 3539 service: ClientService methodName: Multi size: 2.3m connection: 9.1.143.58:33337: output error
2014-07-01 07:42:40,892 WARN  [RpcServer.handler=18,port=60020] ipc.RpcServer: RpcServer.handler=18,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-07-01 07:42:41,019 WARN  [RpcServer.handler=23,port=60020] ipc.RpcServer: RpcServer.respondercallId: 3528 service: ClientService methodName: Multi size: 2.4m connection: 9.1.143.58:33337: output error
2014-07-01 07:42:41,019 WARN  [RpcServer.handler=23,port=60020] ipc.RpcServer: RpcServer.handler=23,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-07-01 07:42:41,402 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-01 07:42:41,416 WARN  [RpcServer.handler=30,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":14906,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.58:33337","starttimems":1404225746510,"queuetimems":21152,"class":"HRegionServer","responsesize":13374,"method":"Multi"}
2014-07-01 07:42:41,417 WARN  [RpcServer.handler=30,port=60020] ipc.RpcServer: RpcServer.respondercallId: 3526 service: ClientService methodName: Multi size: 2.3m connection: 9.1.143.58:33337: output error
2014-07-01 07:42:41,417 WARN  [RpcServer.handler=30,port=60020] ipc.RpcServer: RpcServer.handler=30,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-07-01 07:42:41,509 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 5844 synced till here 5813
2014-07-01 07:42:43,722 INFO  [JvmPauseMonitor] util.JvmPauseMonitor: Detected pause in JVM or host machine (eg GC): pause of approximately 1060ms
GC pool 'ParNew' had collection(s): count=1 time=1346ms
2014-07-01 07:42:44,493 INFO  [MemStoreFlusher.0] regionserver.DefaultStoreFlusher: Flushed, sequenceid=1155, memsize=279.0m, hasBloomFilter=true, into tmp file hdfs://master:54310/hbase/data/default/usertable/89cb00f0096b305c3317b7753d66adf1/.tmp/3accfe0de7a347c88e7a4aa33a6a8895
2014-07-01 07:42:44,522 DEBUG [MemStoreFlusher.0] regionserver.HRegionFileSystem: Committing store file hdfs://master:54310/hbase/data/default/usertable/89cb00f0096b305c3317b7753d66adf1/.tmp/3accfe0de7a347c88e7a4aa33a6a8895 as hdfs://master:54310/hbase/data/default/usertable/89cb00f0096b305c3317b7753d66adf1/family/3accfe0de7a347c88e7a4aa33a6a8895
2014-07-01 07:42:44,535 INFO  [MemStoreFlusher.0] regionserver.HStore: Added hdfs://master:54310/hbase/data/default/usertable/89cb00f0096b305c3317b7753d66adf1/family/3accfe0de7a347c88e7a4aa33a6a8895, entries=1015960, sequenceid=1155, filesize=72.4m
2014-07-01 07:42:44,536 INFO  [MemStoreFlusher.0] regionserver.HRegion: Finished memstore flush of ~386.8m/405624480, currentsize=392.9m/412037360 for region usertable,user6,1404225423342.89cb00f0096b305c3317b7753d66adf1. in 54478ms, sequenceid=1155, compaction requested=false
2014-07-01 07:42:44,536 DEBUG [MemStoreFlusher.0] regionserver.HRegion: Started memstore flush for usertable,user2,1404225423342.07064f253c865a7cc0c08893a4f56af9., current region memstore size 593.1m
2014-07-01 07:42:44,582 DEBUG [regionserver60020-smallCompactions-1404225561807] regionserver.HRegionFileSystem: Committing store file hdfs://master:54310/hbase/data/default/usertable/89cb00f0096b305c3317b7753d66adf1/.tmp/b0c123f16c7546dcb79d7654ffd58acc as hdfs://master:54310/hbase/data/default/usertable/89cb00f0096b305c3317b7753d66adf1/family/b0c123f16c7546dcb79d7654ffd58acc
2014-07-01 07:42:45,060 DEBUG [RpcServer.handler=44,port=60020] regionserver.HRegion: Flush requested on usertable,user6,1404225423342.89cb00f0096b305c3317b7753d66adf1.
2014-07-01 07:42:45,062 WARN  [RpcServer.handler=44,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":16835,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.58:33505","starttimems":1404225748226,"queuetimems":5,"class":"HRegionServer","responsesize":10169,"method":"Multi"}
2014-07-01 07:42:45,064 WARN  [RpcServer.handler=14,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":16805,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.58:33505","starttimems":1404225748258,"queuetimems":0,"class":"HRegionServer","responsesize":13394,"method":"Multi"}
2014-07-01 07:42:45,064 WARN  [RpcServer.handler=41,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":16408,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.58:33505","starttimems":1404225748655,"queuetimems":1,"class":"HRegionServer","responsesize":13421,"method":"Multi"}
2014-07-01 07:42:45,065 WARN  [RpcServer.handler=9,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":15319,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.58:33505","starttimems":1404225749745,"queuetimems":18,"class":"HRegionServer","responsesize":11651,"method":"Multi"}
2014-07-01 07:42:45,239 WARN  [RpcServer.handler=34,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":16687,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.58:33505","starttimems":1404225748551,"queuetimems":3,"class":"HRegionServer","responsesize":13674,"method":"Multi"}
2014-07-01 07:42:45,240 WARN  [RpcServer.handler=33,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":14219,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.58:33505","starttimems":1404225751021,"queuetimems":0,"class":"HRegionServer","responsesize":13203,"method":"Multi"}
2014-07-01 07:42:45,242 WARN  [RpcServer.handler=47,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":14405,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.58:33505","starttimems":1404225750836,"queuetimems":19,"class":"HRegionServer","responsesize":13494,"method":"Multi"}
2014-07-01 07:42:45,252 WARN  [RpcServer.handler=36,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":15535,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.58:33505","starttimems":1404225749717,"queuetimems":1,"class":"HRegionServer","responsesize":13275,"method":"Multi"}
2014-07-01 07:42:45,256 WARN  [RpcServer.handler=11,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":17062,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.58:33505","starttimems":1404225748193,"queuetimems":1,"class":"HRegionServer","responsesize":13695,"method":"Multi"}
2014-07-01 07:42:45,257 WARN  [RpcServer.handler=19,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":15422,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.58:33505","starttimems":1404225749834,"queuetimems":1,"class":"HRegionServer","responsesize":13337,"method":"Multi"}
2014-07-01 07:42:45,265 WARN  [RpcServer.handler=16,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":14031,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.58:33505","starttimems":1404225751234,"queuetimems":1,"class":"HRegionServer","responsesize":13294,"method":"Multi"}
2014-07-01 07:42:45,288 DEBUG [regionserver60020-smallCompactions-1404225561807] regionserver.HStore: Removing store files after compaction...
2014-07-01 07:42:45,292 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1404225098235/slave1%2C60020%2C1404225098235.1404225751659 with entries=100, filesize=89.3m; new WAL /hbase/WALs/slave1,60020,1404225098235/slave1%2C60020%2C1404225098235.1404225761403
2014-07-01 07:42:45,298 DEBUG [regionserver60020-smallCompactions-1404225561807] backup.HFileArchiver: Finished archiving from class org.apache.hadoop.hbase.backup.HFileArchiver$FileableStoreFile, file:hdfs://master:54310/hbase/data/default/usertable/89cb00f0096b305c3317b7753d66adf1/family/aaa762fa710f487890b087b543a2e26f, to hdfs://master:54310/hbase/archive/data/default/usertable/89cb00f0096b305c3317b7753d66adf1/family/aaa762fa710f487890b087b543a2e26f
2014-07-01 07:42:45,302 DEBUG [regionserver60020-smallCompactions-1404225561807] backup.HFileArchiver: Finished archiving from class org.apache.hadoop.hbase.backup.HFileArchiver$FileableStoreFile, file:hdfs://master:54310/hbase/data/default/usertable/89cb00f0096b305c3317b7753d66adf1/family/79f30d7a60d941c18054ddab01f26c6c, to hdfs://master:54310/hbase/archive/data/default/usertable/89cb00f0096b305c3317b7753d66adf1/family/79f30d7a60d941c18054ddab01f26c6c
2014-07-01 07:42:45,306 DEBUG [regionserver60020-smallCompactions-1404225561807] backup.HFileArchiver: Finished archiving from class org.apache.hadoop.hbase.backup.HFileArchiver$FileableStoreFile, file:hdfs://master:54310/hbase/data/default/usertable/89cb00f0096b305c3317b7753d66adf1/family/d342f65c4ae34d8ca1a93b4dae5733a5, to hdfs://master:54310/hbase/archive/data/default/usertable/89cb00f0096b305c3317b7753d66adf1/family/d342f65c4ae34d8ca1a93b4dae5733a5
2014-07-01 07:42:45,306 INFO  [regionserver60020-smallCompactions-1404225561807] regionserver.HStore: Completed major compaction of 3 file(s) in family of usertable,user6,1404225423342.89cb00f0096b305c3317b7753d66adf1. into b0c123f16c7546dcb79d7654ffd58acc(size=120.4m), total size for store is 192.8m. This selection was in queue for 0sec, and took 1mins, 4sec to execute.
2014-07-01 07:42:45,307 INFO  [regionserver60020-smallCompactions-1404225561807] regionserver.CompactSplitThread: Completed compaction: Request = regionName=usertable,user6,1404225423342.89cb00f0096b305c3317b7753d66adf1., storeName=family, fileCount=3, fileSize=137.9m, priority=17, time=45811746822054; duration=1mins, 4sec
2014-07-01 07:42:45,307 DEBUG [regionserver60020-smallCompactions-1404225561807] regionserver.CompactSplitThread: CompactSplitThread Status: compaction_queue=(0:0), split_queue=0, merge_queue=0
2014-07-01 07:42:47,622 INFO  [JvmPauseMonitor] util.JvmPauseMonitor: Detected pause in JVM or host machine (eg GC): pause of approximately 1082ms
GC pool 'ParNew' had collection(s): count=1 time=1424ms
2014-07-01 07:42:49,947 INFO  [JvmPauseMonitor] util.JvmPauseMonitor: Detected pause in JVM or host machine (eg GC): pause of approximately 1824ms
GC pool 'ParNew' had collection(s): count=1 time=2166ms
2014-07-01 07:42:51,362 WARN  [RpcServer.handler=5,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":19697,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.58:33505","starttimems":1404225751664,"queuetimems":334,"class":"HRegionServer","responsesize":13510,"method":"Multi"}
2014-07-01 07:42:51,372 WARN  [RpcServer.handler=39,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":19209,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.58:33505","starttimems":1404225752163,"queuetimems":0,"class":"HRegionServer","responsesize":7027,"method":"Multi"}
2014-07-01 07:42:51,372 WARN  [RpcServer.handler=8,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":19709,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.58:33505","starttimems":1404225751663,"queuetimems":377,"class":"HRegionServer","responsesize":10477,"method":"Multi"}
2014-07-01 07:42:51,388 WARN  [RpcServer.handler=26,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":19245,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.58:33505","starttimems":1404225752116,"queuetimems":0,"class":"HRegionServer","responsesize":13514,"method":"Multi"}
2014-07-01 07:42:51,390 WARN  [RpcServer.handler=29,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":19571,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.58:33505","starttimems":1404225751800,"queuetimems":0,"class":"HRegionServer","responsesize":13254,"method":"Multi"}
2014-07-01 07:42:51,390 WARN  [RpcServer.handler=0,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":19626,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.58:33505","starttimems":1404225751763,"queuetimems":138,"class":"HRegionServer","responsesize":13589,"method":"Multi"}
2014-07-01 07:42:51,396 WARN  [RpcServer.handler=46,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":19487,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.58:33505","starttimems":1404225751907,"queuetimems":1,"class":"HRegionServer","responsesize":13204,"method":"Multi"}
2014-07-01 07:42:51,397 WARN  [RpcServer.handler=42,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":19725,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.58:33505","starttimems":1404225751665,"queuetimems":110,"class":"HRegionServer","responsesize":13374,"method":"Multi"}
2014-07-01 07:42:51,409 WARN  [RpcServer.handler=2,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":19614,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.58:33505","starttimems":1404225751762,"queuetimems":259,"class":"HRegionServer","responsesize":13339,"method":"Multi"}
2014-07-01 07:42:55,901 WARN  [RpcServer.handler=27,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":16833,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.58:33505","starttimems":1404225759067,"queuetimems":3040,"class":"HRegionServer","responsesize":8030,"method":"Multi"}
2014-07-01 07:42:56,815 WARN  [RpcServer.handler=40,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":17820,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.58:33505","starttimems":1404225758995,"queuetimems":3649,"class":"HRegionServer","responsesize":13693,"method":"Multi"}
2014-07-01 07:42:56,816 WARN  [RpcServer.handler=21,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":17821,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.58:33505","starttimems":1404225758994,"queuetimems":3673,"class":"HRegionServer","responsesize":13727,"method":"Multi"}
2014-07-01 07:42:56,816 WARN  [RpcServer.handler=12,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":17821,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.58:33505","starttimems":1404225758994,"queuetimems":3683,"class":"HRegionServer","responsesize":11940,"method":"Multi"}
2014-07-01 07:42:56,822 WARN  [RpcServer.handler=31,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":17808,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.58:33505","starttimems":1404225759008,"queuetimems":3614,"class":"HRegionServer","responsesize":12977,"method":"Multi"}
2014-07-01 07:42:56,823 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-01 07:42:56,824 WARN  [RpcServer.handler=32,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":17764,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.58:33505","starttimems":1404225759059,"queuetimems":3056,"class":"HRegionServer","responsesize":13489,"method":"Multi"}
2014-07-01 07:42:56,824 WARN  [RpcServer.handler=48,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":17804,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.58:33505","starttimems":1404225759020,"queuetimems":3573,"class":"HRegionServer","responsesize":13130,"method":"Multi"}
2014-07-01 07:42:56,825 WARN  [RpcServer.handler=25,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":17830,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.58:33505","starttimems":1404225758993,"queuetimems":6760,"class":"HRegionServer","responsesize":13523,"method":"Multi"}
2014-07-01 07:42:56,826 WARN  [RpcServer.handler=13,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":15969,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.58:33505","starttimems":1404225760856,"queuetimems":3483,"class":"HRegionServer","responsesize":9219,"method":"Multi"}
2014-07-01 07:42:56,826 WARN  [RpcServer.handler=22,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":15994,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.58:33505","starttimems":1404225760831,"queuetimems":4379,"class":"HRegionServer","responsesize":13436,"method":"Multi"}
2014-07-01 07:42:56,826 WARN  [RpcServer.handler=28,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":15999,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.58:33505","starttimems":1404225760827,"queuetimems":4678,"class":"HRegionServer","responsesize":13665,"method":"Multi"}
2014-07-01 07:42:56,828 WARN  [RpcServer.handler=43,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":15936,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.58:33505","starttimems":1404225760891,"queuetimems":2735,"class":"HRegionServer","responsesize":13694,"method":"Multi"}
2014-07-01 07:42:56,825 WARN  [RpcServer.handler=17,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":15995,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.58:33505","starttimems":1404225760829,"queuetimems":4473,"class":"HRegionServer","responsesize":6175,"method":"Multi"}
2014-07-01 07:42:56,828 WARN  [RpcServer.handler=35,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":15955,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.58:33505","starttimems":1404225760869,"queuetimems":3448,"class":"HRegionServer","responsesize":13005,"method":"Multi"}
2014-07-01 07:42:57,298 WARN  [RpcServer.handler=18,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":16406,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.58:33505","starttimems":1404225760892,"queuetimems":2685,"class":"HRegionServer","responsesize":13550,"method":"Multi"}
2014-07-01 07:42:57,299 WARN  [RpcServer.handler=6,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":16449,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.58:33505","starttimems":1404225760849,"queuetimems":3532,"class":"HRegionServer","responsesize":12848,"method":"Multi"}
2014-07-01 07:42:57,299 WARN  [RpcServer.handler=3,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":16585,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.58:33505","starttimems":1404225760713,"queuetimems":4647,"class":"HRegionServer","responsesize":13543,"method":"Multi"}
2014-07-01 07:42:57,305 WARN  [RpcServer.handler=23,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":16286,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.58:33505","starttimems":1404225761019,"queuetimems":2779,"class":"HRegionServer","responsesize":12867,"method":"Multi"}
2014-07-01 07:42:57,311 WARN  [RpcServer.handler=15,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":16436,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.58:33505","starttimems":1404225760875,"queuetimems":2821,"class":"HRegionServer","responsesize":13237,"method":"Multi"}
2014-07-01 07:42:57,311 WARN  [RpcServer.handler=10,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":16482,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.58:33505","starttimems":1404225760829,"queuetimems":4483,"class":"HRegionServer","responsesize":13178,"method":"Multi"}
2014-07-01 07:42:57,316 WARN  [RpcServer.handler=45,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":16424,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.58:33505","starttimems":1404225760891,"queuetimems":2783,"class":"HRegionServer","responsesize":13633,"method":"Multi"}
2014-07-01 07:42:57,319 WARN  [RpcServer.handler=4,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":16482,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.58:33505","starttimems":1404225760837,"queuetimems":4152,"class":"HRegionServer","responsesize":13540,"method":"Multi"}
2014-07-01 07:42:57,323 WARN  [RpcServer.handler=1,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":16497,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.58:33505","starttimems":1404225760825,"queuetimems":4723,"class":"HRegionServer","responsesize":13497,"method":"Multi"}
2014-07-01 07:42:57,325 WARN  [RpcServer.handler=7,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":16478,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.58:33505","starttimems":1404225760845,"queuetimems":3642,"class":"HRegionServer","responsesize":13271,"method":"Multi"}
2014-07-01 07:42:57,376 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 5958 synced till here 5942
2014-07-01 07:42:59,735 INFO  [JvmPauseMonitor] util.JvmPauseMonitor: Detected pause in JVM or host machine (eg GC): pause of approximately 1108ms
GC pool 'ParNew' had collection(s): count=1 time=1459ms
2014-07-01 07:43:01,466 INFO  [JvmPauseMonitor] util.JvmPauseMonitor: Detected pause in JVM or host machine (eg GC): pause of approximately 1230ms
GC pool 'ParNew' had collection(s): count=1 time=1442ms
2014-07-01 07:43:01,805 DEBUG [MemStoreFlusher.0] util.FSUtils: DFS Client does not support most favored nodes create; using default create
2014-07-01 07:43:02,704 WARN  [RpcServer.handler=20,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":21865,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.58:33505","starttimems":1404225760838,"queuetimems":3817,"class":"HRegionServer","responsesize":11784,"method":"Multi"}
2014-07-01 07:43:02,744 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1404225098235/slave1%2C60020%2C1404225098235.1404225761403 with entries=114, filesize=98.9m; new WAL /hbase/WALs/slave1,60020,1404225098235/slave1%2C60020%2C1404225098235.1404225776823
2014-07-01 07:43:05,183 INFO  [JvmPauseMonitor] util.JvmPauseMonitor: Detected pause in JVM or host machine (eg GC): pause of approximately 2120ms
GC pool 'ParNew' had collection(s): count=1 time=2428ms
2014-07-01 07:43:06,778 INFO  [JvmPauseMonitor] util.JvmPauseMonitor: Detected pause in JVM or host machine (eg GC): pause of approximately 1094ms
GC pool 'ParNew' had collection(s): count=1 time=1316ms
2014-07-01 07:43:08,431 WARN  [RpcServer.handler=30,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":26990,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.58:33505","starttimems":1404225761417,"queuetimems":2420,"class":"HRegionServer","responsesize":13577,"method":"Multi"}
2014-07-01 07:43:08,982 WARN  [RpcServer.handler=14,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":23916,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.58:33505","starttimems":1404225765065,"queuetimems":4800,"class":"HRegionServer","responsesize":13752,"method":"Multi"}
2014-07-01 07:43:08,995 WARN  [RpcServer.handler=34,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":23740,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.58:33505","starttimems":1404225765254,"queuetimems":4582,"class":"HRegionServer","responsesize":8030,"method":"Multi"}
2014-07-01 07:43:08,995 WARN  [RpcServer.handler=11,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":23738,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.58:33505","starttimems":1404225765257,"queuetimems":4572,"class":"HRegionServer","responsesize":7027,"method":"Multi"}
2014-07-01 07:43:08,999 WARN  [RpcServer.handler=49,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":23922,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.58:33505","starttimems":1404225765061,"queuetimems":5960,"class":"HRegionServer","responsesize":9005,"method":"Multi"}
2014-07-01 07:43:08,999 WARN  [RpcServer.handler=24,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":23759,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.58:33505","starttimems":1404225765240,"queuetimems":4711,"class":"HRegionServer","responsesize":13631,"method":"Multi"}
2014-07-01 07:43:09,000 WARN  [RpcServer.handler=33,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":23759,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.58:33505","starttimems":1404225765241,"queuetimems":4623,"class":"HRegionServer","responsesize":6175,"method":"Multi"}
2014-07-01 07:43:10,171 WARN  [RpcServer.handler=41,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":23929,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.58:33505","starttimems":1404225765065,"queuetimems":4774,"class":"HRegionServer","responsesize":13382,"method":"Multi"}
2014-07-01 07:43:10,173 INFO  [MemStoreFlusher.1] regionserver.DefaultStoreFlusher: Flushed, sequenceid=1164, memsize=308.0m, hasBloomFilter=true, into tmp file hdfs://master:54310/hbase/data/default/usertable/c7604442b843793f8451582c14f6bf59/.tmp/f71ce0ee32524c36b94cd1b4c15da3d1
2014-07-01 07:43:10,242 DEBUG [MemStoreFlusher.1] regionserver.HRegionFileSystem: Committing store file hdfs://master:54310/hbase/data/default/usertable/c7604442b843793f8451582c14f6bf59/.tmp/f71ce0ee32524c36b94cd1b4c15da3d1 as hdfs://master:54310/hbase/data/default/usertable/c7604442b843793f8451582c14f6bf59/family/f71ce0ee32524c36b94cd1b4c15da3d1
2014-07-01 07:43:11,011 INFO  [MemStoreFlusher.1] regionserver.HStore: Added hdfs://master:54310/hbase/data/default/usertable/c7604442b843793f8451582c14f6bf59/family/f71ce0ee32524c36b94cd1b4c15da3d1, entries=1121350, sequenceid=1164, filesize=79.9m
2014-07-01 07:43:11,012 INFO  [MemStoreFlusher.1] regionserver.HRegion: Finished memstore flush of ~432.5m/453475200, currentsize=487.3m/510964240 for region usertable,user5,1404225423342.c7604442b843793f8451582c14f6bf59. in 81068ms, sequenceid=1164, compaction requested=false
2014-07-01 07:43:11,016 DEBUG [MemStoreFlusher.1] regionserver.HRegion: Started memstore flush for usertable,user6,1404225423342.89cb00f0096b305c3317b7753d66adf1., current region memstore size 478.8m
2014-07-01 07:43:11,981 DEBUG [RpcServer.handler=27,port=60020] regionserver.HRegion: Flush requested on usertable,user5,1404225423342.c7604442b843793f8451582c14f6bf59.
2014-07-01 07:43:12,196 WARN  [RpcServer.handler=38,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":24476,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.58:33505","starttimems":1404225767719,"queuetimems":7012,"class":"HRegionServer","responsesize":11651,"method":"Multi"}
2014-07-01 07:43:12,196 WARN  [RpcServer.handler=9,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":27129,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.58:33505","starttimems":1404225765066,"queuetimems":4603,"class":"HRegionServer","responsesize":13535,"method":"Multi"}
2014-07-01 07:43:12,196 WARN  [RpcServer.handler=5,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":20832,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.58:33505","starttimems":1404225771363,"queuetimems":10534,"class":"HRegionServer","responsesize":10477,"method":"Multi"}
2014-07-01 07:43:12,215 WARN  [RpcServer.handler=29,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":20824,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.58:33505","starttimems":1404225771390,"queuetimems":7634,"class":"HRegionServer","responsesize":13337,"method":"Multi"}
2014-07-01 07:43:12,215 WARN  [RpcServer.handler=19,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":18801,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.58:33505","starttimems":1404225773414,"queuetimems":8968,"class":"HRegionServer","responsesize":13254,"method":"Multi"}
2014-07-01 07:43:12,215 WARN  [RpcServer.handler=46,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":20815,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.58:33505","starttimems":1404225771399,"queuetimems":7568,"class":"HRegionServer","responsesize":13275,"method":"Multi"}
2014-07-01 07:43:12,216 WARN  [RpcServer.handler=8,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":20841,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.58:33505","starttimems":1404225771373,"queuetimems":9067,"class":"HRegionServer","responsesize":13374,"method":"Multi"}
2014-07-01 07:43:12,221 WARN  [RpcServer.handler=36,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":26967,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.58:33505","starttimems":1404225765253,"queuetimems":4593,"class":"HRegionServer","responsesize":13695,"method":"Multi"}
2014-07-01 07:43:12,224 WARN  [RpcServer.handler=16,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":18796,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.58:33505","starttimems":1404225773419,"queuetimems":8951,"class":"HRegionServer","responsesize":13674,"method":"Multi"}
2014-07-01 07:43:12,229 WARN  [RpcServer.handler=0,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":20831,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.58:33505","starttimems":1404225771390,"queuetimems":7585,"class":"HRegionServer","responsesize":13510,"method":"Multi"}
2014-07-01 07:43:12,229 WARN  [RpcServer.handler=2,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":20805,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.58:33505","starttimems":1404225771416,"queuetimems":7543,"class":"HRegionServer","responsesize":13204,"method":"Multi"}
2014-07-01 07:43:12,230 WARN  [RpcServer.handler=39,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":20847,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.58:33505","starttimems":1404225771373,"queuetimems":9098,"class":"HRegionServer","responsesize":13514,"method":"Multi"}
2014-07-01 07:43:12,231 WARN  [RpcServer.handler=47,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":20853,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.58:33505","starttimems":1404225771367,"queuetimems":9147,"class":"HRegionServer","responsesize":13339,"method":"Multi"}
2014-07-01 07:43:12,236 WARN  [RpcServer.handler=26,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":20830,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.58:33505","starttimems":1404225771390,"queuetimems":7644,"class":"HRegionServer","responsesize":13589,"method":"Multi"}
2014-07-01 07:43:12,244 WARN  [RpcServer.handler=42,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":20819,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.58:33505","starttimems":1404225771401,"queuetimems":7550,"class":"HRegionServer","responsesize":13494,"method":"Multi"}
2014-07-01 07:43:12,347 WARN  [RpcServer.handler=37,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":26981,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.58:33505","starttimems":1404225765240,"queuetimems":4753,"class":"HRegionServer","responsesize":13397,"method":"Multi"}
2014-07-01 07:43:12,356 WARN  [RpcServer.handler=44,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":20831,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.58:33505","starttimems":1404225771364,"queuetimems":10501,"class":"HRegionServer","responsesize":10169,"method":"Multi"}
2014-07-01 07:43:13,954 DEBUG [MemStoreFlusher.1] util.FSUtils: DFS Client does not support most favored nodes create; using default create
2014-07-01 07:43:14,081 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-01 07:43:14,086 WARN  [RpcServer.handler=27,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":18181,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.58:33505","starttimems":1404225775903,"queuetimems":11403,"class":"HRegionServer","responsesize":13203,"method":"Multi"}
2014-07-01 07:43:15,197 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 6062 synced till here 6042
2014-07-01 07:43:16,390 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1404225098235/slave1%2C60020%2C1404225098235.1404225776823 with entries=104, filesize=92.2m; new WAL /hbase/WALs/slave1,60020,1404225098235/slave1%2C60020%2C1404225098235.1404225794082
2014-07-01 07:43:16,391 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1404225098235/slave1%2C60020%2C1404225098235.1404225642711
2014-07-01 07:43:16,428 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1404225098235/slave1%2C60020%2C1404225098235.1404225649895
2014-07-01 07:43:16,428 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1404225098235/slave1%2C60020%2C1404225098235.1404225656019
2014-07-01 07:43:16,429 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1404225098235/slave1%2C60020%2C1404225098235.1404225659164
2014-07-01 07:43:16,429 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1404225098235/slave1%2C60020%2C1404225098235.1404225669200
2014-07-01 07:43:16,429 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1404225098235/slave1%2C60020%2C1404225098235.1404225677414
2014-07-01 07:43:16,429 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1404225098235/slave1%2C60020%2C1404225098235.1404225685291
2014-07-01 07:43:19,784 WARN  [RpcServer.handler=45,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":22467,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.58:33505","starttimems":1404225777316,"queuetimems":9600,"class":"HRegionServer","responsesize":13497,"method":"Multi"}
2014-07-01 07:43:19,784 WARN  [RpcServer.handler=43,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":22440,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.58:33505","starttimems":1404225777343,"queuetimems":7304,"class":"HRegionServer","responsesize":11784,"method":"Multi"}
2014-07-01 07:43:19,784 WARN  [RpcServer.handler=31,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":22959,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.58:33505","starttimems":1404225776824,"queuetimems":12018,"class":"HRegionServer","responsesize":13727,"method":"Multi"}
2014-07-01 07:43:19,795 WARN  [RpcServer.handler=21,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":22978,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.58:33505","starttimems":1404225776816,"queuetimems":12243,"class":"HRegionServer","responsesize":13294,"method":"Multi"}
2014-07-01 07:43:19,802 WARN  [RpcServer.handler=32,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":22977,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.58:33505","starttimems":1404225776824,"queuetimems":12003,"class":"HRegionServer","responsesize":11940,"method":"Multi"}
2014-07-01 07:43:19,969 WARN  [RpcServer.handler=25,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":23142,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.58:33505","starttimems":1404225776826,"queuetimems":11972,"class":"HRegionServer","responsesize":13523,"method":"Multi"}
2014-07-01 07:43:19,969 WARN  [RpcServer.handler=48,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":23140,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.58:33505","starttimems":1404225776828,"queuetimems":10725,"class":"HRegionServer","responsesize":13130,"method":"Multi"}
2014-07-01 07:43:19,970 WARN  [RpcServer.handler=28,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":22671,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.58:33505","starttimems":1404225777299,"queuetimems":11177,"class":"HRegionServer","responsesize":12977,"method":"Multi"}
2014-07-01 07:43:19,972 WARN  [RpcServer.handler=3,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":22644,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.58:33505","starttimems":1404225777327,"queuetimems":7374,"class":"HRegionServer","responsesize":12848,"method":"Multi"}
2014-07-01 07:43:19,984 WARN  [RpcServer.handler=23,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":22677,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.58:33505","starttimems":1404225777306,"queuetimems":9626,"class":"HRegionServer","responsesize":13436,"method":"Multi"}
2014-07-01 07:43:19,993 WARN  [RpcServer.handler=13,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":23158,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.58:33505","starttimems":1404225776826,"queuetimems":10775,"class":"HRegionServer","responsesize":13693,"method":"Multi"}
2014-07-01 07:43:19,994 WARN  [RpcServer.handler=22,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":23158,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.58:33505","starttimems":1404225776827,"queuetimems":10747,"class":"HRegionServer","responsesize":13489,"method":"Multi"}
2014-07-01 07:43:19,993 WARN  [RpcServer.handler=1,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":22663,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.58:33505","starttimems":1404225777323,"queuetimems":9586,"class":"HRegionServer","responsesize":13005,"method":"Multi"}
2014-07-01 07:43:19,993 WARN  [RpcServer.handler=17,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":22679,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.58:33505","starttimems":1404225777305,"queuetimems":9669,"class":"HRegionServer","responsesize":13665,"method":"Multi"}
2014-07-01 07:43:19,993 WARN  [RpcServer.handler=10,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":22642,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.58:33505","starttimems":1404225777342,"queuetimems":7328,"class":"HRegionServer","responsesize":13271,"method":"Multi"}
2014-07-01 07:43:19,994 WARN  [RpcServer.handler=40,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":23168,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.58:33505","starttimems":1404225776816,"queuetimems":12282,"class":"HRegionServer","responsesize":13421,"method":"Multi"}
2014-07-01 07:43:20,006 WARN  [RpcServer.handler=4,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":13203,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.58:33505","starttimems":1404225786793,"queuetimems":16485,"class":"HRegionServer","responsesize":9005,"method":"Multi"}
2014-07-01 07:43:20,006 WARN  [RpcServer.handler=12,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":23166,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.58:33505","starttimems":1404225776817,"queuetimems":12223,"class":"HRegionServer","responsesize":13394,"method":"Multi"}
2014-07-01 07:43:20,006 WARN  [RpcServer.handler=7,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":13182,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.58:33505","starttimems":1404225786816,"queuetimems":16475,"class":"HRegionServer","responsesize":13577,"method":"Multi"}
2014-07-01 07:43:20,006 WARN  [RpcServer.handler=20,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":17290,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.58:33505","starttimems":1404225782705,"queuetimems":12408,"class":"HRegionServer","responsesize":13540,"method":"Multi"}
2014-07-01 07:43:20,009 WARN  [RpcServer.handler=15,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":22672,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.58:33505","starttimems":1404225777312,"queuetimems":9625,"class":"HRegionServer","responsesize":13178,"method":"Multi"}
2014-07-01 07:43:20,011 WARN  [RpcServer.handler=6,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":22662,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.58:33505","starttimems":1404225777335,"queuetimems":7370,"class":"HRegionServer","responsesize":9219,"method":"Multi"}
2014-07-01 07:43:20,020 WARN  [RpcServer.handler=35,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":22708,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.58:33505","starttimems":1404225777311,"queuetimems":9687,"class":"HRegionServer","responsesize":13543,"method":"Multi"}
2014-07-01 07:43:21,932 WARN  [RpcServer.handler=49,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":11752,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.58:33505","starttimems":1404225790171,"queuetimems":19493,"class":"HRegionServer","responsesize":13633,"method":"Multi"}
2014-07-01 07:43:21,944 WARN  [RpcServer.handler=34,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":12947,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.58:33505","starttimems":1404225788996,"queuetimems":18602,"class":"HRegionServer","responsesize":13550,"method":"Multi"}
2014-07-01 07:43:23,207 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-01 07:43:23,208 WARN  [RpcServer.handler=2,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":10978,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.58:33505","starttimems":1404225792230,"queuetimems":17628,"class":"HRegionServer","responsesize":6175,"method":"Multi"}
2014-07-01 07:43:23,208 WARN  [RpcServer.handler=24,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":13035,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.58:33505","starttimems":1404225790173,"queuetimems":19450,"class":"HRegionServer","responsesize":13631,"method":"Multi"}
2014-07-01 07:43:23,209 WARN  [RpcServer.handler=38,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":11012,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.58:33505","starttimems":1404225792196,"queuetimems":21437,"class":"HRegionServer","responsesize":13535,"method":"Multi"}
2014-07-01 07:43:23,208 WARN  [RpcServer.handler=11,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":14207,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.58:33505","starttimems":1404225789001,"queuetimems":18332,"class":"HRegionServer","responsesize":13694,"method":"Multi"}
2014-07-01 07:43:23,208 WARN  [RpcServer.handler=41,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":13035,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.58:33505","starttimems":1404225790173,"queuetimems":19480,"class":"HRegionServer","responsesize":13237,"method":"Multi"}
2014-07-01 07:43:23,208 WARN  [RpcServer.handler=16,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":10983,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.58:33505","starttimems":1404225792225,"queuetimems":21146,"class":"HRegionServer","responsesize":7027,"method":"Multi"}
2014-07-01 07:43:23,214 WARN  [RpcServer.handler=33,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":13004,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.58:33505","starttimems":1404225790205,"queuetimems":19460,"class":"HRegionServer","responsesize":13397,"method":"Multi"}
2014-07-01 07:43:23,214 WARN  [RpcServer.handler=14,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":14216,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.58:33505","starttimems":1404225788992,"queuetimems":18628,"class":"HRegionServer","responsesize":12867,"method":"Multi"}
2014-07-01 07:43:24,989 INFO  [JvmPauseMonitor] util.JvmPauseMonitor: Detected pause in JVM or host machine (eg GC): pause of approximately 1348ms
GC pool 'ParNew' had collection(s): count=1 time=1742ms
2014-07-01 07:43:24,996 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 6164 synced till here 6148
2014-07-01 07:43:25,911 WARN  [RpcServer.handler=44,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":13554,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.58:33505","starttimems":1404225792356,"queuetimems":16658,"class":"HRegionServer","responsesize":13204,"method":"Multi"}
2014-07-01 07:43:25,911 WARN  [RpcServer.handler=47,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":13678,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.58:33505","starttimems":1404225792232,"queuetimems":17620,"class":"HRegionServer","responsesize":13695,"method":"Multi"}
2014-07-01 07:43:25,911 WARN  [RpcServer.handler=42,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":13666,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.58:33505","starttimems":1404225792244,"queuetimems":16810,"class":"HRegionServer","responsesize":13514,"method":"Multi"}
2014-07-01 07:43:25,911 WARN  [RpcServer.handler=0,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":13680,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.58:33505","starttimems":1404225792230,"queuetimems":18725,"class":"HRegionServer","responsesize":8030,"method":"Multi"}
2014-07-01 07:43:25,911 WARN  [RpcServer.handler=29,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":13695,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.58:33505","starttimems":1404225792215,"queuetimems":21181,"class":"HRegionServer","responsesize":10169,"method":"Multi"}
2014-07-01 07:43:25,912 WARN  [RpcServer.handler=37,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":13564,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.58:33505","starttimems":1404225792348,"queuetimems":16875,"class":"HRegionServer","responsesize":13674,"method":"Multi"}
2014-07-01 07:43:25,911 WARN  [RpcServer.handler=26,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":13674,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.58:33505","starttimems":1404225792236,"queuetimems":16840,"class":"HRegionServer","responsesize":13337,"method":"Multi"}
2014-07-01 07:43:25,912 WARN  [RpcServer.handler=5,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":13710,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.58:33505","starttimems":1404225792202,"queuetimems":21177,"class":"HRegionServer","responsesize":13752,"method":"Multi"}
2014-07-01 07:43:25,912 WARN  [RpcServer.handler=9,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":13716,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.58:33505","starttimems":1404225792196,"queuetimems":21190,"class":"HRegionServer","responsesize":13382,"method":"Multi"}
2014-07-01 07:43:25,933 WARN  [RpcServer.handler=19,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":13717,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.58:33505","starttimems":1404225792215,"queuetimems":21171,"class":"HRegionServer","responsesize":10477,"method":"Multi"}
2014-07-01 07:43:25,933 WARN  [RpcServer.handler=8,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":13716,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.58:33505","starttimems":1404225792217,"queuetimems":21156,"class":"HRegionServer","responsesize":11651,"method":"Multi"}
2014-07-01 07:43:26,140 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1404225098235/slave1%2C60020%2C1404225098235.1404225794082 with entries=102, filesize=93.6m; new WAL /hbase/WALs/slave1,60020,1404225098235/slave1%2C60020%2C1404225098235.1404225803207
2014-07-01 07:43:29,169 WARN  [RpcServer.handler=30,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":15083,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.58:33505","starttimems":1404225794085,"queuetimems":18361,"class":"HRegionServer","responsesize":13254,"method":"Multi"}
2014-07-01 07:43:29,965 WARN  [RpcServer.handler=39,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":12870,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.58:33505","starttimems":1404225796337,"queuetimems":20382,"class":"HRegionServer","responsesize":13374,"method":"Multi"}
2014-07-01 07:43:29,983 WARN  [RpcServer.handler=46,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":13835,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.58:33505","starttimems":1404225796148,"queuetimems":20221,"class":"HRegionServer","responsesize":13203,"method":"Multi"}
2014-07-01 07:43:29,985 WARN  [RpcServer.handler=36,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":13836,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.58:33505","starttimems":1404225796148,"queuetimems":20236,"class":"HRegionServer","responsesize":13394,"method":"Multi"}
2014-07-01 07:43:30,005 WARN  [RpcServer.handler=27,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":15912,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.58:33505","starttimems":1404225794087,"queuetimems":18342,"class":"HRegionServer","responsesize":13523,"method":"Multi"}
2014-07-01 07:43:30,011 WARN  [RpcServer.handler=18,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":15910,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.58:33505","starttimems":1404225794084,"queuetimems":18376,"class":"HRegionServer","responsesize":13294,"method":"Multi"}
2014-07-01 07:43:31,076 WARN  [RpcServer.handler=43,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":11290,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.58:33505","starttimems":1404225799785,"queuetimems":23619,"class":"HRegionServer","responsesize":13510,"method":"Multi"}
2014-07-01 07:43:31,077 WARN  [RpcServer.handler=3,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":11101,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.58:33505","starttimems":1404225799973,"queuetimems":21836,"class":"HRegionServer","responsesize":13693,"method":"Multi"}
2014-07-01 07:43:31,978 WARN  [RpcServer.handler=22,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":11983,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.58:33505","starttimems":1404225799994,"queuetimems":21787,"class":"HRegionServer","responsesize":12977,"method":"Multi"}
2014-07-01 07:43:32,747 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-01 07:43:32,749 WARN  [RpcServer.handler=45,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":12963,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.58:33505","starttimems":1404225799785,"queuetimems":23648,"class":"HRegionServer","responsesize":13339,"method":"Multi"}
2014-07-01 07:43:32,784 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 6271 synced till here 6234
2014-07-01 07:43:33,396 WARN  [RpcServer.handler=25,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":13426,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.58:33505","starttimems":1404225799969,"queuetimems":23596,"class":"HRegionServer","responsesize":13589,"method":"Multi"}
2014-07-01 07:43:33,396 WARN  [RpcServer.handler=48,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":13426,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.58:33505","starttimems":1404225799970,"queuetimems":23579,"class":"HRegionServer","responsesize":13494,"method":"Multi"}
2014-07-01 07:43:33,396 WARN  [RpcServer.handler=40,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":13399,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.58:33505","starttimems":1404225799997,"queuetimems":20078,"class":"HRegionServer","responsesize":12848,"method":"Multi"}
2014-07-01 07:43:33,397 WARN  [RpcServer.handler=35,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":13370,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.58:33505","starttimems":1404225800026,"queuetimems":18201,"class":"HRegionServer","responsesize":13436,"method":"Multi"}
2014-07-01 07:43:33,396 WARN  [RpcServer.handler=1,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":13399,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.58:33505","starttimems":1404225799996,"queuetimems":20179,"class":"HRegionServer","responsesize":13489,"method":"Multi"}
2014-07-01 07:43:33,396 WARN  [RpcServer.handler=23,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":13412,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.58:33505","starttimems":1404225799984,"queuetimems":21835,"class":"HRegionServer","responsesize":13130,"method":"Multi"}
2014-07-01 07:43:33,397 WARN  [RpcServer.handler=15,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":13387,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.58:33505","starttimems":1404225800010,"queuetimems":18252,"class":"HRegionServer","responsesize":13665,"method":"Multi"}
2014-07-01 07:43:33,399 WARN  [RpcServer.handler=34,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":11455,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.58:33505","starttimems":1404225801944,"queuetimems":19380,"class":"HRegionServer","responsesize":9005,"method":"Multi"}
2014-07-01 07:43:33,400 WARN  [RpcServer.handler=31,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":13614,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.58:33505","starttimems":1404225799785,"queuetimems":23639,"class":"HRegionServer","responsesize":11940,"method":"Multi"}
2014-07-01 07:43:33,401 WARN  [RpcServer.handler=7,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":13392,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.58:33505","starttimems":1404225800008,"queuetimems":18309,"class":"HRegionServer","responsesize":13271,"method":"Multi"}
2014-07-01 07:43:33,402 WARN  [RpcServer.handler=28,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":13430,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.58:33505","starttimems":1404225799971,"queuetimems":22592,"class":"HRegionServer","responsesize":13727,"method":"Multi"}
2014-07-01 07:43:33,402 WARN  [RpcServer.handler=21,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":13606,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.58:33505","starttimems":1404225799795,"queuetimems":23605,"class":"HRegionServer","responsesize":13275,"method":"Multi"}
2014-07-01 07:43:33,400 WARN  [RpcServer.handler=49,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":11466,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.58:33505","starttimems":1404225801933,"queuetimems":20046,"class":"HRegionServer","responsesize":13543,"method":"Multi"}
2014-07-01 07:43:33,406 WARN  [RpcServer.handler=4,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":13399,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.58:33505","starttimems":1404225800007,"queuetimems":18539,"class":"HRegionServer","responsesize":13497,"method":"Multi"}
2014-07-01 07:43:33,399 WARN  [RpcServer.handler=12,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":13392,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.58:33505","starttimems":1404225800007,"queuetimems":18481,"class":"HRegionServer","responsesize":13178,"method":"Multi"}
2014-07-01 07:43:33,420 WARN  [RpcServer.handler=20,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":13413,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.58:33505","starttimems":1404225800007,"queuetimems":18352,"class":"HRegionServer","responsesize":9219,"method":"Multi"}
2014-07-01 07:43:33,420 WARN  [RpcServer.handler=17,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":13417,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.58:33505","starttimems":1404225800003,"queuetimems":20036,"class":"HRegionServer","responsesize":13005,"method":"Multi"}
2014-07-01 07:43:33,423 WARN  [RpcServer.handler=32,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":13620,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.58:33505","starttimems":1404225799802,"queuetimems":23444,"class":"HRegionServer","responsesize":13421,"method":"Multi"}
2014-07-01 07:43:34,115 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1404225098235/slave1%2C60020%2C1404225098235.1404225803207 with entries=107, filesize=95.6m; new WAL /hbase/WALs/slave1,60020,1404225098235/slave1%2C60020%2C1404225098235.1404225812747
2014-07-01 07:43:35,292 WARN  [RpcServer.handler=11,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":12082,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.58:33505","starttimems":1404225803209,"queuetimems":20531,"class":"HRegionServer","responsesize":12867,"method":"Multi"}
2014-07-01 07:45:07,565 WARN  [regionserver60020] util.Sleeper: We slept 92597ms instead of 3000ms, this is likely due to a long garbage collecting pause and it's usually bad, see http://hbase.apache.org/book.html#trouble.rs.runtime.zkexpired
2014-07-01 07:45:07,566 WARN  [regionserver60020.compactionChecker] util.Sleeper: We slept 101940ms instead of 10000ms, this is likely due to a long garbage collecting pause and it's usually bad, see http://hbase.apache.org/book.html#trouble.rs.runtime.zkexpired
2014-07-01 07:45:07,570 INFO  [regionserver60020-SendThread(master:2181)] zookeeper.ClientCnxn: Client session timed out, have not heard from server in 120784ms for sessionid 0x46f25690f50001, closing socket connection and attempting reconnect
2014-07-01 07:45:07,570 INFO  [regionserver60020-SendThread(master:2181)] zookeeper.ClientCnxn: Client session timed out, have not heard from server in 120786ms for sessionid 0x46f25690f50006, closing socket connection and attempting reconnect
2014-07-01 07:45:07,570 INFO  [regionserver60020-SendThread(master:2181)] zookeeper.ClientCnxn: Client session timed out, have not heard from server in 120784ms for sessionid 0x46f25690f50003, closing socket connection and attempting reconnect
2014-07-01 07:45:07,567 WARN  [regionserver60020.nonceCleaner] util.Sleeper: We slept 438297ms instead of 360000ms, this is likely due to a long garbage collecting pause and it's usually bad, see http://hbase.apache.org/book.html#trouble.rs.runtime.zkexpired
2014-07-01 07:45:07,567 WARN  [JvmPauseMonitor] util.JvmPauseMonitor: Detected pause in JVM or host machine (eg GC): pause of approximately 92176ms
GC pool 'ParNew' had collection(s): count=2 time=35ms
GC pool 'ConcurrentMarkSweep' had collection(s): count=1 time=92226ms
2014-07-01 07:45:07,566 WARN  [regionserver60020.periodicFlusher] util.Sleeper: We slept 101940ms instead of 10000ms, this is likely due to a long garbage collecting pause and it's usually bad, see http://hbase.apache.org/book.html#trouble.rs.runtime.zkexpired
2014-07-01 07:45:07,573 WARN  [ResponseProcessor for block blk_2143454676794356893_29088] hdfs.DFSClient: DFSOutputStream ResponseProcessor exception  for block blk_2143454676794356893_29088java.net.SocketTimeoutException: 66000 millis timeout while waiting for channel to be ready for read. ch : java.nio.channels.SocketChannel[connected local=/9.1.143.59:48534 remote=/9.1.143.59:50010]
	at org.apache.hadoop.net.SocketIOWithTimeout.doIO(SocketIOWithTimeout.java:164)
	at org.apache.hadoop.net.SocketInputStream.read(SocketInputStream.java:155)
	at org.apache.hadoop.net.SocketInputStream.read(SocketInputStream.java:128)
	at java.io.DataInputStream.readFully(DataInputStream.java:195)
	at java.io.DataInputStream.readLong(DataInputStream.java:416)
	at org.apache.hadoop.hdfs.protocol.DataTransferProtocol$PipelineAck.readFields(DataTransferProtocol.java:124)
	at org.apache.hadoop.hdfs.DFSClient$DFSOutputStream$ResponseProcessor.run(DFSClient.java:3161)

2014-07-01 07:45:07,574 WARN  [RpcServer.handler=42,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":98856,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.58:33505","starttimems":1404225808717,"queuetimems":20842,"class":"HRegionServer","responsesize":13752,"method":"Multi"}
2014-07-01 07:45:07,573 WARN  [ResponseProcessor for block blk_4372291247462747067_29087] hdfs.DFSClient: DFSOutputStream ResponseProcessor exception  for block blk_4372291247462747067_29087java.net.SocketTimeoutException: 66000 millis timeout while waiting for channel to be ready for read. ch : java.nio.channels.SocketChannel[connected local=/9.1.143.59:48526 remote=/9.1.143.59:50010]
	at org.apache.hadoop.net.SocketIOWithTimeout.doIO(SocketIOWithTimeout.java:164)
	at org.apache.hadoop.net.SocketInputStream.read(SocketInputStream.java:155)
	at org.apache.hadoop.net.SocketInputStream.read(SocketInputStream.java:128)
	at java.io.DataInputStream.readFully(DataInputStream.java:195)
	at java.io.DataInputStream.readLong(DataInputStream.java:416)
	at org.apache.hadoop.hdfs.protocol.DataTransferProtocol$PipelineAck.readFields(DataTransferProtocol.java:124)
	at org.apache.hadoop.hdfs.DFSClient$DFSOutputStream$ResponseProcessor.run(DFSClient.java:3161)

2014-07-01 07:45:07,579 WARN  [DataStreamer for file /hbase/data/default/usertable/07064f253c865a7cc0c08893a4f56af9/.tmp/a17a8b64528148559a6414832c7b909b block blk_4372291247462747067_29087] hdfs.DFSClient: Error Recovery for blk_4372291247462747067_29087 bad datanode[0] 9.1.143.59:50010
2014-07-01 07:45:07,580 WARN  [DataStreamer for file /hbase/data/default/usertable/07064f253c865a7cc0c08893a4f56af9/.tmp/a17a8b64528148559a6414832c7b909b block blk_4372291247462747067_29087] hdfs.DFSClient: Error Recovery for block blk_4372291247462747067_29087 in pipeline 9.1.143.59:50010, 9.1.143.58:50010: bad datanode 9.1.143.59:50010
2014-07-01 07:45:07,574 WARN  [DataStreamer for file /hbase/data/default/usertable/89cb00f0096b305c3317b7753d66adf1/.tmp/e0c57336acca4322aa4da80b9784f631 block blk_2143454676794356893_29088] hdfs.DFSClient: Error Recovery for blk_2143454676794356893_29088 bad datanode[0] 9.1.143.59:50010
2014-07-01 07:45:07,627 WARN  [DataStreamer for file /hbase/data/default/usertable/89cb00f0096b305c3317b7753d66adf1/.tmp/e0c57336acca4322aa4da80b9784f631 block blk_2143454676794356893_29088] hdfs.DFSClient: Error Recovery for block blk_2143454676794356893_29088 in pipeline 9.1.143.59:50010, 9.1.143.58:50010: bad datanode 9.1.143.59:50010
2014-07-01 07:45:07,627 WARN  [RpcServer.handler=41,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":104417,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.58:33505","starttimems":1404225803210,"queuetimems":17800,"class":"HRegionServer","responsesize":13237,"method":"Multi"}
2014-07-01 07:45:07,682 FATAL [regionserver60020] regionserver.HRegionServer: ABORTING region server slave1,60020,1404225098235: org.apache.hadoop.hbase.YouAreDeadException: Server REPORT rejected; currently processing slave1,60020,1404225098235 as dead server
org.apache.hadoop.hbase.YouAreDeadException: org.apache.hadoop.hbase.YouAreDeadException: Server REPORT rejected; currently processing slave1,60020,1404225098235 as dead server
	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:57)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:534)
	at org.apache.hadoop.ipc.RemoteException.instantiateException(RemoteException.java:95)
	at org.apache.hadoop.ipc.RemoteException.unwrapRemoteException(RemoteException.java:79)
	at org.apache.hadoop.hbase.protobuf.ProtobufUtil.getRemoteException(ProtobufUtil.java:285)
	at org.apache.hadoop.hbase.regionserver.HRegionServer.tryRegionServerReport(HRegionServer.java:1065)
	at org.apache.hadoop.hbase.regionserver.HRegionServer.run(HRegionServer.java:901)
	at java.lang.Thread.run(Thread.java:701)
Caused by: org.apache.hadoop.hbase.ipc.RemoteWithExtrasException: org.apache.hadoop.hbase.YouAreDeadException: Server REPORT rejected; currently processing slave1,60020,1404225098235 as dead server
	at org.apache.hadoop.hbase.master.ServerManager.checkIsDead(ServerManager.java:369)
	at org.apache.hadoop.hbase.master.ServerManager.regionServerReport(ServerManager.java:274)
	at org.apache.hadoop.hbase.master.HMaster.regionServerReport(HMaster.java:1357)
	at org.apache.hadoop.hbase.protobuf.generated.RegionServerStatusProtos$RegionServerStatusService$2.callBlockingMethod(RegionServerStatusProtos.java:5087)
	at org.apache.hadoop.hbase.ipc.RpcServer.call(RpcServer.java:2012)
	at org.apache.hadoop.hbase.ipc.CallRunner.run(CallRunner.java:98)
	at org.apache.hadoop.hbase.ipc.FifoRpcScheduler$1.run(FifoRpcScheduler.java:73)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:471)
	at java.util.concurrent.FutureTask$Sync.innerRun(FutureTask.java:334)
	at java.util.concurrent.FutureTask.run(FutureTask.java:166)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1146)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:701)

	at org.apache.hadoop.hbase.ipc.RpcClient.call(RpcClient.java:1453)
	at org.apache.hadoop.hbase.ipc.RpcClient.callBlockingMethod(RpcClient.java:1657)
	at org.apache.hadoop.hbase.ipc.RpcClient$BlockingRpcChannelImplementation.callBlockingMethod(RpcClient.java:1715)
	at org.apache.hadoop.hbase.protobuf.generated.RegionServerStatusProtos$RegionServerStatusService$BlockingStub.regionServerReport(RegionServerStatusProtos.java:5414)
	at org.apache.hadoop.hbase.regionserver.HRegionServer.tryRegionServerReport(HRegionServer.java:1063)
	... 2 more
2014-07-01 07:45:07,683 FATAL [regionserver60020] regionserver.HRegionServer: RegionServer abort: loaded coprocessors are: []
2014-07-01 07:45:07,682 WARN  [RpcServer.handler=38,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":104418,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.58:33505","starttimems":1404225803209,"queuetimems":20563,"class":"HRegionServer","responsesize":13550,"method":"Multi"}
2014-07-01 07:45:07,682 WARN  [DataStreamer for file /hbase/WALs/slave1,60020,1404225098235/slave1%2C60020%2C1404225098235.1404225812747 block blk_-5973222587916760426_29091] hdfs.DFSClient: DataStreamer Exception: java.io.IOException: Broken pipe
	at sun.nio.ch.FileDispatcher.write0(Native Method)
	at sun.nio.ch.SocketDispatcher.write(SocketDispatcher.java:47)
	at sun.nio.ch.IOUtil.writeFromNativeBuffer(IOUtil.java:122)
	at sun.nio.ch.IOUtil.write(IOUtil.java:93)
	at sun.nio.ch.SocketChannelImpl.write(SocketChannelImpl.java:352)
	at org.apache.hadoop.net.SocketOutputStream$Writer.performIO(SocketOutputStream.java:55)
	at org.apache.hadoop.net.SocketIOWithTimeout.doIO(SocketIOWithTimeout.java:142)
	at org.apache.hadoop.net.SocketOutputStream.write(SocketOutputStream.java:146)
	at org.apache.hadoop.net.SocketOutputStream.write(SocketOutputStream.java:107)
	at java.io.BufferedOutputStream.write(BufferedOutputStream.java:122)
	at java.io.DataOutputStream.write(DataOutputStream.java:107)
	at org.apache.hadoop.hdfs.DFSClient$DFSOutputStream$DataStreamer.run(DFSClient.java:3051)

2014-07-01 07:45:07,684 WARN  [DataStreamer for file /hbase/WALs/slave1,60020,1404225098235/slave1%2C60020%2C1404225098235.1404225812747 block blk_-5973222587916760426_29091] hdfs.DFSClient: Error Recovery for blk_-5973222587916760426_29091 bad datanode[0] 9.1.143.59:50010
2014-07-01 07:45:07,688 WARN  [DataStreamer for file /hbase/WALs/slave1,60020,1404225098235/slave1%2C60020%2C1404225098235.1404225812747 block blk_-5973222587916760426_29091] hdfs.DFSClient: Error Recovery for block blk_-5973222587916760426_29091 in pipeline 9.1.143.59:50010, 9.1.143.58:50010: bad datanode 9.1.143.59:50010
2014-07-01 07:45:07,733 WARN  [RpcServer.handler=42,port=60020] ipc.RpcServer: RpcServer.respondercallId: 3900 service: ClientService methodName: Multi size: 2.4m connection: 9.1.143.58:33505: output error
2014-07-01 07:45:07,743 WARN  [RpcServer.handler=42,port=60020] ipc.RpcServer: RpcServer.handler=42,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-07-01 07:45:07,743 WARN  [RpcServer.handler=38,port=60020] ipc.RpcServer: RpcServer.respondercallId: 3891 service: ClientService methodName: Multi size: 2.4m connection: 9.1.143.58:33505: output error
2014-07-01 07:45:07,743 WARN  [RpcServer.handler=38,port=60020] ipc.RpcServer: RpcServer.handler=38,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-07-01 07:45:07,743 WARN  [RpcServer.handler=41,port=60020] ipc.RpcServer: RpcServer.respondercallId: 3889 service: ClientService methodName: Multi size: 2.3m connection: 9.1.143.58:33505: output error
2014-07-01 07:45:07,744 WARN  [RpcServer.handler=41,port=60020] ipc.RpcServer: RpcServer.handler=41,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-07-01 07:45:07,765 WARN  [DataStreamer for file /hbase/data/default/usertable/07064f253c865a7cc0c08893a4f56af9/.tmp/a17a8b64528148559a6414832c7b909b block blk_4372291247462747067_29087] hdfs.DFSClient: Failed recovery attempt #0 from primary datanode 9.1.143.58:50010
org.apache.hadoop.ipc.RemoteException: org.apache.hadoop.ipc.RemoteException: java.io.IOException: blk_4372291247462747067_29087 is missing
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.nextGenerationStampForBlock(FSNamesystem.java:5943)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.nextGenerationStamp(NameNode.java:835)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:622)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:587)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1432)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1428)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:416)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1190)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:1426)

	at org.apache.hadoop.ipc.Client.call(Client.java:1113)
	at org.apache.hadoop.ipc.RPC$Invoker.invoke(RPC.java:229)
	at com.sun.proxy.$Proxy5.nextGenerationStamp(Unknown Source)
	at org.apache.hadoop.hdfs.server.datanode.DataNode.syncBlock(DataNode.java:2127)
	at org.apache.hadoop.hdfs.server.datanode.DataNode.recoverBlock(DataNode.java:2095)
	at org.apache.hadoop.hdfs.server.datanode.DataNode.recoverBlock(DataNode.java:2175)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:622)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:587)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1432)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1428)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:416)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1190)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:1426)

	at org.apache.hadoop.ipc.Client.call(Client.java:1113)
	at org.apache.hadoop.ipc.RPC$Invoker.invoke(RPC.java:229)
	at com.sun.proxy.$Proxy13.recoverBlock(Unknown Source)
	at org.apache.hadoop.hdfs.DFSClient$DFSOutputStream.processDatanodeError(DFSClient.java:3319)
	at org.apache.hadoop.hdfs.DFSClient$DFSOutputStream.access$2200(DFSClient.java:2783)
	at org.apache.hadoop.hdfs.DFSClient$DFSOutputStream$DataStreamer.run(DFSClient.java:2987)
2014-07-01 07:45:07,777 WARN  [DataStreamer for file /hbase/WALs/slave1,60020,1404225098235/slave1%2C60020%2C1404225098235.1404225812747 block blk_-5973222587916760426_29091] hdfs.DFSClient: Failed recovery attempt #0 from primary datanode 9.1.143.58:50010
org.apache.hadoop.ipc.RemoteException: org.apache.hadoop.ipc.RemoteException: java.io.IOException: blk_-5973222587916760426_29091 has out of date GS 29091 found 29152, may already be committed
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.nextGenerationStampForBlock(FSNamesystem.java:5943)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.nextGenerationStamp(NameNode.java:835)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:622)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:587)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1432)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1428)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:416)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1190)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:1426)

	at org.apache.hadoop.ipc.Client.call(Client.java:1113)
	at org.apache.hadoop.ipc.RPC$Invoker.invoke(RPC.java:229)
	at com.sun.proxy.$Proxy5.nextGenerationStamp(Unknown Source)
	at org.apache.hadoop.hdfs.server.datanode.DataNode.syncBlock(DataNode.java:2127)
	at org.apache.hadoop.hdfs.server.datanode.DataNode.recoverBlock(DataNode.java:2095)
	at org.apache.hadoop.hdfs.server.datanode.DataNode.recoverBlock(DataNode.java:2175)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:622)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:587)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1432)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1428)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:416)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1190)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:1426)

	at org.apache.hadoop.ipc.Client.call(Client.java:1113)
	at org.apache.hadoop.ipc.RPC$Invoker.invoke(RPC.java:229)
	at com.sun.proxy.$Proxy13.recoverBlock(Unknown Source)
	at org.apache.hadoop.hdfs.DFSClient$DFSOutputStream.processDatanodeError(DFSClient.java:3319)
	at org.apache.hadoop.hdfs.DFSClient$DFSOutputStream.access$2200(DFSClient.java:2783)
	at org.apache.hadoop.hdfs.DFSClient$DFSOutputStream$DataStreamer.run(DFSClient.java:2987)
2014-07-01 07:45:07,785 WARN  [DataStreamer for file /hbase/data/default/usertable/07064f253c865a7cc0c08893a4f56af9/.tmp/a17a8b64528148559a6414832c7b909b block blk_4372291247462747067_29087] hdfs.DFSClient: Error Recovery for block blk_4372291247462747067_29087 failed  because recovery from primary datanode 9.1.143.58:50010 failed 1 times.  Pipeline was 9.1.143.59:50010, 9.1.143.58:50010. Will retry...
2014-07-01 07:45:07,786 WARN  [DataStreamer for file /hbase/WALs/slave1,60020,1404225098235/slave1%2C60020%2C1404225098235.1404225812747 block blk_-5973222587916760426_29091] hdfs.DFSClient: Error Recovery for block blk_-5973222587916760426_29091 failed  because recovery from primary datanode 9.1.143.58:50010 failed 1 times.  Pipeline was 9.1.143.59:50010, 9.1.143.58:50010. Will retry...
2014-07-01 07:45:07,787 WARN  [DataStreamer for file /hbase/data/default/usertable/07064f253c865a7cc0c08893a4f56af9/.tmp/a17a8b64528148559a6414832c7b909b block blk_4372291247462747067_29087] hdfs.DFSClient: Error Recovery for blk_4372291247462747067_29087 bad datanode[0] 9.1.143.59:50010
2014-07-01 07:45:07,787 WARN  [DataStreamer for file /hbase/data/default/usertable/07064f253c865a7cc0c08893a4f56af9/.tmp/a17a8b64528148559a6414832c7b909b block blk_4372291247462747067_29087] hdfs.DFSClient: Error Recovery for block blk_4372291247462747067_29087 in pipeline 9.1.143.59:50010, 9.1.143.58:50010: bad datanode 9.1.143.59:50010
2014-07-01 07:45:07,787 WARN  [DataStreamer for file /hbase/WALs/slave1,60020,1404225098235/slave1%2C60020%2C1404225098235.1404225812747 block blk_-5973222587916760426_29091] hdfs.DFSClient: Error Recovery for blk_-5973222587916760426_29091 bad datanode[0] 9.1.143.59:50010
2014-07-01 07:45:07,787 WARN  [DataStreamer for file /hbase/WALs/slave1,60020,1404225098235/slave1%2C60020%2C1404225098235.1404225812747 block blk_-5973222587916760426_29091] hdfs.DFSClient: Error Recovery for block blk_-5973222587916760426_29091 in pipeline 9.1.143.59:50010, 9.1.143.58:50010: bad datanode 9.1.143.59:50010
2014-07-01 07:45:07,837 WARN  [DataStreamer for file /hbase/data/default/usertable/07064f253c865a7cc0c08893a4f56af9/.tmp/a17a8b64528148559a6414832c7b909b block blk_4372291247462747067_29087] hdfs.DFSClient: Failed recovery attempt #1 from primary datanode 9.1.143.58:50010
org.apache.hadoop.ipc.RemoteException: org.apache.hadoop.ipc.RemoteException: java.io.IOException: blk_4372291247462747067_29087 is missing
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.nextGenerationStampForBlock(FSNamesystem.java:5943)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.nextGenerationStamp(NameNode.java:835)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:622)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:587)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1432)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1428)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:416)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1190)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:1426)

	at org.apache.hadoop.ipc.Client.call(Client.java:1113)
	at org.apache.hadoop.ipc.RPC$Invoker.invoke(RPC.java:229)
	at com.sun.proxy.$Proxy5.nextGenerationStamp(Unknown Source)
	at org.apache.hadoop.hdfs.server.datanode.DataNode.syncBlock(DataNode.java:2127)
	at org.apache.hadoop.hdfs.server.datanode.DataNode.recoverBlock(DataNode.java:2095)
	at org.apache.hadoop.hdfs.server.datanode.DataNode.recoverBlock(DataNode.java:2175)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:622)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:587)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1432)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1428)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:416)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1190)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:1426)

	at org.apache.hadoop.ipc.Client.call(Client.java:1113)
	at org.apache.hadoop.ipc.RPC$Invoker.invoke(RPC.java:229)
	at com.sun.proxy.$Proxy13.recoverBlock(Unknown Source)
	at org.apache.hadoop.hdfs.DFSClient$DFSOutputStream.processDatanodeError(DFSClient.java:3319)
	at org.apache.hadoop.hdfs.DFSClient$DFSOutputStream.access$2200(DFSClient.java:2783)
	at org.apache.hadoop.hdfs.DFSClient$DFSOutputStream$DataStreamer.run(DFSClient.java:2987)
2014-07-01 07:45:07,838 WARN  [DataStreamer for file /hbase/data/default/usertable/07064f253c865a7cc0c08893a4f56af9/.tmp/a17a8b64528148559a6414832c7b909b block blk_4372291247462747067_29087] hdfs.DFSClient: Error Recovery for block blk_4372291247462747067_29087 failed  because recovery from primary datanode 9.1.143.58:50010 failed 2 times.  Pipeline was 9.1.143.59:50010, 9.1.143.58:50010. Will retry...
2014-07-01 07:45:07,843 WARN  [DataStreamer for file /hbase/WALs/slave1,60020,1404225098235/slave1%2C60020%2C1404225098235.1404225812747 block blk_-5973222587916760426_29091] hdfs.DFSClient: Failed recovery attempt #1 from primary datanode 9.1.143.58:50010
org.apache.hadoop.ipc.RemoteException: org.apache.hadoop.ipc.RemoteException: java.io.IOException: blk_-5973222587916760426_29091 has out of date GS 29091 found 29152, may already be committed
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.nextGenerationStampForBlock(FSNamesystem.java:5943)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.nextGenerationStamp(NameNode.java:835)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:622)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:587)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1432)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1428)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:416)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1190)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:1426)

	at org.apache.hadoop.ipc.Client.call(Client.java:1113)
	at org.apache.hadoop.ipc.RPC$Invoker.invoke(RPC.java:229)
	at com.sun.proxy.$Proxy5.nextGenerationStamp(Unknown Source)
	at org.apache.hadoop.hdfs.server.datanode.DataNode.syncBlock(DataNode.java:2127)
	at org.apache.hadoop.hdfs.server.datanode.DataNode.recoverBlock(DataNode.java:2095)
	at org.apache.hadoop.hdfs.server.datanode.DataNode.recoverBlock(DataNode.java:2175)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:622)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:587)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1432)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1428)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:416)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1190)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:1426)

	at org.apache.hadoop.ipc.Client.call(Client.java:1113)
	at org.apache.hadoop.ipc.RPC$Invoker.invoke(RPC.java:229)
	at com.sun.proxy.$Proxy13.recoverBlock(Unknown Source)
	at org.apache.hadoop.hdfs.DFSClient$DFSOutputStream.processDatanodeError(DFSClient.java:3319)
	at org.apache.hadoop.hdfs.DFSClient$DFSOutputStream.access$2200(DFSClient.java:2783)
	at org.apache.hadoop.hdfs.DFSClient$DFSOutputStream$DataStreamer.run(DFSClient.java:2987)
2014-07-01 07:45:07,843 WARN  [DataStreamer for file /hbase/WALs/slave1,60020,1404225098235/slave1%2C60020%2C1404225098235.1404225812747 block blk_-5973222587916760426_29091] hdfs.DFSClient: Error Recovery for block blk_-5973222587916760426_29091 failed  because recovery from primary datanode 9.1.143.58:50010 failed 2 times.  Pipeline was 9.1.143.59:50010, 9.1.143.58:50010. Will retry...
2014-07-01 07:45:07,844 WARN  [DataStreamer for file /hbase/WALs/slave1,60020,1404225098235/slave1%2C60020%2C1404225098235.1404225812747 block blk_-5973222587916760426_29091] hdfs.DFSClient: Error Recovery for blk_-5973222587916760426_29091 bad datanode[0] 9.1.143.59:50010
2014-07-01 07:45:07,844 WARN  [DataStreamer for file /hbase/WALs/slave1,60020,1404225098235/slave1%2C60020%2C1404225098235.1404225812747 block blk_-5973222587916760426_29091] hdfs.DFSClient: Error Recovery for block blk_-5973222587916760426_29091 in pipeline 9.1.143.59:50010, 9.1.143.58:50010: bad datanode 9.1.143.59:50010
2014-07-01 07:45:07,872 WARN  [DataStreamer for file /hbase/WALs/slave1,60020,1404225098235/slave1%2C60020%2C1404225098235.1404225812747 block blk_-5973222587916760426_29091] hdfs.DFSClient: Failed recovery attempt #2 from primary datanode 9.1.143.58:50010
org.apache.hadoop.ipc.RemoteException: org.apache.hadoop.ipc.RemoteException: java.io.IOException: blk_-5973222587916760426_29091 has out of date GS 29091 found 29152, may already be committed
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.nextGenerationStampForBlock(FSNamesystem.java:5943)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.nextGenerationStamp(NameNode.java:835)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:622)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:587)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1432)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1428)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:416)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1190)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:1426)

	at org.apache.hadoop.ipc.Client.call(Client.java:1113)
	at org.apache.hadoop.ipc.RPC$Invoker.invoke(RPC.java:229)
	at com.sun.proxy.$Proxy5.nextGenerationStamp(Unknown Source)
	at org.apache.hadoop.hdfs.server.datanode.DataNode.syncBlock(DataNode.java:2127)
	at org.apache.hadoop.hdfs.server.datanode.DataNode.recoverBlock(DataNode.java:2095)
	at org.apache.hadoop.hdfs.server.datanode.DataNode.recoverBlock(DataNode.java:2175)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:622)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:587)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1432)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1428)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:416)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1190)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:1426)

	at org.apache.hadoop.ipc.Client.call(Client.java:1113)
	at org.apache.hadoop.ipc.RPC$Invoker.invoke(RPC.java:229)
	at com.sun.proxy.$Proxy13.recoverBlock(Unknown Source)
	at org.apache.hadoop.hdfs.DFSClient$DFSOutputStream.processDatanodeError(DFSClient.java:3319)
	at org.apache.hadoop.hdfs.DFSClient$DFSOutputStream.access$2200(DFSClient.java:2783)
	at org.apache.hadoop.hdfs.DFSClient$DFSOutputStream$DataStreamer.run(DFSClient.java:2987)
2014-07-01 07:45:07,873 WARN  [DataStreamer for file /hbase/WALs/slave1,60020,1404225098235/slave1%2C60020%2C1404225098235.1404225812747 block blk_-5973222587916760426_29091] hdfs.DFSClient: Error Recovery for block blk_-5973222587916760426_29091 failed  because recovery from primary datanode 9.1.143.58:50010 failed 3 times.  Pipeline was 9.1.143.59:50010, 9.1.143.58:50010. Will retry...
2014-07-01 07:45:07,874 WARN  [DataStreamer for file /hbase/WALs/slave1,60020,1404225098235/slave1%2C60020%2C1404225098235.1404225812747 block blk_-5973222587916760426_29091] hdfs.DFSClient: Error Recovery for blk_-5973222587916760426_29091 bad datanode[0] 9.1.143.59:50010
2014-07-01 07:45:07,874 WARN  [DataStreamer for file /hbase/WALs/slave1,60020,1404225098235/slave1%2C60020%2C1404225098235.1404225812747 block blk_-5973222587916760426_29091] hdfs.DFSClient: Error Recovery for block blk_-5973222587916760426_29091 in pipeline 9.1.143.59:50010, 9.1.143.58:50010: bad datanode 9.1.143.59:50010
2014-07-01 07:45:07,928 WARN  [DataStreamer for file /hbase/WALs/slave1,60020,1404225098235/slave1%2C60020%2C1404225098235.1404225812747 block blk_-5973222587916760426_29091] hdfs.DFSClient: Failed recovery attempt #3 from primary datanode 9.1.143.58:50010
org.apache.hadoop.ipc.RemoteException: org.apache.hadoop.ipc.RemoteException: java.io.IOException: blk_-5973222587916760426_29091 has out of date GS 29091 found 29152, may already be committed
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.nextGenerationStampForBlock(FSNamesystem.java:5943)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.nextGenerationStamp(NameNode.java:835)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:622)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:587)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1432)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1428)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:416)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1190)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:1426)

	at org.apache.hadoop.ipc.Client.call(Client.java:1113)
	at org.apache.hadoop.ipc.RPC$Invoker.invoke(RPC.java:229)
	at com.sun.proxy.$Proxy5.nextGenerationStamp(Unknown Source)
	at org.apache.hadoop.hdfs.server.datanode.DataNode.syncBlock(DataNode.java:2127)
	at org.apache.hadoop.hdfs.server.datanode.DataNode.recoverBlock(DataNode.java:2095)
	at org.apache.hadoop.hdfs.server.datanode.DataNode.recoverBlock(DataNode.java:2175)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:622)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:587)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1432)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1428)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:416)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1190)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:1426)

	at org.apache.hadoop.ipc.Client.call(Client.java:1113)
	at org.apache.hadoop.ipc.RPC$Invoker.invoke(RPC.java:229)
	at com.sun.proxy.$Proxy13.recoverBlock(Unknown Source)
	at org.apache.hadoop.hdfs.DFSClient$DFSOutputStream.processDatanodeError(DFSClient.java:3319)
	at org.apache.hadoop.hdfs.DFSClient$DFSOutputStream.access$2200(DFSClient.java:2783)
	at org.apache.hadoop.hdfs.DFSClient$DFSOutputStream$DataStreamer.run(DFSClient.java:2987)
2014-07-01 07:45:07,929 WARN  [DataStreamer for file /hbase/WALs/slave1,60020,1404225098235/slave1%2C60020%2C1404225098235.1404225812747 block blk_-5973222587916760426_29091] hdfs.DFSClient: Error Recovery for block blk_-5973222587916760426_29091 failed  because recovery from primary datanode 9.1.143.58:50010 failed 4 times.  Pipeline was 9.1.143.59:50010, 9.1.143.58:50010. Will retry...
2014-07-01 07:45:07,930 WARN  [DataStreamer for file /hbase/WALs/slave1,60020,1404225098235/slave1%2C60020%2C1404225098235.1404225812747 block blk_-5973222587916760426_29091] hdfs.DFSClient: Error Recovery for blk_-5973222587916760426_29091 bad datanode[0] 9.1.143.59:50010
2014-07-01 07:45:07,930 WARN  [DataStreamer for file /hbase/WALs/slave1,60020,1404225098235/slave1%2C60020%2C1404225098235.1404225812747 block blk_-5973222587916760426_29091] hdfs.DFSClient: Error Recovery for block blk_-5973222587916760426_29091 in pipeline 9.1.143.59:50010, 9.1.143.58:50010: bad datanode 9.1.143.59:50010
2014-07-01 07:45:07,953 WARN  [DataStreamer for file /hbase/WALs/slave1,60020,1404225098235/slave1%2C60020%2C1404225098235.1404225812747 block blk_-5973222587916760426_29091] hdfs.DFSClient: Failed recovery attempt #4 from primary datanode 9.1.143.58:50010
org.apache.hadoop.ipc.RemoteException: org.apache.hadoop.ipc.RemoteException: java.io.IOException: blk_-5973222587916760426_29091 has out of date GS 29091 found 29152, may already be committed
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.nextGenerationStampForBlock(FSNamesystem.java:5943)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.nextGenerationStamp(NameNode.java:835)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:622)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:587)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1432)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1428)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:416)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1190)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:1426)

	at org.apache.hadoop.ipc.Client.call(Client.java:1113)
	at org.apache.hadoop.ipc.RPC$Invoker.invoke(RPC.java:229)
	at com.sun.proxy.$Proxy5.nextGenerationStamp(Unknown Source)
	at org.apache.hadoop.hdfs.server.datanode.DataNode.syncBlock(DataNode.java:2127)
	at org.apache.hadoop.hdfs.server.datanode.DataNode.recoverBlock(DataNode.java:2095)
	at org.apache.hadoop.hdfs.server.datanode.DataNode.recoverBlock(DataNode.java:2175)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:622)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:587)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1432)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1428)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:416)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1190)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:1426)

	at org.apache.hadoop.ipc.Client.call(Client.java:1113)
	at org.apache.hadoop.ipc.RPC$Invoker.invoke(RPC.java:229)
	at com.sun.proxy.$Proxy13.recoverBlock(Unknown Source)
	at org.apache.hadoop.hdfs.DFSClient$DFSOutputStream.processDatanodeError(DFSClient.java:3319)
	at org.apache.hadoop.hdfs.DFSClient$DFSOutputStream.access$2200(DFSClient.java:2783)
	at org.apache.hadoop.hdfs.DFSClient$DFSOutputStream$DataStreamer.run(DFSClient.java:2987)
2014-07-01 07:45:07,954 WARN  [DataStreamer for file /hbase/WALs/slave1,60020,1404225098235/slave1%2C60020%2C1404225098235.1404225812747 block blk_-5973222587916760426_29091] hdfs.DFSClient: Error Recovery for block blk_-5973222587916760426_29091 failed  because recovery from primary datanode 9.1.143.58:50010 failed 5 times.  Pipeline was 9.1.143.59:50010, 9.1.143.58:50010. Will retry...
2014-07-01 07:45:07,955 WARN  [DataStreamer for file /hbase/WALs/slave1,60020,1404225098235/slave1%2C60020%2C1404225098235.1404225812747 block blk_-5973222587916760426_29091] hdfs.DFSClient: Error Recovery for blk_-5973222587916760426_29091 bad datanode[0] 9.1.143.59:50010
2014-07-01 07:45:07,955 WARN  [DataStreamer for file /hbase/WALs/slave1,60020,1404225098235/slave1%2C60020%2C1404225098235.1404225812747 block blk_-5973222587916760426_29091] hdfs.DFSClient: Error Recovery for block blk_-5973222587916760426_29091 in pipeline 9.1.143.59:50010, 9.1.143.58:50010: bad datanode 9.1.143.59:50010
2014-07-01 07:45:07,970 WARN  [DataStreamer for file /hbase/WALs/slave1,60020,1404225098235/slave1%2C60020%2C1404225098235.1404225812747 block blk_-5973222587916760426_29091] hdfs.DFSClient: Failed recovery attempt #5 from primary datanode 9.1.143.58:50010
org.apache.hadoop.ipc.RemoteException: org.apache.hadoop.ipc.RemoteException: java.io.IOException: blk_-5973222587916760426_29091 has out of date GS 29091 found 29152, may already be committed
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.nextGenerationStampForBlock(FSNamesystem.java:5943)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.nextGenerationStamp(NameNode.java:835)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:622)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:587)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1432)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1428)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:416)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1190)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:1426)

	at org.apache.hadoop.ipc.Client.call(Client.java:1113)
	at org.apache.hadoop.ipc.RPC$Invoker.invoke(RPC.java:229)
	at com.sun.proxy.$Proxy5.nextGenerationStamp(Unknown Source)
	at org.apache.hadoop.hdfs.server.datanode.DataNode.syncBlock(DataNode.java:2127)
	at org.apache.hadoop.hdfs.server.datanode.DataNode.recoverBlock(DataNode.java:2095)
	at org.apache.hadoop.hdfs.server.datanode.DataNode.recoverBlock(DataNode.java:2175)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:622)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:587)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1432)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1428)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:416)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1190)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:1426)

	at org.apache.hadoop.ipc.Client.call(Client.java:1113)
	at org.apache.hadoop.ipc.RPC$Invoker.invoke(RPC.java:229)
	at com.sun.proxy.$Proxy13.recoverBlock(Unknown Source)
	at org.apache.hadoop.hdfs.DFSClient$DFSOutputStream.processDatanodeError(DFSClient.java:3319)
	at org.apache.hadoop.hdfs.DFSClient$DFSOutputStream.access$2200(DFSClient.java:2783)
	at org.apache.hadoop.hdfs.DFSClient$DFSOutputStream$DataStreamer.run(DFSClient.java:2987)
2014-07-01 07:45:07,970 INFO  [regionserver60020] regionserver.HRegionServer: STOPPED: org.apache.hadoop.hbase.YouAreDeadException: Server REPORT rejected; currently processing slave1,60020,1404225098235 as dead server
2014-07-01 07:45:07,971 INFO  [regionserver60020] ipc.RpcServer: Stopping server on 60020
2014-07-01 07:45:07,971 INFO  [RpcServer.listener,port=60020] ipc.RpcServer: RpcServer.listener,port=60020: stopping
2014-07-01 07:45:07,971 WARN  [DataStreamer for file /hbase/WALs/slave1,60020,1404225098235/slave1%2C60020%2C1404225098235.1404225812747 block blk_-5973222587916760426_29091] hdfs.DFSClient: Error Recovery for block blk_-5973222587916760426_29091 failed  because recovery from primary datanode 9.1.143.58:50010 failed 6 times.  Pipeline was 9.1.143.59:50010, 9.1.143.58:50010. Marking primary datanode as bad.
2014-07-01 07:45:07,972 INFO  [RpcServer.responder] ipc.RpcServer: RpcServer.responder: stopped
2014-07-01 07:45:07,972 DEBUG [RpcServer.handler=3,port=60020] wal.FSHLog: interrupted while waiting for notification from AsyncNotifier
2014-07-01 07:45:07,972 INFO  [RpcServer.responder] ipc.RpcServer: RpcServer.responder: stopping
2014-07-01 07:45:07,972 DEBUG [RpcServer.handler=18,port=60020] wal.FSHLog: interrupted while waiting for notification from AsyncNotifier
2014-07-01 07:45:07,972 DEBUG [RpcServer.handler=6,port=60020] wal.FSHLog: interrupted while waiting for notification from AsyncNotifier
2014-07-01 07:45:07,972 DEBUG [RpcServer.handler=9,port=60020] wal.FSHLog: interrupted while waiting for notification from AsyncNotifier
2014-07-01 07:45:07,973 DEBUG [RpcServer.handler=43,port=60020] wal.FSHLog: interrupted while waiting for notification from AsyncNotifier
2014-07-01 07:45:07,974 INFO  [regionserver60020] regionserver.SplitLogWorker: Sending interrupt to stop the worker thread
2014-07-01 07:45:07,974 INFO  [regionserver60020] regionserver.HRegionServer: Stopping infoServer
2014-07-01 07:45:07,974 DEBUG [RpcServer.handler=46,port=60020] wal.FSHLog: interrupted while waiting for notification from AsyncNotifier
2014-07-01 07:45:07,974 INFO  [SplitLogWorker-slave1,60020,1404225098235] regionserver.SplitLogWorker: SplitLogWorker interrupted while waiting for task, exiting: java.lang.InterruptedException
2014-07-01 07:45:07,991 INFO  [SplitLogWorker-slave1,60020,1404225098235] regionserver.SplitLogWorker: SplitLogWorker slave1,60020,1404225098235 exiting
2014-07-01 07:45:07,991 INFO  [regionserver60020] mortbay.log: Stopped SelectChannelConnector@0.0.0.0:60030
2014-07-01 07:45:07,991 DEBUG [RpcServer.handler=34,port=60020] wal.FSHLog: interrupted while waiting for notification from AsyncNotifier
2014-07-01 07:45:07,991 DEBUG [RpcServer.handler=39,port=60020] wal.FSHLog: interrupted while waiting for notification from AsyncNotifier
2014-07-01 07:45:07,992 DEBUG [RpcServer.handler=37,port=60020] wal.FSHLog: interrupted while waiting for notification from AsyncNotifier
2014-07-01 07:45:07,992 DEBUG [RpcServer.handler=36,port=60020] wal.FSHLog: interrupted while waiting for notification from AsyncNotifier
2014-07-01 07:45:07,992 DEBUG [RpcServer.handler=22,port=60020] wal.FSHLog: interrupted while waiting for notification from AsyncNotifier
2014-07-01 07:45:07,992 DEBUG [RpcServer.handler=27,port=60020] wal.FSHLog: interrupted while waiting for notification from AsyncNotifier
2014-07-01 07:45:07,998 DEBUG [RpcServer.handler=47,port=60020] wal.FSHLog: interrupted while waiting for notification from AsyncNotifier
2014-07-01 07:45:08,051 WARN  [DataStreamer for file /hbase/WALs/slave1,60020,1404225098235/slave1%2C60020%2C1404225098235.1404225812747 block blk_-5973222587916760426_29091] hdfs.DFSClient: Failed recovery attempt #0 from primary datanode 9.1.143.59:50010
org.apache.hadoop.ipc.RemoteException: org.apache.hadoop.ipc.RemoteException: java.io.IOException: blk_-5973222587916760426_29091 has out of date GS 29091 found 29152, may already be committed
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.nextGenerationStampForBlock(FSNamesystem.java:5943)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.nextGenerationStamp(NameNode.java:835)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:622)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:587)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1432)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1428)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:416)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1190)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:1426)

	at org.apache.hadoop.ipc.Client.call(Client.java:1113)
	at org.apache.hadoop.ipc.RPC$Invoker.invoke(RPC.java:229)
	at com.sun.proxy.$Proxy5.nextGenerationStamp(Unknown Source)
	at org.apache.hadoop.hdfs.server.datanode.DataNode.syncBlock(DataNode.java:2127)
	at org.apache.hadoop.hdfs.server.datanode.DataNode.recoverBlock(DataNode.java:2095)
	at org.apache.hadoop.hdfs.server.datanode.DataNode.recoverBlock(DataNode.java:2175)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:622)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:587)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1432)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1428)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:416)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1190)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:1426)

	at org.apache.hadoop.ipc.Client.call(Client.java:1113)
	at org.apache.hadoop.ipc.RPC$Invoker.invoke(RPC.java:229)
	at com.sun.proxy.$Proxy13.recoverBlock(Unknown Source)
	at org.apache.hadoop.hdfs.DFSClient$DFSOutputStream.processDatanodeError(DFSClient.java:3319)
	at org.apache.hadoop.hdfs.DFSClient$DFSOutputStream.access$2200(DFSClient.java:2783)
	at org.apache.hadoop.hdfs.DFSClient$DFSOutputStream$DataStreamer.run(DFSClient.java:2987)
2014-07-01 07:45:08,051 WARN  [DataStreamer for file /hbase/WALs/slave1,60020,1404225098235/slave1%2C60020%2C1404225098235.1404225812747 block blk_-5973222587916760426_29091] hdfs.DFSClient: Error Recovery for block blk_-5973222587916760426_29091 failed  because recovery from primary datanode 9.1.143.59:50010 failed 1 times.  Pipeline was 9.1.143.59:50010. Will retry...
2014-07-01 07:45:08,075 WARN  [DataStreamer for file /hbase/WALs/slave1,60020,1404225098235/slave1%2C60020%2C1404225098235.1404225812747 block blk_-5973222587916760426_29091] hdfs.DFSClient: Failed recovery attempt #1 from primary datanode 9.1.143.59:50010
org.apache.hadoop.ipc.RemoteException: org.apache.hadoop.ipc.RemoteException: java.io.IOException: blk_-5973222587916760426_29091 has out of date GS 29091 found 29152, may already be committed
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.nextGenerationStampForBlock(FSNamesystem.java:5943)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.nextGenerationStamp(NameNode.java:835)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:622)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:587)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1432)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1428)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:416)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1190)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:1426)

	at org.apache.hadoop.ipc.Client.call(Client.java:1113)
	at org.apache.hadoop.ipc.RPC$Invoker.invoke(RPC.java:229)
	at com.sun.proxy.$Proxy5.nextGenerationStamp(Unknown Source)
	at org.apache.hadoop.hdfs.server.datanode.DataNode.syncBlock(DataNode.java:2127)
	at org.apache.hadoop.hdfs.server.datanode.DataNode.recoverBlock(DataNode.java:2095)
	at org.apache.hadoop.hdfs.server.datanode.DataNode.recoverBlock(DataNode.java:2175)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:622)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:587)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1432)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1428)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:416)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1190)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:1426)

	at org.apache.hadoop.ipc.Client.call(Client.java:1113)
	at org.apache.hadoop.ipc.RPC$Invoker.invoke(RPC.java:229)
	at com.sun.proxy.$Proxy13.recoverBlock(Unknown Source)
	at org.apache.hadoop.hdfs.DFSClient$DFSOutputStream.processDatanodeError(DFSClient.java:3319)
	at org.apache.hadoop.hdfs.DFSClient$DFSOutputStream.access$2200(DFSClient.java:2783)
	at org.apache.hadoop.hdfs.DFSClient$DFSOutputStream$DataStreamer.run(DFSClient.java:2987)
2014-07-01 07:45:08,075 WARN  [DataStreamer for file /hbase/WALs/slave1,60020,1404225098235/slave1%2C60020%2C1404225098235.1404225812747 block blk_-5973222587916760426_29091] hdfs.DFSClient: Error Recovery for block blk_-5973222587916760426_29091 failed  because recovery from primary datanode 9.1.143.59:50010 failed 2 times.  Pipeline was 9.1.143.59:50010. Will retry...
2014-07-01 07:45:08,105 WARN  [DataStreamer for file /hbase/WALs/slave1,60020,1404225098235/slave1%2C60020%2C1404225098235.1404225812747 block blk_-5973222587916760426_29091] hdfs.DFSClient: Failed recovery attempt #2 from primary datanode 9.1.143.59:50010
org.apache.hadoop.ipc.RemoteException: org.apache.hadoop.ipc.RemoteException: java.io.IOException: blk_-5973222587916760426_29091 has out of date GS 29091 found 29152, may already be committed
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.nextGenerationStampForBlock(FSNamesystem.java:5943)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.nextGenerationStamp(NameNode.java:835)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:622)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:587)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1432)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1428)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:416)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1190)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:1426)

	at org.apache.hadoop.ipc.Client.call(Client.java:1113)
	at org.apache.hadoop.ipc.RPC$Invoker.invoke(RPC.java:229)
	at com.sun.proxy.$Proxy5.nextGenerationStamp(Unknown Source)
	at org.apache.hadoop.hdfs.server.datanode.DataNode.syncBlock(DataNode.java:2127)
	at org.apache.hadoop.hdfs.server.datanode.DataNode.recoverBlock(DataNode.java:2095)
	at org.apache.hadoop.hdfs.server.datanode.DataNode.recoverBlock(DataNode.java:2175)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:622)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:587)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1432)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1428)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:416)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1190)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:1426)

	at org.apache.hadoop.ipc.Client.call(Client.java:1113)
	at org.apache.hadoop.ipc.RPC$Invoker.invoke(RPC.java:229)
	at com.sun.proxy.$Proxy13.recoverBlock(Unknown Source)
	at org.apache.hadoop.hdfs.DFSClient$DFSOutputStream.processDatanodeError(DFSClient.java:3319)
	at org.apache.hadoop.hdfs.DFSClient$DFSOutputStream.access$2200(DFSClient.java:2783)
	at org.apache.hadoop.hdfs.DFSClient$DFSOutputStream$DataStreamer.run(DFSClient.java:2987)
2014-07-01 07:45:08,109 WARN  [DataStreamer for file /hbase/WALs/slave1,60020,1404225098235/slave1%2C60020%2C1404225098235.1404225812747 block blk_-5973222587916760426_29091] hdfs.DFSClient: Error Recovery for block blk_-5973222587916760426_29091 failed  because recovery from primary datanode 9.1.143.59:50010 failed 3 times.  Pipeline was 9.1.143.59:50010. Will retry...
2014-07-01 07:45:08,135 WARN  [DataStreamer for file /hbase/WALs/slave1,60020,1404225098235/slave1%2C60020%2C1404225098235.1404225812747 block blk_-5973222587916760426_29091] hdfs.DFSClient: Failed recovery attempt #3 from primary datanode 9.1.143.59:50010
org.apache.hadoop.ipc.RemoteException: org.apache.hadoop.ipc.RemoteException: java.io.IOException: blk_-5973222587916760426_29091 has out of date GS 29091 found 29152, may already be committed
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.nextGenerationStampForBlock(FSNamesystem.java:5943)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.nextGenerationStamp(NameNode.java:835)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:622)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:587)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1432)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1428)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:416)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1190)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:1426)

	at org.apache.hadoop.ipc.Client.call(Client.java:1113)
	at org.apache.hadoop.ipc.RPC$Invoker.invoke(RPC.java:229)
	at com.sun.proxy.$Proxy5.nextGenerationStamp(Unknown Source)
	at org.apache.hadoop.hdfs.server.datanode.DataNode.syncBlock(DataNode.java:2127)
	at org.apache.hadoop.hdfs.server.datanode.DataNode.recoverBlock(DataNode.java:2095)
	at org.apache.hadoop.hdfs.server.datanode.DataNode.recoverBlock(DataNode.java:2175)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:622)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:587)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1432)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1428)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:416)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1190)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:1426)

	at org.apache.hadoop.ipc.Client.call(Client.java:1113)
	at org.apache.hadoop.ipc.RPC$Invoker.invoke(RPC.java:229)
	at com.sun.proxy.$Proxy13.recoverBlock(Unknown Source)
	at org.apache.hadoop.hdfs.DFSClient$DFSOutputStream.processDatanodeError(DFSClient.java:3319)
	at org.apache.hadoop.hdfs.DFSClient$DFSOutputStream.access$2200(DFSClient.java:2783)
	at org.apache.hadoop.hdfs.DFSClient$DFSOutputStream$DataStreamer.run(DFSClient.java:2987)
2014-07-01 07:45:08,136 WARN  [DataStreamer for file /hbase/WALs/slave1,60020,1404225098235/slave1%2C60020%2C1404225098235.1404225812747 block blk_-5973222587916760426_29091] hdfs.DFSClient: Error Recovery for block blk_-5973222587916760426_29091 failed  because recovery from primary datanode 9.1.143.59:50010 failed 4 times.  Pipeline was 9.1.143.59:50010. Will retry...
2014-07-01 07:45:08,138 INFO  [regionserver60020-SendThread(sceplus-vm49.almaden.ibm.com:2181)] zookeeper.ClientCnxn: Opening socket connection to server sceplus-vm49.almaden.ibm.com/9.1.143.59:2181. Will not attempt to authenticate using SASL (unknown error)
2014-07-01 07:45:08,139 INFO  [regionserver60020-SendThread(sceplus-vm49.almaden.ibm.com:2181)] zookeeper.ClientCnxn: Socket connection established to sceplus-vm49.almaden.ibm.com/9.1.143.59:2181, initiating session
2014-07-01 07:45:08,151 WARN  [DataStreamer for file /hbase/WALs/slave1,60020,1404225098235/slave1%2C60020%2C1404225098235.1404225812747 block blk_-5973222587916760426_29091] hdfs.DFSClient: Failed recovery attempt #4 from primary datanode 9.1.143.59:50010
org.apache.hadoop.ipc.RemoteException: org.apache.hadoop.ipc.RemoteException: java.io.IOException: blk_-5973222587916760426_29091 has out of date GS 29091 found 29152, may already be committed
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.nextGenerationStampForBlock(FSNamesystem.java:5943)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.nextGenerationStamp(NameNode.java:835)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:622)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:587)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1432)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1428)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:416)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1190)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:1426)

	at org.apache.hadoop.ipc.Client.call(Client.java:1113)
	at org.apache.hadoop.ipc.RPC$Invoker.invoke(RPC.java:229)
	at com.sun.proxy.$Proxy5.nextGenerationStamp(Unknown Source)
	at org.apache.hadoop.hdfs.server.datanode.DataNode.syncBlock(DataNode.java:2127)
	at org.apache.hadoop.hdfs.server.datanode.DataNode.recoverBlock(DataNode.java:2095)
	at org.apache.hadoop.hdfs.server.datanode.DataNode.recoverBlock(DataNode.java:2175)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:622)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:587)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1432)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1428)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:416)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1190)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:1426)

	at org.apache.hadoop.ipc.Client.call(Client.java:1113)
	at org.apache.hadoop.ipc.RPC$Invoker.invoke(RPC.java:229)
	at com.sun.proxy.$Proxy13.recoverBlock(Unknown Source)
	at org.apache.hadoop.hdfs.DFSClient$DFSOutputStream.processDatanodeError(DFSClient.java:3319)
	at org.apache.hadoop.hdfs.DFSClient$DFSOutputStream.access$2200(DFSClient.java:2783)
	at org.apache.hadoop.hdfs.DFSClient$DFSOutputStream$DataStreamer.run(DFSClient.java:2987)
2014-07-01 07:45:08,151 INFO  [regionserver60020-SendThread(sceplus-vm49.almaden.ibm.com:2181)] zookeeper.ClientCnxn: Unable to reconnect to ZooKeeper service, session 0x46f25690f50001 has expired, closing socket connection
2014-07-01 07:45:08,151 WARN  [DataStreamer for file /hbase/WALs/slave1,60020,1404225098235/slave1%2C60020%2C1404225098235.1404225812747 block blk_-5973222587916760426_29091] hdfs.DFSClient: Error Recovery for block blk_-5973222587916760426_29091 failed  because recovery from primary datanode 9.1.143.59:50010 failed 5 times.  Pipeline was 9.1.143.59:50010. Will retry...
2014-07-01 07:45:08,151 FATAL [regionserver60020-EventThread] regionserver.HRegionServer: ABORTING region server slave1,60020,1404225098235: regionserver:60020-0x46f25690f50001-0x46f25690f50001, quorum=sceplus-vm49.almaden.ibm.com:2181,sceplus-vm48.almaden.ibm.com:2181, baseZNode=/hbase regionserver:60020-0x46f25690f50001-0x46f25690f50001 received expired from ZooKeeper, aborting
org.apache.zookeeper.KeeperException$SessionExpiredException: KeeperErrorCode = Session expired
	at org.apache.hadoop.hbase.zookeeper.ZooKeeperWatcher.connectionEvent(ZooKeeperWatcher.java:401)
	at org.apache.hadoop.hbase.zookeeper.ZooKeeperWatcher.process(ZooKeeperWatcher.java:319)
	at org.apache.zookeeper.ClientCnxn$EventThread.processEvent(ClientCnxn.java:522)
	at org.apache.zookeeper.ClientCnxn$EventThread.run(ClientCnxn.java:498)
2014-07-01 07:45:08,153 FATAL [regionserver60020-EventThread] regionserver.HRegionServer: RegionServer abort: loaded coprocessors are: []
2014-07-01 07:45:08,169 WARN  [DataStreamer for file /hbase/WALs/slave1,60020,1404225098235/slave1%2C60020%2C1404225098235.1404225812747 block blk_-5973222587916760426_29091] hdfs.DFSClient: Failed recovery attempt #5 from primary datanode 9.1.143.59:50010
org.apache.hadoop.ipc.RemoteException: org.apache.hadoop.ipc.RemoteException: java.io.IOException: blk_-5973222587916760426_29091 has out of date GS 29091 found 29152, may already be committed
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.nextGenerationStampForBlock(FSNamesystem.java:5943)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.nextGenerationStamp(NameNode.java:835)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:622)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:587)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1432)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1428)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:416)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1190)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:1426)

	at org.apache.hadoop.ipc.Client.call(Client.java:1113)
	at org.apache.hadoop.ipc.RPC$Invoker.invoke(RPC.java:229)
	at com.sun.proxy.$Proxy5.nextGenerationStamp(Unknown Source)
	at org.apache.hadoop.hdfs.server.datanode.DataNode.syncBlock(DataNode.java:2127)
	at org.apache.hadoop.hdfs.server.datanode.DataNode.recoverBlock(DataNode.java:2095)
	at org.apache.hadoop.hdfs.server.datanode.DataNode.recoverBlock(DataNode.java:2175)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:622)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:587)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1432)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1428)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:416)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1190)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:1426)

	at org.apache.hadoop.ipc.Client.call(Client.java:1113)
	at org.apache.hadoop.ipc.RPC$Invoker.invoke(RPC.java:229)
	at com.sun.proxy.$Proxy13.recoverBlock(Unknown Source)
	at org.apache.hadoop.hdfs.DFSClient$DFSOutputStream.processDatanodeError(DFSClient.java:3319)
	at org.apache.hadoop.hdfs.DFSClient$DFSOutputStream.access$2200(DFSClient.java:2783)
	at org.apache.hadoop.hdfs.DFSClient$DFSOutputStream$DataStreamer.run(DFSClient.java:2987)
2014-07-01 07:45:08,169 WARN  [DataStreamer for file /hbase/data/default/usertable/07064f253c865a7cc0c08893a4f56af9/.tmp/a17a8b64528148559a6414832c7b909b block blk_4372291247462747067_29087] hdfs.DFSClient: Error Recovery for blk_4372291247462747067_29087 bad datanode[0] 9.1.143.59:50010
2014-07-01 07:45:08,169 WARN  [DataStreamer for file /hbase/WALs/slave1,60020,1404225098235/slave1%2C60020%2C1404225098235.1404225812747 block blk_-5973222587916760426_29091] hdfs.DFSClient: Error Recovery for block blk_-5973222587916760426_29091 failed  because recovery from primary datanode 9.1.143.59:50010 failed 6 times.  Pipeline was 9.1.143.59:50010. Aborting...
2014-07-01 07:45:08,169 WARN  [DataStreamer for file /hbase/data/default/usertable/07064f253c865a7cc0c08893a4f56af9/.tmp/a17a8b64528148559a6414832c7b909b block blk_4372291247462747067_29087] hdfs.DFSClient: Error Recovery for block blk_4372291247462747067_29087 in pipeline 9.1.143.59:50010, 9.1.143.58:50010: bad datanode 9.1.143.59:50010
2014-07-01 07:45:08,169 WARN  [regionserver60020-WAL.AsyncSyncer0] hdfs.DFSClient: Error while syncing
java.io.IOException: Error Recovery for block blk_-5973222587916760426_29091 failed  because recovery from primary datanode 9.1.143.59:50010 failed 6 times.  Pipeline was 9.1.143.59:50010. Aborting...
	at org.apache.hadoop.hdfs.DFSClient$DFSOutputStream.processDatanodeError(DFSClient.java:3355)
	at org.apache.hadoop.hdfs.DFSClient$DFSOutputStream.access$2200(DFSClient.java:2783)
	at org.apache.hadoop.hdfs.DFSClient$DFSOutputStream$DataStreamer.run(DFSClient.java:2987)
2014-07-01 07:45:08,169 INFO  [regionserver60020-EventThread] zookeeper.ClientCnxn: EventThread shut down
2014-07-01 07:45:08,170 FATAL [regionserver60020-WAL.AsyncSyncer0] wal.FSHLog: Error while AsyncSyncer sync, request close of hlog 
java.io.IOException: Error Recovery for block blk_-5973222587916760426_29091 failed  because recovery from primary datanode 9.1.143.59:50010 failed 6 times.  Pipeline was 9.1.143.59:50010. Aborting...
	at org.apache.hadoop.hdfs.DFSClient$DFSOutputStream.processDatanodeError(DFSClient.java:3355)
	at org.apache.hadoop.hdfs.DFSClient$DFSOutputStream.access$2200(DFSClient.java:2783)
	at org.apache.hadoop.hdfs.DFSClient$DFSOutputStream$DataStreamer.run(DFSClient.java:2987)
2014-07-01 07:45:08,169 ERROR [regionserver60020-WAL.AsyncWriter] wal.FSHLog: Error while AsyncWriter write, request close of hlog 
java.io.IOException: Error Recovery for block blk_-5973222587916760426_29091 failed  because recovery from primary datanode 9.1.143.59:50010 failed 6 times.  Pipeline was 9.1.143.59:50010. Aborting...
	at org.apache.hadoop.hdfs.DFSClient$DFSOutputStream.processDatanodeError(DFSClient.java:3355)
	at org.apache.hadoop.hdfs.DFSClient$DFSOutputStream.access$2200(DFSClient.java:2783)
	at org.apache.hadoop.hdfs.DFSClient$DFSOutputStream$DataStreamer.run(DFSClient.java:2987)
2014-07-01 07:45:08,171 WARN  [regionserver60020-WAL.AsyncSyncer0] wal.FSHLog: HDFS pipeline error detected. Found 1 replicas but expecting no less than 2 replicas.  Requesting close of hlog.
2014-07-01 07:45:08,187 FATAL [regionserver60020-WAL.AsyncSyncer0] wal.FSHLog: Error while AsyncSyncer sync, request close of hlog 
java.io.IOException: DFSOutputStream is closed
	at org.apache.hadoop.hdfs.DFSClient$DFSOutputStream.sync(DFSClient.java:3879)
	at org.apache.hadoop.fs.FSDataOutputStream.sync(FSDataOutputStream.java:97)
	at org.apache.hadoop.hbase.regionserver.wal.ProtobufLogWriter.sync(ProtobufLogWriter.java:158)
	at org.apache.hadoop.hbase.regionserver.wal.FSHLog$AsyncSyncer.run(FSHLog.java:1240)
	at java.lang.Thread.run(Thread.java:701)
2014-07-01 07:45:08,188 WARN  [regionserver60020-WAL.AsyncSyncer0] wal.FSHLog: HDFS pipeline error detected. Found 1 replicas but expecting no less than 2 replicas.  Requesting close of hlog.
2014-07-01 07:45:08,171 INFO  [regionserver60020.logRoller] regionserver.LogRoller: LogRoller exiting.
2014-07-01 07:45:08,201 WARN  [DataStreamer for file /hbase/data/default/usertable/07064f253c865a7cc0c08893a4f56af9/.tmp/a17a8b64528148559a6414832c7b909b block blk_4372291247462747067_29087] hdfs.DFSClient: Failed recovery attempt #2 from primary datanode 9.1.143.58:50010
org.apache.hadoop.ipc.RemoteException: org.apache.hadoop.ipc.RemoteException: java.io.IOException: blk_4372291247462747067_29087 is missing
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.nextGenerationStampForBlock(FSNamesystem.java:5943)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.nextGenerationStamp(NameNode.java:835)
	at sun.reflect.GeneratedMethodAccessor18.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:622)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:587)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1432)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1428)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:416)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1190)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:1426)

	at org.apache.hadoop.ipc.Client.call(Client.java:1113)
	at org.apache.hadoop.ipc.RPC$Invoker.invoke(RPC.java:229)
	at com.sun.proxy.$Proxy5.nextGenerationStamp(Unknown Source)
	at org.apache.hadoop.hdfs.server.datanode.DataNode.syncBlock(DataNode.java:2127)
	at org.apache.hadoop.hdfs.server.datanode.DataNode.recoverBlock(DataNode.java:2095)
	at org.apache.hadoop.hdfs.server.datanode.DataNode.recoverBlock(DataNode.java:2175)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:622)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:587)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1432)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1428)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:416)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1190)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:1426)

	at org.apache.hadoop.ipc.Client.call(Client.java:1113)
	at org.apache.hadoop.ipc.RPC$Invoker.invoke(RPC.java:229)
	at com.sun.proxy.$Proxy13.recoverBlock(Unknown Source)
	at org.apache.hadoop.hdfs.DFSClient$DFSOutputStream.processDatanodeError(DFSClient.java:3319)
	at org.apache.hadoop.hdfs.DFSClient$DFSOutputStream.access$2200(DFSClient.java:2783)
	at org.apache.hadoop.hdfs.DFSClient$DFSOutputStream$DataStreamer.run(DFSClient.java:2987)
2014-07-01 07:45:08,201 WARN  [DataStreamer for file /hbase/data/default/usertable/07064f253c865a7cc0c08893a4f56af9/.tmp/a17a8b64528148559a6414832c7b909b block blk_4372291247462747067_29087] hdfs.DFSClient: Error Recovery for block blk_4372291247462747067_29087 failed  because recovery from primary datanode 9.1.143.58:50010 failed 3 times.  Pipeline was 9.1.143.59:50010, 9.1.143.58:50010. Will retry...
2014-07-01 07:45:08,242 ERROR [regionserver60020-WAL.AsyncWriter] wal.FSHLog: Error while AsyncWriter write, request close of hlog 
java.io.IOException: Error Recovery for block blk_-5973222587916760426_29091 failed  because recovery from primary datanode 9.1.143.59:50010 failed 6 times.  Pipeline was 9.1.143.59:50010. Aborting...
	at org.apache.hadoop.hdfs.DFSClient$DFSOutputStream.processDatanodeError(DFSClient.java:3355)
	at org.apache.hadoop.hdfs.DFSClient$DFSOutputStream.access$2200(DFSClient.java:2783)
	at org.apache.hadoop.hdfs.DFSClient$DFSOutputStream$DataStreamer.run(DFSClient.java:2987)
2014-07-01 07:45:08,243 FATAL [regionserver60020-WAL.AsyncSyncer0] wal.FSHLog: Error while AsyncSyncer sync, request close of hlog 
java.io.IOException: DFSOutputStream is closed
	at org.apache.hadoop.hdfs.DFSClient$DFSOutputStream.sync(DFSClient.java:3879)
	at org.apache.hadoop.fs.FSDataOutputStream.sync(FSDataOutputStream.java:97)
	at org.apache.hadoop.hbase.regionserver.wal.ProtobufLogWriter.sync(ProtobufLogWriter.java:158)
	at org.apache.hadoop.hbase.regionserver.wal.FSHLog$AsyncSyncer.run(FSHLog.java:1240)
	at java.lang.Thread.run(Thread.java:701)
2014-07-01 07:45:08,243 WARN  [regionserver60020-WAL.AsyncSyncer0] wal.FSHLog: HDFS pipeline error detected. Found 1 replicas but expecting no less than 2 replicas.  Requesting close of hlog.
2014-07-01 07:45:08,243 DEBUG [RpcServer.handler=11,port=60020] wal.FSHLog: interrupted while waiting for notification from AsyncNotifier
2014-07-01 07:45:08,275 INFO  [regionserver60020-SendThread(sceplus-vm49.almaden.ibm.com:2181)] zookeeper.ClientCnxn: Opening socket connection to server sceplus-vm49.almaden.ibm.com/9.1.143.59:2181. Will not attempt to authenticate using SASL (unknown error)
2014-07-01 07:45:08,275 INFO  [regionserver60020-SendThread(sceplus-vm49.almaden.ibm.com:2181)] zookeeper.ClientCnxn: Socket connection established to sceplus-vm49.almaden.ibm.com/9.1.143.59:2181, initiating session
2014-07-01 07:45:08,277 INFO  [regionserver60020-SendThread(sceplus-vm49.almaden.ibm.com:2181)] zookeeper.ClientCnxn: Unable to reconnect to ZooKeeper service, session 0x46f25690f50003 has expired, closing socket connection
2014-07-01 07:45:08,277 WARN  [regionserver60020-EventThread] client.HConnectionManager$HConnectionImplementation: This client just lost it's session with ZooKeeper, closing it. It will be recreated next time someone needs it
org.apache.zookeeper.KeeperException$SessionExpiredException: KeeperErrorCode = Session expired
	at org.apache.hadoop.hbase.zookeeper.ZooKeeperWatcher.connectionEvent(ZooKeeperWatcher.java:401)
	at org.apache.hadoop.hbase.zookeeper.ZooKeeperWatcher.process(ZooKeeperWatcher.java:319)
	at org.apache.zookeeper.ClientCnxn$EventThread.processEvent(ClientCnxn.java:522)
	at org.apache.zookeeper.ClientCnxn$EventThread.run(ClientCnxn.java:498)
2014-07-01 07:45:08,277 INFO  [regionserver60020-EventThread] client.HConnectionManager$HConnectionImplementation: Closing zookeeper sessionid=0x46f25690f50003
2014-07-01 07:45:08,278 INFO  [regionserver60020-EventThread] zookeeper.ClientCnxn: EventThread shut down
2014-07-01 07:45:08,278 WARN  [DataStreamer for file /hbase/data/default/usertable/07064f253c865a7cc0c08893a4f56af9/.tmp/a17a8b64528148559a6414832c7b909b block blk_4372291247462747067_29087] hdfs.DFSClient: Error Recovery for blk_4372291247462747067_29087 bad datanode[0] 9.1.143.59:50010
2014-07-01 07:45:08,279 WARN  [DataStreamer for file /hbase/data/default/usertable/07064f253c865a7cc0c08893a4f56af9/.tmp/a17a8b64528148559a6414832c7b909b block blk_4372291247462747067_29087] hdfs.DFSClient: Error Recovery for block blk_4372291247462747067_29087 in pipeline 9.1.143.59:50010, 9.1.143.58:50010: bad datanode 9.1.143.59:50010
2014-07-01 07:45:08,281 ERROR [regionserver60020-WAL.AsyncWriter] wal.FSHLog: Error while AsyncWriter write, request close of hlog 
java.io.IOException: Error Recovery for block blk_-5973222587916760426_29091 failed  because recovery from primary datanode 9.1.143.59:50010 failed 6 times.  Pipeline was 9.1.143.59:50010. Aborting...
	at org.apache.hadoop.hdfs.DFSClient$DFSOutputStream.processDatanodeError(DFSClient.java:3355)
	at org.apache.hadoop.hdfs.DFSClient$DFSOutputStream.access$2200(DFSClient.java:2783)
	at org.apache.hadoop.hdfs.DFSClient$DFSOutputStream$DataStreamer.run(DFSClient.java:2987)
2014-07-01 07:45:08,281 FATAL [regionserver60020-WAL.AsyncSyncer0] wal.FSHLog: Error while AsyncSyncer sync, request close of hlog 
java.io.IOException: DFSOutputStream is closed
	at org.apache.hadoop.hdfs.DFSClient$DFSOutputStream.sync(DFSClient.java:3879)
	at org.apache.hadoop.fs.FSDataOutputStream.sync(FSDataOutputStream.java:97)
	at org.apache.hadoop.hbase.regionserver.wal.ProtobufLogWriter.sync(ProtobufLogWriter.java:158)
	at org.apache.hadoop.hbase.regionserver.wal.FSHLog$AsyncSyncer.run(FSHLog.java:1240)
	at java.lang.Thread.run(Thread.java:701)
2014-07-01 07:45:08,281 DEBUG [RpcServer.handler=1,port=60020] wal.FSHLog: interrupted while waiting for notification from AsyncNotifier
2014-07-01 07:45:08,282 WARN  [regionserver60020-WAL.AsyncSyncer0] wal.FSHLog: HDFS pipeline error detected. Found 1 replicas but expecting no less than 2 replicas.  Requesting close of hlog.
2014-07-01 07:45:08,284 ERROR [regionserver60020-WAL.AsyncWriter] wal.FSHLog: Error while AsyncWriter write, request close of hlog 
java.io.IOException: Error Recovery for block blk_-5973222587916760426_29091 failed  because recovery from primary datanode 9.1.143.59:50010 failed 6 times.  Pipeline was 9.1.143.59:50010. Aborting...
	at org.apache.hadoop.hdfs.DFSClient$DFSOutputStream.processDatanodeError(DFSClient.java:3355)
	at org.apache.hadoop.hdfs.DFSClient$DFSOutputStream.access$2200(DFSClient.java:2783)
	at org.apache.hadoop.hdfs.DFSClient$DFSOutputStream$DataStreamer.run(DFSClient.java:2987)
2014-07-01 07:45:08,284 FATAL [regionserver60020-WAL.AsyncSyncer0] wal.FSHLog: Error while AsyncSyncer sync, request close of hlog 
java.io.IOException: DFSOutputStream is closed
	at org.apache.hadoop.hdfs.DFSClient$DFSOutputStream.sync(DFSClient.java:3879)
	at org.apache.hadoop.fs.FSDataOutputStream.sync(FSDataOutputStream.java:97)
	at org.apache.hadoop.hbase.regionserver.wal.ProtobufLogWriter.sync(ProtobufLogWriter.java:158)
	at org.apache.hadoop.hbase.regionserver.wal.FSHLog$AsyncSyncer.run(FSHLog.java:1240)
	at java.lang.Thread.run(Thread.java:701)
2014-07-01 07:45:08,284 WARN  [regionserver60020-WAL.AsyncSyncer0] wal.FSHLog: HDFS pipeline error detected. Found 1 replicas but expecting no less than 2 replicas.  Requesting close of hlog.
2014-07-01 07:45:08,291 ERROR [regionserver60020-WAL.AsyncWriter] wal.FSHLog: Error while AsyncWriter write, request close of hlog 
java.io.IOException: Error Recovery for block blk_-5973222587916760426_29091 failed  because recovery from primary datanode 9.1.143.59:50010 failed 6 times.  Pipeline was 9.1.143.59:50010. Aborting...
	at org.apache.hadoop.hdfs.DFSClient$DFSOutputStream.processDatanodeError(DFSClient.java:3355)
	at org.apache.hadoop.hdfs.DFSClient$DFSOutputStream.access$2200(DFSClient.java:2783)
	at org.apache.hadoop.hdfs.DFSClient$DFSOutputStream$DataStreamer.run(DFSClient.java:2987)
2014-07-01 07:45:08,291 DEBUG [RpcServer.handler=32,port=60020] wal.FSHLog: interrupted while waiting for notification from AsyncNotifier
2014-07-01 07:45:08,291 FATAL [regionserver60020-WAL.AsyncSyncer0] wal.FSHLog: Error while AsyncSyncer sync, request close of hlog 
java.io.IOException: DFSOutputStream is closed
	at org.apache.hadoop.hdfs.DFSClient$DFSOutputStream.sync(DFSClient.java:3879)
	at org.apache.hadoop.fs.FSDataOutputStream.sync(FSDataOutputStream.java:97)
	at org.apache.hadoop.hbase.regionserver.wal.ProtobufLogWriter.sync(ProtobufLogWriter.java:158)
	at org.apache.hadoop.hbase.regionserver.wal.FSHLog$AsyncSyncer.run(FSHLog.java:1240)
	at java.lang.Thread.run(Thread.java:701)
2014-07-01 07:45:08,292 WARN  [regionserver60020-WAL.AsyncSyncer0] wal.FSHLog: Too many consecutive RollWriter requests, it's a sign of the total number of live datanodes is lower than the tolerable replicas.
2014-07-01 07:45:08,293 ERROR [regionserver60020-WAL.AsyncWriter] wal.FSHLog: Error while AsyncWriter write, request close of hlog 
java.io.IOException: Error Recovery for block blk_-5973222587916760426_29091 failed  because recovery from primary datanode 9.1.143.59:50010 failed 6 times.  Pipeline was 9.1.143.59:50010. Aborting...
	at org.apache.hadoop.hdfs.DFSClient$DFSOutputStream.processDatanodeError(DFSClient.java:3355)
	at org.apache.hadoop.hdfs.DFSClient$DFSOutputStream.access$2200(DFSClient.java:2783)
	at org.apache.hadoop.hdfs.DFSClient$DFSOutputStream$DataStreamer.run(DFSClient.java:2987)
2014-07-01 07:45:08,294 DEBUG [RpcServer.handler=45,port=60020] wal.FSHLog: interrupted while waiting for notification from AsyncNotifier
2014-07-01 07:45:08,294 FATAL [regionserver60020-WAL.AsyncSyncer0] wal.FSHLog: Error while AsyncSyncer sync, request close of hlog 
java.io.IOException: DFSOutputStream is closed
	at org.apache.hadoop.hdfs.DFSClient$DFSOutputStream.sync(DFSClient.java:3879)
	at org.apache.hadoop.fs.FSDataOutputStream.sync(FSDataOutputStream.java:97)
	at org.apache.hadoop.hbase.regionserver.wal.ProtobufLogWriter.sync(ProtobufLogWriter.java:158)
	at org.apache.hadoop.hbase.regionserver.wal.FSHLog$AsyncSyncer.run(FSHLog.java:1240)
	at java.lang.Thread.run(Thread.java:701)
2014-07-01 07:45:08,306 WARN  [DataStreamer for file /hbase/data/default/usertable/07064f253c865a7cc0c08893a4f56af9/.tmp/a17a8b64528148559a6414832c7b909b block blk_4372291247462747067_29087] hdfs.DFSClient: Failed recovery attempt #3 from primary datanode 9.1.143.58:50010
org.apache.hadoop.ipc.RemoteException: org.apache.hadoop.ipc.RemoteException: java.io.IOException: blk_4372291247462747067_29087 is missing
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.nextGenerationStampForBlock(FSNamesystem.java:5943)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.nextGenerationStamp(NameNode.java:835)
	at sun.reflect.GeneratedMethodAccessor18.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:622)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:587)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1432)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1428)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:416)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1190)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:1426)

	at org.apache.hadoop.ipc.Client.call(Client.java:1113)
	at org.apache.hadoop.ipc.RPC$Invoker.invoke(RPC.java:229)
	at com.sun.proxy.$Proxy5.nextGenerationStamp(Unknown Source)
	at org.apache.hadoop.hdfs.server.datanode.DataNode.syncBlock(DataNode.java:2127)
	at org.apache.hadoop.hdfs.server.datanode.DataNode.recoverBlock(DataNode.java:2095)
	at org.apache.hadoop.hdfs.server.datanode.DataNode.recoverBlock(DataNode.java:2175)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:622)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:587)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1432)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1428)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:416)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1190)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:1426)

	at org.apache.hadoop.ipc.Client.call(Client.java:1113)
	at org.apache.hadoop.ipc.RPC$Invoker.invoke(RPC.java:229)
	at com.sun.proxy.$Proxy13.recoverBlock(Unknown Source)
	at org.apache.hadoop.hdfs.DFSClient$DFSOutputStream.processDatanodeError(DFSClient.java:3319)
	at org.apache.hadoop.hdfs.DFSClient$DFSOutputStream.access$2200(DFSClient.java:2783)
	at org.apache.hadoop.hdfs.DFSClient$DFSOutputStream$DataStreamer.run(DFSClient.java:2987)
2014-07-01 07:45:08,306 WARN  [DataStreamer for file /hbase/data/default/usertable/07064f253c865a7cc0c08893a4f56af9/.tmp/a17a8b64528148559a6414832c7b909b block blk_4372291247462747067_29087] hdfs.DFSClient: Error Recovery for block blk_4372291247462747067_29087 failed  because recovery from primary datanode 9.1.143.58:50010 failed 4 times.  Pipeline was 9.1.143.59:50010, 9.1.143.58:50010. Will retry...
2014-07-01 07:45:08,313 ERROR [regionserver60020-WAL.AsyncWriter] wal.FSHLog: Error while AsyncWriter write, request close of hlog 
java.io.IOException: Error Recovery for block blk_-5973222587916760426_29091 failed  because recovery from primary datanode 9.1.143.59:50010 failed 6 times.  Pipeline was 9.1.143.59:50010. Aborting...
	at org.apache.hadoop.hdfs.DFSClient$DFSOutputStream.processDatanodeError(DFSClient.java:3355)
	at org.apache.hadoop.hdfs.DFSClient$DFSOutputStream.access$2200(DFSClient.java:2783)
	at org.apache.hadoop.hdfs.DFSClient$DFSOutputStream$DataStreamer.run(DFSClient.java:2987)
2014-07-01 07:45:08,313 DEBUG [RpcServer.handler=48,port=60020] wal.FSHLog: interrupted while waiting for notification from AsyncNotifier
2014-07-01 07:45:08,313 FATAL [regionserver60020-WAL.AsyncSyncer0] wal.FSHLog: Error while AsyncSyncer sync, request close of hlog 
java.io.IOException: DFSOutputStream is closed
	at org.apache.hadoop.hdfs.DFSClient$DFSOutputStream.sync(DFSClient.java:3879)
	at org.apache.hadoop.fs.FSDataOutputStream.sync(FSDataOutputStream.java:97)
	at org.apache.hadoop.hbase.regionserver.wal.ProtobufLogWriter.sync(ProtobufLogWriter.java:158)
	at org.apache.hadoop.hbase.regionserver.wal.FSHLog$AsyncSyncer.run(FSHLog.java:1240)
	at java.lang.Thread.run(Thread.java:701)
2014-07-01 07:45:08,316 ERROR [regionserver60020-WAL.AsyncWriter] wal.FSHLog: Error while AsyncWriter write, request close of hlog 
java.io.IOException: Error Recovery for block blk_-5973222587916760426_29091 failed  because recovery from primary datanode 9.1.143.59:50010 failed 6 times.  Pipeline was 9.1.143.59:50010. Aborting...
	at org.apache.hadoop.hdfs.DFSClient$DFSOutputStream.processDatanodeError(DFSClient.java:3355)
	at org.apache.hadoop.hdfs.DFSClient$DFSOutputStream.access$2200(DFSClient.java:2783)
	at org.apache.hadoop.hdfs.DFSClient$DFSOutputStream$DataStreamer.run(DFSClient.java:2987)
2014-07-01 07:45:08,316 DEBUG [RpcServer.handler=40,port=60020] wal.FSHLog: interrupted while waiting for notification from AsyncNotifier
2014-07-01 07:45:08,316 FATAL [regionserver60020-WAL.AsyncSyncer0] wal.FSHLog: Error while AsyncSyncer sync, request close of hlog 
java.io.IOException: DFSOutputStream is closed
	at org.apache.hadoop.hdfs.DFSClient$DFSOutputStream.sync(DFSClient.java:3879)
	at org.apache.hadoop.fs.FSDataOutputStream.sync(FSDataOutputStream.java:97)
	at org.apache.hadoop.hbase.regionserver.wal.ProtobufLogWriter.sync(ProtobufLogWriter.java:158)
	at org.apache.hadoop.hbase.regionserver.wal.FSHLog$AsyncSyncer.run(FSHLog.java:1240)
	at java.lang.Thread.run(Thread.java:701)
2014-07-01 07:45:08,317 ERROR [regionserver60020-WAL.AsyncWriter] wal.FSHLog: Error while AsyncWriter write, request close of hlog 
java.io.IOException: Error Recovery for block blk_-5973222587916760426_29091 failed  because recovery from primary datanode 9.1.143.59:50010 failed 6 times.  Pipeline was 9.1.143.59:50010. Aborting...
	at org.apache.hadoop.hdfs.DFSClient$DFSOutputStream.processDatanodeError(DFSClient.java:3355)
	at org.apache.hadoop.hdfs.DFSClient$DFSOutputStream.access$2200(DFSClient.java:2783)
	at org.apache.hadoop.hdfs.DFSClient$DFSOutputStream$DataStreamer.run(DFSClient.java:2987)
2014-07-01 07:45:08,317 FATAL [regionserver60020-WAL.AsyncSyncer0] wal.FSHLog: Error while AsyncSyncer sync, request close of hlog 
java.io.IOException: DFSOutputStream is closed
	at org.apache.hadoop.hdfs.DFSClient$DFSOutputStream.sync(DFSClient.java:3879)
	at org.apache.hadoop.fs.FSDataOutputStream.sync(FSDataOutputStream.java:97)
	at org.apache.hadoop.hbase.regionserver.wal.ProtobufLogWriter.sync(ProtobufLogWriter.java:158)
	at org.apache.hadoop.hbase.regionserver.wal.FSHLog$AsyncSyncer.run(FSHLog.java:1240)
	at java.lang.Thread.run(Thread.java:701)
2014-07-01 07:45:08,317 DEBUG [RpcServer.handler=49,port=60020] wal.FSHLog: interrupted while waiting for notification from AsyncNotifier
2014-07-01 07:45:08,329 ERROR [regionserver60020-WAL.AsyncWriter] wal.FSHLog: Error while AsyncWriter write, request close of hlog 
java.io.IOException: Error Recovery for block blk_-5973222587916760426_29091 failed  because recovery from primary datanode 9.1.143.59:50010 failed 6 times.  Pipeline was 9.1.143.59:50010. Aborting...
	at org.apache.hadoop.hdfs.DFSClient$DFSOutputStream.processDatanodeError(DFSClient.java:3355)
	at org.apache.hadoop.hdfs.DFSClient$DFSOutputStream.access$2200(DFSClient.java:2783)
	at org.apache.hadoop.hdfs.DFSClient$DFSOutputStream$DataStreamer.run(DFSClient.java:2987)
2014-07-01 07:45:08,329 DEBUG [RpcServer.handler=35,port=60020] wal.FSHLog: interrupted while waiting for notification from AsyncNotifier
2014-07-01 07:45:08,329 FATAL [regionserver60020-WAL.AsyncSyncer0] wal.FSHLog: Error while AsyncSyncer sync, request close of hlog 
java.io.IOException: DFSOutputStream is closed
	at org.apache.hadoop.hdfs.DFSClient$DFSOutputStream.sync(DFSClient.java:3879)
	at org.apache.hadoop.fs.FSDataOutputStream.sync(FSDataOutputStream.java:97)
	at org.apache.hadoop.hbase.regionserver.wal.ProtobufLogWriter.sync(ProtobufLogWriter.java:158)
	at org.apache.hadoop.hbase.regionserver.wal.FSHLog$AsyncSyncer.run(FSHLog.java:1240)
	at java.lang.Thread.run(Thread.java:701)
2014-07-01 07:45:08,333 ERROR [regionserver60020-WAL.AsyncWriter] wal.FSHLog: Error while AsyncWriter write, request close of hlog 
java.io.IOException: Error Recovery for block blk_-5973222587916760426_29091 failed  because recovery from primary datanode 9.1.143.59:50010 failed 6 times.  Pipeline was 9.1.143.59:50010. Aborting...
	at org.apache.hadoop.hdfs.DFSClient$DFSOutputStream.processDatanodeError(DFSClient.java:3355)
	at org.apache.hadoop.hdfs.DFSClient$DFSOutputStream.access$2200(DFSClient.java:2783)
	at org.apache.hadoop.hdfs.DFSClient$DFSOutputStream$DataStreamer.run(DFSClient.java:2987)
2014-07-01 07:45:08,334 DEBUG [RpcServer.handler=12,port=60020] wal.FSHLog: interrupted while waiting for notification from AsyncNotifier
2014-07-01 07:45:08,334 FATAL [regionserver60020-WAL.AsyncSyncer0] wal.FSHLog: Error while AsyncSyncer sync, request close of hlog 
java.io.IOException: DFSOutputStream is closed
	at org.apache.hadoop.hdfs.DFSClient$DFSOutputStream.sync(DFSClient.java:3879)
	at org.apache.hadoop.fs.FSDataOutputStream.sync(FSDataOutputStream.java:97)
	at org.apache.hadoop.hbase.regionserver.wal.ProtobufLogWriter.sync(ProtobufLogWriter.java:158)
	at org.apache.hadoop.hbase.regionserver.wal.FSHLog$AsyncSyncer.run(FSHLog.java:1240)
	at java.lang.Thread.run(Thread.java:701)
2014-07-01 07:45:08,337 ERROR [regionserver60020-WAL.AsyncWriter] wal.FSHLog: Error while AsyncWriter write, request close of hlog 
java.io.IOException: Error Recovery for block blk_-5973222587916760426_29091 failed  because recovery from primary datanode 9.1.143.59:50010 failed 6 times.  Pipeline was 9.1.143.59:50010. Aborting...
	at org.apache.hadoop.hdfs.DFSClient$DFSOutputStream.processDatanodeError(DFSClient.java:3355)
	at org.apache.hadoop.hdfs.DFSClient$DFSOutputStream.access$2200(DFSClient.java:2783)
	at org.apache.hadoop.hdfs.DFSClient$DFSOutputStream$DataStreamer.run(DFSClient.java:2987)
2014-07-01 07:45:08,338 FATAL [regionserver60020-WAL.AsyncSyncer0] wal.FSHLog: Error while AsyncSyncer sync, request close of hlog 
java.io.IOException: DFSOutputStream is closed
	at org.apache.hadoop.hdfs.DFSClient$DFSOutputStream.sync(DFSClient.java:3879)
	at org.apache.hadoop.fs.FSDataOutputStream.sync(FSDataOutputStream.java:97)
	at org.apache.hadoop.hbase.regionserver.wal.ProtobufLogWriter.sync(ProtobufLogWriter.java:158)
	at org.apache.hadoop.hbase.regionserver.wal.FSHLog$AsyncSyncer.run(FSHLog.java:1240)
	at java.lang.Thread.run(Thread.java:701)
2014-07-01 07:45:08,338 DEBUG [RpcServer.handler=17,port=60020] wal.FSHLog: interrupted while waiting for notification from AsyncNotifier
2014-07-01 07:45:08,344 ERROR [regionserver60020-WAL.AsyncWriter] wal.FSHLog: Error while AsyncWriter write, request close of hlog 
java.io.IOException: Error Recovery for block blk_-5973222587916760426_29091 failed  because recovery from primary datanode 9.1.143.59:50010 failed 6 times.  Pipeline was 9.1.143.59:50010. Aborting...
	at org.apache.hadoop.hdfs.DFSClient$DFSOutputStream.processDatanodeError(DFSClient.java:3355)
	at org.apache.hadoop.hdfs.DFSClient$DFSOutputStream.access$2200(DFSClient.java:2783)
	at org.apache.hadoop.hdfs.DFSClient$DFSOutputStream$DataStreamer.run(DFSClient.java:2987)
2014-07-01 07:45:08,344 DEBUG [RpcServer.handler=4,port=60020] wal.FSHLog: interrupted while waiting for notification from AsyncNotifier
2014-07-01 07:45:08,344 FATAL [regionserver60020-WAL.AsyncSyncer0] wal.FSHLog: Error while AsyncSyncer sync, request close of hlog 
java.io.IOException: DFSOutputStream is closed
	at org.apache.hadoop.hdfs.DFSClient$DFSOutputStream.sync(DFSClient.java:3879)
	at org.apache.hadoop.fs.FSDataOutputStream.sync(FSDataOutputStream.java:97)
	at org.apache.hadoop.hbase.regionserver.wal.ProtobufLogWriter.sync(ProtobufLogWriter.java:158)
	at org.apache.hadoop.hbase.regionserver.wal.FSHLog$AsyncSyncer.run(FSHLog.java:1240)
	at java.lang.Thread.run(Thread.java:701)
2014-07-01 07:45:08,347 ERROR [regionserver60020-WAL.AsyncWriter] wal.FSHLog: Error while AsyncWriter write, request close of hlog 
java.io.IOException: Error Recovery for block blk_-5973222587916760426_29091 failed  because recovery from primary datanode 9.1.143.59:50010 failed 6 times.  Pipeline was 9.1.143.59:50010. Aborting...
	at org.apache.hadoop.hdfs.DFSClient$DFSOutputStream.processDatanodeError(DFSClient.java:3355)
	at org.apache.hadoop.hdfs.DFSClient$DFSOutputStream.access$2200(DFSClient.java:2783)
	at org.apache.hadoop.hdfs.DFSClient$DFSOutputStream$DataStreamer.run(DFSClient.java:2987)
2014-07-01 07:45:08,347 DEBUG [RpcServer.handler=15,port=60020] wal.FSHLog: interrupted while waiting for notification from AsyncNotifier
2014-07-01 07:45:08,347 FATAL [regionserver60020-WAL.AsyncSyncer0] wal.FSHLog: Error while AsyncSyncer sync, request close of hlog 
java.io.IOException: DFSOutputStream is closed
	at org.apache.hadoop.hdfs.DFSClient$DFSOutputStream.sync(DFSClient.java:3879)
	at org.apache.hadoop.fs.FSDataOutputStream.sync(FSDataOutputStream.java:97)
	at org.apache.hadoop.hbase.regionserver.wal.ProtobufLogWriter.sync(ProtobufLogWriter.java:158)
	at org.apache.hadoop.hbase.regionserver.wal.FSHLog$AsyncSyncer.run(FSHLog.java:1240)
	at java.lang.Thread.run(Thread.java:701)
2014-07-01 07:45:08,353 ERROR [regionserver60020-WAL.AsyncWriter] wal.FSHLog: Error while AsyncWriter write, request close of hlog 
java.io.IOException: Error Recovery for block blk_-5973222587916760426_29091 failed  because recovery from primary datanode 9.1.143.59:50010 failed 6 times.  Pipeline was 9.1.143.59:50010. Aborting...
	at org.apache.hadoop.hdfs.DFSClient$DFSOutputStream.processDatanodeError(DFSClient.java:3355)
	at org.apache.hadoop.hdfs.DFSClient$DFSOutputStream.access$2200(DFSClient.java:2783)
	at org.apache.hadoop.hdfs.DFSClient$DFSOutputStream$DataStreamer.run(DFSClient.java:2987)
2014-07-01 07:45:08,353 FATAL [regionserver60020-WAL.AsyncSyncer0] wal.FSHLog: Error while AsyncSyncer sync, request close of hlog 
java.io.IOException: DFSOutputStream is closed
	at org.apache.hadoop.hdfs.DFSClient$DFSOutputStream.sync(DFSClient.java:3879)
	at org.apache.hadoop.fs.FSDataOutputStream.sync(FSDataOutputStream.java:97)
	at org.apache.hadoop.hbase.regionserver.wal.ProtobufLogWriter.sync(ProtobufLogWriter.java:158)
	at org.apache.hadoop.hbase.regionserver.wal.FSHLog$AsyncSyncer.run(FSHLog.java:1240)
	at java.lang.Thread.run(Thread.java:701)
2014-07-01 07:45:08,353 DEBUG [RpcServer.handler=21,port=60020] wal.FSHLog: interrupted while waiting for notification from AsyncNotifier
2014-07-01 07:45:08,357 DEBUG [RpcServer.handler=23,port=60020] wal.FSHLog: interrupted while waiting for notification from AsyncNotifier
2014-07-01 07:45:08,360 ERROR [regionserver60020-WAL.AsyncWriter] wal.FSHLog: Error while AsyncWriter write, request close of hlog 
java.io.IOException: Error Recovery for block blk_-5973222587916760426_29091 failed  because recovery from primary datanode 9.1.143.59:50010 failed 6 times.  Pipeline was 9.1.143.59:50010. Aborting...
	at org.apache.hadoop.hdfs.DFSClient$DFSOutputStream.processDatanodeError(DFSClient.java:3355)
	at org.apache.hadoop.hdfs.DFSClient$DFSOutputStream.access$2200(DFSClient.java:2783)
	at org.apache.hadoop.hdfs.DFSClient$DFSOutputStream$DataStreamer.run(DFSClient.java:2987)
2014-07-01 07:45:08,360 FATAL [regionserver60020-WAL.AsyncSyncer0] wal.FSHLog: Error while AsyncSyncer sync, request close of hlog 
java.io.IOException: DFSOutputStream is closed
	at org.apache.hadoop.hdfs.DFSClient$DFSOutputStream.sync(DFSClient.java:3879)
	at org.apache.hadoop.fs.FSDataOutputStream.sync(FSDataOutputStream.java:97)
	at org.apache.hadoop.hbase.regionserver.wal.ProtobufLogWriter.sync(ProtobufLogWriter.java:158)
	at org.apache.hadoop.hbase.regionserver.wal.FSHLog$AsyncSyncer.run(FSHLog.java:1240)
	at java.lang.Thread.run(Thread.java:701)
2014-07-01 07:45:08,362 ERROR [regionserver60020-WAL.AsyncWriter] wal.FSHLog: Error while AsyncWriter write, request close of hlog 
java.io.IOException: Error Recovery for block blk_-5973222587916760426_29091 failed  because recovery from primary datanode 9.1.143.59:50010 failed 6 times.  Pipeline was 9.1.143.59:50010. Aborting...
	at org.apache.hadoop.hdfs.DFSClient$DFSOutputStream.processDatanodeError(DFSClient.java:3355)
	at org.apache.hadoop.hdfs.DFSClient$DFSOutputStream.access$2200(DFSClient.java:2783)
	at org.apache.hadoop.hdfs.DFSClient$DFSOutputStream$DataStreamer.run(DFSClient.java:2987)
2014-07-01 07:45:08,362 FATAL [regionserver60020-WAL.AsyncSyncer0] wal.FSHLog: Error while AsyncSyncer sync, request close of hlog 
java.io.IOException: DFSOutputStream is closed
	at org.apache.hadoop.hdfs.DFSClient$DFSOutputStream.sync(DFSClient.java:3879)
	at org.apache.hadoop.fs.FSDataOutputStream.sync(FSDataOutputStream.java:97)
	at org.apache.hadoop.hbase.regionserver.wal.ProtobufLogWriter.sync(ProtobufLogWriter.java:158)
	at org.apache.hadoop.hbase.regionserver.wal.FSHLog$AsyncSyncer.run(FSHLog.java:1240)
	at java.lang.Thread.run(Thread.java:701)
2014-07-01 07:45:08,360 DEBUG [RpcServer.handler=20,port=60020] wal.FSHLog: interrupted while waiting for notification from AsyncNotifier
2014-07-01 07:45:08,363 DEBUG [RpcServer.handler=7,port=60020] wal.FSHLog: interrupted while waiting for notification from AsyncNotifier
2014-07-01 07:45:08,370 WARN  [DataStreamer for file /hbase/data/default/usertable/07064f253c865a7cc0c08893a4f56af9/.tmp/a17a8b64528148559a6414832c7b909b block blk_4372291247462747067_29087] hdfs.DFSClient: Error Recovery for blk_4372291247462747067_29087 bad datanode[0] 9.1.143.59:50010
2014-07-01 07:45:08,370 WARN  [DataStreamer for file /hbase/data/default/usertable/07064f253c865a7cc0c08893a4f56af9/.tmp/a17a8b64528148559a6414832c7b909b block blk_4372291247462747067_29087] hdfs.DFSClient: Error Recovery for block blk_4372291247462747067_29087 in pipeline 9.1.143.59:50010, 9.1.143.58:50010: bad datanode 9.1.143.59:50010
2014-07-01 07:45:08,370 ERROR [regionserver60020-WAL.AsyncWriter] wal.FSHLog: Error while AsyncWriter write, request close of hlog 
java.io.IOException: Error Recovery for block blk_-5973222587916760426_29091 failed  because recovery from primary datanode 9.1.143.59:50010 failed 6 times.  Pipeline was 9.1.143.59:50010. Aborting...
	at org.apache.hadoop.hdfs.DFSClient$DFSOutputStream.processDatanodeError(DFSClient.java:3355)
	at org.apache.hadoop.hdfs.DFSClient$DFSOutputStream.access$2200(DFSClient.java:2783)
	at org.apache.hadoop.hdfs.DFSClient$DFSOutputStream$DataStreamer.run(DFSClient.java:2987)
2014-07-01 07:45:08,370 FATAL [regionserver60020-WAL.AsyncSyncer0] wal.FSHLog: Error while AsyncSyncer sync, request close of hlog 
java.io.IOException: DFSOutputStream is closed
	at org.apache.hadoop.hdfs.DFSClient$DFSOutputStream.sync(DFSClient.java:3879)
	at org.apache.hadoop.fs.FSDataOutputStream.sync(FSDataOutputStream.java:97)
	at org.apache.hadoop.hbase.regionserver.wal.ProtobufLogWriter.sync(ProtobufLogWriter.java:158)
	at org.apache.hadoop.hbase.regionserver.wal.FSHLog$AsyncSyncer.run(FSHLog.java:1240)
	at java.lang.Thread.run(Thread.java:701)
2014-07-01 07:45:08,370 DEBUG [RpcServer.handler=28,port=60020] wal.FSHLog: interrupted while waiting for notification from AsyncNotifier
2014-07-01 07:45:08,372 ERROR [regionserver60020-WAL.AsyncWriter] wal.FSHLog: Error while AsyncWriter write, request close of hlog 
java.io.IOException: Error Recovery for block blk_-5973222587916760426_29091 failed  because recovery from primary datanode 9.1.143.59:50010 failed 6 times.  Pipeline was 9.1.143.59:50010. Aborting...
	at org.apache.hadoop.hdfs.DFSClient$DFSOutputStream.processDatanodeError(DFSClient.java:3355)
	at org.apache.hadoop.hdfs.DFSClient$DFSOutputStream.access$2200(DFSClient.java:2783)
	at org.apache.hadoop.hdfs.DFSClient$DFSOutputStream$DataStreamer.run(DFSClient.java:2987)
2014-07-01 07:45:08,373 FATAL [regionserver60020-WAL.AsyncSyncer0] wal.FSHLog: Error while AsyncSyncer sync, request close of hlog 
java.io.IOException: DFSOutputStream is closed
	at org.apache.hadoop.hdfs.DFSClient$DFSOutputStream.sync(DFSClient.java:3879)
	at org.apache.hadoop.fs.FSDataOutputStream.sync(FSDataOutputStream.java:97)
	at org.apache.hadoop.hbase.regionserver.wal.ProtobufLogWriter.sync(ProtobufLogWriter.java:158)
	at org.apache.hadoop.hbase.regionserver.wal.FSHLog$AsyncSyncer.run(FSHLog.java:1240)
	at java.lang.Thread.run(Thread.java:701)
2014-07-01 07:45:08,372 DEBUG [RpcServer.handler=25,port=60020] wal.FSHLog: interrupted while waiting for notification from AsyncNotifier
2014-07-01 07:45:08,403 WARN  [DataStreamer for file /hbase/data/default/usertable/07064f253c865a7cc0c08893a4f56af9/.tmp/a17a8b64528148559a6414832c7b909b block blk_4372291247462747067_29087] hdfs.DFSClient: Failed recovery attempt #4 from primary datanode 9.1.143.58:50010
org.apache.hadoop.ipc.RemoteException: org.apache.hadoop.ipc.RemoteException: java.io.IOException: blk_4372291247462747067_29087 is missing
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.nextGenerationStampForBlock(FSNamesystem.java:5943)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.nextGenerationStamp(NameNode.java:835)
	at sun.reflect.GeneratedMethodAccessor18.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:622)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:587)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1432)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1428)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:416)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1190)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:1426)

	at org.apache.hadoop.ipc.Client.call(Client.java:1113)
	at org.apache.hadoop.ipc.RPC$Invoker.invoke(RPC.java:229)
	at com.sun.proxy.$Proxy5.nextGenerationStamp(Unknown Source)
	at org.apache.hadoop.hdfs.server.datanode.DataNode.syncBlock(DataNode.java:2127)
	at org.apache.hadoop.hdfs.server.datanode.DataNode.recoverBlock(DataNode.java:2095)
	at org.apache.hadoop.hdfs.server.datanode.DataNode.recoverBlock(DataNode.java:2175)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:622)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:587)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1432)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1428)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:416)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1190)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:1426)

	at org.apache.hadoop.ipc.Client.call(Client.java:1113)
	at org.apache.hadoop.ipc.RPC$Invoker.invoke(RPC.java:229)
	at com.sun.proxy.$Proxy13.recoverBlock(Unknown Source)
	at org.apache.hadoop.hdfs.DFSClient$DFSOutputStream.processDatanodeError(DFSClient.java:3319)
	at org.apache.hadoop.hdfs.DFSClient$DFSOutputStream.access$2200(DFSClient.java:2783)
	at org.apache.hadoop.hdfs.DFSClient$DFSOutputStream$DataStreamer.run(DFSClient.java:2987)
2014-07-01 07:45:08,404 WARN  [DataStreamer for file /hbase/data/default/usertable/07064f253c865a7cc0c08893a4f56af9/.tmp/a17a8b64528148559a6414832c7b909b block blk_4372291247462747067_29087] hdfs.DFSClient: Error Recovery for block blk_4372291247462747067_29087 failed  because recovery from primary datanode 9.1.143.58:50010 failed 5 times.  Pipeline was 9.1.143.59:50010, 9.1.143.58:50010. Will retry...
2014-07-01 07:45:08,477 WARN  [DataStreamer for file /hbase/data/default/usertable/07064f253c865a7cc0c08893a4f56af9/.tmp/a17a8b64528148559a6414832c7b909b block blk_4372291247462747067_29087] hdfs.DFSClient: Error Recovery for blk_4372291247462747067_29087 bad datanode[0] 9.1.143.59:50010
2014-07-01 07:45:08,477 WARN  [DataStreamer for file /hbase/data/default/usertable/07064f253c865a7cc0c08893a4f56af9/.tmp/a17a8b64528148559a6414832c7b909b block blk_4372291247462747067_29087] hdfs.DFSClient: Error Recovery for block blk_4372291247462747067_29087 in pipeline 9.1.143.59:50010, 9.1.143.58:50010: bad datanode 9.1.143.59:50010
2014-07-01 07:45:08,513 WARN  [DataStreamer for file /hbase/data/default/usertable/07064f253c865a7cc0c08893a4f56af9/.tmp/a17a8b64528148559a6414832c7b909b block blk_4372291247462747067_29087] hdfs.DFSClient: Failed recovery attempt #5 from primary datanode 9.1.143.58:50010
org.apache.hadoop.ipc.RemoteException: org.apache.hadoop.ipc.RemoteException: java.io.IOException: blk_4372291247462747067_29087 is missing
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.nextGenerationStampForBlock(FSNamesystem.java:5943)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.nextGenerationStamp(NameNode.java:835)
	at sun.reflect.GeneratedMethodAccessor18.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:622)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:587)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1432)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1428)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:416)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1190)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:1426)

	at org.apache.hadoop.ipc.Client.call(Client.java:1113)
	at org.apache.hadoop.ipc.RPC$Invoker.invoke(RPC.java:229)
	at com.sun.proxy.$Proxy5.nextGenerationStamp(Unknown Source)
	at org.apache.hadoop.hdfs.server.datanode.DataNode.syncBlock(DataNode.java:2127)
	at org.apache.hadoop.hdfs.server.datanode.DataNode.recoverBlock(DataNode.java:2095)
	at org.apache.hadoop.hdfs.server.datanode.DataNode.recoverBlock(DataNode.java:2175)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:622)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:587)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1432)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1428)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:416)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1190)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:1426)

	at org.apache.hadoop.ipc.Client.call(Client.java:1113)
	at org.apache.hadoop.ipc.RPC$Invoker.invoke(RPC.java:229)
	at com.sun.proxy.$Proxy13.recoverBlock(Unknown Source)
	at org.apache.hadoop.hdfs.DFSClient$DFSOutputStream.processDatanodeError(DFSClient.java:3319)
	at org.apache.hadoop.hdfs.DFSClient$DFSOutputStream.access$2200(DFSClient.java:2783)
	at org.apache.hadoop.hdfs.DFSClient$DFSOutputStream$DataStreamer.run(DFSClient.java:2987)
2014-07-01 07:45:08,514 WARN  [DataStreamer for file /hbase/data/default/usertable/07064f253c865a7cc0c08893a4f56af9/.tmp/a17a8b64528148559a6414832c7b909b block blk_4372291247462747067_29087] hdfs.DFSClient: Error Recovery for block blk_4372291247462747067_29087 failed  because recovery from primary datanode 9.1.143.58:50010 failed 6 times.  Pipeline was 9.1.143.59:50010, 9.1.143.58:50010. Marking primary datanode as bad.
2014-07-01 07:45:08,546 INFO  [regionserver60020-SendThread(sceplus-vm49.almaden.ibm.com:2181)] zookeeper.ClientCnxn: Opening socket connection to server sceplus-vm49.almaden.ibm.com/9.1.143.59:2181. Will not attempt to authenticate using SASL (unknown error)
2014-07-01 07:45:08,547 INFO  [regionserver60020-SendThread(sceplus-vm49.almaden.ibm.com:2181)] zookeeper.ClientCnxn: Socket connection established to sceplus-vm49.almaden.ibm.com/9.1.143.59:2181, initiating session
2014-07-01 07:45:08,560 INFO  [regionserver60020-SendThread(sceplus-vm49.almaden.ibm.com:2181)] zookeeper.ClientCnxn: Unable to reconnect to ZooKeeper service, session 0x46f25690f50006 has expired, closing socket connection
2014-07-01 07:45:08,560 WARN  [regionserver60020-EventThread] client.HConnectionManager$HConnectionImplementation: This client just lost it's session with ZooKeeper, closing it. It will be recreated next time someone needs it
org.apache.zookeeper.KeeperException$SessionExpiredException: KeeperErrorCode = Session expired
	at org.apache.hadoop.hbase.zookeeper.ZooKeeperWatcher.connectionEvent(ZooKeeperWatcher.java:401)
	at org.apache.hadoop.hbase.zookeeper.ZooKeeperWatcher.process(ZooKeeperWatcher.java:319)
	at org.apache.zookeeper.ClientCnxn$EventThread.processEvent(ClientCnxn.java:522)
	at org.apache.zookeeper.ClientCnxn$EventThread.run(ClientCnxn.java:498)
2014-07-01 07:45:08,982 INFO  [regionserver60020-EventThread] client.HConnectionManager$HConnectionImplementation: Closing zookeeper sessionid=0x46f25690f50006
2014-07-01 07:45:08,982 INFO  [regionserver60020-EventThread] zookeeper.ClientCnxn: EventThread shut down
2014-07-01 07:45:08,993 WARN  [DataStreamer for file /hbase/data/default/usertable/07064f253c865a7cc0c08893a4f56af9/.tmp/a17a8b64528148559a6414832c7b909b block blk_4372291247462747067_29087] hdfs.DFSClient: Failed recovery attempt #0 from primary datanode 9.1.143.59:50010
org.apache.hadoop.ipc.RemoteException: org.apache.hadoop.ipc.RemoteException: java.io.IOException: blk_4372291247462747067_29087 is missing
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.nextGenerationStampForBlock(FSNamesystem.java:5943)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.nextGenerationStamp(NameNode.java:835)
	at sun.reflect.GeneratedMethodAccessor18.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:622)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:587)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1432)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1428)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:416)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1190)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:1426)

	at org.apache.hadoop.ipc.Client.call(Client.java:1113)
	at org.apache.hadoop.ipc.RPC$Invoker.invoke(RPC.java:229)
	at com.sun.proxy.$Proxy5.nextGenerationStamp(Unknown Source)
	at org.apache.hadoop.hdfs.server.datanode.DataNode.syncBlock(DataNode.java:2127)
	at org.apache.hadoop.hdfs.server.datanode.DataNode.recoverBlock(DataNode.java:2095)
	at org.apache.hadoop.hdfs.server.datanode.DataNode.recoverBlock(DataNode.java:2175)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:622)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:587)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1432)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1428)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:416)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1190)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:1426)

	at org.apache.hadoop.ipc.Client.call(Client.java:1113)
	at org.apache.hadoop.ipc.RPC$Invoker.invoke(RPC.java:229)
	at com.sun.proxy.$Proxy13.recoverBlock(Unknown Source)
	at org.apache.hadoop.hdfs.DFSClient$DFSOutputStream.processDatanodeError(DFSClient.java:3319)
	at org.apache.hadoop.hdfs.DFSClient$DFSOutputStream.access$2200(DFSClient.java:2783)
	at org.apache.hadoop.hdfs.DFSClient$DFSOutputStream$DataStreamer.run(DFSClient.java:2987)
2014-07-01 07:45:08,993 WARN  [DataStreamer for file /hbase/data/default/usertable/07064f253c865a7cc0c08893a4f56af9/.tmp/a17a8b64528148559a6414832c7b909b block blk_4372291247462747067_29087] hdfs.DFSClient: Error Recovery for block blk_4372291247462747067_29087 failed  because recovery from primary datanode 9.1.143.59:50010 failed 1 times.  Pipeline was 9.1.143.59:50010. Will retry...
2014-07-01 07:45:09,017 DEBUG [RpcServer.handler=48,port=60020] regionserver.HRegion: rollbackMemstore rolled back 6140 keyvalues from start:0 to end:614
2014-07-01 07:45:09,031 DEBUG [RpcServer.handler=49,port=60020] regionserver.HRegion: rollbackMemstore rolled back 4230 keyvalues from start:0 to end:423
2014-07-01 07:45:09,050 DEBUG [RpcServer.handler=21,port=60020] regionserver.HRegion: rollbackMemstore rolled back 6170 keyvalues from start:0 to end:617
2014-07-01 07:45:09,055 DEBUG [RpcServer.handler=4,port=60020] regionserver.HRegion: rollbackMemstore rolled back 6400 keyvalues from start:0 to end:640
2014-07-01 07:45:09,059 WARN  [DataStreamer for file /hbase/data/default/usertable/07064f253c865a7cc0c08893a4f56af9/.tmp/a17a8b64528148559a6414832c7b909b block blk_4372291247462747067_29087] hdfs.DFSClient: Failed recovery attempt #1 from primary datanode 9.1.143.59:50010
org.apache.hadoop.ipc.RemoteException: org.apache.hadoop.ipc.RemoteException: java.io.IOException: blk_4372291247462747067_29087 is missing
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.nextGenerationStampForBlock(FSNamesystem.java:5943)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.nextGenerationStamp(NameNode.java:835)
	at sun.reflect.GeneratedMethodAccessor18.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:622)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:587)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1432)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1428)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:416)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1190)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:1426)

	at org.apache.hadoop.ipc.Client.call(Client.java:1113)
	at org.apache.hadoop.ipc.RPC$Invoker.invoke(RPC.java:229)
	at com.sun.proxy.$Proxy5.nextGenerationStamp(Unknown Source)
	at org.apache.hadoop.hdfs.server.datanode.DataNode.syncBlock(DataNode.java:2127)
	at org.apache.hadoop.hdfs.server.datanode.DataNode.recoverBlock(DataNode.java:2095)
	at org.apache.hadoop.hdfs.server.datanode.DataNode.recoverBlock(DataNode.java:2175)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:622)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:587)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1432)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1428)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:416)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1190)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:1426)

	at org.apache.hadoop.ipc.Client.call(Client.java:1113)
	at org.apache.hadoop.ipc.RPC$Invoker.invoke(RPC.java:229)
	at com.sun.proxy.$Proxy13.recoverBlock(Unknown Source)
	at org.apache.hadoop.hdfs.DFSClient$DFSOutputStream.processDatanodeError(DFSClient.java:3319)
	at org.apache.hadoop.hdfs.DFSClient$DFSOutputStream.access$2200(DFSClient.java:2783)
	at org.apache.hadoop.hdfs.DFSClient$DFSOutputStream$DataStreamer.run(DFSClient.java:2987)
2014-07-01 07:45:09,059 WARN  [DataStreamer for file /hbase/data/default/usertable/07064f253c865a7cc0c08893a4f56af9/.tmp/a17a8b64528148559a6414832c7b909b block blk_4372291247462747067_29087] hdfs.DFSClient: Error Recovery for block blk_4372291247462747067_29087 failed  because recovery from primary datanode 9.1.143.59:50010 failed 2 times.  Pipeline was 9.1.143.59:50010. Will retry...
2014-07-01 07:45:09,065 DEBUG [RpcServer.handler=20,port=60020] regionserver.HRegion: rollbackMemstore rolled back 6560 keyvalues from start:0 to end:656
2014-07-01 07:45:09,065 DEBUG [RpcServer.handler=1,port=60020] regionserver.HRegion: rollbackMemstore rolled back 6250 keyvalues from start:0 to end:625
2014-07-01 07:45:09,069 DEBUG [RpcServer.handler=12,port=60020] regionserver.HRegion: rollbackMemstore rolled back 6530 keyvalues from start:0 to end:653
2014-07-01 07:45:09,072 DEBUG [RpcServer.handler=11,port=60020] regionserver.HRegion: rollbackMemstore rolled back 6470 keyvalues from start:0 to end:647
2014-07-01 07:45:09,075 DEBUG [RpcServer.handler=28,port=60020] regionserver.HRegion: rollbackMemstore rolled back 6610 keyvalues from start:0 to end:661
2014-07-01 07:45:09,079 DEBUG [RpcServer.handler=17,port=60020] regionserver.HRegion: rollbackMemstore rolled back 6360 keyvalues from start:0 to end:636
2014-07-01 07:45:09,090 DEBUG [RpcServer.handler=40,port=60020] regionserver.HRegion: rollbackMemstore rolled back 6270 keyvalues from start:0 to end:627
2014-07-01 07:45:09,090 DEBUG [RpcServer.handler=25,port=60020] regionserver.HRegion: rollbackMemstore rolled back 6780 keyvalues from start:0 to end:678
2014-07-01 07:45:09,095 DEBUG [RpcServer.handler=32,port=60020] regionserver.HRegion: rollbackMemstore rolled back 6150 keyvalues from start:0 to end:615
2014-07-01 07:45:09,115 DEBUG [RpcServer.handler=7,port=60020] regionserver.HRegion: rollbackMemstore rolled back 6530 keyvalues from start:0 to end:653
2014-07-01 07:45:09,115 DEBUG [RpcServer.handler=23,port=60020] regionserver.HRegion: rollbackMemstore rolled back 5910 keyvalues from start:0 to end:591
2014-07-01 07:45:09,117 DEBUG [RpcServer.handler=35,port=60020] regionserver.HRegion: rollbackMemstore rolled back 6350 keyvalues from start:0 to end:635
2014-07-01 07:45:09,118 DEBUG [RpcServer.handler=15,port=60020] regionserver.HRegion: rollbackMemstore rolled back 6570 keyvalues from start:0 to end:657
2014-07-01 07:45:09,142 WARN  [DataStreamer for file /hbase/data/default/usertable/07064f253c865a7cc0c08893a4f56af9/.tmp/a17a8b64528148559a6414832c7b909b block blk_4372291247462747067_29087] hdfs.DFSClient: Failed recovery attempt #2 from primary datanode 9.1.143.59:50010
org.apache.hadoop.ipc.RemoteException: org.apache.hadoop.ipc.RemoteException: java.io.IOException: blk_4372291247462747067_29087 is missing
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.nextGenerationStampForBlock(FSNamesystem.java:5943)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.nextGenerationStamp(NameNode.java:835)
	at sun.reflect.GeneratedMethodAccessor18.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:622)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:587)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1432)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1428)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:416)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1190)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:1426)

	at org.apache.hadoop.ipc.Client.call(Client.java:1113)
	at org.apache.hadoop.ipc.RPC$Invoker.invoke(RPC.java:229)
	at com.sun.proxy.$Proxy5.nextGenerationStamp(Unknown Source)
	at org.apache.hadoop.hdfs.server.datanode.DataNode.syncBlock(DataNode.java:2127)
	at org.apache.hadoop.hdfs.server.datanode.DataNode.recoverBlock(DataNode.java:2095)
	at org.apache.hadoop.hdfs.server.datanode.DataNode.recoverBlock(DataNode.java:2175)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:622)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:587)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1432)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1428)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:416)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1190)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:1426)

	at org.apache.hadoop.ipc.Client.call(Client.java:1113)
	at org.apache.hadoop.ipc.RPC$Invoker.invoke(RPC.java:229)
	at com.sun.proxy.$Proxy13.recoverBlock(Unknown Source)
	at org.apache.hadoop.hdfs.DFSClient$DFSOutputStream.processDatanodeError(DFSClient.java:3319)
	at org.apache.hadoop.hdfs.DFSClient$DFSOutputStream.access$2200(DFSClient.java:2783)
	at org.apache.hadoop.hdfs.DFSClient$DFSOutputStream$DataStreamer.run(DFSClient.java:2987)
2014-07-01 07:45:09,142 WARN  [RpcServer.handler=11,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":93832,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.58:33505","starttimems":1404225815299,"queuetimems":21338,"class":"HRegionServer","responsesize":297496,"method":"Multi"}
2014-07-01 07:45:09,142 WARN  [DataStreamer for file /hbase/data/default/usertable/07064f253c865a7cc0c08893a4f56af9/.tmp/a17a8b64528148559a6414832c7b909b block blk_4372291247462747067_29087] hdfs.DFSClient: Error Recovery for block blk_4372291247462747067_29087 failed  because recovery from primary datanode 9.1.143.59:50010 failed 3 times.  Pipeline was 9.1.143.59:50010. Will retry...
2014-07-01 07:45:09,145 WARN  [RpcServer.handler=11,port=60020] ipc.RpcServer: RpcServer.respondercallId: 3961 service: ClientService methodName: Multi size: 808.0k connection: 9.1.143.58:33505: output error
2014-07-01 07:45:09,145 WARN  [RpcServer.handler=11,port=60020] ipc.RpcServer: RpcServer.handler=11,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-07-01 07:45:09,147 DEBUG [RpcServer.handler=45,port=60020] regionserver.HRegion: rollbackMemstore rolled back 6690 keyvalues from start:0 to end:669
2014-07-01 07:45:09,221 DEBUG [RpcServer.handler=18,port=60020] regionserver.HRegion: rollbackMemstore rolled back 5690 keyvalues from start:0 to end:569
2014-07-01 07:45:09,232 DEBUG [RpcServer.handler=47,port=60020] regionserver.HRegion: rollbackMemstore rolled back 5590 keyvalues from start:0 to end:559
2014-07-01 07:45:09,233 WARN  [RpcServer.handler=2,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":100515,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.58:33505","starttimems":1404225808717,"queuetimems":20828,"class":"HRegionServer","responsesize":13382,"method":"Multi"}
2014-07-01 07:45:09,233 WARN  [RpcServer.handler=5,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":103113,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.58:33505","starttimems":1404225806119,"queuetimems":18609,"class":"HRegionServer","responsesize":13535,"method":"Multi"}
2014-07-01 07:45:09,233 WARN  [RpcServer.handler=16,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":100515,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.58:33505","starttimems":1404225808717,"queuetimems":20817,"class":"HRegionServer","responsesize":10169,"method":"Multi"}
2014-07-01 07:45:09,233 WARN  [RpcServer.handler=2,port=60020] ipc.RpcServer: RpcServer.respondercallId: 3899 service: ClientService methodName: Multi size: 2.3m connection: 9.1.143.58:33505: output error
2014-07-01 07:45:09,233 WARN  [RpcServer.handler=2,port=60020] ipc.RpcServer: RpcServer.handler=2,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-07-01 07:45:09,233 WARN  [RpcServer.handler=16,port=60020] ipc.RpcServer: RpcServer.respondercallId: 3898 service: ClientService methodName: Multi size: 1.8m connection: 9.1.143.58:33505: output error
2014-07-01 07:45:09,233 WARN  [RpcServer.handler=16,port=60020] ipc.RpcServer: RpcServer.handler=16,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-07-01 07:45:09,233 WARN  [RpcServer.handler=5,port=60020] ipc.RpcServer: RpcServer.respondercallId: 3905 service: ClientService methodName: Multi size: 2.4m connection: 9.1.143.58:33505: output error
2014-07-01 07:45:09,233 WARN  [RpcServer.handler=5,port=60020] ipc.RpcServer: RpcServer.handler=5,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-07-01 07:45:09,241 WARN  [RpcServer.handler=14,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":106027,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.58:33505","starttimems":1404225803214,"queuetimems":16428,"class":"HRegionServer","responsesize":13577,"method":"Multi"}
2014-07-01 07:45:09,241 WARN  [RpcServer.handler=30,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":100071,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.58:33505","starttimems":1404225809170,"queuetimems":21232,"class":"HRegionServer","responsesize":13494,"method":"Multi"}
2014-07-01 07:45:09,241 WARN  [RpcServer.handler=24,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":106032,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.58:33505","starttimems":1404225803209,"queuetimems":20603,"class":"HRegionServer","responsesize":13633,"method":"Multi"}
2014-07-01 07:45:09,241 WARN  [RpcServer.handler=14,port=60020] ipc.RpcServer: RpcServer.respondercallId: 3884 service: ClientService methodName: Multi size: 2.4m connection: 9.1.143.58:33505: output error
2014-07-01 07:45:09,241 WARN  [RpcServer.handler=14,port=60020] ipc.RpcServer: RpcServer.handler=14,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-07-01 07:45:09,241 WARN  [RpcServer.handler=30,port=60020] ipc.RpcServer: RpcServer.respondercallId: 3938 service: ClientService methodName: Multi size: 2.4m connection: 9.1.143.58:33505: output error
2014-07-01 07:45:09,241 WARN  [RpcServer.handler=30,port=60020] ipc.RpcServer: RpcServer.handler=30,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-07-01 07:45:09,241 WARN  [RpcServer.handler=24,port=60020] ipc.RpcServer: RpcServer.respondercallId: 3892 service: ClientService methodName: Multi size: 2.4m connection: 9.1.143.58:33505: output error
2014-07-01 07:45:09,241 WARN  [RpcServer.handler=24,port=60020] ipc.RpcServer: RpcServer.handler=24,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-07-01 07:45:09,243 WARN  [RpcServer.handler=29,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":103311,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.58:33505","starttimems":1404225805932,"queuetimems":19063,"class":"HRegionServer","responsesize":13695,"method":"Multi"}
2014-07-01 07:45:09,244 WARN  [RpcServer.handler=29,port=60020] ipc.RpcServer: RpcServer.respondercallId: 3918 service: ClientService methodName: Multi size: 2.4m connection: 9.1.143.58:33505: output error
2014-07-01 07:45:09,244 WARN  [RpcServer.handler=29,port=60020] ipc.RpcServer: RpcServer.handler=29,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-07-01 07:45:09,244 WARN  [RpcServer.handler=26,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":103312,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.58:33505","starttimems":1404225805932,"queuetimems":19057,"class":"HRegionServer","responsesize":6175,"method":"Multi"}
2014-07-01 07:45:09,244 WARN  [RpcServer.handler=44,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":103332,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.58:33505","starttimems":1404225805912,"queuetimems":19092,"class":"HRegionServer","responsesize":13540,"method":"Multi"}
2014-07-01 07:45:09,244 WARN  [RpcServer.handler=33,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":106030,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.58:33505","starttimems":1404225803214,"queuetimems":17786,"class":"HRegionServer","responsesize":13694,"method":"Multi"}
2014-07-01 07:45:09,244 WARN  [RpcServer.handler=13,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":103123,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.58:33505","starttimems":1404225806121,"queuetimems":18304,"class":"HRegionServer","responsesize":13631,"method":"Multi"}
2014-07-01 07:45:09,244 WARN  [RpcServer.handler=26,port=60020] ipc.RpcServer: RpcServer.respondercallId: 3915 service: ClientService methodName: Multi size: 1.1m connection: 9.1.143.58:33505: output error
2014-07-01 07:45:09,244 WARN  [RpcServer.handler=8,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":103310,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.58:33505","starttimems":1404225805934,"queuetimems":18446,"class":"HRegionServer","responsesize":13397,"method":"Multi"}
2014-07-01 07:45:09,245 WARN  [RpcServer.handler=13,port=60020] ipc.RpcServer: RpcServer.respondercallId: 3902 service: ClientService methodName: Multi size: 2.4m connection: 9.1.143.58:33505: output error
2014-07-01 07:45:09,245 WARN  [RpcServer.handler=13,port=60020] ipc.RpcServer: RpcServer.handler=13,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-07-01 07:45:09,245 WARN  [RpcServer.handler=26,port=60020] ipc.RpcServer: RpcServer.handler=26,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-07-01 07:45:09,245 WARN  [RpcServer.handler=8,port=60020] ipc.RpcServer: RpcServer.respondercallId: 3906 service: ClientService methodName: Multi size: 2.3m connection: 9.1.143.58:33505: output error
2014-07-01 07:45:09,245 WARN  [RpcServer.handler=8,port=60020] ipc.RpcServer: RpcServer.handler=8,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-07-01 07:45:09,245 WARN  [RpcServer.handler=33,port=60020] ipc.RpcServer: RpcServer.respondercallId: 3886 service: ClientService methodName: Multi size: 2.4m connection: 9.1.143.58:33505: output error
2014-07-01 07:45:09,245 WARN  [RpcServer.handler=33,port=60020] ipc.RpcServer: RpcServer.handler=33,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-07-01 07:45:09,245 WARN  [RpcServer.handler=44,port=60020] ipc.RpcServer: RpcServer.respondercallId: 3881 service: ClientService methodName: Multi size: 2.4m connection: 9.1.143.58:33505: output error
2014-07-01 07:45:09,245 WARN  [RpcServer.handler=44,port=60020] ipc.RpcServer: RpcServer.handler=44,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-07-01 07:45:09,249 INFO  [RpcServer.handler=31,port=60020] regionserver.HRegion: Interrupted while waiting for a lock
2014-07-01 07:45:09,249 WARN  [RpcServer.handler=10,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":103300,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.58:33505","starttimems":1404225805933,"queuetimems":18488,"class":"HRegionServer","responsesize":7027,"method":"Multi"}
2014-07-01 07:45:09,249 WARN  [RpcServer.handler=10,port=60020] ipc.RpcServer: RpcServer.respondercallId: 3908 service: ClientService methodName: Multi size: 1.2m connection: 9.1.143.58:33505: output error
2014-07-01 07:45:09,250 WARN  [RpcServer.handler=10,port=60020] ipc.RpcServer: RpcServer.handler=10,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-07-01 07:45:09,250 WARN  [RpcServer.handler=19,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":103316,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.58:33505","starttimems":1404225805933,"queuetimems":18468,"class":"HRegionServer","responsesize":10477,"method":"Multi"}
2014-07-01 07:45:09,250 WARN  [RpcServer.handler=0,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":103317,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.58:33505","starttimems":1404225805932,"queuetimems":18505,"class":"HRegionServer","responsesize":8030,"method":"Multi"}
2014-07-01 07:45:09,250 WARN  [RpcServer.handler=0,port=60020] ipc.RpcServer: RpcServer.respondercallId: 3914 service: ClientService methodName: Multi size: 1.4m connection: 9.1.143.58:33505: output error
2014-07-01 07:45:09,250 WARN  [RpcServer.handler=0,port=60020] ipc.RpcServer: RpcServer.handler=0,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-07-01 07:45:09,250 WARN  [RpcServer.handler=19,port=60020] ipc.RpcServer: RpcServer.respondercallId: 3907 service: ClientService methodName: Multi size: 1.8m connection: 9.1.143.58:33505: output error
2014-07-01 07:45:09,250 WARN  [RpcServer.handler=19,port=60020] ipc.RpcServer: RpcServer.handler=19,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-07-01 07:45:09,302 DEBUG [RpcServer.handler=46,port=60020] regionserver.HRegion: rollbackMemstore rolled back 6430 keyvalues from start:0 to end:643
2014-07-01 07:45:09,313 WARN  [DataStreamer for file /hbase/data/default/usertable/07064f253c865a7cc0c08893a4f56af9/.tmp/a17a8b64528148559a6414832c7b909b block blk_4372291247462747067_29087] hdfs.DFSClient: Failed recovery attempt #3 from primary datanode 9.1.143.59:50010
org.apache.hadoop.ipc.RemoteException: org.apache.hadoop.ipc.RemoteException: java.io.IOException: blk_4372291247462747067_29087 is missing
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.nextGenerationStampForBlock(FSNamesystem.java:5943)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.nextGenerationStamp(NameNode.java:835)
	at sun.reflect.GeneratedMethodAccessor18.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:622)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:587)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1432)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1428)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:416)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1190)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:1426)

	at org.apache.hadoop.ipc.Client.call(Client.java:1113)
	at org.apache.hadoop.ipc.RPC$Invoker.invoke(RPC.java:229)
	at com.sun.proxy.$Proxy5.nextGenerationStamp(Unknown Source)
	at org.apache.hadoop.hdfs.server.datanode.DataNode.syncBlock(DataNode.java:2127)
	at org.apache.hadoop.hdfs.server.datanode.DataNode.recoverBlock(DataNode.java:2095)
	at org.apache.hadoop.hdfs.server.datanode.DataNode.recoverBlock(DataNode.java:2175)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:622)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:587)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1432)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1428)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:416)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1190)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:1426)

	at org.apache.hadoop.ipc.Client.call(Client.java:1113)
	at org.apache.hadoop.ipc.RPC$Invoker.invoke(RPC.java:229)
	at com.sun.proxy.$Proxy13.recoverBlock(Unknown Source)
	at org.apache.hadoop.hdfs.DFSClient$DFSOutputStream.processDatanodeError(DFSClient.java:3319)
	at org.apache.hadoop.hdfs.DFSClient$DFSOutputStream.access$2200(DFSClient.java:2783)
	at org.apache.hadoop.hdfs.DFSClient$DFSOutputStream$DataStreamer.run(DFSClient.java:2987)
2014-07-01 07:45:09,313 WARN  [DataStreamer for file /hbase/data/default/usertable/07064f253c865a7cc0c08893a4f56af9/.tmp/a17a8b64528148559a6414832c7b909b block blk_4372291247462747067_29087] hdfs.DFSClient: Error Recovery for block blk_4372291247462747067_29087 failed  because recovery from primary datanode 9.1.143.59:50010 failed 4 times.  Pipeline was 9.1.143.59:50010. Will retry...
2014-07-01 07:45:09,334 DEBUG [RpcServer.handler=39,port=60020] regionserver.HRegion: rollbackMemstore rolled back 6530 keyvalues from start:0 to end:653
2014-07-01 07:45:09,335 DEBUG [RpcServer.handler=27,port=60020] regionserver.HRegion: rollbackMemstore rolled back 6380 keyvalues from start:0 to end:638
2014-07-01 07:45:09,339 DEBUG [RpcServer.handler=37,port=60020] regionserver.HRegion: rollbackMemstore rolled back 6270 keyvalues from start:0 to end:627
2014-07-01 07:45:09,344 DEBUG [RpcServer.handler=6,port=60020] regionserver.HRegion: rollbackMemstore rolled back 6030 keyvalues from start:0 to end:603
2014-07-01 07:45:09,362 DEBUG [RpcServer.handler=43,port=60020] regionserver.HRegion: rollbackMemstore rolled back 6230 keyvalues from start:0 to end:623
2014-07-01 07:45:09,369 DEBUG [RpcServer.handler=22,port=60020] regionserver.HRegion: rollbackMemstore rolled back 6800 keyvalues from start:0 to end:680
2014-07-01 07:45:09,385 DEBUG [RpcServer.handler=36,port=60020] regionserver.HRegion: rollbackMemstore rolled back 6300 keyvalues from start:0 to end:630
2014-07-01 07:45:09,485 DEBUG [RpcServer.handler=3,port=60020] regionserver.HRegion: rollbackMemstore rolled back 6480 keyvalues from start:0 to end:648
2014-07-01 07:45:09,659 ERROR [regionserver60020-WAL.AsyncWriter] wal.FSHLog: Error while AsyncWriter write, request close of hlog 
java.io.IOException: Error Recovery for block blk_-5973222587916760426_29091 failed  because recovery from primary datanode 9.1.143.59:50010 failed 6 times.  Pipeline was 9.1.143.59:50010. Aborting...
	at org.apache.hadoop.hdfs.DFSClient$DFSOutputStream.processDatanodeError(DFSClient.java:3355)
	at org.apache.hadoop.hdfs.DFSClient$DFSOutputStream.access$2200(DFSClient.java:2783)
	at org.apache.hadoop.hdfs.DFSClient$DFSOutputStream$DataStreamer.run(DFSClient.java:2987)
2014-07-01 07:45:09,659 FATAL [regionserver60020-WAL.AsyncSyncer0] wal.FSHLog: Error while AsyncSyncer sync, request close of hlog 
java.io.IOException: DFSOutputStream is closed
	at org.apache.hadoop.hdfs.DFSClient$DFSOutputStream.sync(DFSClient.java:3879)
	at org.apache.hadoop.fs.FSDataOutputStream.sync(FSDataOutputStream.java:97)
	at org.apache.hadoop.hbase.regionserver.wal.ProtobufLogWriter.sync(ProtobufLogWriter.java:158)
	at org.apache.hadoop.hbase.regionserver.wal.FSHLog$AsyncSyncer.run(FSHLog.java:1240)
	at java.lang.Thread.run(Thread.java:701)
2014-07-01 07:45:09,884 WARN  [DataStreamer for file /hbase/data/default/usertable/07064f253c865a7cc0c08893a4f56af9/.tmp/a17a8b64528148559a6414832c7b909b block blk_4372291247462747067_29087] hdfs.DFSClient: Failed recovery attempt #4 from primary datanode 9.1.143.59:50010
org.apache.hadoop.ipc.RemoteException: org.apache.hadoop.ipc.RemoteException: java.io.IOException: blk_4372291247462747067_29087 is missing
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.nextGenerationStampForBlock(FSNamesystem.java:5943)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.nextGenerationStamp(NameNode.java:835)
	at sun.reflect.GeneratedMethodAccessor18.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:622)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:587)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1432)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1428)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:416)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1190)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:1426)

	at org.apache.hadoop.ipc.Client.call(Client.java:1113)
	at org.apache.hadoop.ipc.RPC$Invoker.invoke(RPC.java:229)
	at com.sun.proxy.$Proxy5.nextGenerationStamp(Unknown Source)
	at org.apache.hadoop.hdfs.server.datanode.DataNode.syncBlock(DataNode.java:2127)
	at org.apache.hadoop.hdfs.server.datanode.DataNode.recoverBlock(DataNode.java:2095)
	at org.apache.hadoop.hdfs.server.datanode.DataNode.recoverBlock(DataNode.java:2175)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:622)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:587)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1432)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1428)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:416)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1190)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:1426)

	at org.apache.hadoop.ipc.Client.call(Client.java:1113)
	at org.apache.hadoop.ipc.RPC$Invoker.invoke(RPC.java:229)
	at com.sun.proxy.$Proxy13.recoverBlock(Unknown Source)
	at org.apache.hadoop.hdfs.DFSClient$DFSOutputStream.processDatanodeError(DFSClient.java:3319)
	at org.apache.hadoop.hdfs.DFSClient$DFSOutputStream.access$2200(DFSClient.java:2783)
	at org.apache.hadoop.hdfs.DFSClient$DFSOutputStream$DataStreamer.run(DFSClient.java:2987)
2014-07-01 07:45:09,885 WARN  [RpcServer.handler=47,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":103769,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.58:33505","starttimems":1404225805912,"queuetimems":19079,"class":"HRegionServer","responsesize":290726,"method":"Multi"}
2014-07-01 07:45:09,901 WARN  [RpcServer.handler=47,port=60020] ipc.RpcServer: RpcServer.respondercallId: 3880 service: ClientService methodName: Multi size: 2.1m connection: 9.1.143.58:33505: output error
2014-07-01 07:45:09,901 WARN  [RpcServer.handler=47,port=60020] ipc.RpcServer: RpcServer.handler=47,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-07-01 07:45:10,108 WARN  [RpcServer.handler=37,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":100706,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.58:33505","starttimems":1404225809195,"queuetimems":20256,"class":"HRegionServer","responsesize":297326,"method":"Multi"}
2014-07-01 07:45:10,110 WARN  [RpcServer.handler=37,port=60020] ipc.RpcServer: RpcServer.respondercallId: 3937 service: ClientService methodName: Multi size: 2.3m connection: 9.1.143.58:33505: output error
2014-07-01 07:45:10,110 WARN  [RpcServer.handler=37,port=60020] ipc.RpcServer: RpcServer.handler=37,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-07-01 07:45:10,130 ERROR [regionserver60020-WAL.AsyncWriter] wal.FSHLog: Error while AsyncWriter write, request close of hlog 
java.io.IOException: Error Recovery for block blk_-5973222587916760426_29091 failed  because recovery from primary datanode 9.1.143.59:50010 failed 6 times.  Pipeline was 9.1.143.59:50010. Aborting...
	at org.apache.hadoop.hdfs.DFSClient$DFSOutputStream.processDatanodeError(DFSClient.java:3355)
	at org.apache.hadoop.hdfs.DFSClient$DFSOutputStream.access$2200(DFSClient.java:2783)
	at org.apache.hadoop.hdfs.DFSClient$DFSOutputStream$DataStreamer.run(DFSClient.java:2987)
2014-07-01 07:45:10,131 FATAL [regionserver60020-WAL.AsyncSyncer0] wal.FSHLog: Error while AsyncSyncer sync, request close of hlog 
java.io.IOException: DFSOutputStream is closed
	at org.apache.hadoop.hdfs.DFSClient$DFSOutputStream.sync(DFSClient.java:3879)
	at org.apache.hadoop.fs.FSDataOutputStream.sync(FSDataOutputStream.java:97)
	at org.apache.hadoop.hbase.regionserver.wal.ProtobufLogWriter.sync(ProtobufLogWriter.java:158)
	at org.apache.hadoop.hbase.regionserver.wal.FSHLog$AsyncSyncer.run(FSHLog.java:1240)
	at java.lang.Thread.run(Thread.java:701)
2014-07-01 07:45:10,132 WARN  [RpcServer.handler=36,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":100142,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.58:33505","starttimems":1404225809985,"queuetimems":19812,"class":"HRegionServer","responsesize":327894,"method":"Multi"}
2014-07-01 07:45:10,139 WARN  [RpcServer.handler=36,port=60020] ipc.RpcServer: RpcServer.respondercallId: 3933 service: ClientService methodName: Multi size: 2.4m connection: 9.1.143.58:33505: output error
2014-07-01 07:45:10,139 WARN  [RpcServer.handler=36,port=60020] ipc.RpcServer: RpcServer.handler=36,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-07-01 07:45:10,132 WARN  [RpcServer.handler=46,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":100143,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.58:33505","starttimems":1404225809984,"queuetimems":20997,"class":"HRegionServer","responsesize":334372,"method":"Multi"}
2014-07-01 07:45:10,140 WARN  [DataStreamer for file /hbase/data/default/usertable/07064f253c865a7cc0c08893a4f56af9/.tmp/a17a8b64528148559a6414832c7b909b block blk_4372291247462747067_29087] hdfs.DFSClient: Error Recovery for block blk_4372291247462747067_29087 failed  because recovery from primary datanode 9.1.143.59:50010 failed 5 times.  Pipeline was 9.1.143.59:50010. Will retry...
2014-07-01 07:45:10,153 WARN  [RpcServer.handler=18,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":99155,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.58:33505","starttimems":1404225810971,"queuetimems":20779,"class":"HRegionServer","responsesize":269698,"method":"Multi"}
2014-07-01 07:45:10,152 ERROR [regionserver60020-WAL.AsyncWriter] wal.FSHLog: Error while AsyncWriter write, request close of hlog 
java.io.IOException: Error Recovery for block blk_-5973222587916760426_29091 failed  because recovery from primary datanode 9.1.143.59:50010 failed 6 times.  Pipeline was 9.1.143.59:50010. Aborting...
	at org.apache.hadoop.hdfs.DFSClient$DFSOutputStream.processDatanodeError(DFSClient.java:3355)
	at org.apache.hadoop.hdfs.DFSClient$DFSOutputStream.access$2200(DFSClient.java:2783)
	at org.apache.hadoop.hdfs.DFSClient$DFSOutputStream$DataStreamer.run(DFSClient.java:2987)
2014-07-01 07:45:10,161 FATAL [regionserver60020-WAL.AsyncSyncer0] wal.FSHLog: Error while AsyncSyncer sync, request close of hlog 
java.io.IOException: DFSOutputStream is closed
	at org.apache.hadoop.hdfs.DFSClient$DFSOutputStream.sync(DFSClient.java:3879)
	at org.apache.hadoop.fs.FSDataOutputStream.sync(FSDataOutputStream.java:97)
	at org.apache.hadoop.hbase.regionserver.wal.ProtobufLogWriter.sync(ProtobufLogWriter.java:158)
	at org.apache.hadoop.hbase.regionserver.wal.FSHLog$AsyncSyncer.run(FSHLog.java:1240)
	at java.lang.Thread.run(Thread.java:701)
2014-07-01 07:45:10,151 WARN  [RpcServer.handler=27,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":100133,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.58:33505","starttimems":1404225810006,"queuetimems":19823,"class":"HRegionServer","responsesize":302436,"method":"Multi"}
2014-07-01 07:45:10,148 WARN  [RpcServer.handler=3,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":99058,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.58:33505","starttimems":1404225811079,"queuetimems":20844,"class":"HRegionServer","responsesize":306882,"method":"Multi"}
2014-07-01 07:45:10,162 ERROR [regionserver60020-WAL.AsyncWriter] wal.FSHLog: Error while AsyncWriter write, request close of hlog 
java.io.IOException: Error Recovery for block blk_-5973222587916760426_29091 failed  because recovery from primary datanode 9.1.143.59:50010 failed 6 times.  Pipeline was 9.1.143.59:50010. Aborting...
	at org.apache.hadoop.hdfs.DFSClient$DFSOutputStream.processDatanodeError(DFSClient.java:3355)
	at org.apache.hadoop.hdfs.DFSClient$DFSOutputStream.access$2200(DFSClient.java:2783)
	at org.apache.hadoop.hdfs.DFSClient$DFSOutputStream$DataStreamer.run(DFSClient.java:2987)
2014-07-01 07:45:10,162 FATAL [regionserver60020-WAL.AsyncSyncer0] wal.FSHLog: Error while AsyncSyncer sync, request close of hlog 
java.io.IOException: DFSOutputStream is closed
	at org.apache.hadoop.hdfs.DFSClient$DFSOutputStream.sync(DFSClient.java:3879)
	at org.apache.hadoop.fs.FSDataOutputStream.sync(FSDataOutputStream.java:97)
	at org.apache.hadoop.hbase.regionserver.wal.ProtobufLogWriter.sync(ProtobufLogWriter.java:158)
	at org.apache.hadoop.hbase.regionserver.wal.FSHLog$AsyncSyncer.run(FSHLog.java:1240)
	at java.lang.Thread.run(Thread.java:701)
2014-07-01 07:45:10,165 ERROR [regionserver60020-WAL.AsyncWriter] wal.FSHLog: Error while AsyncWriter write, request close of hlog 
java.io.IOException: Error Recovery for block blk_-5973222587916760426_29091 failed  because recovery from primary datanode 9.1.143.59:50010 failed 6 times.  Pipeline was 9.1.143.59:50010. Aborting...
	at org.apache.hadoop.hdfs.DFSClient$DFSOutputStream.processDatanodeError(DFSClient.java:3355)
	at org.apache.hadoop.hdfs.DFSClient$DFSOutputStream.access$2200(DFSClient.java:2783)
	at org.apache.hadoop.hdfs.DFSClient$DFSOutputStream$DataStreamer.run(DFSClient.java:2987)
2014-07-01 07:45:10,166 FATAL [regionserver60020-WAL.AsyncSyncer0] wal.FSHLog: Error while AsyncSyncer sync, request close of hlog 
java.io.IOException: DFSOutputStream is closed
	at org.apache.hadoop.hdfs.DFSClient$DFSOutputStream.sync(DFSClient.java:3879)
	at org.apache.hadoop.fs.FSDataOutputStream.sync(FSDataOutputStream.java:97)
	at org.apache.hadoop.hbase.regionserver.wal.ProtobufLogWriter.sync(ProtobufLogWriter.java:158)
	at org.apache.hadoop.hbase.regionserver.wal.FSHLog$AsyncSyncer.run(FSHLog.java:1240)
	at java.lang.Thread.run(Thread.java:701)
2014-07-01 07:45:10,150 WARN  [RpcServer.handler=39,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":100172,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.58:33505","starttimems":1404225809972,"queuetimems":20993,"class":"HRegionServer","responsesize":309064,"method":"Multi"}
2014-07-01 07:45:10,150 WARN  [RpcServer.handler=46,port=60020] ipc.RpcServer: RpcServer.respondercallId: 3934 service: ClientService methodName: Multi size: 2.4m connection: 9.1.143.58:33505: output error
2014-07-01 07:45:10,167 WARN  [RpcServer.handler=46,port=60020] ipc.RpcServer: RpcServer.handler=46,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-07-01 07:45:10,150 WARN  [RpcServer.handler=43,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":99050,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.58:33505","starttimems":1404225811076,"queuetimems":20866,"class":"HRegionServer","responsesize":295559,"method":"Multi"}
2014-07-01 07:45:10,167 WARN  [RpcServer.handler=3,port=60020] ipc.RpcServer: RpcServer.respondercallId: 3929 service: ClientService methodName: Multi size: 2.3m connection: 9.1.143.58:33505: output error
2014-07-01 07:45:10,167 WARN  [RpcServer.handler=3,port=60020] ipc.RpcServer: RpcServer.handler=3,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-07-01 07:45:10,168 WARN  [RpcServer.handler=18,port=60020] ipc.RpcServer: RpcServer.respondercallId: 3931 service: ClientService methodName: Multi size: 2.1m connection: 9.1.143.58:33505: output error
2014-07-01 07:45:10,168 WARN  [RpcServer.handler=18,port=60020] ipc.RpcServer: RpcServer.handler=18,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-07-01 07:45:10,171 WARN  [RpcServer.handler=39,port=60020] ipc.RpcServer: RpcServer.respondercallId: 3935 service: ClientService methodName: Multi size: 2.3m connection: 9.1.143.58:33505: output error
2014-07-01 07:45:10,171 WARN  [RpcServer.handler=39,port=60020] ipc.RpcServer: RpcServer.handler=39,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-07-01 07:45:10,172 WARN  [RpcServer.handler=27,port=60020] ipc.RpcServer: RpcServer.respondercallId: 3932 service: ClientService methodName: Multi size: 2.3m connection: 9.1.143.58:33505: output error
2014-07-01 07:45:10,172 WARN  [RpcServer.handler=27,port=60020] ipc.RpcServer: RpcServer.handler=27,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-07-01 07:45:10,173 WARN  [RpcServer.handler=6,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":100964,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.58:33505","starttimems":1404225809203,"queuetimems":20233,"class":"HRegionServer","responsesize":286435,"method":"Multi"}
2014-07-01 07:45:10,178 WARN  [RpcServer.handler=6,port=60020] ipc.RpcServer: RpcServer.respondercallId: 3936 service: ClientService methodName: Multi size: 2.3m connection: 9.1.143.58:33505: output error
2014-07-01 07:45:10,178 WARN  [RpcServer.handler=6,port=60020] ipc.RpcServer: RpcServer.handler=6,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-07-01 07:45:10,178 WARN  [RpcServer.handler=43,port=60020] ipc.RpcServer: RpcServer.respondercallId: 3930 service: ClientService methodName: Multi size: 2.3m connection: 9.1.143.58:33505: output error
2014-07-01 07:45:10,178 WARN  [RpcServer.handler=43,port=60020] ipc.RpcServer: RpcServer.handler=43,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-07-01 07:45:10,180 ERROR [regionserver60020-WAL.AsyncWriter] wal.FSHLog: Error while AsyncWriter write, request close of hlog 
java.io.IOException: Error Recovery for block blk_-5973222587916760426_29091 failed  because recovery from primary datanode 9.1.143.59:50010 failed 6 times.  Pipeline was 9.1.143.59:50010. Aborting...
	at org.apache.hadoop.hdfs.DFSClient$DFSOutputStream.processDatanodeError(DFSClient.java:3355)
	at org.apache.hadoop.hdfs.DFSClient$DFSOutputStream.access$2200(DFSClient.java:2783)
	at org.apache.hadoop.hdfs.DFSClient$DFSOutputStream$DataStreamer.run(DFSClient.java:2987)
2014-07-01 07:45:10,180 FATAL [regionserver60020-WAL.AsyncSyncer0] wal.FSHLog: Error while AsyncSyncer sync, request close of hlog 
java.io.IOException: DFSOutputStream is closed
	at org.apache.hadoop.hdfs.DFSClient$DFSOutputStream.sync(DFSClient.java:3879)
	at org.apache.hadoop.fs.FSDataOutputStream.sync(FSDataOutputStream.java:97)
	at org.apache.hadoop.hbase.regionserver.wal.ProtobufLogWriter.sync(ProtobufLogWriter.java:158)
	at org.apache.hadoop.hbase.regionserver.wal.FSHLog$AsyncSyncer.run(FSHLog.java:1240)
	at java.lang.Thread.run(Thread.java:701)
2014-07-01 07:45:10,182 WARN  [RpcServer.handler=22,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":98198,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.58:33505","starttimems":1404225811982,"queuetimems":21724,"class":"HRegionServer","responsesize":321435,"method":"Multi"}
2014-07-01 07:45:10,191 ERROR [regionserver60020-WAL.AsyncWriter] wal.FSHLog: Error while AsyncWriter write, request close of hlog 
java.io.IOException: Error Recovery for block blk_-5973222587916760426_29091 failed  because recovery from primary datanode 9.1.143.59:50010 failed 6 times.  Pipeline was 9.1.143.59:50010. Aborting...
	at org.apache.hadoop.hdfs.DFSClient$DFSOutputStream.processDatanodeError(DFSClient.java:3355)
	at org.apache.hadoop.hdfs.DFSClient$DFSOutputStream.access$2200(DFSClient.java:2783)
	at org.apache.hadoop.hdfs.DFSClient$DFSOutputStream$DataStreamer.run(DFSClient.java:2987)
2014-07-01 07:45:10,191 WARN  [RpcServer.handler=22,port=60020] ipc.RpcServer: RpcServer.respondercallId: 3928 service: ClientService methodName: Multi size: 2.3m connection: 9.1.143.58:33505: output error
2014-07-01 07:45:10,192 FATAL [regionserver60020-WAL.AsyncSyncer0] wal.FSHLog: Error while AsyncSyncer sync, request close of hlog 
java.io.IOException: DFSOutputStream is closed
	at org.apache.hadoop.hdfs.DFSClient$DFSOutputStream.sync(DFSClient.java:3879)
	at org.apache.hadoop.fs.FSDataOutputStream.sync(FSDataOutputStream.java:97)
	at org.apache.hadoop.hbase.regionserver.wal.ProtobufLogWriter.sync(ProtobufLogWriter.java:158)
	at org.apache.hadoop.hbase.regionserver.wal.FSHLog$AsyncSyncer.run(FSHLog.java:1240)
	at java.lang.Thread.run(Thread.java:701)
2014-07-01 07:45:10,192 WARN  [RpcServer.handler=22,port=60020] ipc.RpcServer: RpcServer.handler=22,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-07-01 07:45:10,194 ERROR [regionserver60020-WAL.AsyncWriter] wal.FSHLog: Error while AsyncWriter write, request close of hlog 
java.io.IOException: Error Recovery for block blk_-5973222587916760426_29091 failed  because recovery from primary datanode 9.1.143.59:50010 failed 6 times.  Pipeline was 9.1.143.59:50010. Aborting...
	at org.apache.hadoop.hdfs.DFSClient$DFSOutputStream.processDatanodeError(DFSClient.java:3355)
	at org.apache.hadoop.hdfs.DFSClient$DFSOutputStream.access$2200(DFSClient.java:2783)
	at org.apache.hadoop.hdfs.DFSClient$DFSOutputStream$DataStreamer.run(DFSClient.java:2987)
2014-07-01 07:45:10,194 FATAL [regionserver60020-WAL.AsyncSyncer0] wal.FSHLog: Error while AsyncSyncer sync, request close of hlog 
java.io.IOException: DFSOutputStream is closed
	at org.apache.hadoop.hdfs.DFSClient$DFSOutputStream.sync(DFSClient.java:3879)
	at org.apache.hadoop.fs.FSDataOutputStream.sync(FSDataOutputStream.java:97)
	at org.apache.hadoop.hbase.regionserver.wal.ProtobufLogWriter.sync(ProtobufLogWriter.java:158)
	at org.apache.hadoop.hbase.regionserver.wal.FSHLog$AsyncSyncer.run(FSHLog.java:1240)
	at java.lang.Thread.run(Thread.java:701)
2014-07-01 07:45:10,204 ERROR [regionserver60020-WAL.AsyncWriter] wal.FSHLog: Error while AsyncWriter write, request close of hlog 
java.io.IOException: Error Recovery for block blk_-5973222587916760426_29091 failed  because recovery from primary datanode 9.1.143.59:50010 failed 6 times.  Pipeline was 9.1.143.59:50010. Aborting...
	at org.apache.hadoop.hdfs.DFSClient$DFSOutputStream.processDatanodeError(DFSClient.java:3355)
	at org.apache.hadoop.hdfs.DFSClient$DFSOutputStream.access$2200(DFSClient.java:2783)
	at org.apache.hadoop.hdfs.DFSClient$DFSOutputStream$DataStreamer.run(DFSClient.java:2987)
2014-07-01 07:45:10,204 FATAL [regionserver60020-WAL.AsyncSyncer0] wal.FSHLog: Error while AsyncSyncer sync, request close of hlog 
java.io.IOException: DFSOutputStream is closed
	at org.apache.hadoop.hdfs.DFSClient$DFSOutputStream.sync(DFSClient.java:3879)
	at org.apache.hadoop.fs.FSDataOutputStream.sync(FSDataOutputStream.java:97)
	at org.apache.hadoop.hbase.regionserver.wal.ProtobufLogWriter.sync(ProtobufLogWriter.java:158)
	at org.apache.hadoop.hbase.regionserver.wal.FSHLog$AsyncSyncer.run(FSHLog.java:1240)
	at java.lang.Thread.run(Thread.java:701)
2014-07-01 07:45:10,211 ERROR [regionserver60020-WAL.AsyncWriter] wal.FSHLog: Error while AsyncWriter write, request close of hlog 
java.io.IOException: Error Recovery for block blk_-5973222587916760426_29091 failed  because recovery from primary datanode 9.1.143.59:50010 failed 6 times.  Pipeline was 9.1.143.59:50010. Aborting...
	at org.apache.hadoop.hdfs.DFSClient$DFSOutputStream.processDatanodeError(DFSClient.java:3355)
	at org.apache.hadoop.hdfs.DFSClient$DFSOutputStream.access$2200(DFSClient.java:2783)
	at org.apache.hadoop.hdfs.DFSClient$DFSOutputStream$DataStreamer.run(DFSClient.java:2987)
2014-07-01 07:45:10,211 FATAL [regionserver60020-WAL.AsyncSyncer0] wal.FSHLog: Error while AsyncSyncer sync, request close of hlog 
java.io.IOException: DFSOutputStream is closed
	at org.apache.hadoop.hdfs.DFSClient$DFSOutputStream.sync(DFSClient.java:3879)
	at org.apache.hadoop.fs.FSDataOutputStream.sync(FSDataOutputStream.java:97)
	at org.apache.hadoop.hbase.regionserver.wal.ProtobufLogWriter.sync(ProtobufLogWriter.java:158)
	at org.apache.hadoop.hbase.regionserver.wal.FSHLog$AsyncSyncer.run(FSHLog.java:1240)
	at java.lang.Thread.run(Thread.java:701)
2014-07-01 07:45:10,213 ERROR [regionserver60020-WAL.AsyncWriter] wal.FSHLog: Error while AsyncWriter write, request close of hlog 
java.io.IOException: Error Recovery for block blk_-5973222587916760426_29091 failed  because recovery from primary datanode 9.1.143.59:50010 failed 6 times.  Pipeline was 9.1.143.59:50010. Aborting...
	at org.apache.hadoop.hdfs.DFSClient$DFSOutputStream.processDatanodeError(DFSClient.java:3355)
	at org.apache.hadoop.hdfs.DFSClient$DFSOutputStream.access$2200(DFSClient.java:2783)
	at org.apache.hadoop.hdfs.DFSClient$DFSOutputStream$DataStreamer.run(DFSClient.java:2987)
2014-07-01 07:45:10,213 FATAL [regionserver60020-WAL.AsyncSyncer0] wal.FSHLog: Error while AsyncSyncer sync, request close of hlog 
java.io.IOException: DFSOutputStream is closed
	at org.apache.hadoop.hdfs.DFSClient$DFSOutputStream.sync(DFSClient.java:3879)
	at org.apache.hadoop.fs.FSDataOutputStream.sync(FSDataOutputStream.java:97)
	at org.apache.hadoop.hbase.regionserver.wal.ProtobufLogWriter.sync(ProtobufLogWriter.java:158)
	at org.apache.hadoop.hbase.regionserver.wal.FSHLog$AsyncSyncer.run(FSHLog.java:1240)
	at java.lang.Thread.run(Thread.java:701)
2014-07-01 07:45:10,224 ERROR [regionserver60020-WAL.AsyncWriter] wal.FSHLog: Error while AsyncWriter write, request close of hlog 
java.io.IOException: Error Recovery for block blk_-5973222587916760426_29091 failed  because recovery from primary datanode 9.1.143.59:50010 failed 6 times.  Pipeline was 9.1.143.59:50010. Aborting...
	at org.apache.hadoop.hdfs.DFSClient$DFSOutputStream.processDatanodeError(DFSClient.java:3355)
	at org.apache.hadoop.hdfs.DFSClient$DFSOutputStream.access$2200(DFSClient.java:2783)
	at org.apache.hadoop.hdfs.DFSClient$DFSOutputStream$DataStreamer.run(DFSClient.java:2987)
2014-07-01 07:45:10,224 FATAL [regionserver60020-WAL.AsyncSyncer0] wal.FSHLog: Error while AsyncSyncer sync, request close of hlog 
java.io.IOException: DFSOutputStream is closed
	at org.apache.hadoop.hdfs.DFSClient$DFSOutputStream.sync(DFSClient.java:3879)
	at org.apache.hadoop.fs.FSDataOutputStream.sync(FSDataOutputStream.java:97)
	at org.apache.hadoop.hbase.regionserver.wal.ProtobufLogWriter.sync(ProtobufLogWriter.java:158)
	at org.apache.hadoop.hbase.regionserver.wal.FSHLog$AsyncSyncer.run(FSHLog.java:1240)
	at java.lang.Thread.run(Thread.java:701)
2014-07-01 07:45:10,227 ERROR [regionserver60020-WAL.AsyncWriter] wal.FSHLog: Error while AsyncWriter write, request close of hlog 
java.io.IOException: Error Recovery for block blk_-5973222587916760426_29091 failed  because recovery from primary datanode 9.1.143.59:50010 failed 6 times.  Pipeline was 9.1.143.59:50010. Aborting...
	at org.apache.hadoop.hdfs.DFSClient$DFSOutputStream.processDatanodeError(DFSClient.java:3355)
	at org.apache.hadoop.hdfs.DFSClient$DFSOutputStream.access$2200(DFSClient.java:2783)
	at org.apache.hadoop.hdfs.DFSClient$DFSOutputStream$DataStreamer.run(DFSClient.java:2987)
2014-07-01 07:45:10,231 FATAL [regionserver60020-WAL.AsyncSyncer0] wal.FSHLog: Error while AsyncSyncer sync, request close of hlog 
java.io.IOException: DFSOutputStream is closed
	at org.apache.hadoop.hdfs.DFSClient$DFSOutputStream.sync(DFSClient.java:3879)
	at org.apache.hadoop.fs.FSDataOutputStream.sync(FSDataOutputStream.java:97)
	at org.apache.hadoop.hbase.regionserver.wal.ProtobufLogWriter.sync(ProtobufLogWriter.java:158)
	at org.apache.hadoop.hbase.regionserver.wal.FSHLog$AsyncSyncer.run(FSHLog.java:1240)
	at java.lang.Thread.run(Thread.java:701)
2014-07-01 07:45:10,232 ERROR [regionserver60020-WAL.AsyncWriter] wal.FSHLog: Error while AsyncWriter write, request close of hlog 
java.io.IOException: Error Recovery for block blk_-5973222587916760426_29091 failed  because recovery from primary datanode 9.1.143.59:50010 failed 6 times.  Pipeline was 9.1.143.59:50010. Aborting...
	at org.apache.hadoop.hdfs.DFSClient$DFSOutputStream.processDatanodeError(DFSClient.java:3355)
	at org.apache.hadoop.hdfs.DFSClient$DFSOutputStream.access$2200(DFSClient.java:2783)
	at org.apache.hadoop.hdfs.DFSClient$DFSOutputStream$DataStreamer.run(DFSClient.java:2987)
2014-07-01 07:45:10,232 FATAL [regionserver60020-WAL.AsyncSyncer0] wal.FSHLog: Error while AsyncSyncer sync, request close of hlog 
java.io.IOException: DFSOutputStream is closed
	at org.apache.hadoop.hdfs.DFSClient$DFSOutputStream.sync(DFSClient.java:3879)
	at org.apache.hadoop.fs.FSDataOutputStream.sync(FSDataOutputStream.java:97)
	at org.apache.hadoop.hbase.regionserver.wal.ProtobufLogWriter.sync(ProtobufLogWriter.java:158)
	at org.apache.hadoop.hbase.regionserver.wal.FSHLog$AsyncSyncer.run(FSHLog.java:1240)
	at java.lang.Thread.run(Thread.java:701)
2014-07-01 07:45:10,233 WARN  [RpcServer.handler=31,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":96830,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.58:33505","starttimems":1404225813400,"queuetimems":21671,"class":"HRegionServer","responsesize":1216521,"method":"Multi"}
2014-07-01 07:45:10,235 ERROR [regionserver60020-WAL.AsyncWriter] wal.FSHLog: Error while AsyncWriter write, request close of hlog 
java.io.IOException: Error Recovery for block blk_-5973222587916760426_29091 failed  because recovery from primary datanode 9.1.143.59:50010 failed 6 times.  Pipeline was 9.1.143.59:50010. Aborting...
	at org.apache.hadoop.hdfs.DFSClient$DFSOutputStream.processDatanodeError(DFSClient.java:3355)
	at org.apache.hadoop.hdfs.DFSClient$DFSOutputStream.access$2200(DFSClient.java:2783)
	at org.apache.hadoop.hdfs.DFSClient$DFSOutputStream$DataStreamer.run(DFSClient.java:2987)
2014-07-01 07:45:10,235 FATAL [regionserver60020-WAL.AsyncSyncer0] wal.FSHLog: Error while AsyncSyncer sync, request close of hlog 
java.io.IOException: DFSOutputStream is closed
	at org.apache.hadoop.hdfs.DFSClient$DFSOutputStream.sync(DFSClient.java:3879)
	at org.apache.hadoop.fs.FSDataOutputStream.sync(FSDataOutputStream.java:97)
	at org.apache.hadoop.hbase.regionserver.wal.ProtobufLogWriter.sync(ProtobufLogWriter.java:158)
	at org.apache.hadoop.hbase.regionserver.wal.FSHLog$AsyncSyncer.run(FSHLog.java:1240)
	at java.lang.Thread.run(Thread.java:701)
2014-07-01 07:45:10,236 WARN  [RpcServer.handler=31,port=60020] ipc.RpcServer: RpcServer.respondercallId: 3946 service: ClientService methodName: Multi size: 2.4m connection: 9.1.143.58:33505: output error
2014-07-01 07:45:10,236 WARN  [RpcServer.handler=31,port=60020] ipc.RpcServer: RpcServer.handler=31,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-07-01 07:45:10,238 ERROR [regionserver60020-WAL.AsyncWriter] wal.FSHLog: Error while AsyncWriter write, request close of hlog 
java.io.IOException: Error Recovery for block blk_-5973222587916760426_29091 failed  because recovery from primary datanode 9.1.143.59:50010 failed 6 times.  Pipeline was 9.1.143.59:50010. Aborting...
	at org.apache.hadoop.hdfs.DFSClient$DFSOutputStream.processDatanodeError(DFSClient.java:3355)
	at org.apache.hadoop.hdfs.DFSClient$DFSOutputStream.access$2200(DFSClient.java:2783)
	at org.apache.hadoop.hdfs.DFSClient$DFSOutputStream$DataStreamer.run(DFSClient.java:2987)
2014-07-01 07:45:10,238 FATAL [regionserver60020-WAL.AsyncSyncer0] wal.FSHLog: Error while AsyncSyncer sync, request close of hlog 
java.io.IOException: DFSOutputStream is closed
	at org.apache.hadoop.hdfs.DFSClient$DFSOutputStream.sync(DFSClient.java:3879)
	at org.apache.hadoop.fs.FSDataOutputStream.sync(FSDataOutputStream.java:97)
	at org.apache.hadoop.hbase.regionserver.wal.ProtobufLogWriter.sync(ProtobufLogWriter.java:158)
	at org.apache.hadoop.hbase.regionserver.wal.FSHLog$AsyncSyncer.run(FSHLog.java:1240)
	at java.lang.Thread.run(Thread.java:701)
2014-07-01 07:45:10,327 DEBUG [RpcServer.handler=15,port=60020] regionserver.HRegion: rollbackMemstore rolled back 6530 keyvalues from start:0 to end:653
2014-07-01 07:45:10,341 DEBUG [RpcServer.handler=45,port=60020] regionserver.HRegion: rollbackMemstore rolled back 6580 keyvalues from start:0 to end:658
2014-07-01 07:45:10,535 DEBUG [RpcServer.handler=35,port=60020] regionserver.HRegion: rollbackMemstore rolled back 6400 keyvalues from start:0 to end:640
2014-07-01 07:45:10,540 WARN  [RpcServer.handler=17,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":97116,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.58:33505","starttimems":1404225813422,"queuetimems":20389,"class":"HRegionServer","responsesize":301545,"method":"Multi"}
2014-07-01 07:45:10,535 DEBUG [RpcServer.handler=12,port=60020] regionserver.HRegion: rollbackMemstore rolled back 6430 keyvalues from start:0 to end:643
2014-07-01 07:45:10,542 WARN  [RpcServer.handler=17,port=60020] ipc.RpcServer: RpcServer.respondercallId: 3957 service: ClientService methodName: Multi size: 2.3m connection: 9.1.143.58:33505: output error
2014-07-01 07:45:10,543 WARN  [RpcServer.handler=17,port=60020] ipc.RpcServer: RpcServer.handler=17,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-07-01 07:45:10,551 DEBUG [RpcServer.handler=34,port=60020] regionserver.HRegion: rollbackMemstore rolled back 6050 keyvalues from start:0 to end:605
2014-07-01 07:45:10,558 DEBUG [RpcServer.handler=25,port=60020] regionserver.HRegion: rollbackMemstore rolled back 6190 keyvalues from start:0 to end:619
2014-07-01 07:45:10,582 DEBUG [RpcServer.handler=28,port=60020] regionserver.HRegion: rollbackMemstore rolled back 6370 keyvalues from start:0 to end:637
2014-07-01 07:45:10,584 DEBUG [RpcServer.handler=21,port=60020] regionserver.HRegion: rollbackMemstore rolled back 6290 keyvalues from start:0 to end:629
2014-07-01 07:45:10,614 DEBUG [RpcServer.handler=9,port=60020] regionserver.HRegion: rollbackMemstore rolled back 6590 keyvalues from start:0 to end:659
2014-07-01 07:45:10,847 WARN  [RpcServer.handler=45,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":98095,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.58:33505","starttimems":1404225812750,"queuetimems":21766,"class":"HRegionServer","responsesize":614807,"method":"Multi"}
2014-07-01 07:45:10,851 DEBUG [RpcServer.handler=32,port=60020] regionserver.HRegion: rollbackMemstore rolled back 6330 keyvalues from start:0 to end:633
2014-07-01 07:45:10,884 WARN  [RpcServer.handler=40,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":97482,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.58:33505","starttimems":1404225813401,"queuetimems":21653,"class":"HRegionServer","responsesize":297759,"method":"Multi"}
2014-07-01 07:45:10,885 WARN  [RpcServer.handler=40,port=60020] ipc.RpcServer: RpcServer.respondercallId: 3942 service: ClientService methodName: Multi size: 2.4m connection: 9.1.143.58:33505: output error
2014-07-01 07:45:10,886 WARN  [RpcServer.handler=40,port=60020] ipc.RpcServer: RpcServer.handler=40,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-07-01 07:45:11,190 WARN  [DataStreamer for file /hbase/data/default/usertable/07064f253c865a7cc0c08893a4f56af9/.tmp/a17a8b64528148559a6414832c7b909b block blk_4372291247462747067_29087] hdfs.DFSClient: Failed recovery attempt #5 from primary datanode 9.1.143.59:50010
org.apache.hadoop.ipc.RemoteException: org.apache.hadoop.ipc.RemoteException: java.io.IOException: blk_4372291247462747067_29087 is missing
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.nextGenerationStampForBlock(FSNamesystem.java:5943)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.nextGenerationStamp(NameNode.java:835)
	at sun.reflect.GeneratedMethodAccessor18.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:622)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:587)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1432)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1428)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:416)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1190)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:1426)

	at org.apache.hadoop.ipc.Client.call(Client.java:1113)
	at org.apache.hadoop.ipc.RPC$Invoker.invoke(RPC.java:229)
	at com.sun.proxy.$Proxy5.nextGenerationStamp(Unknown Source)
	at org.apache.hadoop.hdfs.server.datanode.DataNode.syncBlock(DataNode.java:2127)
	at org.apache.hadoop.hdfs.server.datanode.DataNode.recoverBlock(DataNode.java:2095)
	at org.apache.hadoop.hdfs.server.datanode.DataNode.recoverBlock(DataNode.java:2175)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:622)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:587)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1432)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1428)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:416)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1190)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:1426)

	at org.apache.hadoop.ipc.Client.call(Client.java:1113)
	at org.apache.hadoop.ipc.RPC$Invoker.invoke(RPC.java:229)
	at com.sun.proxy.$Proxy13.recoverBlock(Unknown Source)
	at org.apache.hadoop.hdfs.DFSClient$DFSOutputStream.processDatanodeError(DFSClient.java:3319)
	at org.apache.hadoop.hdfs.DFSClient$DFSOutputStream.access$2200(DFSClient.java:2783)
	at org.apache.hadoop.hdfs.DFSClient$DFSOutputStream$DataStreamer.run(DFSClient.java:2987)
2014-07-01 07:45:11,191 WARN  [DataStreamer for file /hbase/data/default/usertable/07064f253c865a7cc0c08893a4f56af9/.tmp/a17a8b64528148559a6414832c7b909b block blk_4372291247462747067_29087] hdfs.DFSClient: Error Recovery for block blk_4372291247462747067_29087 failed  because recovery from primary datanode 9.1.143.59:50010 failed 6 times.  Pipeline was 9.1.143.59:50010. Aborting...
2014-07-01 07:45:11,195 WARN  [MemStoreFlusher.0] regionserver.HStore: Failed flushing store file, retrying num=0
java.io.IOException: Error Recovery for block blk_4372291247462747067_29087 failed  because recovery from primary datanode 9.1.143.59:50010 failed 6 times.  Pipeline was 9.1.143.59:50010. Aborting...
	at org.apache.hadoop.hdfs.DFSClient$DFSOutputStream.processDatanodeError(DFSClient.java:3355)
	at org.apache.hadoop.hdfs.DFSClient$DFSOutputStream.access$2200(DFSClient.java:2783)
	at org.apache.hadoop.hdfs.DFSClient$DFSOutputStream$DataStreamer.run(DFSClient.java:2987)
2014-07-01 07:45:11,293 WARN  [RpcServer.handler=45,port=60020] ipc.RpcServer: RpcServer.respondercallId: 3927 service: ClientService methodName: Multi size: 2.4m connection: 9.1.143.58:33505: output error
2014-07-01 07:45:11,293 WARN  [RpcServer.handler=45,port=60020] ipc.RpcServer: RpcServer.handler=45,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-07-01 07:45:11,295 DEBUG [RpcServer.handler=1,port=60020] regionserver.HRegion: rollbackMemstore rolled back 6110 keyvalues from start:0 to end:611
2014-07-01 07:45:11,296 WARN  [RpcServer.handler=15,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":97896,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.58:33505","starttimems":1404225813398,"queuetimems":21917,"class":"HRegionServer","responsesize":606942,"method":"Multi"}
2014-07-01 07:45:11,297 WARN  [RpcServer.handler=15,port=60020] ipc.RpcServer: RpcServer.respondercallId: 3923 service: ClientService methodName: Multi size: 2.4m connection: 9.1.143.58:33505: output error
2014-07-01 07:45:11,297 WARN  [RpcServer.handler=15,port=60020] ipc.RpcServer: RpcServer.handler=15,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-07-01 07:45:11,327 DEBUG [RpcServer.handler=49,port=60020] regionserver.HRegion: rollbackMemstore rolled back 4500 keyvalues from start:0 to end:450
2014-07-01 07:45:11,342 DEBUG [RpcServer.handler=4,port=60020] regionserver.HRegion: rollbackMemstore rolled back 6430 keyvalues from start:0 to end:643
2014-07-01 07:45:11,346 DEBUG [RpcServer.handler=20,port=60020] regionserver.HRegion: rollbackMemstore rolled back 6430 keyvalues from start:0 to end:643
2014-07-01 07:45:11,365 DEBUG [RpcServer.handler=48,port=60020] regionserver.HRegion: rollbackMemstore rolled back 6570 keyvalues from start:0 to end:657
2014-07-01 07:45:11,378 WARN  [RpcServer.handler=35,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":97955,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.58:33505","starttimems":1404225813397,"queuetimems":22394,"class":"HRegionServer","responsesize":590780,"method":"Multi"}
2014-07-01 07:45:11,378 DEBUG [RpcServer.handler=7,port=60020] regionserver.HRegion: rollbackMemstore rolled back 6300 keyvalues from start:0 to end:630
2014-07-01 07:45:11,392 WARN  [RpcServer.handler=35,port=60020] ipc.RpcServer: RpcServer.respondercallId: 3925 service: ClientService methodName: Multi size: 2.3m connection: 9.1.143.58:33505: output error
2014-07-01 07:45:11,392 WARN  [RpcServer.handler=35,port=60020] ipc.RpcServer: RpcServer.handler=35,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-07-01 07:45:11,473 WARN  [RpcServer.handler=32,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":98048,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.58:33505","starttimems":1404225813423,"queuetimems":20355,"class":"HRegionServer","responsesize":578351,"method":"Multi"}
2014-07-01 07:45:11,476 WARN  [RpcServer.handler=32,port=60020] ipc.RpcServer: RpcServer.respondercallId: 3956 service: ClientService methodName: Multi size: 2.3m connection: 9.1.143.58:33505: output error
2014-07-01 07:45:11,477 WARN  [RpcServer.handler=32,port=60020] ipc.RpcServer: RpcServer.handler=32,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-07-01 07:45:11,527 DEBUG [RpcServer.handler=23,port=60020] regionserver.HRegion: rollbackMemstore rolled back 6260 keyvalues from start:0 to end:626
2014-07-01 07:45:11,842 WARN  [RpcServer.handler=21,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":98439,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.58:33505","starttimems":1404225813402,"queuetimems":21438,"class":"HRegionServer","responsesize":577570,"method":"Multi"}
2014-07-01 07:45:11,842 WARN  [RpcServer.handler=28,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":98439,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.58:33505","starttimems":1404225813402,"queuetimems":21604,"class":"HRegionServer","responsesize":601689,"method":"Multi"}
2014-07-01 07:45:12,287 WARN  [RpcServer.handler=49,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":98883,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.58:33505","starttimems":1404225813403,"queuetimems":20495,"class":"HRegionServer","responsesize":404690,"method":"Multi"}
2014-07-01 07:45:12,286 WARN  [RpcServer.handler=1,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":98887,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.58:33505","starttimems":1404225813398,"queuetimems":22378,"class":"HRegionServer","responsesize":601220,"method":"Multi"}
2014-07-01 07:45:12,285 WARN  [RpcServer.handler=25,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":98886,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.58:33505","starttimems":1404225813398,"queuetimems":21905,"class":"HRegionServer","responsesize":629606,"method":"Multi"}
2014-07-01 07:45:12,282 WARN  [RpcServer.handler=21,port=60020] ipc.RpcServer: RpcServer.respondercallId: 3950 service: ClientService methodName: Multi size: 2.3m connection: 9.1.143.58:33505: output error
2014-07-01 07:45:12,288 WARN  [RpcServer.handler=23,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":98865,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.58:33505","starttimems":1404225813422,"queuetimems":20432,"class":"HRegionServer","responsesize":592947,"method":"Multi"}
2014-07-01 07:45:12,289 WARN  [RpcServer.handler=7,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":98887,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.58:33505","starttimems":1404225813401,"queuetimems":21638,"class":"HRegionServer","responsesize":594178,"method":"Multi"}
2014-07-01 07:45:12,289 WARN  [RpcServer.handler=20,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":98857,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.58:33505","starttimems":1404225813431,"queuetimems":20324,"class":"HRegionServer","responsesize":601992,"method":"Multi"}
2014-07-01 07:45:12,288 WARN  [RpcServer.handler=21,port=60020] ipc.RpcServer: RpcServer.handler=21,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-07-01 07:45:12,289 WARN  [RpcServer.handler=23,port=60020] ipc.RpcServer: RpcServer.respondercallId: 3958 service: ClientService methodName: Multi size: 2.2m connection: 9.1.143.58:33505: output error
2014-07-01 07:45:12,289 WARN  [RpcServer.handler=23,port=60020] ipc.RpcServer: RpcServer.handler=23,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-07-01 07:45:12,290 WARN  [RpcServer.handler=28,port=60020] ipc.RpcServer: RpcServer.respondercallId: 3951 service: ClientService methodName: Multi size: 2.4m connection: 9.1.143.58:33505: output error
2014-07-01 07:45:12,290 WARN  [RpcServer.handler=28,port=60020] ipc.RpcServer: RpcServer.handler=28,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-07-01 07:45:12,290 WARN  [RpcServer.handler=12,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":98856,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.58:33505","starttimems":1404225813431,"queuetimems":19569,"class":"HRegionServer","responsesize":630165,"method":"Multi"}
2014-07-01 07:45:12,290 WARN  [RpcServer.handler=49,port=60020] ipc.RpcServer: RpcServer.respondercallId: 3955 service: ClientService methodName: Multi size: 1.6m connection: 9.1.143.58:33505: output error
2014-07-01 07:45:12,291 WARN  [RpcServer.handler=49,port=60020] ipc.RpcServer: RpcServer.handler=49,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-07-01 07:45:12,291 WARN  [RpcServer.handler=1,port=60020] ipc.RpcServer: RpcServer.respondercallId: 3924 service: ClientService methodName: Multi size: 2.3m connection: 9.1.143.58:33505: output error
2014-07-01 07:45:12,291 WARN  [RpcServer.handler=1,port=60020] ipc.RpcServer: RpcServer.handler=1,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-07-01 07:45:12,291 WARN  [RpcServer.handler=20,port=60020] ipc.RpcServer: RpcServer.respondercallId: 3965 service: ClientService methodName: Multi size: 2.4m connection: 9.1.143.58:33505: output error
2014-07-01 07:45:12,291 WARN  [RpcServer.handler=20,port=60020] ipc.RpcServer: RpcServer.handler=20,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-07-01 07:45:12,292 WARN  [RpcServer.handler=7,port=60020] ipc.RpcServer: RpcServer.respondercallId: 3952 service: ClientService methodName: Multi size: 2.3m connection: 9.1.143.58:33505: output error
2014-07-01 07:45:12,292 WARN  [RpcServer.handler=7,port=60020] ipc.RpcServer: RpcServer.handler=7,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-07-01 07:45:12,292 WARN  [RpcServer.handler=25,port=60020] ipc.RpcServer: RpcServer.respondercallId: 3922 service: ClientService methodName: Multi size: 2.4m connection: 9.1.143.58:33505: output error
2014-07-01 07:45:12,292 WARN  [RpcServer.handler=25,port=60020] ipc.RpcServer: RpcServer.handler=25,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-07-01 07:45:12,292 WARN  [RpcServer.handler=12,port=60020] ipc.RpcServer: RpcServer.respondercallId: 3964 service: ClientService methodName: Multi size: 2.4m connection: 9.1.143.58:33505: output error
2014-07-01 07:45:12,293 WARN  [RpcServer.handler=12,port=60020] ipc.RpcServer: RpcServer.handler=12,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-07-01 07:45:12,295 WARN  [RpcServer.handler=48,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":98897,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.58:33505","starttimems":1404225813397,"queuetimems":22403,"class":"HRegionServer","responsesize":589139,"method":"Multi"}
2014-07-01 07:45:12,296 WARN  [RpcServer.handler=48,port=60020] ipc.RpcServer: RpcServer.respondercallId: 3926 service: ClientService methodName: Multi size: 2.3m connection: 9.1.143.58:33505: output error
2014-07-01 07:45:12,296 WARN  [RpcServer.handler=48,port=60020] ipc.RpcServer: RpcServer.handler=48,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-07-01 07:45:12,296 WARN  [RpcServer.handler=4,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":98873,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.58:33505","starttimems":1404225813422,"queuetimems":20473,"class":"HRegionServer","responsesize":594472,"method":"Multi"}
2014-07-01 07:45:12,297 WARN  [RpcServer.handler=4,port=60020] ipc.RpcServer: RpcServer.respondercallId: 3959 service: ClientService methodName: Multi size: 2.3m connection: 9.1.143.58:33505: output error
2014-07-01 07:45:12,297 WARN  [RpcServer.handler=4,port=60020] ipc.RpcServer: RpcServer.handler=4,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-07-01 07:45:12,348 ERROR [regionserver60020-WAL.AsyncWriter] wal.FSHLog: Error while AsyncWriter write, request close of hlog 
java.io.IOException: Error Recovery for block blk_-5973222587916760426_29091 failed  because recovery from primary datanode 9.1.143.59:50010 failed 6 times.  Pipeline was 9.1.143.59:50010. Aborting...
	at org.apache.hadoop.hdfs.DFSClient$DFSOutputStream.processDatanodeError(DFSClient.java:3355)
	at org.apache.hadoop.hdfs.DFSClient$DFSOutputStream.access$2200(DFSClient.java:2783)
	at org.apache.hadoop.hdfs.DFSClient$DFSOutputStream$DataStreamer.run(DFSClient.java:2987)
2014-07-01 07:45:12,348 FATAL [regionserver60020-WAL.AsyncSyncer0] wal.FSHLog: Error while AsyncSyncer sync, request close of hlog 
java.io.IOException: DFSOutputStream is closed
	at org.apache.hadoop.hdfs.DFSClient$DFSOutputStream.sync(DFSClient.java:3879)
	at org.apache.hadoop.fs.FSDataOutputStream.sync(FSDataOutputStream.java:97)
	at org.apache.hadoop.hbase.regionserver.wal.ProtobufLogWriter.sync(ProtobufLogWriter.java:158)
	at org.apache.hadoop.hbase.regionserver.wal.FSHLog$AsyncSyncer.run(FSHLog.java:1240)
	at java.lang.Thread.run(Thread.java:701)
2014-07-01 07:45:12,390 ERROR [regionserver60020-WAL.AsyncWriter] wal.FSHLog: Error while AsyncWriter write, request close of hlog 
java.io.IOException: Error Recovery for block blk_-5973222587916760426_29091 failed  because recovery from primary datanode 9.1.143.59:50010 failed 6 times.  Pipeline was 9.1.143.59:50010. Aborting...
	at org.apache.hadoop.hdfs.DFSClient$DFSOutputStream.processDatanodeError(DFSClient.java:3355)
	at org.apache.hadoop.hdfs.DFSClient$DFSOutputStream.access$2200(DFSClient.java:2783)
	at org.apache.hadoop.hdfs.DFSClient$DFSOutputStream$DataStreamer.run(DFSClient.java:2987)
2014-07-01 07:45:12,390 FATAL [regionserver60020-WAL.AsyncSyncer0] wal.FSHLog: Error while AsyncSyncer sync, request close of hlog 
java.io.IOException: DFSOutputStream is closed
	at org.apache.hadoop.hdfs.DFSClient$DFSOutputStream.sync(DFSClient.java:3879)
	at org.apache.hadoop.fs.FSDataOutputStream.sync(FSDataOutputStream.java:97)
	at org.apache.hadoop.hbase.regionserver.wal.ProtobufLogWriter.sync(ProtobufLogWriter.java:158)
	at org.apache.hadoop.hbase.regionserver.wal.FSHLog$AsyncSyncer.run(FSHLog.java:1240)
	at java.lang.Thread.run(Thread.java:701)
2014-07-01 07:45:12,492 DEBUG [RpcServer.handler=34,port=60020] regionserver.HRegion: rollbackMemstore rolled back 6560 keyvalues from start:0 to end:656
2014-07-01 07:45:12,531 DEBUG [RpcServer.handler=9,port=60020] regionserver.HRegion: rollbackMemstore rolled back 6460 keyvalues from start:0 to end:646
2014-07-01 07:45:12,581 ERROR [regionserver60020-WAL.AsyncWriter] wal.FSHLog: Error while AsyncWriter write, request close of hlog 
java.io.IOException: Error Recovery for block blk_-5973222587916760426_29091 failed  because recovery from primary datanode 9.1.143.59:50010 failed 6 times.  Pipeline was 9.1.143.59:50010. Aborting...
	at org.apache.hadoop.hdfs.DFSClient$DFSOutputStream.processDatanodeError(DFSClient.java:3355)
	at org.apache.hadoop.hdfs.DFSClient$DFSOutputStream.access$2200(DFSClient.java:2783)
	at org.apache.hadoop.hdfs.DFSClient$DFSOutputStream$DataStreamer.run(DFSClient.java:2987)
2014-07-01 07:45:12,582 FATAL [regionserver60020-WAL.AsyncSyncer0] wal.FSHLog: Error while AsyncSyncer sync, request close of hlog 
java.io.IOException: DFSOutputStream is closed
	at org.apache.hadoop.hdfs.DFSClient$DFSOutputStream.sync(DFSClient.java:3879)
	at org.apache.hadoop.fs.FSDataOutputStream.sync(FSDataOutputStream.java:97)
	at org.apache.hadoop.hbase.regionserver.wal.ProtobufLogWriter.sync(ProtobufLogWriter.java:158)
	at org.apache.hadoop.hbase.regionserver.wal.FSHLog$AsyncSyncer.run(FSHLog.java:1240)
	at java.lang.Thread.run(Thread.java:701)
2014-07-01 07:45:12,604 ERROR [regionserver60020-WAL.AsyncWriter] wal.FSHLog: Error while AsyncWriter write, request close of hlog 
java.io.IOException: Error Recovery for block blk_-5973222587916760426_29091 failed  because recovery from primary datanode 9.1.143.59:50010 failed 6 times.  Pipeline was 9.1.143.59:50010. Aborting...
	at org.apache.hadoop.hdfs.DFSClient$DFSOutputStream.processDatanodeError(DFSClient.java:3355)
	at org.apache.hadoop.hdfs.DFSClient$DFSOutputStream.access$2200(DFSClient.java:2783)
	at org.apache.hadoop.hdfs.DFSClient$DFSOutputStream$DataStreamer.run(DFSClient.java:2987)
2014-07-01 07:45:12,604 FATAL [regionserver60020-WAL.AsyncSyncer0] wal.FSHLog: Error while AsyncSyncer sync, request close of hlog 
java.io.IOException: DFSOutputStream is closed
	at org.apache.hadoop.hdfs.DFSClient$DFSOutputStream.sync(DFSClient.java:3879)
	at org.apache.hadoop.fs.FSDataOutputStream.sync(FSDataOutputStream.java:97)
	at org.apache.hadoop.hbase.regionserver.wal.ProtobufLogWriter.sync(ProtobufLogWriter.java:158)
	at org.apache.hadoop.hbase.regionserver.wal.FSHLog$AsyncSyncer.run(FSHLog.java:1240)
	at java.lang.Thread.run(Thread.java:701)
2014-07-01 07:45:12,804 DEBUG [RpcServer.handler=34,port=60020] regionserver.HRegion: rollbackMemstore rolled back 6270 keyvalues from start:0 to end:627
2014-07-01 07:45:12,820 WARN  [RpcServer.handler=34,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":98147,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.58:33505","starttimems":1404225814671,"queuetimems":20767,"class":"HRegionServer","responsesize":896275,"method":"Multi"}
2014-07-01 07:45:12,822 WARN  [RpcServer.handler=34,port=60020] ipc.RpcServer: RpcServer.respondercallId: 3963 service: ClientService methodName: Multi size: 2.3m connection: 9.1.143.58:33505: output error
2014-07-01 07:45:12,822 WARN  [RpcServer.handler=34,port=60020] ipc.RpcServer: RpcServer.handler=34,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-07-01 07:45:12,832 DEBUG [RpcServer.handler=9,port=60020] regionserver.HRegion: rollbackMemstore rolled back 6540 keyvalues from start:0 to end:654
2014-07-01 07:45:12,854 WARN  [RpcServer.handler=9,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":97552,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.58:33505","starttimems":1404225815299,"queuetimems":21350,"class":"HRegionServer","responsesize":901095,"method":"Multi"}
2014-07-01 07:45:12,857 WARN  [RpcServer.handler=9,port=60020] ipc.RpcServer: RpcServer.respondercallId: 3962 service: ClientService methodName: Multi size: 2.4m connection: 9.1.143.58:33505: output error
2014-07-01 07:45:12,857 WARN  [RpcServer.handler=9,port=60020] ipc.RpcServer: RpcServer.handler=9,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-07-01 07:45:12,896 DEBUG [MemStoreFlusher.0] util.FSUtils: DFS Client does not support most favored nodes create; using default create
2014-07-01 07:45:14,935 INFO  [MemStoreFlusher.1] regionserver.DefaultStoreFlusher: Flushed, sequenceid=1463, memsize=86.6m, hasBloomFilter=true, into tmp file hdfs://master:54310/hbase/data/default/usertable/89cb00f0096b305c3317b7753d66adf1/.tmp/e0c57336acca4322aa4da80b9784f631
2014-07-01 07:45:14,961 DEBUG [MemStoreFlusher.1] regionserver.HRegionFileSystem: Committing store file hdfs://master:54310/hbase/data/default/usertable/89cb00f0096b305c3317b7753d66adf1/.tmp/e0c57336acca4322aa4da80b9784f631 as hdfs://master:54310/hbase/data/default/usertable/89cb00f0096b305c3317b7753d66adf1/family/e0c57336acca4322aa4da80b9784f631
2014-07-01 07:45:14,980 INFO  [MemStoreFlusher.1] regionserver.HStore: Added hdfs://master:54310/hbase/data/default/usertable/89cb00f0096b305c3317b7753d66adf1/family/e0c57336acca4322aa4da80b9784f631, entries=315420, sequenceid=1463, filesize=22.5m
2014-07-01 07:45:14,980 INFO  [MemStoreFlusher.1] regionserver.HRegion: Finished memstore flush of ~507.6m/532299200, currentsize=157.8m/165446880 for region usertable,user6,1404225423342.89cb00f0096b305c3317b7753d66adf1. in 123964ms, sequenceid=1463, compaction requested=true
2014-07-01 07:45:14,981 INFO  [MemStoreFlusher.1] regionserver.MemStoreFlusher: MemStoreFlusher.1 exiting
2014-07-01 07:45:17,567 INFO  [regionserver60020.leaseChecker] regionserver.Leases: regionserver60020.leaseChecker closing leases
2014-07-01 07:45:17,567 INFO  [regionserver60020.leaseChecker] regionserver.Leases: regionserver60020.leaseChecker closed leases
2014-07-01 07:45:17,571 INFO  [regionserver60020.compactionChecker] regionserver.HRegionServer$CompactionChecker: regionserver60020.compactionChecker exiting
2014-07-01 07:45:17,572 INFO  [regionserver60020.periodicFlusher] regionserver.HRegionServer$PeriodicMemstoreFlusher: regionserver60020.periodicFlusher exiting
2014-07-01 07:45:18,458 INFO  [MemStoreFlusher.0] regionserver.DefaultStoreFlusher: Flushed, sequenceid=1433, memsize=119.5m, hasBloomFilter=true, into tmp file hdfs://master:54310/hbase/data/default/usertable/07064f253c865a7cc0c08893a4f56af9/.tmp/a18a9a0f87424b4997a4a694be09af50
2014-07-01 07:45:18,474 DEBUG [MemStoreFlusher.0] regionserver.HRegionFileSystem: Committing store file hdfs://master:54310/hbase/data/default/usertable/07064f253c865a7cc0c08893a4f56af9/.tmp/a18a9a0f87424b4997a4a694be09af50 as hdfs://master:54310/hbase/data/default/usertable/07064f253c865a7cc0c08893a4f56af9/family/a18a9a0f87424b4997a4a694be09af50
2014-07-01 07:45:18,485 INFO  [MemStoreFlusher.0] regionserver.HStore: Added hdfs://master:54310/hbase/data/default/usertable/07064f253c865a7cc0c08893a4f56af9/family/a18a9a0f87424b4997a4a694be09af50, entries=435190, sequenceid=1433, filesize=31.0m
2014-07-01 07:45:18,485 INFO  [MemStoreFlusher.0] regionserver.HRegion: Finished memstore flush of ~622.8m/653047120, currentsize=242.0m/253715840 for region usertable,user2,1404225423342.07064f253c865a7cc0c08893a4f56af9. in 153949ms, sequenceid=1433, compaction requested=true
2014-07-01 07:45:18,486 INFO  [MemStoreFlusher.0] regionserver.MemStoreFlusher: MemStoreFlusher.0 exiting
2014-07-01 07:45:18,487 INFO  [regionserver60020] snapshot.RegionServerSnapshotManager: Stopping RegionServerSnapshotManager abruptly.
2014-07-01 07:45:18,487 INFO  [regionserver60020.nonceCleaner] regionserver.ServerNonceManager$1: regionserver60020.nonceCleaner exiting
2014-07-01 07:45:18,490 DEBUG [RS_CLOSE_REGION-slave1:60020-0] handler.CloseRegionHandler: Processing close of usertable,,1404225423341.387a171caad6315f0b61271a30313d1a.
2014-07-01 07:45:18,490 DEBUG [RS_CLOSE_REGION-slave1:60020-1] handler.CloseRegionHandler: Processing close of usertable,user5,1404225423342.c7604442b843793f8451582c14f6bf59.
2014-07-01 07:45:18,491 DEBUG [RS_CLOSE_REGION-slave1:60020-0] regionserver.HRegion: Closing usertable,,1404225423341.387a171caad6315f0b61271a30313d1a.: disabling compactions & flushes
2014-07-01 07:45:18,491 DEBUG [RS_CLOSE_REGION-slave1:60020-1] regionserver.HRegion: Closing usertable,user5,1404225423342.c7604442b843793f8451582c14f6bf59.: disabling compactions & flushes
2014-07-01 07:45:18,491 INFO  [regionserver60020] regionserver.HRegionServer: aborting server slave1,60020,1404225098235
2014-07-01 07:45:18,491 DEBUG [RS_CLOSE_REGION-slave1:60020-2] handler.CloseRegionHandler: Processing close of usertable,user2,1404225423342.07064f253c865a7cc0c08893a4f56af9.
2014-07-01 07:45:18,491 DEBUG [RS_CLOSE_REGION-slave1:60020-1] regionserver.HRegion: Updates disabled for region usertable,user5,1404225423342.c7604442b843793f8451582c14f6bf59.
2014-07-01 07:45:18,491 DEBUG [RS_CLOSE_REGION-slave1:60020-2] regionserver.HRegion: Closing usertable,user2,1404225423342.07064f253c865a7cc0c08893a4f56af9.: disabling compactions & flushes
2014-07-01 07:45:18,491 DEBUG [RS_CLOSE_REGION-slave1:60020-0] regionserver.HRegion: Updates disabled for region usertable,,1404225423341.387a171caad6315f0b61271a30313d1a.
2014-07-01 07:45:18,491 DEBUG [RS_CLOSE_REGION-slave1:60020-2] regionserver.HRegion: Updates disabled for region usertable,user2,1404225423342.07064f253c865a7cc0c08893a4f56af9.
2014-07-01 07:45:18,491 DEBUG [regionserver60020] catalog.CatalogTracker: Stopping catalog tracker org.apache.hadoop.hbase.catalog.CatalogTracker@1a6bfc15
2014-07-01 07:45:18,495 INFO  [regionserver60020] regionserver.HRegionServer: Waiting on 4 regions to close
2014-07-01 07:45:18,496 DEBUG [regionserver60020] regionserver.HRegionServer: {387a171caad6315f0b61271a30313d1a=usertable,,1404225423341.387a171caad6315f0b61271a30313d1a., c7604442b843793f8451582c14f6bf59=usertable,user5,1404225423342.c7604442b843793f8451582c14f6bf59., 07064f253c865a7cc0c08893a4f56af9=usertable,user2,1404225423342.07064f253c865a7cc0c08893a4f56af9., 89cb00f0096b305c3317b7753d66adf1=usertable,user6,1404225423342.89cb00f0096b305c3317b7753d66adf1.}
2014-07-01 07:45:18,496 INFO  [StoreCloserThread-usertable,user5,1404225423342.c7604442b843793f8451582c14f6bf59.-1] regionserver.HStore: Closed family
2014-07-01 07:45:18,497 ERROR [RS_CLOSE_REGION-slave1:60020-1] regionserver.HRegion: Memstore size is 693425200
2014-07-01 07:45:18,497 INFO  [StoreCloserThread-usertable,user2,1404225423342.07064f253c865a7cc0c08893a4f56af9.-1] regionserver.HStore: Closed family
2014-07-01 07:45:18,498 ERROR [RS_CLOSE_REGION-slave1:60020-2] regionserver.HRegion: Memstore size is 253715840
2014-07-01 07:45:18,498 INFO  [StoreCloserThread-usertable,,1404225423341.387a171caad6315f0b61271a30313d1a.-1] regionserver.HStore: Closed family
2014-07-01 07:45:18,499 ERROR [RS_CLOSE_REGION-slave1:60020-0] regionserver.HRegion: Memstore size is 211210080
2014-07-01 07:45:18,509 INFO  [RS_CLOSE_REGION-slave1:60020-1] regionserver.HRegion: Closed usertable,user5,1404225423342.c7604442b843793f8451582c14f6bf59.
2014-07-01 07:45:18,509 INFO  [RS_CLOSE_REGION-slave1:60020-2] regionserver.HRegion: Closed usertable,user2,1404225423342.07064f253c865a7cc0c08893a4f56af9.
2014-07-01 07:45:18,509 DEBUG [RS_CLOSE_REGION-slave1:60020-1] handler.CloseRegionHandler: Closed usertable,user5,1404225423342.c7604442b843793f8451582c14f6bf59.
2014-07-01 07:45:18,509 INFO  [RS_CLOSE_REGION-slave1:60020-0] regionserver.HRegion: Closed usertable,,1404225423341.387a171caad6315f0b61271a30313d1a.
2014-07-01 07:45:18,509 DEBUG [RS_CLOSE_REGION-slave1:60020-1] handler.CloseRegionHandler: Processing close of usertable,user6,1404225423342.89cb00f0096b305c3317b7753d66adf1.
2014-07-01 07:45:18,509 DEBUG [RS_CLOSE_REGION-slave1:60020-2] handler.CloseRegionHandler: Closed usertable,user2,1404225423342.07064f253c865a7cc0c08893a4f56af9.
2014-07-01 07:45:18,509 DEBUG [RS_CLOSE_REGION-slave1:60020-1] regionserver.HRegion: Closing usertable,user6,1404225423342.89cb00f0096b305c3317b7753d66adf1.: disabling compactions & flushes
2014-07-01 07:45:18,509 DEBUG [RS_CLOSE_REGION-slave1:60020-0] handler.CloseRegionHandler: Closed usertable,,1404225423341.387a171caad6315f0b61271a30313d1a.
2014-07-01 07:45:18,509 DEBUG [RS_CLOSE_REGION-slave1:60020-1] regionserver.HRegion: Updates disabled for region usertable,user6,1404225423342.89cb00f0096b305c3317b7753d66adf1.
2014-07-01 07:45:18,511 INFO  [StoreCloserThread-usertable,user6,1404225423342.89cb00f0096b305c3317b7753d66adf1.-1] regionserver.HStore: Closed family
2014-07-01 07:45:18,512 ERROR [RS_CLOSE_REGION-slave1:60020-1] regionserver.HRegion: Memstore size is 165446880
2014-07-01 07:45:18,512 INFO  [RS_CLOSE_REGION-slave1:60020-1] regionserver.HRegion: Closed usertable,user6,1404225423342.89cb00f0096b305c3317b7753d66adf1.
2014-07-01 07:45:18,512 DEBUG [RS_CLOSE_REGION-slave1:60020-1] handler.CloseRegionHandler: Closed usertable,user6,1404225423342.89cb00f0096b305c3317b7753d66adf1.
2014-07-01 07:45:18,696 INFO  [regionserver60020] regionserver.HRegionServer: stopping server slave1,60020,1404225098235; all regions closed.
2014-07-01 07:45:18,697 DEBUG [regionserver60020-WAL.AsyncNotifier] wal.FSHLog: regionserver60020-WAL.AsyncNotifier interrupted while waiting for  notification from AsyncSyncer thread
2014-07-01 07:45:18,697 INFO  [regionserver60020-WAL.AsyncNotifier] wal.FSHLog: regionserver60020-WAL.AsyncNotifier exiting
2014-07-01 07:45:18,697 DEBUG [regionserver60020-WAL.AsyncSyncer0] wal.FSHLog: regionserver60020-WAL.AsyncSyncer0 interrupted while waiting for notification from AsyncWriter thread
2014-07-01 07:45:18,698 INFO  [regionserver60020-WAL.AsyncSyncer0] wal.FSHLog: regionserver60020-WAL.AsyncSyncer0 exiting
2014-07-01 07:45:18,698 DEBUG [regionserver60020-WAL.AsyncSyncer1] wal.FSHLog: regionserver60020-WAL.AsyncSyncer1 interrupted while waiting for notification from AsyncWriter thread
2014-07-01 07:45:18,698 INFO  [regionserver60020-WAL.AsyncSyncer1] wal.FSHLog: regionserver60020-WAL.AsyncSyncer1 exiting
2014-07-01 07:45:18,716 DEBUG [regionserver60020-WAL.AsyncSyncer2] wal.FSHLog: regionserver60020-WAL.AsyncSyncer2 interrupted while waiting for notification from AsyncWriter thread
2014-07-01 07:45:18,716 INFO  [regionserver60020-WAL.AsyncSyncer2] wal.FSHLog: regionserver60020-WAL.AsyncSyncer2 exiting
2014-07-01 07:45:18,717 DEBUG [regionserver60020-WAL.AsyncSyncer3] wal.FSHLog: regionserver60020-WAL.AsyncSyncer3 interrupted while waiting for notification from AsyncWriter thread
2014-07-01 07:45:18,717 INFO  [regionserver60020-WAL.AsyncSyncer3] wal.FSHLog: regionserver60020-WAL.AsyncSyncer3 exiting
2014-07-01 07:45:18,734 DEBUG [regionserver60020-WAL.AsyncSyncer4] wal.FSHLog: regionserver60020-WAL.AsyncSyncer4 interrupted while waiting for notification from AsyncWriter thread
2014-07-01 07:45:18,734 INFO  [regionserver60020-WAL.AsyncSyncer4] wal.FSHLog: regionserver60020-WAL.AsyncSyncer4 exiting
2014-07-01 07:45:18,735 DEBUG [regionserver60020-WAL.AsyncWriter] wal.FSHLog: regionserver60020-WAL.AsyncWriter interrupted while waiting for newer writes added to local buffer
2014-07-01 07:45:18,735 INFO  [regionserver60020-WAL.AsyncWriter] wal.FSHLog: regionserver60020-WAL.AsyncWriter exiting
2014-07-01 07:45:18,735 DEBUG [regionserver60020] wal.FSHLog: Closing WAL writer in hdfs://master:54310/hbase/WALs/slave1,60020,1404225098235
2014-07-01 07:45:18,736 ERROR [regionserver60020] regionserver.HRegionServer: Close and delete failed
java.io.IOException: Error Recovery for block blk_-5973222587916760426_29091 failed  because recovery from primary datanode 9.1.143.59:50010 failed 6 times.  Pipeline was 9.1.143.59:50010. Aborting...
	at org.apache.hadoop.hdfs.DFSClient$DFSOutputStream.processDatanodeError(DFSClient.java:3355)
	at org.apache.hadoop.hdfs.DFSClient$DFSOutputStream.access$2200(DFSClient.java:2783)
	at org.apache.hadoop.hdfs.DFSClient$DFSOutputStream$DataStreamer.run(DFSClient.java:2987)
2014-07-01 07:45:18,737 INFO  [regionserver60020] regionserver.Leases: regionserver60020 closing leases
2014-07-01 07:45:18,737 INFO  [regionserver60020] regionserver.Leases: regionserver60020 closed leases
2014-07-01 07:45:18,738 INFO  [regionserver60020] regionserver.CompactSplitThread: Waiting for Split Thread to finish...
2014-07-01 07:45:18,738 INFO  [regionserver60020] regionserver.CompactSplitThread: Waiting for Merge Thread to finish...
2014-07-01 07:45:18,738 INFO  [regionserver60020] regionserver.CompactSplitThread: Waiting for Large Compaction Thread to finish...
2014-07-01 07:45:18,738 INFO  [regionserver60020] regionserver.CompactSplitThread: Waiting for Small Compaction Thread to finish...
2014-07-01 07:45:18,739 WARN  [regionserver60020] zookeeper.RecoverableZooKeeper: Possibly transient ZooKeeper, quorum=sceplus-vm49.almaden.ibm.com:2181,sceplus-vm48.almaden.ibm.com:2181, exception=org.apache.zookeeper.KeeperException$SessionExpiredException: KeeperErrorCode = Session expired for /hbase/replication/rs/slave1,60020,1404225098235
2014-07-01 07:45:18,740 INFO  [regionserver60020] util.RetryCounter: Sleeping 1000ms before retry #0...
2014-07-01 07:45:19,740 WARN  [regionserver60020] zookeeper.RecoverableZooKeeper: Possibly transient ZooKeeper, quorum=sceplus-vm49.almaden.ibm.com:2181,sceplus-vm48.almaden.ibm.com:2181, exception=org.apache.zookeeper.KeeperException$SessionExpiredException: KeeperErrorCode = Session expired for /hbase/replication/rs/slave1,60020,1404225098235
2014-07-01 07:45:19,741 INFO  [regionserver60020] util.RetryCounter: Sleeping 2000ms before retry #1...
2014-07-01 07:45:21,741 WARN  [regionserver60020] zookeeper.RecoverableZooKeeper: Possibly transient ZooKeeper, quorum=sceplus-vm49.almaden.ibm.com:2181,sceplus-vm48.almaden.ibm.com:2181, exception=org.apache.zookeeper.KeeperException$SessionExpiredException: KeeperErrorCode = Session expired for /hbase/replication/rs/slave1,60020,1404225098235
2014-07-01 07:45:21,741 INFO  [regionserver60020] util.RetryCounter: Sleeping 4000ms before retry #2...
2014-07-01 07:45:25,742 WARN  [regionserver60020] zookeeper.RecoverableZooKeeper: Possibly transient ZooKeeper, quorum=sceplus-vm49.almaden.ibm.com:2181,sceplus-vm48.almaden.ibm.com:2181, exception=org.apache.zookeeper.KeeperException$SessionExpiredException: KeeperErrorCode = Session expired for /hbase/replication/rs/slave1,60020,1404225098235
2014-07-01 07:45:25,742 INFO  [regionserver60020] util.RetryCounter: Sleeping 8000ms before retry #3...
2014-07-01 07:45:33,743 WARN  [regionserver60020] zookeeper.RecoverableZooKeeper: Possibly transient ZooKeeper, quorum=sceplus-vm49.almaden.ibm.com:2181,sceplus-vm48.almaden.ibm.com:2181, exception=org.apache.zookeeper.KeeperException$SessionExpiredException: KeeperErrorCode = Session expired for /hbase/replication/rs/slave1,60020,1404225098235
2014-07-01 07:45:33,743 ERROR [regionserver60020] zookeeper.RecoverableZooKeeper: ZooKeeper getChildren failed after 4 attempts
2014-07-01 07:45:33,762 WARN  [regionserver60020] zookeeper.RecoverableZooKeeper: Possibly transient ZooKeeper, quorum=sceplus-vm49.almaden.ibm.com:2181,sceplus-vm48.almaden.ibm.com:2181, exception=org.apache.zookeeper.KeeperException$SessionExpiredException: KeeperErrorCode = Session expired for /hbase/rs/slave1,60020,1404225098235
2014-07-01 07:45:33,762 INFO  [regionserver60020] util.RetryCounter: Sleeping 1000ms before retry #0...
2014-07-01 07:45:34,762 WARN  [regionserver60020] zookeeper.RecoverableZooKeeper: Possibly transient ZooKeeper, quorum=sceplus-vm49.almaden.ibm.com:2181,sceplus-vm48.almaden.ibm.com:2181, exception=org.apache.zookeeper.KeeperException$SessionExpiredException: KeeperErrorCode = Session expired for /hbase/rs/slave1,60020,1404225098235
2014-07-01 07:45:34,763 INFO  [regionserver60020] util.RetryCounter: Sleeping 2000ms before retry #1...
2014-07-01 07:45:36,763 WARN  [regionserver60020] zookeeper.RecoverableZooKeeper: Possibly transient ZooKeeper, quorum=sceplus-vm49.almaden.ibm.com:2181,sceplus-vm48.almaden.ibm.com:2181, exception=org.apache.zookeeper.KeeperException$SessionExpiredException: KeeperErrorCode = Session expired for /hbase/rs/slave1,60020,1404225098235
2014-07-01 07:45:36,763 INFO  [regionserver60020] util.RetryCounter: Sleeping 4000ms before retry #2...
2014-07-01 07:45:40,764 WARN  [regionserver60020] zookeeper.RecoverableZooKeeper: Possibly transient ZooKeeper, quorum=sceplus-vm49.almaden.ibm.com:2181,sceplus-vm48.almaden.ibm.com:2181, exception=org.apache.zookeeper.KeeperException$SessionExpiredException: KeeperErrorCode = Session expired for /hbase/rs/slave1,60020,1404225098235
2014-07-01 07:45:40,764 INFO  [regionserver60020] util.RetryCounter: Sleeping 8000ms before retry #3...
2014-07-01 07:45:48,764 WARN  [regionserver60020] zookeeper.RecoverableZooKeeper: Possibly transient ZooKeeper, quorum=sceplus-vm49.almaden.ibm.com:2181,sceplus-vm48.almaden.ibm.com:2181, exception=org.apache.zookeeper.KeeperException$SessionExpiredException: KeeperErrorCode = Session expired for /hbase/rs/slave1,60020,1404225098235
2014-07-01 07:45:48,765 ERROR [regionserver60020] zookeeper.RecoverableZooKeeper: ZooKeeper delete failed after 4 attempts
2014-07-01 07:45:48,765 WARN  [regionserver60020] regionserver.HRegionServer: Failed deleting my ephemeral node
org.apache.zookeeper.KeeperException$SessionExpiredException: KeeperErrorCode = Session expired for /hbase/rs/slave1,60020,1404225098235
	at org.apache.zookeeper.KeeperException.create(KeeperException.java:127)
	at org.apache.zookeeper.KeeperException.create(KeeperException.java:51)
	at org.apache.zookeeper.ZooKeeper.delete(ZooKeeper.java:873)
	at org.apache.hadoop.hbase.zookeeper.RecoverableZooKeeper.delete(RecoverableZooKeeper.java:156)
	at org.apache.hadoop.hbase.zookeeper.ZKUtil.deleteNode(ZKUtil.java:1273)
	at org.apache.hadoop.hbase.zookeeper.ZKUtil.deleteNode(ZKUtil.java:1262)
	at org.apache.hadoop.hbase.regionserver.HRegionServer.deleteMyEphemeralNode(HRegionServer.java:1292)
	at org.apache.hadoop.hbase.regionserver.HRegionServer.run(HRegionServer.java:1008)
	at java.lang.Thread.run(Thread.java:701)
2014-07-01 07:45:48,787 INFO  [regionserver60020] regionserver.HRegionServer: stopping server slave1,60020,1404225098235; zookeeper connection closed.
2014-07-01 07:45:48,787 INFO  [regionserver60020] regionserver.HRegionServer: regionserver60020 exiting
2014-07-01 07:45:48,803 ERROR [main] regionserver.HRegionServerCommandLine: Region server exiting
java.lang.RuntimeException: HRegionServer Aborted
	at org.apache.hadoop.hbase.regionserver.HRegionServerCommandLine.start(HRegionServerCommandLine.java:66)
	at org.apache.hadoop.hbase.regionserver.HRegionServerCommandLine.run(HRegionServerCommandLine.java:85)
	at org.apache.hadoop.util.ToolRunner.run(ToolRunner.java:65)
	at org.apache.hadoop.hbase.util.ServerCommandLine.doMain(ServerCommandLine.java:126)
	at org.apache.hadoop.hbase.regionserver.HRegionServer.main(HRegionServer.java:2410)
2014-07-01 07:45:48,826 INFO  [Shutdownhook:regionserver60020] regionserver.ShutdownHook: Shutdown hook starting; hbase.shutdown.hook=true; fsShutdownHook=Thread[Thread-9,5,main]
2014-07-01 07:45:48,826 INFO  [Shutdownhook:regionserver60020] regionserver.ShutdownHook: Starting fs shutdown hook thread.
2014-07-01 07:45:48,827 ERROR [Thread-9] hdfs.DFSClient: Failed to close file /hbase/data/default/usertable/07064f253c865a7cc0c08893a4f56af9/.tmp/a17a8b64528148559a6414832c7b909b
java.io.IOException: Error Recovery for block blk_4372291247462747067_29087 failed  because recovery from primary datanode 9.1.143.59:50010 failed 6 times.  Pipeline was 9.1.143.59:50010. Aborting...
	at org.apache.hadoop.hdfs.DFSClient$DFSOutputStream.processDatanodeError(DFSClient.java:3355)
	at org.apache.hadoop.hdfs.DFSClient$DFSOutputStream.access$2200(DFSClient.java:2783)
	at org.apache.hadoop.hdfs.DFSClient$DFSOutputStream$DataStreamer.run(DFSClient.java:2987)
2014-07-01 07:45:48,827 ERROR [Thread-9] hdfs.DFSClient: Failed to close file /hbase/WALs/slave1,60020,1404225098235/slave1%2C60020%2C1404225098235.1404225812747
java.io.IOException: Error Recovery for block blk_-5973222587916760426_29091 failed  because recovery from primary datanode 9.1.143.59:50010 failed 6 times.  Pipeline was 9.1.143.59:50010. Aborting...
	at org.apache.hadoop.hdfs.DFSClient$DFSOutputStream.processDatanodeError(DFSClient.java:3355)
	at org.apache.hadoop.hdfs.DFSClient$DFSOutputStream.access$2200(DFSClient.java:2783)
	at org.apache.hadoop.hdfs.DFSClient$DFSOutputStream$DataStreamer.run(DFSClient.java:2987)
2014-07-01 07:45:48,827 INFO  [Shutdownhook:regionserver60020] regionserver.ShutdownHook: Shutdown hook finished.
Tue Jul  1 15:31:06 PDT 2014 Starting regionserver on sceplus-vm49
core file size          (blocks, -c) 0
data seg size           (kbytes, -d) unlimited
scheduling priority             (-e) 0
file size               (blocks, -f) unlimited
pending signals                 (-i) 128203
max locked memory       (kbytes, -l) 64
max memory size         (kbytes, -m) unlimited
open files                      (-n) 32768
pipe size            (512 bytes, -p) 8
POSIX message queues     (bytes, -q) 819200
real-time priority              (-r) 0
stack size              (kbytes, -s) 8192
cpu time               (seconds, -t) unlimited
max user processes              (-u) 128203
virtual memory          (kbytes, -v) unlimited
file locks                      (-x) unlimited
2014-07-01 15:31:07,585 INFO  [main] util.VersionInfo: HBase 0.98.3-hadoop1
2014-07-01 15:31:07,586 INFO  [main] util.VersionInfo: Subversion git://acer/usr/src/Hadoop/hbase -r d5e65a9144e315bb0a964e7730871af32f5018d5
2014-07-01 15:31:07,586 INFO  [main] util.VersionInfo: Compiled by apurtell on Sat May 31 19:34:57 PDT 2014
2014-07-01 15:31:07,812 INFO  [main] util.ServerCommandLine: env:JAVA_HOME=/usr/lib/jvm/java-1.6.0-openjdk-amd64/
2014-07-01 15:31:07,812 INFO  [main] util.ServerCommandLine: env:SHLVL=3
2014-07-01 15:31:07,812 INFO  [main] util.ServerCommandLine: env:HBASE_LOG_DIR=/home/hadoop/hbase/bin/../logs
2014-07-01 15:31:07,812 INFO  [main] util.ServerCommandLine: env:HBASE_HOME=/home/hadoop/hbase/bin/..
2014-07-01 15:31:07,812 INFO  [main] util.ServerCommandLine: env:HBASE_OPTS=-XX:+UseConcMarkSweepGC   -Dhbase.log.dir=/home/hadoop/hbase/bin/../logs -Dhbase.log.file=hbase-hadoop-regionserver-sceplus-vm49.log -Dhbase.home.dir=/home/hadoop/hbase/bin/.. -Dhbase.id.str=hadoop -Dhbase.root.logger=INFO,RFA -Djava.library.path=/home/hadoop/hbase/bin/../lib/native/Linux-amd64-64 -Dhbase.security.logger=INFO,RFAS
2014-07-01 15:31:07,812 INFO  [main] util.ServerCommandLine: env:HBASE_ENV_INIT=true
2014-07-01 15:31:07,813 INFO  [main] util.ServerCommandLine: env:SSH_CLIENT=9.1.143.58 43556 22
2014-07-01 15:31:07,813 INFO  [main] util.ServerCommandLine: env:HBASE_HEAPSIZE=10240
2014-07-01 15:31:07,813 INFO  [main] util.ServerCommandLine: env:MAIL=/var/mail/hadoop
2014-07-01 15:31:07,813 INFO  [main] util.ServerCommandLine: env:HBASE_ZNODE_FILE=/var/hadoop/pids/hbase-hadoop-regionserver.znode
2014-07-01 15:31:07,813 INFO  [main] util.ServerCommandLine: env:PWD=/home/hadoop/hbase
2014-07-01 15:31:07,813 INFO  [main] util.ServerCommandLine: env:LOGNAME=hadoop
2014-07-01 15:31:07,813 INFO  [main] util.ServerCommandLine: env:HBASE_MANAGES_ZK=true
2014-07-01 15:31:07,813 INFO  [main] util.ServerCommandLine: env:HBASE_NICENESS=0
2014-07-01 15:31:07,813 INFO  [main] util.ServerCommandLine: env:HBASE_REST_OPTS=
2014-07-01 15:31:07,813 INFO  [main] util.ServerCommandLine: env:LD_LIBRARY_PATH=/usr/lib/jvm/java-6-openjdk-amd64/jre/lib/amd64/server:/usr/lib/jvm/java-6-openjdk-amd64/jre/lib/amd64:/usr/lib/jvm/java-6-openjdk-amd64/jre/../lib/amd64::/home/hadoop/hbase/bin/../lib/native/Linux-amd64-64
2014-07-01 15:31:07,814 INFO  [main] util.ServerCommandLine: env:MALLOC_ARENA_MAX=4
2014-07-01 15:31:07,814 INFO  [main] util.ServerCommandLine: env:SSH_CONNECTION=9.1.143.58 43556 9.1.143.59 22
2014-07-01 15:31:07,814 INFO  [main] util.ServerCommandLine: env:SHELL=/bin/bash
2014-07-01 15:31:07,814 INFO  [main] util.ServerCommandLine: env:HBASE_PID_DIR=/var/hadoop/pids
2014-07-01 15:31:07,814 INFO  [main] util.ServerCommandLine: env:HBASE_ROOT_LOGGER=INFO,RFA
2014-07-01 15:31:07,817 INFO  [main] util.ServerCommandLine: env:CLASSPATH=/home/hadoop/hbase/conf:/usr/lib/jvm/java-1.6.0-openjdk-amd64//lib/tools.jar:/home/hadoop/hbase/bin/..:/home/hadoop/hbase/bin/../lib/activation-1.1.jar:/home/hadoop/hbase/bin/../lib/asm-3.1.jar:/home/hadoop/hbase/bin/../lib/commons-beanutils-1.7.0.jar:/home/hadoop/hbase/bin/../lib/commons-beanutils-core-1.8.0.jar:/home/hadoop/hbase/bin/../lib/commons-cli-1.2.jar:/home/hadoop/hbase/bin/../lib/commons-codec-1.7.jar:/home/hadoop/hbase/bin/../lib/commons-collections-3.2.1.jar:/home/hadoop/hbase/bin/../lib/commons-configuration-1.6.jar:/home/hadoop/hbase/bin/../lib/commons-digester-1.8.jar:/home/hadoop/hbase/bin/../lib/commons-el-1.0.jar:/home/hadoop/hbase/bin/../lib/commons-httpclient-3.1.jar:/home/hadoop/hbase/bin/../lib/commons-io-2.4.jar:/home/hadoop/hbase/bin/../lib/commons-lang-2.6.jar:/home/hadoop/hbase/bin/../lib/commons-logging-1.1.1.jar:/home/hadoop/hbase/bin/../lib/commons-math-2.1.jar:/home/hadoop/hbase/bin/../lib/commons-net-1.4.1.jar:/home/hadoop/hbase/bin/../lib/findbugs-annotations-1.3.9-1.jar:/home/hadoop/hbase/bin/../lib/guava-12.0.1.jar:/home/hadoop/hbase/bin/../lib/hadoop-core-1.2.1.jar:/home/hadoop/hbase/bin/../lib/hamcrest-core-1.3.jar:/home/hadoop/hbase/bin/../lib/hbase-client-0.98.3-hadoop1.jar:/home/hadoop/hbase/bin/../lib/hbase-common-0.98.3-hadoop1.jar:/home/hadoop/hbase/bin/../lib/hbase-common-0.98.3-hadoop1-tests.jar:/home/hadoop/hbase/bin/../lib/hbase-examples-0.98.3-hadoop1.jar:/home/hadoop/hbase/bin/../lib/hbase-hadoop1-compat-0.98.3-hadoop1.jar:/home/hadoop/hbase/bin/../lib/hbase-hadoop-compat-0.98.3-hadoop1.jar:/home/hadoop/hbase/bin/../lib/hbase-it-0.98.3-hadoop1.jar:/home/hadoop/hbase/bin/../lib/hbase-it-0.98.3-hadoop1-tests.jar:/home/hadoop/hbase/bin/../lib/hbase-prefix-tree-0.98.3-hadoop1.jar:/home/hadoop/hbase/bin/../lib/hbase-protocol-0.98.3-hadoop1.jar:/home/hadoop/hbase/bin/../lib/hbase-server-0.98.3-hadoop1.jar:/home/hadoop/hbase/bin/../lib/hbase-server-0.98.3-hadoop1-tests.jar:/home/hadoop/hbase/bin/../lib/hbase-shell-0.98.3-hadoop1.jar:/home/hadoop/hbase/bin/../lib/hbase-testing-util-0.98.3-hadoop1.jar:/home/hadoop/hbase/bin/../lib/hbase-thrift-0.98.3-hadoop1.jar:/home/hadoop/hbase/bin/../lib/high-scale-lib-1.1.1.jar:/home/hadoop/hbase/bin/../lib/htrace-core-2.04.jar:/home/hadoop/hbase/bin/../lib/httpclient-4.1.3.jar:/home/hadoop/hbase/bin/../lib/httpcore-4.1.3.jar:/home/hadoop/hbase/bin/../lib/jackson-core-asl-1.8.8.jar:/home/hadoop/hbase/bin/../lib/jackson-jaxrs-1.8.8.jar:/home/hadoop/hbase/bin/../lib/jackson-mapper-asl-1.8.8.jar:/home/hadoop/hbase/bin/../lib/jackson-xc-1.8.8.jar:/home/hadoop/hbase/bin/../lib/jamon-runtime-2.3.1.jar:/home/hadoop/hbase/bin/../lib/jasper-compiler-5.5.23.jar:/home/hadoop/hbase/bin/../lib/jasper-runtime-5.5.23.jar:/home/hadoop/hbase/bin/../lib/jaxb-api-2.2.2.jar:/home/hadoop/hbase/bin/../lib/jaxb-impl-2.2.3-1.jar:/home/hadoop/hbase/bin/../lib/jersey-core-1.8.jar:/home/hadoop/hbase/bin/../lib/jersey-json-1.8.jar:/home/hadoop/hbase/bin/../lib/jersey-server-1.8.jar:/home/hadoop/hbase/bin/../lib/jettison-1.3.1.jar:/home/hadoop/hbase/bin/../lib/jetty-6.1.26.jar:/home/hadoop/hbase/bin/../lib/jetty-sslengine-6.1.26.jar:/home/hadoop/hbase/bin/../lib/jetty-util-6.1.26.jar:/home/hadoop/hbase/bin/../lib/jruby-complete-1.6.8.jar:/home/hadoop/hbase/bin/../lib/jsp-2.1-6.1.14.jar:/home/hadoop/hbase/bin/../lib/jsp-api-2.1-6.1.14.jar:/home/hadoop/hbase/bin/../lib/jsr305-1.3.9.jar:/home/hadoop/hbase/bin/../lib/junit-4.11.jar:/home/hadoop/hbase/bin/../lib/libthrift-0.9.0.jar:/home/hadoop/hbase/bin/../lib/log4j-1.2.17.jar:/home/hadoop/hbase/bin/../lib/metrics-core-2.1.2.jar:/home/hadoop/hbase/bin/../lib/netty-3.6.6.Final.jar:/home/hadoop/hbase/bin/../lib/protobuf-java-2.5.0.jar:/home/hadoop/hbase/bin/../lib/servlet-api-2.5-6.1.14.jar:/home/hadoop/hbase/bin/../lib/slf4j-api-1.6.4.jar:/home/hadoop/hbase/bin/../lib/slf4j-log4j12-1.6.4.jar:/home/hadoop/hbase/bin/../lib/xmlenc-0.52.jar:/home/hadoop/hbase/bin/../lib/zookeeper-3.4.6.jar:
2014-07-01 15:31:07,817 INFO  [main] util.ServerCommandLine: env:HBASE_THRIFT_OPTS=
2014-07-01 15:31:07,817 INFO  [main] util.ServerCommandLine: env:PATH=/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/usr/games:/usr/local/games
2014-07-01 15:31:07,817 INFO  [main] util.ServerCommandLine: env:USER=hadoop
2014-07-01 15:31:07,817 INFO  [main] util.ServerCommandLine: env:HBASE_SECURITY_LOGGER=INFO,RFAS
2014-07-01 15:31:07,818 INFO  [main] util.ServerCommandLine: env:HOME=/home/hadoop
2014-07-01 15:31:07,818 INFO  [main] util.ServerCommandLine: env:HBASE_LIBRARY_PATH=/home/hadoop/hbase/lib/native/Linux-amd64-64
2014-07-01 15:31:07,818 INFO  [main] util.ServerCommandLine: env:HBASE_START_FILE=/var/hadoop/pids/hbase-hadoop-regionserver.autorestart
2014-07-01 15:31:07,818 INFO  [main] util.ServerCommandLine: env:XDG_SESSION_ID=49
2014-07-01 15:31:07,818 INFO  [main] util.ServerCommandLine: env:HBASE_LOGFILE=hbase-hadoop-regionserver-sceplus-vm49.log
2014-07-01 15:31:07,818 INFO  [main] util.ServerCommandLine: env:XDG_RUNTIME_DIR=/run/user/1001
2014-07-01 15:31:07,818 INFO  [main] util.ServerCommandLine: env:LANG=en_US.UTF-8
2014-07-01 15:31:07,818 INFO  [main] util.ServerCommandLine: env:HBASE_LOG_PREFIX=hbase-hadoop-regionserver-sceplus-vm49
2014-07-01 15:31:07,818 INFO  [main] util.ServerCommandLine: env:HBASE_IDENT_STRING=hadoop
2014-07-01 15:31:07,821 INFO  [main] util.ServerCommandLine: vmName=OpenJDK 64-Bit Server VM, vmVendor=Sun Microsystems Inc., vmVersion=23.25-b01
2014-07-01 15:31:07,829 INFO  [main] util.ServerCommandLine: vmInputArguments=[-Dproc_regionserver, -XX:OnOutOfMemoryError=kill -9 %p, -Xmx10240m, -XX:+UseConcMarkSweepGC, -Dhbase.log.dir=/home/hadoop/hbase/bin/../logs, -Dhbase.log.file=hbase-hadoop-regionserver-sceplus-vm49.log, -Dhbase.home.dir=/home/hadoop/hbase/bin/.., -Dhbase.id.str=hadoop, -Dhbase.root.logger=INFO,RFA, -Djava.library.path=/home/hadoop/hbase/bin/../lib/native/Linux-amd64-64, -Dhbase.security.logger=INFO,RFAS]
2014-07-01 15:31:08,068 DEBUG [main] regionserver.HRegionServer: regionserver/sceplus-vm49.almaden.ibm.com/9.1.143.59:60020 HConnection server-to-server retries=350
2014-07-01 15:31:08,609 INFO  [main] ipc.RpcServer: regionserver/sceplus-vm49.almaden.ibm.com/9.1.143.59:60020: started 10 reader(s).
2014-07-01 15:31:08,717 INFO  [main] impl.MetricsConfig: loaded properties from hadoop-metrics2-hbase.properties
2014-07-01 15:31:08,733 INFO  [main] impl.MetricsSinkAdapter: Sink file-all started
2014-07-01 15:31:08,816 INFO  [main] impl.MetricsSourceAdapter: MBean for source MetricsSystem,sub=Stats registered.
2014-07-01 15:31:08,817 INFO  [main] impl.MetricsSystemImpl: Scheduled snapshot period at 10 second(s).
2014-07-01 15:31:08,817 INFO  [main] impl.MetricsSystemImpl: HBase metrics system started
2014-07-01 15:31:08,822 INFO  [main] impl.MetricsSourceAdapter: MBean for source jvm registered.
2014-07-01 15:31:08,827 INFO  [main] impl.MetricsSourceAdapter: MBean for source IPC,sub=IPC registered.
2014-07-01 15:31:08,924 INFO  [main] impl.MetricsSourceAdapter: MBean for source ugi registered.
2014-07-01 15:31:08,924 WARN  [main] impl.MetricsSystemImpl: Source name ugi already exists!
2014-07-01 15:31:08,929 DEBUG [main] util.DirectMemoryUtils: Failed to retrieve nio.BufferPool direct MemoryUsed attribute.
javax.management.InstanceNotFoundException: java.nio:type=BufferPool,name=direct
	at com.sun.jmx.interceptor.DefaultMBeanServerInterceptor.getMBean(DefaultMBeanServerInterceptor.java:1117)
	at com.sun.jmx.interceptor.DefaultMBeanServerInterceptor.getAttribute(DefaultMBeanServerInterceptor.java:678)
	at com.sun.jmx.mbeanserver.JmxMBeanServer.getAttribute(JmxMBeanServer.java:682)
	at org.apache.hadoop.hbase.util.DirectMemoryUtils.<clinit>(DirectMemoryUtils.java:72)
	at org.apache.hadoop.hbase.io.hfile.CacheConfig.instantiateBlockCache(CacheConfig.java:396)
	at org.apache.hadoop.hbase.io.hfile.CacheConfig.<init>(CacheConfig.java:179)
	at org.apache.hadoop.hbase.regionserver.HRegionServer.<init>(HRegionServer.java:621)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:57)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:534)
	at org.apache.hadoop.hbase.regionserver.HRegionServer.constructRegionServer(HRegionServer.java:2393)
	at org.apache.hadoop.hbase.regionserver.HRegionServerCommandLine.start(HRegionServerCommandLine.java:61)
	at org.apache.hadoop.hbase.regionserver.HRegionServerCommandLine.run(HRegionServerCommandLine.java:85)
	at org.apache.hadoop.util.ToolRunner.run(ToolRunner.java:65)
	at org.apache.hadoop.hbase.util.ServerCommandLine.doMain(ServerCommandLine.java:126)
	at org.apache.hadoop.hbase.regionserver.HRegionServer.main(HRegionServer.java:2410)
2014-07-01 15:31:08,931 INFO  [main] hfile.CacheConfig: Allocating LruBlockCache with maximum size 4.0g
2014-07-01 15:31:09,022 INFO  [main] mortbay.log: Logging to org.slf4j.impl.Log4jLoggerAdapter(org.mortbay.log) via org.mortbay.log.Slf4jLog
2014-07-01 15:31:09,093 INFO  [main] http.HttpServer: Added global filtersafety (class=org.apache.hadoop.http.HttpServer$QuotingInputFilter)
2014-07-01 15:31:09,103 INFO  [main] http.HttpServer: Port returned by webServer.getConnectors()[0].getLocalPort() before open() is -1. Opening the listener on 60030
2014-07-01 15:31:09,104 INFO  [main] http.HttpServer: listener.getLocalPort() returned 60030 webServer.getConnectors()[0].getLocalPort() returned 60030
2014-07-01 15:31:09,104 INFO  [main] http.HttpServer: Jetty bound to port 60030
2014-07-01 15:31:09,104 INFO  [main] mortbay.log: jetty-6.1.26
2014-07-01 15:31:09,439 INFO  [main] mortbay.log: Started SelectChannelConnector@0.0.0.0:60030
2014-07-01 15:31:09,484 INFO  [regionserver60020] zookeeper.ZooKeeper: Client environment:zookeeper.version=3.4.6-1569965, built on 02/20/2014 09:09 GMT
2014-07-01 15:31:09,484 INFO  [regionserver60020] zookeeper.ZooKeeper: Client environment:host.name=sceplus-vm49.almaden.ibm.com
2014-07-01 15:31:09,484 INFO  [regionserver60020] zookeeper.ZooKeeper: Client environment:java.version=1.6.0_31
2014-07-01 15:31:09,484 INFO  [regionserver60020] zookeeper.ZooKeeper: Client environment:java.vendor=Sun Microsystems Inc.
2014-07-01 15:31:09,485 INFO  [regionserver60020] zookeeper.ZooKeeper: Client environment:java.home=/usr/lib/jvm/java-6-openjdk-amd64/jre
2014-07-01 15:31:09,485 INFO  [regionserver60020] zookeeper.ZooKeeper: Client environment:java.class.path=/home/hadoop/hbase/conf:/usr/lib/jvm/java-1.6.0-openjdk-amd64//lib/tools.jar:/home/hadoop/hbase/bin/..:/home/hadoop/hbase/bin/../lib/activation-1.1.jar:/home/hadoop/hbase/bin/../lib/asm-3.1.jar:/home/hadoop/hbase/bin/../lib/commons-beanutils-1.7.0.jar:/home/hadoop/hbase/bin/../lib/commons-beanutils-core-1.8.0.jar:/home/hadoop/hbase/bin/../lib/commons-cli-1.2.jar:/home/hadoop/hbase/bin/../lib/commons-codec-1.7.jar:/home/hadoop/hbase/bin/../lib/commons-collections-3.2.1.jar:/home/hadoop/hbase/bin/../lib/commons-configuration-1.6.jar:/home/hadoop/hbase/bin/../lib/commons-digester-1.8.jar:/home/hadoop/hbase/bin/../lib/commons-el-1.0.jar:/home/hadoop/hbase/bin/../lib/commons-httpclient-3.1.jar:/home/hadoop/hbase/bin/../lib/commons-io-2.4.jar:/home/hadoop/hbase/bin/../lib/commons-lang-2.6.jar:/home/hadoop/hbase/bin/../lib/commons-logging-1.1.1.jar:/home/hadoop/hbase/bin/../lib/commons-math-2.1.jar:/home/hadoop/hbase/bin/../lib/commons-net-1.4.1.jar:/home/hadoop/hbase/bin/../lib/findbugs-annotations-1.3.9-1.jar:/home/hadoop/hbase/bin/../lib/guava-12.0.1.jar:/home/hadoop/hbase/bin/../lib/hadoop-core-1.2.1.jar:/home/hadoop/hbase/bin/../lib/hamcrest-core-1.3.jar:/home/hadoop/hbase/bin/../lib/hbase-client-0.98.3-hadoop1.jar:/home/hadoop/hbase/bin/../lib/hbase-common-0.98.3-hadoop1.jar:/home/hadoop/hbase/bin/../lib/hbase-common-0.98.3-hadoop1-tests.jar:/home/hadoop/hbase/bin/../lib/hbase-examples-0.98.3-hadoop1.jar:/home/hadoop/hbase/bin/../lib/hbase-hadoop1-compat-0.98.3-hadoop1.jar:/home/hadoop/hbase/bin/../lib/hbase-hadoop-compat-0.98.3-hadoop1.jar:/home/hadoop/hbase/bin/../lib/hbase-it-0.98.3-hadoop1.jar:/home/hadoop/hbase/bin/../lib/hbase-it-0.98.3-hadoop1-tests.jar:/home/hadoop/hbase/bin/../lib/hbase-prefix-tree-0.98.3-hadoop1.jar:/home/hadoop/hbase/bin/../lib/hbase-protocol-0.98.3-hadoop1.jar:/home/hadoop/hbase/bin/../lib/hbase-server-0.98.3-hadoop1.jar:/home/hadoop/hbase/bin/../lib/hbase-server-0.98.3-hadoop1-tests.jar:/home/hadoop/hbase/bin/../lib/hbase-shell-0.98.3-hadoop1.jar:/home/hadoop/hbase/bin/../lib/hbase-testing-util-0.98.3-hadoop1.jar:/home/hadoop/hbase/bin/../lib/hbase-thrift-0.98.3-hadoop1.jar:/home/hadoop/hbase/bin/../lib/high-scale-lib-1.1.1.jar:/home/hadoop/hbase/bin/../lib/htrace-core-2.04.jar:/home/hadoop/hbase/bin/../lib/httpclient-4.1.3.jar:/home/hadoop/hbase/bin/../lib/httpcore-4.1.3.jar:/home/hadoop/hbase/bin/../lib/jackson-core-asl-1.8.8.jar:/home/hadoop/hbase/bin/../lib/jackson-jaxrs-1.8.8.jar:/home/hadoop/hbase/bin/../lib/jackson-mapper-asl-1.8.8.jar:/home/hadoop/hbase/bin/../lib/jackson-xc-1.8.8.jar:/home/hadoop/hbase/bin/../lib/jamon-runtime-2.3.1.jar:/home/hadoop/hbase/bin/../lib/jasper-compiler-5.5.23.jar:/home/hadoop/hbase/bin/../lib/jasper-runtime-5.5.23.jar:/home/hadoop/hbase/bin/../lib/jaxb-api-2.2.2.jar:/home/hadoop/hbase/bin/../lib/jaxb-impl-2.2.3-1.jar:/home/hadoop/hbase/bin/../lib/jersey-core-1.8.jar:/home/hadoop/hbase/bin/../lib/jersey-json-1.8.jar:/home/hadoop/hbase/bin/../lib/jersey-server-1.8.jar:/home/hadoop/hbase/bin/../lib/jettison-1.3.1.jar:/home/hadoop/hbase/bin/../lib/jetty-6.1.26.jar:/home/hadoop/hbase/bin/../lib/jetty-sslengine-6.1.26.jar:/home/hadoop/hbase/bin/../lib/jetty-util-6.1.26.jar:/home/hadoop/hbase/bin/../lib/jruby-complete-1.6.8.jar:/home/hadoop/hbase/bin/../lib/jsp-2.1-6.1.14.jar:/home/hadoop/hbase/bin/../lib/jsp-api-2.1-6.1.14.jar:/home/hadoop/hbase/bin/../lib/jsr305-1.3.9.jar:/home/hadoop/hbase/bin/../lib/junit-4.11.jar:/home/hadoop/hbase/bin/../lib/libthrift-0.9.0.jar:/home/hadoop/hbase/bin/../lib/log4j-1.2.17.jar:/home/hadoop/hbase/bin/../lib/metrics-core-2.1.2.jar:/home/hadoop/hbase/bin/../lib/netty-3.6.6.Final.jar:/home/hadoop/hbase/bin/../lib/protobuf-java-2.5.0.jar:/home/hadoop/hbase/bin/../lib/servlet-api-2.5-6.1.14.jar:/home/hadoop/hbase/bin/../lib/slf4j-api-1.6.4.jar:/home/hadoop/hbase/bin/../lib/slf4j-log4j12-1.6.4.jar:/home/hadoop/hbase/bin/../lib/xmlenc-0.52.jar:/home/hadoop/hbase/bin/../lib/zookeeper-3.4.6.jar:
2014-07-01 15:31:09,485 INFO  [regionserver60020] zookeeper.ZooKeeper: Client environment:java.library.path=/home/hadoop/hbase/bin/../lib/native/Linux-amd64-64
2014-07-01 15:31:09,485 INFO  [regionserver60020] zookeeper.ZooKeeper: Client environment:java.io.tmpdir=/tmp
2014-07-01 15:31:09,485 INFO  [regionserver60020] zookeeper.ZooKeeper: Client environment:java.compiler=<NA>
2014-07-01 15:31:09,485 INFO  [regionserver60020] zookeeper.ZooKeeper: Client environment:os.name=Linux
2014-07-01 15:31:09,485 INFO  [regionserver60020] zookeeper.ZooKeeper: Client environment:os.arch=amd64
2014-07-01 15:31:09,485 INFO  [regionserver60020] zookeeper.ZooKeeper: Client environment:os.version=3.13.0-24-generic
2014-07-01 15:31:09,485 INFO  [regionserver60020] zookeeper.ZooKeeper: Client environment:user.name=hadoop
2014-07-01 15:31:09,485 INFO  [regionserver60020] zookeeper.ZooKeeper: Client environment:user.home=/home/hadoop
2014-07-01 15:31:09,485 INFO  [regionserver60020] zookeeper.ZooKeeper: Client environment:user.dir=/home/hadoop/hbase-0.98.3-hadoop1
2014-07-01 15:31:09,487 INFO  [main] regionserver.ShutdownHook: Installed shutdown hook thread: Shutdownhook:regionserver60020
2014-07-01 15:31:09,487 INFO  [regionserver60020] zookeeper.ZooKeeper: Initiating client connection, connectString=sceplus-vm49.almaden.ibm.com:2181,sceplus-vm48.almaden.ibm.com:2181 sessionTimeout=90000 watcher=regionserver:60020, quorum=sceplus-vm49.almaden.ibm.com:2181,sceplus-vm48.almaden.ibm.com:2181, baseZNode=/hbase
2014-07-01 15:31:09,513 INFO  [regionserver60020] zookeeper.RecoverableZooKeeper: Process identifier=regionserver:60020 connecting to ZooKeeper ensemble=sceplus-vm49.almaden.ibm.com:2181,sceplus-vm48.almaden.ibm.com:2181
2014-07-01 15:31:09,526 INFO  [regionserver60020-SendThread(sceplus-vm49.almaden.ibm.com:2181)] zookeeper.ClientCnxn: Opening socket connection to server sceplus-vm49.almaden.ibm.com/9.1.143.59:2181. Will not attempt to authenticate using SASL (unknown error)
2014-07-01 15:31:09,533 INFO  [regionserver60020-SendThread(sceplus-vm49.almaden.ibm.com:2181)] zookeeper.ClientCnxn: Socket connection established to sceplus-vm49.almaden.ibm.com/9.1.143.59:2181, initiating session
2014-07-01 15:31:09,541 INFO  [regionserver60020-SendThread(sceplus-vm49.almaden.ibm.com:2181)] zookeeper.ClientCnxn: Session establishment complete on server sceplus-vm49.almaden.ibm.com/9.1.143.59:2181, sessionid = 0x146f40d8cbe0002, negotiated timeout = 90000
2014-07-01 15:31:09,594 INFO  [regionserver60020] zookeeper.ZooKeeper: Initiating client connection, connectString=sceplus-vm49.almaden.ibm.com:2181,sceplus-vm48.almaden.ibm.com:2181 sessionTimeout=90000 watcher=hconnection-0x313e868, quorum=sceplus-vm49.almaden.ibm.com:2181,sceplus-vm48.almaden.ibm.com:2181, baseZNode=/hbase
2014-07-01 15:31:09,595 INFO  [regionserver60020] zookeeper.RecoverableZooKeeper: Process identifier=hconnection-0x313e868 connecting to ZooKeeper ensemble=sceplus-vm49.almaden.ibm.com:2181,sceplus-vm48.almaden.ibm.com:2181
2014-07-01 15:31:09,595 INFO  [regionserver60020-SendThread(master:2181)] zookeeper.ClientCnxn: Opening socket connection to server master/9.1.143.58:2181. Will not attempt to authenticate using SASL (unknown error)
2014-07-01 15:31:09,595 INFO  [regionserver60020-SendThread(master:2181)] zookeeper.ClientCnxn: Socket connection established to master/9.1.143.58:2181, initiating session
2014-07-01 15:31:09,600 INFO  [regionserver60020-SendThread(master:2181)] zookeeper.ClientCnxn: Session establishment complete on server master/9.1.143.58:2181, sessionid = 0x46f40d941a0003, negotiated timeout = 90000
2014-07-01 15:31:09,850 DEBUG [regionserver60020] catalog.CatalogTracker: Starting catalog tracker org.apache.hadoop.hbase.catalog.CatalogTracker@5589aea3
2014-07-01 15:31:09,853 INFO  [regionserver60020] regionserver.HRegionServer: ClusterId : 3b61b992-e8ee-43f8-b0c6-14cd23a8afbe
2014-07-01 15:31:09,859 INFO  [regionserver60020] procedure.RegionServerProcedureManagerHost: Procedure online-snapshot is initializing
2014-07-01 15:31:09,880 INFO  [regionserver60020] zookeeper.RecoverableZooKeeper: Node /hbase/online-snapshot/acquired already exists and this is not a retry
2014-07-01 15:31:09,908 INFO  [regionserver60020] procedure.RegionServerProcedureManagerHost: Procedure online-snapshot is initialized
2014-07-01 15:31:09,913 INFO  [regionserver60020] regionserver.MemStoreFlusher: globalMemStoreLimit=4.0g, globalMemStoreLimitLowMark=3.8g, maxHeap=9.9g
2014-07-01 15:31:09,916 INFO  [regionserver60020] regionserver.HRegionServer: CompactionChecker runs every 10sec
2014-07-01 15:31:09,933 INFO  [regionserver60020] regionserver.HRegionServer: reportForDuty to master=sceplus-vm48.almaden.ibm.com,60000,1404253869061 with port=60020, startcode=1404253868846
2014-07-01 15:31:10,121 DEBUG [regionserver60020] regionserver.HRegionServer: Config from master: hbase.rootdir=hdfs://master:54310/hbase
2014-07-01 15:31:10,121 DEBUG [regionserver60020] regionserver.HRegionServer: Config from master: fs.default.name=hdfs://master:54310
2014-07-01 15:31:10,121 INFO  [regionserver60020] regionserver.HRegionServer: Master passed us a different hostname to use; was=sceplus-vm49.almaden.ibm.com, but now=slave1
2014-07-01 15:31:10,149 INFO  [regionserver60020] fs.HFileSystem: Added intercepting call to namenode#getBlockLocations so can do block reordering using class class org.apache.hadoop.hbase.fs.HFileSystem$ReorderWALBlocks
2014-07-01 15:31:10,159 DEBUG [regionserver60020] regionserver.HRegionServer: logdir=hdfs://master:54310/hbase/WALs/slave1,60020,1404253868846
2014-07-01 15:31:10,193 DEBUG [regionserver60020] regionserver.Replication: ReplicationStatisticsThread 300
2014-07-01 15:31:10,204 INFO  [regionserver60020] wal.FSHLog: WAL/HLog configuration: blocksize=64 MB, rollsize=60.8 MB, enabled=true
2014-07-01 15:31:10,273 INFO  [regionserver60020] wal.FSHLog: New WAL /hbase/WALs/slave1,60020,1404253868846/slave1%2C60020%2C1404253868846.1404253870211
2014-07-01 15:31:10,283 INFO  [regionserver60020] impl.MetricsSourceAdapter: MBean for source RegionServer,sub=WAL registered.
2014-07-01 15:31:10,288 INFO  [regionserver60020] regionserver.MetricsRegionServerWrapperImpl: Computing regionserver metrics every 5000 milliseconds
2014-07-01 15:31:10,292 INFO  [regionserver60020] impl.MetricsSourceAdapter: MBean for source RegionServer,sub=Server registered.
2014-07-01 15:31:10,296 INFO  [regionserver60020] trace.SpanReceiverHost: SpanReceiver org.cloudera.htrace.impl.LocalFileSpanReceiver was loaded successfully.
2014-07-01 15:31:10,298 DEBUG [regionserver60020] executor.ExecutorService: Starting executor service name=RS_OPEN_REGION-slave1:60020, corePoolSize=3, maxPoolSize=3
2014-07-01 15:31:10,299 DEBUG [regionserver60020] executor.ExecutorService: Starting executor service name=RS_OPEN_META-slave1:60020, corePoolSize=1, maxPoolSize=1
2014-07-01 15:31:10,299 DEBUG [regionserver60020] executor.ExecutorService: Starting executor service name=RS_CLOSE_REGION-slave1:60020, corePoolSize=3, maxPoolSize=3
2014-07-01 15:31:10,299 DEBUG [regionserver60020] executor.ExecutorService: Starting executor service name=RS_CLOSE_META-slave1:60020, corePoolSize=1, maxPoolSize=1
2014-07-01 15:31:10,299 DEBUG [regionserver60020] executor.ExecutorService: Starting executor service name=RS_LOG_REPLAY_OPS-slave1:60020, corePoolSize=2, maxPoolSize=2
2014-07-01 15:31:10,306 INFO  [regionserver60020] regionserver.ReplicationSourceManager: Current list of replicators: [slave1,60020,1404253868846, sceplus-vm48.almaden.ibm.com,60020,1404253870530, sceplus-vm48.almaden.ibm.com,60020,1404225100003] other RSs: [slave1,60020,1404253868846, sceplus-vm48.almaden.ibm.com,60020,1404253870530]
2014-07-01 15:31:10,328 INFO  [regionserver60020] impl.MetricsSourceAdapter: MBean for source RegionServer,sub=Replication registered.
2014-07-01 15:31:10,330 INFO  [regionserver60020] zookeeper.ZooKeeper: Initiating client connection, connectString=sceplus-vm49.almaden.ibm.com:2181,sceplus-vm48.almaden.ibm.com:2181 sessionTimeout=90000 watcher=hconnection-0x134c7b8d, quorum=sceplus-vm49.almaden.ibm.com:2181,sceplus-vm48.almaden.ibm.com:2181, baseZNode=/hbase
2014-07-01 15:31:10,331 INFO  [regionserver60020] zookeeper.RecoverableZooKeeper: Process identifier=hconnection-0x134c7b8d connecting to ZooKeeper ensemble=sceplus-vm49.almaden.ibm.com:2181,sceplus-vm48.almaden.ibm.com:2181
2014-07-01 15:31:10,331 INFO  [regionserver60020-SendThread(sceplus-vm49.almaden.ibm.com:2181)] zookeeper.ClientCnxn: Opening socket connection to server sceplus-vm49.almaden.ibm.com/9.1.143.59:2181. Will not attempt to authenticate using SASL (unknown error)
2014-07-01 15:31:10,332 INFO  [regionserver60020-SendThread(sceplus-vm49.almaden.ibm.com:2181)] zookeeper.ClientCnxn: Socket connection established to sceplus-vm49.almaden.ibm.com/9.1.143.59:2181, initiating session
2014-07-01 15:31:10,334 INFO  [regionserver60020-SendThread(sceplus-vm49.almaden.ibm.com:2181)] zookeeper.ClientCnxn: Session establishment complete on server sceplus-vm49.almaden.ibm.com/9.1.143.59:2181, sessionid = 0x146f40d8cbe0003, negotiated timeout = 90000
2014-07-01 15:31:10,340 INFO  [RpcServer.responder] ipc.RpcServer: RpcServer.responder: starting
2014-07-01 15:31:10,340 INFO  [RpcServer.listener,port=60020] ipc.RpcServer: RpcServer.listener,port=60020: starting
2014-07-01 15:31:10,380 INFO  [regionserver60020] regionserver.HRegionServer: Serving as slave1,60020,1404253868846, RpcServer on sceplus-vm49.almaden.ibm.com/9.1.143.59:60020, sessionid=0x146f40d8cbe0002
2014-07-01 15:31:10,380 INFO  [SplitLogWorker-slave1,60020,1404253868846] regionserver.SplitLogWorker: SplitLogWorker slave1,60020,1404253868846 starting
2014-07-01 15:31:10,381 INFO  [regionserver60020] procedure.RegionServerProcedureManagerHost: Procedure online-snapshot is starting
2014-07-01 15:31:10,381 DEBUG [regionserver60020] snapshot.RegionServerSnapshotManager: Start Snapshot Manager slave1,60020,1404253868846
2014-07-01 15:31:10,381 DEBUG [regionserver60020] procedure.ZKProcedureMemberRpcs: Starting procedure member 'slave1,60020,1404253868846'
2014-07-01 15:31:10,381 DEBUG [regionserver60020] procedure.ZKProcedureMemberRpcs: Checking for aborted procedures on node: '/hbase/online-snapshot/abort'
2014-07-01 15:31:10,382 DEBUG [regionserver60020] procedure.ZKProcedureMemberRpcs: Looking for new procedures under znode:'/hbase/online-snapshot/acquired'
2014-07-01 15:31:10,383 INFO  [regionserver60020] procedure.RegionServerProcedureManagerHost: Procedure online-snapshot is started
2014-07-01 15:31:10,581 DEBUG [regionserver60020-EventThread] regionserver.SplitLogWorker: tasks arrived or departed
2014-07-01 15:31:13,727 INFO  [ReplicationExecutor-0] replication.ReplicationQueuesZKImpl: Moving sceplus-vm48.almaden.ibm.com,60020,1404225100003's hlogs to my queue
2014-07-01 15:31:13,738 INFO  [ReplicationExecutor-0] replication.ReplicationQueuesZKImpl: Won't transfer the queue, another RS took care of it because of: KeeperErrorCode = NoNode for /hbase/replication/rs/sceplus-vm48.almaden.ibm.com,60020,1404225100003/lock
2014-07-01 15:31:14,651 DEBUG [regionserver60020-EventThread] regionserver.SplitLogWorker: tasks arrived or departed
2014-07-01 15:31:14,656 DEBUG [regionserver60020-EventThread] regionserver.SplitLogWorker: tasks arrived or departed
2014-07-01 15:31:14,667 INFO  [SplitLogWorker-slave1,60020,1404253868846] regionserver.SplitLogWorker: worker slave1,60020,1404253868846 acquired task /hbase/splitWAL/WALs%2Fsceplus-vm48.almaden.ibm.com%2C60020%2C1404225100003-splitting%2Fsceplus-vm48.almaden.ibm.com%252C60020%252C1404225100003.1404228290972
2014-07-01 15:31:14,715 INFO  [RS_LOG_REPLAY_OPS-slave1:60020-0] wal.HLogSplitter: Splitting hlog: hdfs://master:54310/hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1404225100003-splitting/sceplus-vm48.almaden.ibm.com%2C60020%2C1404225100003.1404228290972, length=78194409
2014-07-01 15:31:14,715 INFO  [RS_LOG_REPLAY_OPS-slave1:60020-0] wal.HLogSplitter: DistributedLogReplay = false
2014-07-01 15:31:14,721 INFO  [RS_LOG_REPLAY_OPS-slave1:60020-0] util.FSHDFSUtils: Recovering lease on dfs file hdfs://master:54310/hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1404225100003-splitting/sceplus-vm48.almaden.ibm.com%2C60020%2C1404225100003.1404228290972
2014-07-01 15:31:14,723 INFO  [RS_LOG_REPLAY_OPS-slave1:60020-0] util.FSHDFSUtils: recoverLease=true, attempt=0 on file=hdfs://master:54310/hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1404225100003-splitting/sceplus-vm48.almaden.ibm.com%2C60020%2C1404225100003.1404228290972 after 2ms
2014-07-01 15:31:14,755 DEBUG [RS_LOG_REPLAY_OPS-slave1:60020-0-Writer-0] wal.HLogSplitter: Writer thread Thread[RS_LOG_REPLAY_OPS-slave1:60020-0-Writer-0,5,main]: starting
2014-07-01 15:31:14,756 DEBUG [RS_LOG_REPLAY_OPS-slave1:60020-0-Writer-1] wal.HLogSplitter: Writer thread Thread[RS_LOG_REPLAY_OPS-slave1:60020-0-Writer-1,5,main]: starting
2014-07-01 15:31:14,757 DEBUG [RS_LOG_REPLAY_OPS-slave1:60020-0-Writer-2] wal.HLogSplitter: Writer thread Thread[RS_LOG_REPLAY_OPS-slave1:60020-0-Writer-2,5,main]: starting
2014-07-01 15:31:14,969 DEBUG [RS_LOG_REPLAY_OPS-slave1:60020-0-Writer-2] wal.HLogSplitter: Creating writer path=hdfs://master:54310/hbase/data/default/usertable/9cef9e7d34f96f1d9b111be50040a208/recovered.edits/0000000000000000050.temp region=9cef9e7d34f96f1d9b111be50040a208
2014-07-01 15:31:14,993 DEBUG [RS_LOG_REPLAY_OPS-slave1:60020-0-Writer-0] wal.HLogSplitter: Creating writer path=hdfs://master:54310/hbase/data/default/usertable/93253e37c284f3174309fede3f32339f/recovered.edits/0000000000000000047.temp region=93253e37c284f3174309fede3f32339f
2014-07-01 15:31:15,027 DEBUG [RS_LOG_REPLAY_OPS-slave1:60020-0-Writer-1] wal.HLogSplitter: Creating writer path=hdfs://master:54310/hbase/data/default/usertable/c72fc9d54f680bcd54e9fa888600346b/recovered.edits/0000000000000000048.temp region=c72fc9d54f680bcd54e9fa888600346b
2014-07-01 15:31:15,048 DEBUG [RS_LOG_REPLAY_OPS-slave1:60020-0-Writer-2] wal.HLogSplitter: Creating writer path=hdfs://master:54310/hbase/data/default/usertable/1dc0aa61f0e3f43a53375118eb48213f/recovered.edits/0000000000000000044.temp region=1dc0aa61f0e3f43a53375118eb48213f
2014-07-01 15:31:15,193 DEBUG [RS_LOG_REPLAY_OPS-slave1:60020-0-Writer-1] wal.HLogSplitter: Creating writer path=hdfs://master:54310/hbase/data/default/usertable/122e92eabfab38cb15a9965b21db25c5/recovered.edits/0000000000000000044.temp region=122e92eabfab38cb15a9965b21db25c5
2014-07-01 15:31:15,311 DEBUG [RS_LOG_REPLAY_OPS-slave1:60020-0-Writer-1] wal.HLogSplitter: Creating writer path=hdfs://master:54310/hbase/data/default/usertable/52290d336af5a689eef6040bafbe6b15/recovered.edits/0000000000000000044.temp region=52290d336af5a689eef6040bafbe6b15
2014-07-01 15:31:15,374 INFO  [SplitLogWorker-slave1,60020,1404253868846] regionserver.SplitLogWorker: worker slave1,60020,1404253868846 acquired task /hbase/splitWAL/WALs%2Fsceplus-vm48.almaden.ibm.com%2C60020%2C1404225100003-splitting%2Fsceplus-vm48.almaden.ibm.com%252C60020%252C1404225100003.1404228281863
2014-07-01 15:31:15,431 INFO  [RS_LOG_REPLAY_OPS-slave1:60020-1] wal.HLogSplitter: Splitting hlog: hdfs://master:54310/hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1404225100003-splitting/sceplus-vm48.almaden.ibm.com%2C60020%2C1404225100003.1404228281863, length=79996339
2014-07-01 15:31:15,431 INFO  [RS_LOG_REPLAY_OPS-slave1:60020-1] wal.HLogSplitter: DistributedLogReplay = false
2014-07-01 15:31:15,436 INFO  [RS_LOG_REPLAY_OPS-slave1:60020-1] util.FSHDFSUtils: Recovering lease on dfs file hdfs://master:54310/hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1404225100003-splitting/sceplus-vm48.almaden.ibm.com%2C60020%2C1404225100003.1404228281863
2014-07-01 15:31:15,443 INFO  [RS_LOG_REPLAY_OPS-slave1:60020-1] util.FSHDFSUtils: recoverLease=true, attempt=0 on file=hdfs://master:54310/hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1404225100003-splitting/sceplus-vm48.almaden.ibm.com%2C60020%2C1404225100003.1404228281863 after 2ms
2014-07-01 15:31:15,457 DEBUG [RS_LOG_REPLAY_OPS-slave1:60020-1-Writer-0] wal.HLogSplitter: Writer thread Thread[RS_LOG_REPLAY_OPS-slave1:60020-1-Writer-0,5,main]: starting
2014-07-01 15:31:15,457 DEBUG [RS_LOG_REPLAY_OPS-slave1:60020-1-Writer-1] wal.HLogSplitter: Writer thread Thread[RS_LOG_REPLAY_OPS-slave1:60020-1-Writer-1,5,main]: starting
2014-07-01 15:31:15,457 DEBUG [RS_LOG_REPLAY_OPS-slave1:60020-1-Writer-2] wal.HLogSplitter: Writer thread Thread[RS_LOG_REPLAY_OPS-slave1:60020-1-Writer-2,5,main]: starting
2014-07-01 15:31:15,462 DEBUG [RS_LOG_REPLAY_OPS-slave1:60020-0-Writer-2] wal.HLogSplitter: Creating writer path=hdfs://master:54310/hbase/data/default/usertable/667c1f2c34fe37d48ba25a789d8aba98/recovered.edits/0000000000000000044.temp region=667c1f2c34fe37d48ba25a789d8aba98
2014-07-01 15:31:15,483 DEBUG [RS_LOG_REPLAY_OPS-slave1:60020-1-Writer-2] wal.HLogSplitter: Creating writer path=hdfs://master:54310/hbase/data/default/usertable/9cef9e7d34f96f1d9b111be50040a208/recovered.edits/0000000000000000007.temp region=9cef9e7d34f96f1d9b111be50040a208
2014-07-01 15:31:15,503 DEBUG [RS_LOG_REPLAY_OPS-slave1:60020-1-Writer-0] wal.HLogSplitter: Creating writer path=hdfs://master:54310/hbase/data/default/usertable/93253e37c284f3174309fede3f32339f/recovered.edits/0000000000000000005.temp region=93253e37c284f3174309fede3f32339f
2014-07-01 15:31:15,512 DEBUG [RS_LOG_REPLAY_OPS-slave1:60020-1-Writer-1] wal.HLogSplitter: Creating writer path=hdfs://master:54310/hbase/data/default/usertable/122e92eabfab38cb15a9965b21db25c5/recovered.edits/0000000000000000004.temp region=122e92eabfab38cb15a9965b21db25c5
2014-07-01 15:31:15,786 DEBUG [RS_LOG_REPLAY_OPS-slave1:60020-1-Writer-2] wal.HLogSplitter: Creating writer path=hdfs://master:54310/hbase/data/default/usertable/99b73df170b514f6773e742a495216c4/recovered.edits/0000000000000000002.temp region=99b73df170b514f6773e742a495216c4
2014-07-01 15:31:15,788 DEBUG [RS_LOG_REPLAY_OPS-slave1:60020-1-Writer-0] wal.HLogSplitter: Creating writer path=hdfs://master:54310/hbase/data/default/usertable/c72fc9d54f680bcd54e9fa888600346b/recovered.edits/0000000000000000006.temp region=c72fc9d54f680bcd54e9fa888600346b
2014-07-01 15:31:15,790 DEBUG [RS_LOG_REPLAY_OPS-slave1:60020-0-Writer-1] wal.HLogSplitter: Creating writer path=hdfs://master:54310/hbase/data/default/usertable/bb79ae773f71289e0745068f68a0abd9/recovered.edits/0000000000000000044.temp region=bb79ae773f71289e0745068f68a0abd9
2014-07-01 15:31:15,790 DEBUG [RS_LOG_REPLAY_OPS-slave1:60020-0-Writer-0] wal.HLogSplitter: Creating writer path=hdfs://master:54310/hbase/data/default/usertable/99b73df170b514f6773e742a495216c4/recovered.edits/0000000000000000044.temp region=99b73df170b514f6773e742a495216c4
2014-07-01 15:31:15,790 DEBUG [RS_LOG_REPLAY_OPS-slave1:60020-1-Writer-1] wal.HLogSplitter: Creating writer path=hdfs://master:54310/hbase/data/default/usertable/1dc0aa61f0e3f43a53375118eb48213f/recovered.edits/0000000000000000005.temp region=1dc0aa61f0e3f43a53375118eb48213f
2014-07-01 15:31:15,859 INFO  [RS_LOG_REPLAY_OPS-slave1:60020-0] wal.HLogSplitter: Finishing writing output logs and closing down.
2014-07-01 15:31:15,859 INFO  [RS_LOG_REPLAY_OPS-slave1:60020-0] wal.HLogSplitter: Waiting for split writer threads to finish
2014-07-01 15:31:15,935 INFO  [RS_LOG_REPLAY_OPS-slave1:60020-0] wal.HLogSplitter: Split writers finished
2014-07-01 15:31:15,937 DEBUG [RS_LOG_REPLAY_OPS-slave1:60020-0] wal.HLogSplitter: Submitting close of hdfs://master:54310/hbase/data/default/usertable/122e92eabfab38cb15a9965b21db25c5/recovered.edits/0000000000000000044.temp
2014-07-01 15:31:15,938 DEBUG [RS_LOG_REPLAY_OPS-slave1:60020-0] wal.HLogSplitter: Submitting close of hdfs://master:54310/hbase/data/default/usertable/1dc0aa61f0e3f43a53375118eb48213f/recovered.edits/0000000000000000044.temp
2014-07-01 15:31:15,938 DEBUG [split-log-closeStream-1] wal.HLogSplitter: Closing hdfs://master:54310/hbase/data/default/usertable/122e92eabfab38cb15a9965b21db25c5/recovered.edits/0000000000000000044.temp
2014-07-01 15:31:15,938 DEBUG [RS_LOG_REPLAY_OPS-slave1:60020-0] wal.HLogSplitter: Submitting close of hdfs://master:54310/hbase/data/default/usertable/52290d336af5a689eef6040bafbe6b15/recovered.edits/0000000000000000044.temp
2014-07-01 15:31:15,938 DEBUG [split-log-closeStream-2] wal.HLogSplitter: Closing hdfs://master:54310/hbase/data/default/usertable/1dc0aa61f0e3f43a53375118eb48213f/recovered.edits/0000000000000000044.temp
2014-07-01 15:31:15,939 DEBUG [RS_LOG_REPLAY_OPS-slave1:60020-0] wal.HLogSplitter: Submitting close of hdfs://master:54310/hbase/data/default/usertable/667c1f2c34fe37d48ba25a789d8aba98/recovered.edits/0000000000000000044.temp
2014-07-01 15:31:15,939 DEBUG [RS_LOG_REPLAY_OPS-slave1:60020-0] wal.HLogSplitter: Submitting close of hdfs://master:54310/hbase/data/default/usertable/93253e37c284f3174309fede3f32339f/recovered.edits/0000000000000000047.temp
2014-07-01 15:31:15,939 DEBUG [split-log-closeStream-3] wal.HLogSplitter: Closing hdfs://master:54310/hbase/data/default/usertable/52290d336af5a689eef6040bafbe6b15/recovered.edits/0000000000000000044.temp
2014-07-01 15:31:15,939 DEBUG [RS_LOG_REPLAY_OPS-slave1:60020-0] wal.HLogSplitter: Submitting close of hdfs://master:54310/hbase/data/default/usertable/99b73df170b514f6773e742a495216c4/recovered.edits/0000000000000000044.temp
2014-07-01 15:31:15,939 DEBUG [RS_LOG_REPLAY_OPS-slave1:60020-0] wal.HLogSplitter: Submitting close of hdfs://master:54310/hbase/data/default/usertable/9cef9e7d34f96f1d9b111be50040a208/recovered.edits/0000000000000000050.temp
2014-07-01 15:31:15,939 DEBUG [RS_LOG_REPLAY_OPS-slave1:60020-0] wal.HLogSplitter: Submitting close of hdfs://master:54310/hbase/data/default/usertable/bb79ae773f71289e0745068f68a0abd9/recovered.edits/0000000000000000044.temp
2014-07-01 15:31:15,939 DEBUG [RS_LOG_REPLAY_OPS-slave1:60020-0] wal.HLogSplitter: Submitting close of hdfs://master:54310/hbase/data/default/usertable/c72fc9d54f680bcd54e9fa888600346b/recovered.edits/0000000000000000048.temp
2014-07-01 15:31:15,949 DEBUG [RS_LOG_REPLAY_OPS-slave1:60020-1-Writer-0] wal.HLogSplitter: Creating writer path=hdfs://master:54310/hbase/data/default/usertable/52290d336af5a689eef6040bafbe6b15/recovered.edits/0000000000000000004.temp region=52290d336af5a689eef6040bafbe6b15
2014-07-01 15:31:15,950 INFO  [split-log-closeStream-3] wal.HLogSplitter: Closed wap hdfs://master:54310/hbase/data/default/usertable/52290d336af5a689eef6040bafbe6b15/recovered.edits/0000000000000000044.temp (wrote 9 edits in 131ms)
2014-07-01 15:31:15,951 INFO  [split-log-closeStream-1] wal.HLogSplitter: Closed wap hdfs://master:54310/hbase/data/default/usertable/122e92eabfab38cb15a9965b21db25c5/recovered.edits/0000000000000000044.temp (wrote 10 edits in 208ms)
2014-07-01 15:31:15,952 INFO  [split-log-closeStream-2] wal.HLogSplitter: Closed wap hdfs://master:54310/hbase/data/default/usertable/1dc0aa61f0e3f43a53375118eb48213f/recovered.edits/0000000000000000044.temp (wrote 13 edits in 163ms)
2014-07-01 15:31:15,956 DEBUG [split-log-closeStream-3] wal.HLogSplitter: Rename hdfs://master:54310/hbase/data/default/usertable/52290d336af5a689eef6040bafbe6b15/recovered.edits/0000000000000000044.temp to hdfs://master:54310/hbase/data/default/usertable/52290d336af5a689eef6040bafbe6b15/recovered.edits/0000000000000000052
2014-07-01 15:31:15,957 DEBUG [split-log-closeStream-3] wal.HLogSplitter: Closing hdfs://master:54310/hbase/data/default/usertable/667c1f2c34fe37d48ba25a789d8aba98/recovered.edits/0000000000000000044.temp
2014-07-01 15:31:15,962 DEBUG [split-log-closeStream-1] wal.HLogSplitter: Rename hdfs://master:54310/hbase/data/default/usertable/122e92eabfab38cb15a9965b21db25c5/recovered.edits/0000000000000000044.temp to hdfs://master:54310/hbase/data/default/usertable/122e92eabfab38cb15a9965b21db25c5/recovered.edits/0000000000000000053
2014-07-01 15:31:15,963 DEBUG [split-log-closeStream-1] wal.HLogSplitter: Closing hdfs://master:54310/hbase/data/default/usertable/93253e37c284f3174309fede3f32339f/recovered.edits/0000000000000000047.temp
2014-07-01 15:31:15,964 INFO  [split-log-closeStream-3] wal.HLogSplitter: Closed wap hdfs://master:54310/hbase/data/default/usertable/667c1f2c34fe37d48ba25a789d8aba98/recovered.edits/0000000000000000044.temp (wrote 7 edits in 142ms)
2014-07-01 15:31:15,968 DEBUG [split-log-closeStream-2] wal.HLogSplitter: Rename hdfs://master:54310/hbase/data/default/usertable/1dc0aa61f0e3f43a53375118eb48213f/recovered.edits/0000000000000000044.temp to hdfs://master:54310/hbase/data/default/usertable/1dc0aa61f0e3f43a53375118eb48213f/recovered.edits/0000000000000000056
2014-07-01 15:31:15,968 DEBUG [split-log-closeStream-2] wal.HLogSplitter: Closing hdfs://master:54310/hbase/data/default/usertable/99b73df170b514f6773e742a495216c4/recovered.edits/0000000000000000044.temp
2014-07-01 15:31:15,970 INFO  [split-log-closeStream-1] wal.HLogSplitter: Closed wap hdfs://master:54310/hbase/data/default/usertable/93253e37c284f3174309fede3f32339f/recovered.edits/0000000000000000047.temp (wrote 13 edits in 175ms)
2014-07-01 15:31:15,973 DEBUG [split-log-closeStream-3] wal.HLogSplitter: Rename hdfs://master:54310/hbase/data/default/usertable/667c1f2c34fe37d48ba25a789d8aba98/recovered.edits/0000000000000000044.temp to hdfs://master:54310/hbase/data/default/usertable/667c1f2c34fe37d48ba25a789d8aba98/recovered.edits/0000000000000000050
2014-07-01 15:31:15,973 DEBUG [split-log-closeStream-3] wal.HLogSplitter: Closing hdfs://master:54310/hbase/data/default/usertable/9cef9e7d34f96f1d9b111be50040a208/recovered.edits/0000000000000000050.temp
2014-07-01 15:31:15,977 INFO  [split-log-closeStream-2] wal.HLogSplitter: Closed wap hdfs://master:54310/hbase/data/default/usertable/99b73df170b514f6773e742a495216c4/recovered.edits/0000000000000000044.temp (wrote 4 edits in 291ms)
2014-07-01 15:31:15,979 DEBUG [split-log-closeStream-1] wal.HLogSplitter: Rename hdfs://master:54310/hbase/data/default/usertable/93253e37c284f3174309fede3f32339f/recovered.edits/0000000000000000047.temp to hdfs://master:54310/hbase/data/default/usertable/93253e37c284f3174309fede3f32339f/recovered.edits/0000000000000000059
2014-07-01 15:31:15,980 DEBUG [split-log-closeStream-1] wal.HLogSplitter: Closing hdfs://master:54310/hbase/data/default/usertable/bb79ae773f71289e0745068f68a0abd9/recovered.edits/0000000000000000044.temp
2014-07-01 15:31:15,981 INFO  [split-log-closeStream-3] wal.HLogSplitter: Closed wap hdfs://master:54310/hbase/data/default/usertable/9cef9e7d34f96f1d9b111be50040a208/recovered.edits/0000000000000000050.temp (wrote 14 edits in 227ms)
2014-07-01 15:31:15,984 DEBUG [split-log-closeStream-2] wal.HLogSplitter: Rename hdfs://master:54310/hbase/data/default/usertable/99b73df170b514f6773e742a495216c4/recovered.edits/0000000000000000044.temp to hdfs://master:54310/hbase/data/default/usertable/99b73df170b514f6773e742a495216c4/recovered.edits/0000000000000000047
2014-07-01 15:31:15,984 DEBUG [split-log-closeStream-2] wal.HLogSplitter: Closing hdfs://master:54310/hbase/data/default/usertable/c72fc9d54f680bcd54e9fa888600346b/recovered.edits/0000000000000000048.temp
2014-07-01 15:31:15,985 INFO  [split-log-closeStream-1] wal.HLogSplitter: Closed wap hdfs://master:54310/hbase/data/default/usertable/bb79ae773f71289e0745068f68a0abd9/recovered.edits/0000000000000000044.temp (wrote 3 edits in 189ms)
2014-07-01 15:31:15,995 DEBUG [split-log-closeStream-3] wal.HLogSplitter: Rename hdfs://master:54310/hbase/data/default/usertable/9cef9e7d34f96f1d9b111be50040a208/recovered.edits/0000000000000000050.temp to hdfs://master:54310/hbase/data/default/usertable/9cef9e7d34f96f1d9b111be50040a208/recovered.edits/0000000000000000063
2014-07-01 15:31:15,996 INFO  [split-log-closeStream-2] wal.HLogSplitter: Closed wap hdfs://master:54310/hbase/data/default/usertable/c72fc9d54f680bcd54e9fa888600346b/recovered.edits/0000000000000000048.temp (wrote 16 edits in 284ms)
2014-07-01 15:31:15,997 DEBUG [split-log-closeStream-1] wal.HLogSplitter: Rename hdfs://master:54310/hbase/data/default/usertable/bb79ae773f71289e0745068f68a0abd9/recovered.edits/0000000000000000044.temp to hdfs://master:54310/hbase/data/default/usertable/bb79ae773f71289e0745068f68a0abd9/recovered.edits/0000000000000000046
2014-07-01 15:31:15,998 DEBUG [RS_LOG_REPLAY_OPS-slave1:60020-1-Writer-2] wal.HLogSplitter: Creating writer path=hdfs://master:54310/hbase/data/default/usertable/667c1f2c34fe37d48ba25a789d8aba98/recovered.edits/0000000000000000003.temp region=667c1f2c34fe37d48ba25a789d8aba98
2014-07-01 15:31:16,046 DEBUG [split-log-closeStream-2] wal.HLogSplitter: Rename hdfs://master:54310/hbase/data/default/usertable/c72fc9d54f680bcd54e9fa888600346b/recovered.edits/0000000000000000048.temp to hdfs://master:54310/hbase/data/default/usertable/c72fc9d54f680bcd54e9fa888600346b/recovered.edits/0000000000000000063
2014-07-01 15:31:16,047 INFO  [RS_LOG_REPLAY_OPS-slave1:60020-0] wal.HLogSplitter: Processed 89 edits across 9 regions; log file=hdfs://master:54310/hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1404225100003-splitting/sceplus-vm48.almaden.ibm.com%2C60020%2C1404225100003.1404228290972 is corrupted = false progress failed = false
2014-07-01 15:31:16,054 INFO  [RS_LOG_REPLAY_OPS-slave1:60020-0] handler.HLogSplitterHandler: successfully transitioned task /hbase/splitWAL/WALs%2Fsceplus-vm48.almaden.ibm.com%2C60020%2C1404225100003-splitting%2Fsceplus-vm48.almaden.ibm.com%252C60020%252C1404225100003.1404228290972 to final state DONE slave1,60020,1404253868846
2014-07-01 15:31:16,054 INFO  [RS_LOG_REPLAY_OPS-slave1:60020-0] handler.HLogSplitterHandler: worker slave1,60020,1404253868846 done with task /hbase/splitWAL/WALs%2Fsceplus-vm48.almaden.ibm.com%2C60020%2C1404225100003-splitting%2Fsceplus-vm48.almaden.ibm.com%252C60020%252C1404225100003.1404228290972 in 1380ms
2014-07-01 15:31:16,273 INFO  [RS_LOG_REPLAY_OPS-slave1:60020-1] wal.HLogSplitter: Finishing writing output logs and closing down.
2014-07-01 15:31:16,274 INFO  [RS_LOG_REPLAY_OPS-slave1:60020-1] wal.HLogSplitter: Waiting for split writer threads to finish
2014-07-01 15:31:16,298 DEBUG [RS_LOG_REPLAY_OPS-slave1:60020-1-Writer-2] wal.HLogSplitter: Creating writer path=hdfs://master:54310/hbase/data/default/usertable/bb79ae773f71289e0745068f68a0abd9/recovered.edits/0000000000000000002.temp region=bb79ae773f71289e0745068f68a0abd9
2014-07-01 15:31:16,330 INFO  [RS_LOG_REPLAY_OPS-slave1:60020-1] wal.HLogSplitter: Split writers finished
2014-07-01 15:31:16,330 DEBUG [RS_LOG_REPLAY_OPS-slave1:60020-1] wal.HLogSplitter: Submitting close of hdfs://master:54310/hbase/data/default/usertable/122e92eabfab38cb15a9965b21db25c5/recovered.edits/0000000000000000004.temp
2014-07-01 15:31:16,331 DEBUG [RS_LOG_REPLAY_OPS-slave1:60020-1] wal.HLogSplitter: Submitting close of hdfs://master:54310/hbase/data/default/usertable/1dc0aa61f0e3f43a53375118eb48213f/recovered.edits/0000000000000000005.temp
2014-07-01 15:31:16,331 DEBUG [split-log-closeStream-1] wal.HLogSplitter: Closing hdfs://master:54310/hbase/data/default/usertable/122e92eabfab38cb15a9965b21db25c5/recovered.edits/0000000000000000004.temp
2014-07-01 15:31:16,331 DEBUG [RS_LOG_REPLAY_OPS-slave1:60020-1] wal.HLogSplitter: Submitting close of hdfs://master:54310/hbase/data/default/usertable/52290d336af5a689eef6040bafbe6b15/recovered.edits/0000000000000000004.temp
2014-07-01 15:31:16,331 INFO  [SplitLogWorker-slave1,60020,1404253868846] regionserver.SplitLogWorker: worker slave1,60020,1404253868846 acquired task /hbase/splitWAL/WALs%2Fsceplus-vm48.almaden.ibm.com%2C60020%2C1404225100003-splitting%2Fsceplus-vm48.almaden.ibm.com%252C60020%252C1404225100003.1404228283151
2014-07-01 15:31:16,331 DEBUG [split-log-closeStream-2] wal.HLogSplitter: Closing hdfs://master:54310/hbase/data/default/usertable/1dc0aa61f0e3f43a53375118eb48213f/recovered.edits/0000000000000000005.temp
2014-07-01 15:31:16,331 DEBUG [RS_LOG_REPLAY_OPS-slave1:60020-1] wal.HLogSplitter: Submitting close of hdfs://master:54310/hbase/data/default/usertable/667c1f2c34fe37d48ba25a789d8aba98/recovered.edits/0000000000000000003.temp
2014-07-01 15:31:16,331 DEBUG [split-log-closeStream-3] wal.HLogSplitter: Closing hdfs://master:54310/hbase/data/default/usertable/52290d336af5a689eef6040bafbe6b15/recovered.edits/0000000000000000004.temp
2014-07-01 15:31:16,331 DEBUG [RS_LOG_REPLAY_OPS-slave1:60020-1] wal.HLogSplitter: Submitting close of hdfs://master:54310/hbase/data/default/usertable/93253e37c284f3174309fede3f32339f/recovered.edits/0000000000000000005.temp
2014-07-01 15:31:16,331 DEBUG [RS_LOG_REPLAY_OPS-slave1:60020-1] wal.HLogSplitter: Submitting close of hdfs://master:54310/hbase/data/default/usertable/99b73df170b514f6773e742a495216c4/recovered.edits/0000000000000000002.temp
2014-07-01 15:31:16,331 DEBUG [RS_LOG_REPLAY_OPS-slave1:60020-1] wal.HLogSplitter: Submitting close of hdfs://master:54310/hbase/data/default/usertable/9cef9e7d34f96f1d9b111be50040a208/recovered.edits/0000000000000000007.temp
2014-07-01 15:31:16,332 DEBUG [RS_LOG_REPLAY_OPS-slave1:60020-1] wal.HLogSplitter: Submitting close of hdfs://master:54310/hbase/data/default/usertable/bb79ae773f71289e0745068f68a0abd9/recovered.edits/0000000000000000002.temp
2014-07-01 15:31:16,332 DEBUG [RS_LOG_REPLAY_OPS-slave1:60020-1] wal.HLogSplitter: Submitting close of hdfs://master:54310/hbase/data/default/usertable/c72fc9d54f680bcd54e9fa888600346b/recovered.edits/0000000000000000006.temp
2014-07-01 15:31:16,338 INFO  [split-log-closeStream-2] wal.HLogSplitter: Closed wap hdfs://master:54310/hbase/data/default/usertable/1dc0aa61f0e3f43a53375118eb48213f/recovered.edits/0000000000000000005.temp (wrote 11 edits in 357ms)
2014-07-01 15:31:16,339 INFO  [split-log-closeStream-1] wal.HLogSplitter: Closed wap hdfs://master:54310/hbase/data/default/usertable/122e92eabfab38cb15a9965b21db25c5/recovered.edits/0000000000000000004.temp (wrote 11 edits in 191ms)
2014-07-01 15:31:16,339 INFO  [split-log-closeStream-3] wal.HLogSplitter: Closed wap hdfs://master:54310/hbase/data/default/usertable/52290d336af5a689eef6040bafbe6b15/recovered.edits/0000000000000000004.temp (wrote 10 edits in 238ms)
2014-07-01 15:31:16,343 DEBUG [split-log-closeStream-2] wal.HLogSplitter: Rename hdfs://master:54310/hbase/data/default/usertable/1dc0aa61f0e3f43a53375118eb48213f/recovered.edits/0000000000000000005.temp to hdfs://master:54310/hbase/data/default/usertable/1dc0aa61f0e3f43a53375118eb48213f/recovered.edits/0000000000000000015
2014-07-01 15:31:16,343 DEBUG [split-log-closeStream-2] wal.HLogSplitter: Closing hdfs://master:54310/hbase/data/default/usertable/667c1f2c34fe37d48ba25a789d8aba98/recovered.edits/0000000000000000003.temp
2014-07-01 15:31:16,343 DEBUG [split-log-closeStream-1] wal.HLogSplitter: Rename hdfs://master:54310/hbase/data/default/usertable/122e92eabfab38cb15a9965b21db25c5/recovered.edits/0000000000000000004.temp to hdfs://master:54310/hbase/data/default/usertable/122e92eabfab38cb15a9965b21db25c5/recovered.edits/0000000000000000014
2014-07-01 15:31:16,343 DEBUG [split-log-closeStream-1] wal.HLogSplitter: Closing hdfs://master:54310/hbase/data/default/usertable/93253e37c284f3174309fede3f32339f/recovered.edits/0000000000000000005.temp
2014-07-01 15:31:16,362 DEBUG [split-log-closeStream-3] wal.HLogSplitter: Rename hdfs://master:54310/hbase/data/default/usertable/52290d336af5a689eef6040bafbe6b15/recovered.edits/0000000000000000004.temp to hdfs://master:54310/hbase/data/default/usertable/52290d336af5a689eef6040bafbe6b15/recovered.edits/0000000000000000013
2014-07-01 15:31:16,362 INFO  [split-log-closeStream-2] wal.HLogSplitter: Closed wap hdfs://master:54310/hbase/data/default/usertable/667c1f2c34fe37d48ba25a789d8aba98/recovered.edits/0000000000000000003.temp (wrote 9 edits in 134ms)
2014-07-01 15:31:16,362 DEBUG [split-log-closeStream-3] wal.HLogSplitter: Closing hdfs://master:54310/hbase/data/default/usertable/99b73df170b514f6773e742a495216c4/recovered.edits/0000000000000000002.temp
2014-07-01 15:31:16,363 INFO  [split-log-closeStream-1] wal.HLogSplitter: Closed wap hdfs://master:54310/hbase/data/default/usertable/93253e37c284f3174309fede3f32339f/recovered.edits/0000000000000000005.temp (wrote 12 edits in 220ms)
2014-07-01 15:31:16,367 INFO  [split-log-closeStream-3] wal.HLogSplitter: Closed wap hdfs://master:54310/hbase/data/default/usertable/99b73df170b514f6773e742a495216c4/recovered.edits/0000000000000000002.temp (wrote 7 edits in 512ms)
2014-07-01 15:31:16,368 DEBUG [split-log-closeStream-2] wal.HLogSplitter: Rename hdfs://master:54310/hbase/data/default/usertable/667c1f2c34fe37d48ba25a789d8aba98/recovered.edits/0000000000000000003.temp to hdfs://master:54310/hbase/data/default/usertable/667c1f2c34fe37d48ba25a789d8aba98/recovered.edits/0000000000000000011
2014-07-01 15:31:16,368 DEBUG [split-log-closeStream-2] wal.HLogSplitter: Closing hdfs://master:54310/hbase/data/default/usertable/9cef9e7d34f96f1d9b111be50040a208/recovered.edits/0000000000000000007.temp
2014-07-01 15:31:16,368 DEBUG [split-log-closeStream-1] wal.HLogSplitter: Rename hdfs://master:54310/hbase/data/default/usertable/93253e37c284f3174309fede3f32339f/recovered.edits/0000000000000000005.temp to hdfs://master:54310/hbase/data/default/usertable/93253e37c284f3174309fede3f32339f/recovered.edits/0000000000000000016
2014-07-01 15:31:16,369 DEBUG [split-log-closeStream-1] wal.HLogSplitter: Closing hdfs://master:54310/hbase/data/default/usertable/bb79ae773f71289e0745068f68a0abd9/recovered.edits/0000000000000000002.temp
2014-07-01 15:31:16,373 INFO  [RS_LOG_REPLAY_OPS-slave1:60020-0] wal.HLogSplitter: Splitting hlog: hdfs://master:54310/hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1404225100003-splitting/sceplus-vm48.almaden.ibm.com%2C60020%2C1404225100003.1404228283151, length=83313861
2014-07-01 15:31:16,373 INFO  [RS_LOG_REPLAY_OPS-slave1:60020-0] wal.HLogSplitter: DistributedLogReplay = false
2014-07-01 15:31:16,374 INFO  [split-log-closeStream-2] wal.HLogSplitter: Closed wap hdfs://master:54310/hbase/data/default/usertable/9cef9e7d34f96f1d9b111be50040a208/recovered.edits/0000000000000000007.temp (wrote 15 edits in 308ms)
2014-07-01 15:31:16,375 INFO  [split-log-closeStream-1] wal.HLogSplitter: Closed wap hdfs://master:54310/hbase/data/default/usertable/bb79ae773f71289e0745068f68a0abd9/recovered.edits/0000000000000000002.temp (wrote 7 edits in 30ms)
2014-07-01 15:31:16,376 INFO  [RS_LOG_REPLAY_OPS-slave1:60020-0] util.FSHDFSUtils: Recovering lease on dfs file hdfs://master:54310/hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1404225100003-splitting/sceplus-vm48.almaden.ibm.com%2C60020%2C1404225100003.1404228283151
2014-07-01 15:31:16,376 DEBUG [split-log-closeStream-3] wal.HLogSplitter: Rename hdfs://master:54310/hbase/data/default/usertable/99b73df170b514f6773e742a495216c4/recovered.edits/0000000000000000002.temp to hdfs://master:54310/hbase/data/default/usertable/99b73df170b514f6773e742a495216c4/recovered.edits/0000000000000000008
2014-07-01 15:31:16,376 DEBUG [split-log-closeStream-3] wal.HLogSplitter: Closing hdfs://master:54310/hbase/data/default/usertable/c72fc9d54f680bcd54e9fa888600346b/recovered.edits/0000000000000000006.temp
2014-07-01 15:31:16,378 INFO  [RS_LOG_REPLAY_OPS-slave1:60020-0] util.FSHDFSUtils: recoverLease=true, attempt=0 on file=hdfs://master:54310/hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1404225100003-splitting/sceplus-vm48.almaden.ibm.com%2C60020%2C1404225100003.1404228283151 after 2ms
2014-07-01 15:31:16,382 DEBUG [split-log-closeStream-2] wal.HLogSplitter: Rename hdfs://master:54310/hbase/data/default/usertable/9cef9e7d34f96f1d9b111be50040a208/recovered.edits/0000000000000000007.temp to hdfs://master:54310/hbase/data/default/usertable/9cef9e7d34f96f1d9b111be50040a208/recovered.edits/0000000000000000021
2014-07-01 15:31:16,383 DEBUG [split-log-closeStream-1] wal.HLogSplitter: Rename hdfs://master:54310/hbase/data/default/usertable/bb79ae773f71289e0745068f68a0abd9/recovered.edits/0000000000000000002.temp to hdfs://master:54310/hbase/data/default/usertable/bb79ae773f71289e0745068f68a0abd9/recovered.edits/0000000000000000008
2014-07-01 15:31:16,383 INFO  [split-log-closeStream-3] wal.HLogSplitter: Closed wap hdfs://master:54310/hbase/data/default/usertable/c72fc9d54f680bcd54e9fa888600346b/recovered.edits/0000000000000000006.temp (wrote 12 edits in 543ms)
2014-07-01 15:31:16,390 DEBUG [RS_LOG_REPLAY_OPS-slave1:60020-0-Writer-0] wal.HLogSplitter: Writer thread Thread[RS_LOG_REPLAY_OPS-slave1:60020-0-Writer-0,5,main]: starting
2014-07-01 15:31:16,390 DEBUG [RS_LOG_REPLAY_OPS-slave1:60020-0-Writer-1] wal.HLogSplitter: Writer thread Thread[RS_LOG_REPLAY_OPS-slave1:60020-0-Writer-1,5,main]: starting
2014-07-01 15:31:16,390 DEBUG [RS_LOG_REPLAY_OPS-slave1:60020-0-Writer-2] wal.HLogSplitter: Writer thread Thread[RS_LOG_REPLAY_OPS-slave1:60020-0-Writer-2,5,main]: starting
2014-07-01 15:31:16,405 DEBUG [split-log-closeStream-3] wal.HLogSplitter: Rename hdfs://master:54310/hbase/data/default/usertable/c72fc9d54f680bcd54e9fa888600346b/recovered.edits/0000000000000000006.temp to hdfs://master:54310/hbase/data/default/usertable/c72fc9d54f680bcd54e9fa888600346b/recovered.edits/0000000000000000017
2014-07-01 15:31:16,405 INFO  [RS_LOG_REPLAY_OPS-slave1:60020-1] wal.HLogSplitter: Processed 94 edits across 9 regions; log file=hdfs://master:54310/hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1404225100003-splitting/sceplus-vm48.almaden.ibm.com%2C60020%2C1404225100003.1404228281863 is corrupted = false progress failed = false
2014-07-01 15:31:16,412 INFO  [RS_LOG_REPLAY_OPS-slave1:60020-1] handler.HLogSplitterHandler: successfully transitioned task /hbase/splitWAL/WALs%2Fsceplus-vm48.almaden.ibm.com%2C60020%2C1404225100003-splitting%2Fsceplus-vm48.almaden.ibm.com%252C60020%252C1404225100003.1404228281863 to final state DONE slave1,60020,1404253868846
2014-07-01 15:31:16,412 INFO  [RS_LOG_REPLAY_OPS-slave1:60020-1] handler.HLogSplitterHandler: worker slave1,60020,1404253868846 done with task /hbase/splitWAL/WALs%2Fsceplus-vm48.almaden.ibm.com%2C60020%2C1404225100003-splitting%2Fsceplus-vm48.almaden.ibm.com%252C60020%252C1404225100003.1404228281863 in 1037ms
2014-07-01 15:31:16,439 DEBUG [RS_LOG_REPLAY_OPS-slave1:60020-0-Writer-2] wal.HLogSplitter: Creating writer path=hdfs://master:54310/hbase/data/default/usertable/9cef9e7d34f96f1d9b111be50040a208/recovered.edits/0000000000000000022.temp region=9cef9e7d34f96f1d9b111be50040a208
2014-07-01 15:31:16,445 DEBUG [RS_LOG_REPLAY_OPS-slave1:60020-0-Writer-0] wal.HLogSplitter: Creating writer path=hdfs://master:54310/hbase/data/default/usertable/667c1f2c34fe37d48ba25a789d8aba98/recovered.edits/0000000000000000012.temp region=667c1f2c34fe37d48ba25a789d8aba98
2014-07-01 15:31:16,457 DEBUG [RS_LOG_REPLAY_OPS-slave1:60020-0-Writer-1] wal.HLogSplitter: Creating writer path=hdfs://master:54310/hbase/data/default/usertable/c72fc9d54f680bcd54e9fa888600346b/recovered.edits/0000000000000000018.temp region=c72fc9d54f680bcd54e9fa888600346b
2014-07-01 15:31:16,480 DEBUG [RS_LOG_REPLAY_OPS-slave1:60020-0-Writer-2] wal.HLogSplitter: Creating writer path=hdfs://master:54310/hbase/data/default/usertable/99b73df170b514f6773e742a495216c4/recovered.edits/0000000000000000009.temp region=99b73df170b514f6773e742a495216c4
2014-07-01 15:31:16,483 DEBUG [RS_LOG_REPLAY_OPS-slave1:60020-0-Writer-0] wal.HLogSplitter: Creating writer path=hdfs://master:54310/hbase/data/default/usertable/93253e37c284f3174309fede3f32339f/recovered.edits/0000000000000000017.temp region=93253e37c284f3174309fede3f32339f
2014-07-01 15:31:16,499 DEBUG [RS_LOG_REPLAY_OPS-slave1:60020-0-Writer-1] wal.HLogSplitter: Creating writer path=hdfs://master:54310/hbase/data/default/usertable/52290d336af5a689eef6040bafbe6b15/recovered.edits/0000000000000000014.temp region=52290d336af5a689eef6040bafbe6b15
2014-07-01 15:31:16,530 DEBUG [RS_LOG_REPLAY_OPS-slave1:60020-0-Writer-2] wal.HLogSplitter: Creating writer path=hdfs://master:54310/hbase/data/default/usertable/122e92eabfab38cb15a9965b21db25c5/recovered.edits/0000000000000000015.temp region=122e92eabfab38cb15a9965b21db25c5
2014-07-01 15:31:16,575 DEBUG [RS_LOG_REPLAY_OPS-slave1:60020-0-Writer-0] wal.HLogSplitter: Creating writer path=hdfs://master:54310/hbase/data/default/usertable/1dc0aa61f0e3f43a53375118eb48213f/recovered.edits/0000000000000000016.temp region=1dc0aa61f0e3f43a53375118eb48213f
2014-07-01 15:31:16,576 DEBUG [RS_LOG_REPLAY_OPS-slave1:60020-0-Writer-1] wal.HLogSplitter: Creating writer path=hdfs://master:54310/hbase/data/default/usertable/bb79ae773f71289e0745068f68a0abd9/recovered.edits/0000000000000000009.temp region=bb79ae773f71289e0745068f68a0abd9
2014-07-01 15:31:16,964 INFO  [RS_LOG_REPLAY_OPS-slave1:60020-0] wal.HLogSplitter: Finishing writing output logs and closing down.
2014-07-01 15:31:16,964 INFO  [RS_LOG_REPLAY_OPS-slave1:60020-0] wal.HLogSplitter: Waiting for split writer threads to finish
2014-07-01 15:31:16,972 INFO  [RS_LOG_REPLAY_OPS-slave1:60020-0] wal.HLogSplitter: Split writers finished
2014-07-01 15:31:16,972 DEBUG [RS_LOG_REPLAY_OPS-slave1:60020-0] wal.HLogSplitter: Submitting close of hdfs://master:54310/hbase/data/default/usertable/122e92eabfab38cb15a9965b21db25c5/recovered.edits/0000000000000000015.temp
2014-07-01 15:31:16,972 DEBUG [RS_LOG_REPLAY_OPS-slave1:60020-0] wal.HLogSplitter: Submitting close of hdfs://master:54310/hbase/data/default/usertable/1dc0aa61f0e3f43a53375118eb48213f/recovered.edits/0000000000000000016.temp
2014-07-01 15:31:16,972 DEBUG [RS_LOG_REPLAY_OPS-slave1:60020-0] wal.HLogSplitter: Submitting close of hdfs://master:54310/hbase/data/default/usertable/52290d336af5a689eef6040bafbe6b15/recovered.edits/0000000000000000014.temp
2014-07-01 15:31:16,973 DEBUG [RS_LOG_REPLAY_OPS-slave1:60020-0] wal.HLogSplitter: Submitting close of hdfs://master:54310/hbase/data/default/usertable/667c1f2c34fe37d48ba25a789d8aba98/recovered.edits/0000000000000000012.temp
2014-07-01 15:31:16,973 DEBUG [RS_LOG_REPLAY_OPS-slave1:60020-0] wal.HLogSplitter: Submitting close of hdfs://master:54310/hbase/data/default/usertable/93253e37c284f3174309fede3f32339f/recovered.edits/0000000000000000017.temp
2014-07-01 15:31:16,973 DEBUG [RS_LOG_REPLAY_OPS-slave1:60020-0] wal.HLogSplitter: Submitting close of hdfs://master:54310/hbase/data/default/usertable/99b73df170b514f6773e742a495216c4/recovered.edits/0000000000000000009.temp
2014-07-01 15:31:16,973 DEBUG [RS_LOG_REPLAY_OPS-slave1:60020-0] wal.HLogSplitter: Submitting close of hdfs://master:54310/hbase/data/default/usertable/9cef9e7d34f96f1d9b111be50040a208/recovered.edits/0000000000000000022.temp
2014-07-01 15:31:16,973 DEBUG [RS_LOG_REPLAY_OPS-slave1:60020-0] wal.HLogSplitter: Submitting close of hdfs://master:54310/hbase/data/default/usertable/bb79ae773f71289e0745068f68a0abd9/recovered.edits/0000000000000000009.temp
2014-07-01 15:31:16,973 DEBUG [RS_LOG_REPLAY_OPS-slave1:60020-0] wal.HLogSplitter: Submitting close of hdfs://master:54310/hbase/data/default/usertable/c72fc9d54f680bcd54e9fa888600346b/recovered.edits/0000000000000000018.temp
2014-07-01 15:31:16,973 DEBUG [split-log-closeStream-1] wal.HLogSplitter: Closing hdfs://master:54310/hbase/data/default/usertable/122e92eabfab38cb15a9965b21db25c5/recovered.edits/0000000000000000015.temp
2014-07-01 15:31:16,974 DEBUG [split-log-closeStream-2] wal.HLogSplitter: Closing hdfs://master:54310/hbase/data/default/usertable/1dc0aa61f0e3f43a53375118eb48213f/recovered.edits/0000000000000000016.temp
2014-07-01 15:31:16,974 DEBUG [split-log-closeStream-3] wal.HLogSplitter: Closing hdfs://master:54310/hbase/data/default/usertable/52290d336af5a689eef6040bafbe6b15/recovered.edits/0000000000000000014.temp
2014-07-01 15:31:16,979 INFO  [split-log-closeStream-1] wal.HLogSplitter: Closed wap hdfs://master:54310/hbase/data/default/usertable/122e92eabfab38cb15a9965b21db25c5/recovered.edits/0000000000000000015.temp (wrote 11 edits in 154ms)
2014-07-01 15:31:16,981 INFO  [split-log-closeStream-2] wal.HLogSplitter: Closed wap hdfs://master:54310/hbase/data/default/usertable/1dc0aa61f0e3f43a53375118eb48213f/recovered.edits/0000000000000000016.temp (wrote 13 edits in 144ms)
2014-07-01 15:31:16,983 INFO  [split-log-closeStream-3] wal.HLogSplitter: Closed wap hdfs://master:54310/hbase/data/default/usertable/52290d336af5a689eef6040bafbe6b15/recovered.edits/0000000000000000014.temp (wrote 9 edits in 92ms)
2014-07-01 15:31:16,985 DEBUG [split-log-closeStream-1] wal.HLogSplitter: Rename hdfs://master:54310/hbase/data/default/usertable/122e92eabfab38cb15a9965b21db25c5/recovered.edits/0000000000000000015.temp to hdfs://master:54310/hbase/data/default/usertable/122e92eabfab38cb15a9965b21db25c5/recovered.edits/0000000000000000025
2014-07-01 15:31:16,985 DEBUG [split-log-closeStream-1] wal.HLogSplitter: Closing hdfs://master:54310/hbase/data/default/usertable/667c1f2c34fe37d48ba25a789d8aba98/recovered.edits/0000000000000000012.temp
2014-07-01 15:31:16,986 DEBUG [split-log-closeStream-2] wal.HLogSplitter: Rename hdfs://master:54310/hbase/data/default/usertable/1dc0aa61f0e3f43a53375118eb48213f/recovered.edits/0000000000000000016.temp to hdfs://master:54310/hbase/data/default/usertable/1dc0aa61f0e3f43a53375118eb48213f/recovered.edits/0000000000000000028
2014-07-01 15:31:16,986 DEBUG [split-log-closeStream-2] wal.HLogSplitter: Closing hdfs://master:54310/hbase/data/default/usertable/93253e37c284f3174309fede3f32339f/recovered.edits/0000000000000000017.temp
2014-07-01 15:31:16,991 INFO  [split-log-closeStream-1] wal.HLogSplitter: Closed wap hdfs://master:54310/hbase/data/default/usertable/667c1f2c34fe37d48ba25a789d8aba98/recovered.edits/0000000000000000012.temp (wrote 8 edits in 97ms)
2014-07-01 15:31:16,992 INFO  [split-log-closeStream-2] wal.HLogSplitter: Closed wap hdfs://master:54310/hbase/data/default/usertable/93253e37c284f3174309fede3f32339f/recovered.edits/0000000000000000017.temp (wrote 13 edits in 114ms)
2014-07-01 15:31:16,992 DEBUG [split-log-closeStream-3] wal.HLogSplitter: Rename hdfs://master:54310/hbase/data/default/usertable/52290d336af5a689eef6040bafbe6b15/recovered.edits/0000000000000000014.temp to hdfs://master:54310/hbase/data/default/usertable/52290d336af5a689eef6040bafbe6b15/recovered.edits/0000000000000000022
2014-07-01 15:31:16,992 DEBUG [split-log-closeStream-3] wal.HLogSplitter: Closing hdfs://master:54310/hbase/data/default/usertable/99b73df170b514f6773e742a495216c4/recovered.edits/0000000000000000009.temp
2014-07-01 15:31:16,995 DEBUG [split-log-closeStream-1] wal.HLogSplitter: Rename hdfs://master:54310/hbase/data/default/usertable/667c1f2c34fe37d48ba25a789d8aba98/recovered.edits/0000000000000000012.temp to hdfs://master:54310/hbase/data/default/usertable/667c1f2c34fe37d48ba25a789d8aba98/recovered.edits/0000000000000000019
2014-07-01 15:31:16,995 DEBUG [split-log-closeStream-1] wal.HLogSplitter: Closing hdfs://master:54310/hbase/data/default/usertable/9cef9e7d34f96f1d9b111be50040a208/recovered.edits/0000000000000000022.temp
2014-07-01 15:31:16,996 DEBUG [split-log-closeStream-2] wal.HLogSplitter: Rename hdfs://master:54310/hbase/data/default/usertable/93253e37c284f3174309fede3f32339f/recovered.edits/0000000000000000017.temp to hdfs://master:54310/hbase/data/default/usertable/93253e37c284f3174309fede3f32339f/recovered.edits/0000000000000000029
2014-07-01 15:31:16,996 DEBUG [split-log-closeStream-2] wal.HLogSplitter: Closing hdfs://master:54310/hbase/data/default/usertable/bb79ae773f71289e0745068f68a0abd9/recovered.edits/0000000000000000009.temp
2014-07-01 15:31:17,001 INFO  [split-log-closeStream-3] wal.HLogSplitter: Closed wap hdfs://master:54310/hbase/data/default/usertable/99b73df170b514f6773e742a495216c4/recovered.edits/0000000000000000009.temp (wrote 8 edits in 103ms)
2014-07-01 15:31:17,002 INFO  [split-log-closeStream-2] wal.HLogSplitter: Closed wap hdfs://master:54310/hbase/data/default/usertable/bb79ae773f71289e0745068f68a0abd9/recovered.edits/0000000000000000009.temp (wrote 8 edits in 89ms)
2014-07-01 15:31:17,002 INFO  [split-log-closeStream-1] wal.HLogSplitter: Closed wap hdfs://master:54310/hbase/data/default/usertable/9cef9e7d34f96f1d9b111be50040a208/recovered.edits/0000000000000000022.temp (wrote 13 edits in 143ms)
2014-07-01 15:31:17,005 DEBUG [split-log-closeStream-3] wal.HLogSplitter: Rename hdfs://master:54310/hbase/data/default/usertable/99b73df170b514f6773e742a495216c4/recovered.edits/0000000000000000009.temp to hdfs://master:54310/hbase/data/default/usertable/99b73df170b514f6773e742a495216c4/recovered.edits/0000000000000000016
2014-07-01 15:31:17,005 DEBUG [split-log-closeStream-3] wal.HLogSplitter: Closing hdfs://master:54310/hbase/data/default/usertable/c72fc9d54f680bcd54e9fa888600346b/recovered.edits/0000000000000000018.temp
2014-07-01 15:31:17,005 DEBUG [split-log-closeStream-2] wal.HLogSplitter: Rename hdfs://master:54310/hbase/data/default/usertable/bb79ae773f71289e0745068f68a0abd9/recovered.edits/0000000000000000009.temp to hdfs://master:54310/hbase/data/default/usertable/bb79ae773f71289e0745068f68a0abd9/recovered.edits/0000000000000000016
2014-07-01 15:31:17,009 DEBUG [split-log-closeStream-1] wal.HLogSplitter: Rename hdfs://master:54310/hbase/data/default/usertable/9cef9e7d34f96f1d9b111be50040a208/recovered.edits/0000000000000000022.temp to hdfs://master:54310/hbase/data/default/usertable/9cef9e7d34f96f1d9b111be50040a208/recovered.edits/0000000000000000034
2014-07-01 15:31:17,048 INFO  [split-log-closeStream-3] wal.HLogSplitter: Closed wap hdfs://master:54310/hbase/data/default/usertable/c72fc9d54f680bcd54e9fa888600346b/recovered.edits/0000000000000000018.temp (wrote 15 edits in 158ms)
2014-07-01 15:31:17,054 DEBUG [split-log-closeStream-3] wal.HLogSplitter: Rename hdfs://master:54310/hbase/data/default/usertable/c72fc9d54f680bcd54e9fa888600346b/recovered.edits/0000000000000000018.temp to hdfs://master:54310/hbase/data/default/usertable/c72fc9d54f680bcd54e9fa888600346b/recovered.edits/0000000000000000032
2014-07-01 15:31:17,054 INFO  [RS_LOG_REPLAY_OPS-slave1:60020-0] wal.HLogSplitter: Processed 98 edits across 9 regions; log file=hdfs://master:54310/hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1404225100003-splitting/sceplus-vm48.almaden.ibm.com%2C60020%2C1404225100003.1404228283151 is corrupted = false progress failed = false
2014-07-01 15:31:17,058 INFO  [RS_LOG_REPLAY_OPS-slave1:60020-0] handler.HLogSplitterHandler: successfully transitioned task /hbase/splitWAL/WALs%2Fsceplus-vm48.almaden.ibm.com%2C60020%2C1404225100003-splitting%2Fsceplus-vm48.almaden.ibm.com%252C60020%252C1404225100003.1404228283151 to final state DONE slave1,60020,1404253868846
2014-07-01 15:31:17,058 INFO  [RS_LOG_REPLAY_OPS-slave1:60020-0] handler.HLogSplitterHandler: worker slave1,60020,1404253868846 done with task /hbase/splitWAL/WALs%2Fsceplus-vm48.almaden.ibm.com%2C60020%2C1404225100003-splitting%2Fsceplus-vm48.almaden.ibm.com%252C60020%252C1404225100003.1404228283151 in 727ms
2014-07-01 15:31:17,263 INFO  [SplitLogWorker-slave1,60020,1404253868846] regionserver.SplitLogWorker: worker slave1,60020,1404253868846 acquired task /hbase/splitWAL/WALs%2Fsceplus-vm48.almaden.ibm.com%2C60020%2C1404225100003-splitting%2Fsceplus-vm48.almaden.ibm.com%252C60020%252C1404225100003.1404228308103
2014-07-01 15:31:17,284 INFO  [RS_LOG_REPLAY_OPS-slave1:60020-1] wal.HLogSplitter: Splitting hlog: hdfs://master:54310/hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1404225100003-splitting/sceplus-vm48.almaden.ibm.com%2C60020%2C1404225100003.1404228308103, length=84651673
2014-07-01 15:31:17,284 INFO  [RS_LOG_REPLAY_OPS-slave1:60020-1] wal.HLogSplitter: DistributedLogReplay = false
2014-07-01 15:31:17,289 INFO  [RS_LOG_REPLAY_OPS-slave1:60020-1] util.FSHDFSUtils: Recovering lease on dfs file hdfs://master:54310/hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1404225100003-splitting/sceplus-vm48.almaden.ibm.com%2C60020%2C1404225100003.1404228308103
2014-07-01 15:31:17,290 INFO  [RS_LOG_REPLAY_OPS-slave1:60020-1] util.FSHDFSUtils: recoverLease=true, attempt=0 on file=hdfs://master:54310/hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1404225100003-splitting/sceplus-vm48.almaden.ibm.com%2C60020%2C1404225100003.1404228308103 after 1ms
2014-07-01 15:31:17,303 DEBUG [RS_LOG_REPLAY_OPS-slave1:60020-1-Writer-0] wal.HLogSplitter: Writer thread Thread[RS_LOG_REPLAY_OPS-slave1:60020-1-Writer-0,5,main]: starting
2014-07-01 15:31:17,303 DEBUG [RS_LOG_REPLAY_OPS-slave1:60020-1-Writer-1] wal.HLogSplitter: Writer thread Thread[RS_LOG_REPLAY_OPS-slave1:60020-1-Writer-1,5,main]: starting
2014-07-01 15:31:17,303 DEBUG [RS_LOG_REPLAY_OPS-slave1:60020-1-Writer-2] wal.HLogSplitter: Writer thread Thread[RS_LOG_REPLAY_OPS-slave1:60020-1-Writer-2,5,main]: starting
2014-07-01 15:31:17,325 DEBUG [RS_LOG_REPLAY_OPS-slave1:60020-1-Writer-1] wal.HLogSplitter: Creating writer path=hdfs://master:54310/hbase/data/default/usertable/bb79ae773f71289e0745068f68a0abd9/recovered.edits/0000000000000000078.temp region=bb79ae773f71289e0745068f68a0abd9
2014-07-01 15:31:17,333 DEBUG [RS_LOG_REPLAY_OPS-slave1:60020-1-Writer-2] wal.HLogSplitter: Creating writer path=hdfs://master:54310/hbase/data/default/usertable/93253e37c284f3174309fede3f32339f/recovered.edits/0000000000000000088.temp region=93253e37c284f3174309fede3f32339f
2014-07-01 15:31:17,339 DEBUG [RS_LOG_REPLAY_OPS-slave1:60020-1-Writer-0] wal.HLogSplitter: Creating writer path=hdfs://master:54310/hbase/data/default/usertable/122e92eabfab38cb15a9965b21db25c5/recovered.edits/0000000000000000086.temp region=122e92eabfab38cb15a9965b21db25c5
2014-07-01 15:31:17,349 DEBUG [RS_LOG_REPLAY_OPS-slave1:60020-1-Writer-1] wal.HLogSplitter: Creating writer path=hdfs://master:54310/hbase/data/default/usertable/667c1f2c34fe37d48ba25a789d8aba98/recovered.edits/0000000000000000082.temp region=667c1f2c34fe37d48ba25a789d8aba98
2014-07-01 15:31:17,361 DEBUG [RS_LOG_REPLAY_OPS-slave1:60020-1-Writer-0] wal.HLogSplitter: Creating writer path=hdfs://master:54310/hbase/data/default/usertable/99b73df170b514f6773e742a495216c4/recovered.edits/0000000000000000081.temp region=99b73df170b514f6773e742a495216c4
2014-07-01 15:31:17,374 DEBUG [RS_LOG_REPLAY_OPS-slave1:60020-1-Writer-2] wal.HLogSplitter: Creating writer path=hdfs://master:54310/hbase/data/default/usertable/9cef9e7d34f96f1d9b111be50040a208/recovered.edits/0000000000000000091.temp region=9cef9e7d34f96f1d9b111be50040a208
2014-07-01 15:31:17,403 DEBUG [RS_LOG_REPLAY_OPS-slave1:60020-1-Writer-0] wal.HLogSplitter: Creating writer path=hdfs://master:54310/hbase/data/default/usertable/c72fc9d54f680bcd54e9fa888600346b/recovered.edits/0000000000000000089.temp region=c72fc9d54f680bcd54e9fa888600346b
2014-07-01 15:31:17,409 DEBUG [RS_LOG_REPLAY_OPS-slave1:60020-1-Writer-2] wal.HLogSplitter: Creating writer path=hdfs://master:54310/hbase/data/default/usertable/52290d336af5a689eef6040bafbe6b15/recovered.edits/0000000000000000085.temp region=52290d336af5a689eef6040bafbe6b15
2014-07-01 15:31:17,419 DEBUG [RS_LOG_REPLAY_OPS-slave1:60020-1-Writer-1] wal.HLogSplitter: Creating writer path=hdfs://master:54310/hbase/data/default/usertable/1dc0aa61f0e3f43a53375118eb48213f/recovered.edits/0000000000000000088.temp region=1dc0aa61f0e3f43a53375118eb48213f
2014-07-01 15:31:17,731 DEBUG [regionserver60020-EventThread] regionserver.SplitLogWorker: tasks arrived or departed
2014-07-01 15:31:17,836 INFO  [RS_LOG_REPLAY_OPS-slave1:60020-1] wal.HLogSplitter: Finishing writing output logs and closing down.
2014-07-01 15:31:17,836 INFO  [RS_LOG_REPLAY_OPS-slave1:60020-1] wal.HLogSplitter: Waiting for split writer threads to finish
2014-07-01 15:31:17,842 INFO  [RS_LOG_REPLAY_OPS-slave1:60020-1] wal.HLogSplitter: Split writers finished
2014-07-01 15:31:17,842 DEBUG [RS_LOG_REPLAY_OPS-slave1:60020-1] wal.HLogSplitter: Submitting close of hdfs://master:54310/hbase/data/default/usertable/122e92eabfab38cb15a9965b21db25c5/recovered.edits/0000000000000000086.temp
2014-07-01 15:31:17,843 DEBUG [RS_LOG_REPLAY_OPS-slave1:60020-1] wal.HLogSplitter: Submitting close of hdfs://master:54310/hbase/data/default/usertable/1dc0aa61f0e3f43a53375118eb48213f/recovered.edits/0000000000000000088.temp
2014-07-01 15:31:17,843 DEBUG [split-log-closeStream-1] wal.HLogSplitter: Closing hdfs://master:54310/hbase/data/default/usertable/122e92eabfab38cb15a9965b21db25c5/recovered.edits/0000000000000000086.temp
2014-07-01 15:31:17,843 DEBUG [RS_LOG_REPLAY_OPS-slave1:60020-1] wal.HLogSplitter: Submitting close of hdfs://master:54310/hbase/data/default/usertable/52290d336af5a689eef6040bafbe6b15/recovered.edits/0000000000000000085.temp
2014-07-01 15:31:17,843 DEBUG [split-log-closeStream-2] wal.HLogSplitter: Closing hdfs://master:54310/hbase/data/default/usertable/1dc0aa61f0e3f43a53375118eb48213f/recovered.edits/0000000000000000088.temp
2014-07-01 15:31:17,843 DEBUG [RS_LOG_REPLAY_OPS-slave1:60020-1] wal.HLogSplitter: Submitting close of hdfs://master:54310/hbase/data/default/usertable/667c1f2c34fe37d48ba25a789d8aba98/recovered.edits/0000000000000000082.temp
2014-07-01 15:31:17,844 DEBUG [RS_LOG_REPLAY_OPS-slave1:60020-1] wal.HLogSplitter: Submitting close of hdfs://master:54310/hbase/data/default/usertable/93253e37c284f3174309fede3f32339f/recovered.edits/0000000000000000088.temp
2014-07-01 15:31:17,844 DEBUG [RS_LOG_REPLAY_OPS-slave1:60020-1] wal.HLogSplitter: Submitting close of hdfs://master:54310/hbase/data/default/usertable/99b73df170b514f6773e742a495216c4/recovered.edits/0000000000000000081.temp
2014-07-01 15:31:17,844 DEBUG [split-log-closeStream-3] wal.HLogSplitter: Closing hdfs://master:54310/hbase/data/default/usertable/52290d336af5a689eef6040bafbe6b15/recovered.edits/0000000000000000085.temp
2014-07-01 15:31:17,844 DEBUG [RS_LOG_REPLAY_OPS-slave1:60020-1] wal.HLogSplitter: Submitting close of hdfs://master:54310/hbase/data/default/usertable/9cef9e7d34f96f1d9b111be50040a208/recovered.edits/0000000000000000091.temp
2014-07-01 15:31:17,844 DEBUG [RS_LOG_REPLAY_OPS-slave1:60020-1] wal.HLogSplitter: Submitting close of hdfs://master:54310/hbase/data/default/usertable/bb79ae773f71289e0745068f68a0abd9/recovered.edits/0000000000000000078.temp
2014-07-01 15:31:17,844 DEBUG [RS_LOG_REPLAY_OPS-slave1:60020-1] wal.HLogSplitter: Submitting close of hdfs://master:54310/hbase/data/default/usertable/c72fc9d54f680bcd54e9fa888600346b/recovered.edits/0000000000000000089.temp
2014-07-01 15:31:17,852 INFO  [split-log-closeStream-1] wal.HLogSplitter: Closed wap hdfs://master:54310/hbase/data/default/usertable/122e92eabfab38cb15a9965b21db25c5/recovered.edits/0000000000000000086.temp (wrote 9 edits in 77ms)
2014-07-01 15:31:17,854 INFO  [split-log-closeStream-2] wal.HLogSplitter: Closed wap hdfs://master:54310/hbase/data/default/usertable/1dc0aa61f0e3f43a53375118eb48213f/recovered.edits/0000000000000000088.temp (wrote 10 edits in 93ms)
2014-07-01 15:31:17,855 INFO  [split-log-closeStream-3] wal.HLogSplitter: Closed wap hdfs://master:54310/hbase/data/default/usertable/52290d336af5a689eef6040bafbe6b15/recovered.edits/0000000000000000085.temp (wrote 9 edits in 103ms)
2014-07-01 15:31:17,860 DEBUG [split-log-closeStream-1] wal.HLogSplitter: Rename hdfs://master:54310/hbase/data/default/usertable/122e92eabfab38cb15a9965b21db25c5/recovered.edits/0000000000000000086.temp to hdfs://master:54310/hbase/data/default/usertable/122e92eabfab38cb15a9965b21db25c5/recovered.edits/0000000000000000094
2014-07-01 15:31:17,860 DEBUG [split-log-closeStream-1] wal.HLogSplitter: Closing hdfs://master:54310/hbase/data/default/usertable/667c1f2c34fe37d48ba25a789d8aba98/recovered.edits/0000000000000000082.temp
2014-07-01 15:31:17,860 DEBUG [split-log-closeStream-2] wal.HLogSplitter: Rename hdfs://master:54310/hbase/data/default/usertable/1dc0aa61f0e3f43a53375118eb48213f/recovered.edits/0000000000000000088.temp to hdfs://master:54310/hbase/data/default/usertable/1dc0aa61f0e3f43a53375118eb48213f/recovered.edits/0000000000000000097
2014-07-01 15:31:17,860 DEBUG [split-log-closeStream-2] wal.HLogSplitter: Closing hdfs://master:54310/hbase/data/default/usertable/93253e37c284f3174309fede3f32339f/recovered.edits/0000000000000000088.temp
2014-07-01 15:31:17,867 DEBUG [split-log-closeStream-3] wal.HLogSplitter: Rename hdfs://master:54310/hbase/data/default/usertable/52290d336af5a689eef6040bafbe6b15/recovered.edits/0000000000000000085.temp to hdfs://master:54310/hbase/data/default/usertable/52290d336af5a689eef6040bafbe6b15/recovered.edits/0000000000000000093
2014-07-01 15:31:17,867 DEBUG [split-log-closeStream-3] wal.HLogSplitter: Closing hdfs://master:54310/hbase/data/default/usertable/99b73df170b514f6773e742a495216c4/recovered.edits/0000000000000000081.temp
2014-07-01 15:31:17,868 INFO  [split-log-closeStream-1] wal.HLogSplitter: Closed wap hdfs://master:54310/hbase/data/default/usertable/667c1f2c34fe37d48ba25a789d8aba98/recovered.edits/0000000000000000082.temp (wrote 9 edits in 95ms)
2014-07-01 15:31:17,868 INFO  [split-log-closeStream-2] wal.HLogSplitter: Closed wap hdfs://master:54310/hbase/data/default/usertable/93253e37c284f3174309fede3f32339f/recovered.edits/0000000000000000088.temp (wrote 13 edits in 134ms)
2014-07-01 15:31:17,872 DEBUG [split-log-closeStream-1] wal.HLogSplitter: Rename hdfs://master:54310/hbase/data/default/usertable/667c1f2c34fe37d48ba25a789d8aba98/recovered.edits/0000000000000000082.temp to hdfs://master:54310/hbase/data/default/usertable/667c1f2c34fe37d48ba25a789d8aba98/recovered.edits/0000000000000000090
2014-07-01 15:31:17,872 DEBUG [split-log-closeStream-1] wal.HLogSplitter: Closing hdfs://master:54310/hbase/data/default/usertable/9cef9e7d34f96f1d9b111be50040a208/recovered.edits/0000000000000000091.temp
2014-07-01 15:31:17,873 INFO  [split-log-closeStream-3] wal.HLogSplitter: Closed wap hdfs://master:54310/hbase/data/default/usertable/99b73df170b514f6773e742a495216c4/recovered.edits/0000000000000000081.temp (wrote 9 edits in 110ms)
2014-07-01 15:31:17,874 DEBUG [split-log-closeStream-2] wal.HLogSplitter: Rename hdfs://master:54310/hbase/data/default/usertable/93253e37c284f3174309fede3f32339f/recovered.edits/0000000000000000088.temp to hdfs://master:54310/hbase/data/default/usertable/93253e37c284f3174309fede3f32339f/recovered.edits/0000000000000000100
2014-07-01 15:31:17,874 DEBUG [split-log-closeStream-2] wal.HLogSplitter: Closing hdfs://master:54310/hbase/data/default/usertable/bb79ae773f71289e0745068f68a0abd9/recovered.edits/0000000000000000078.temp
2014-07-01 15:31:17,879 INFO  [split-log-closeStream-2] wal.HLogSplitter: Closed wap hdfs://master:54310/hbase/data/default/usertable/bb79ae773f71289e0745068f68a0abd9/recovered.edits/0000000000000000078.temp (wrote 11 edits in 46ms)
2014-07-01 15:31:17,879 INFO  [split-log-closeStream-1] wal.HLogSplitter: Closed wap hdfs://master:54310/hbase/data/default/usertable/9cef9e7d34f96f1d9b111be50040a208/recovered.edits/0000000000000000091.temp (wrote 17 edits in 144ms)
2014-07-01 15:31:17,881 DEBUG [split-log-closeStream-3] wal.HLogSplitter: Rename hdfs://master:54310/hbase/data/default/usertable/99b73df170b514f6773e742a495216c4/recovered.edits/0000000000000000081.temp to hdfs://master:54310/hbase/data/default/usertable/99b73df170b514f6773e742a495216c4/recovered.edits/0000000000000000089
2014-07-01 15:31:17,881 DEBUG [split-log-closeStream-3] wal.HLogSplitter: Closing hdfs://master:54310/hbase/data/default/usertable/c72fc9d54f680bcd54e9fa888600346b/recovered.edits/0000000000000000089.temp
2014-07-01 15:31:17,882 DEBUG [split-log-closeStream-2] wal.HLogSplitter: Rename hdfs://master:54310/hbase/data/default/usertable/bb79ae773f71289e0745068f68a0abd9/recovered.edits/0000000000000000078.temp to hdfs://master:54310/hbase/data/default/usertable/bb79ae773f71289e0745068f68a0abd9/recovered.edits/0000000000000000088
2014-07-01 15:31:17,887 INFO  [split-log-closeStream-3] wal.HLogSplitter: Closed wap hdfs://master:54310/hbase/data/default/usertable/c72fc9d54f680bcd54e9fa888600346b/recovered.edits/0000000000000000089.temp (wrote 15 edits in 136ms)
2014-07-01 15:31:17,888 DEBUG [split-log-closeStream-1] wal.HLogSplitter: Rename hdfs://master:54310/hbase/data/default/usertable/9cef9e7d34f96f1d9b111be50040a208/recovered.edits/0000000000000000091.temp to hdfs://master:54310/hbase/data/default/usertable/9cef9e7d34f96f1d9b111be50040a208/recovered.edits/0000000000000000107
2014-07-01 15:31:17,930 DEBUG [split-log-closeStream-3] wal.HLogSplitter: Rename hdfs://master:54310/hbase/data/default/usertable/c72fc9d54f680bcd54e9fa888600346b/recovered.edits/0000000000000000089.temp to hdfs://master:54310/hbase/data/default/usertable/c72fc9d54f680bcd54e9fa888600346b/recovered.edits/0000000000000000103
2014-07-01 15:31:17,931 INFO  [RS_LOG_REPLAY_OPS-slave1:60020-1] wal.HLogSplitter: Processed 102 edits across 9 regions; log file=hdfs://master:54310/hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1404225100003-splitting/sceplus-vm48.almaden.ibm.com%2C60020%2C1404225100003.1404228308103 is corrupted = false progress failed = false
2014-07-01 15:31:17,936 INFO  [RS_LOG_REPLAY_OPS-slave1:60020-1] handler.HLogSplitterHandler: successfully transitioned task /hbase/splitWAL/WALs%2Fsceplus-vm48.almaden.ibm.com%2C60020%2C1404225100003-splitting%2Fsceplus-vm48.almaden.ibm.com%252C60020%252C1404225100003.1404228308103 to final state DONE slave1,60020,1404253868846
2014-07-01 15:31:17,936 INFO  [RS_LOG_REPLAY_OPS-slave1:60020-1] handler.HLogSplitterHandler: worker slave1,60020,1404253868846 done with task /hbase/splitWAL/WALs%2Fsceplus-vm48.almaden.ibm.com%2C60020%2C1404225100003-splitting%2Fsceplus-vm48.almaden.ibm.com%252C60020%252C1404225100003.1404228308103 in 672ms
2014-07-01 15:31:18,091 INFO  [SplitLogWorker-slave1,60020,1404253868846] regionserver.SplitLogWorker: worker slave1,60020,1404253868846 acquired task /hbase/splitWAL/WALs%2Fsceplus-vm48.almaden.ibm.com%2C60020%2C1404225100003-splitting%2Fsceplus-vm48.almaden.ibm.com%252C60020%252C1404225100003.1404228312119
2014-07-01 15:31:18,111 INFO  [RS_LOG_REPLAY_OPS-slave1:60020-0] wal.HLogSplitter: Splitting hlog: hdfs://master:54310/hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1404225100003-splitting/sceplus-vm48.almaden.ibm.com%2C60020%2C1404225100003.1404228312119, length=72537683
2014-07-01 15:31:18,112 INFO  [RS_LOG_REPLAY_OPS-slave1:60020-0] wal.HLogSplitter: DistributedLogReplay = false
2014-07-01 15:31:18,114 INFO  [RS_LOG_REPLAY_OPS-slave1:60020-0] util.FSHDFSUtils: Recovering lease on dfs file hdfs://master:54310/hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1404225100003-splitting/sceplus-vm48.almaden.ibm.com%2C60020%2C1404225100003.1404228312119
2014-07-01 15:31:18,116 INFO  [RS_LOG_REPLAY_OPS-slave1:60020-0] util.FSHDFSUtils: recoverLease=true, attempt=0 on file=hdfs://master:54310/hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1404225100003-splitting/sceplus-vm48.almaden.ibm.com%2C60020%2C1404225100003.1404228312119 after 1ms
2014-07-01 15:31:18,129 DEBUG [RS_LOG_REPLAY_OPS-slave1:60020-0-Writer-0] wal.HLogSplitter: Writer thread Thread[RS_LOG_REPLAY_OPS-slave1:60020-0-Writer-0,5,main]: starting
2014-07-01 15:31:18,129 DEBUG [RS_LOG_REPLAY_OPS-slave1:60020-0-Writer-1] wal.HLogSplitter: Writer thread Thread[RS_LOG_REPLAY_OPS-slave1:60020-0-Writer-1,5,main]: starting
2014-07-01 15:31:18,129 DEBUG [RS_LOG_REPLAY_OPS-slave1:60020-0-Writer-2] wal.HLogSplitter: Writer thread Thread[RS_LOG_REPLAY_OPS-slave1:60020-0-Writer-2,5,main]: starting
2014-07-01 15:31:18,155 DEBUG [RS_LOG_REPLAY_OPS-slave1:60020-0-Writer-2] wal.HLogSplitter: Creating writer path=hdfs://master:54310/hbase/data/default/usertable/52290d336af5a689eef6040bafbe6b15/recovered.edits/0000000000000000094.temp region=52290d336af5a689eef6040bafbe6b15
2014-07-01 15:31:18,163 DEBUG [RS_LOG_REPLAY_OPS-slave1:60020-0-Writer-0] wal.HLogSplitter: Creating writer path=hdfs://master:54310/hbase/data/default/usertable/667c1f2c34fe37d48ba25a789d8aba98/recovered.edits/0000000000000000091.temp region=667c1f2c34fe37d48ba25a789d8aba98
2014-07-01 15:31:18,179 DEBUG [RS_LOG_REPLAY_OPS-slave1:60020-0-Writer-1] wal.HLogSplitter: Creating writer path=hdfs://master:54310/hbase/data/default/usertable/122e92eabfab38cb15a9965b21db25c5/recovered.edits/0000000000000000095.temp region=122e92eabfab38cb15a9965b21db25c5
2014-07-01 15:31:18,187 DEBUG [RS_LOG_REPLAY_OPS-slave1:60020-0-Writer-0] wal.HLogSplitter: Creating writer path=hdfs://master:54310/hbase/data/default/usertable/c72fc9d54f680bcd54e9fa888600346b/recovered.edits/0000000000000000104.temp region=c72fc9d54f680bcd54e9fa888600346b
2014-07-01 15:31:18,192 DEBUG [RS_LOG_REPLAY_OPS-slave1:60020-0-Writer-2] wal.HLogSplitter: Creating writer path=hdfs://master:54310/hbase/data/default/usertable/9cef9e7d34f96f1d9b111be50040a208/recovered.edits/0000000000000000108.temp region=9cef9e7d34f96f1d9b111be50040a208
2014-07-01 15:31:18,261 DEBUG [RS_LOG_REPLAY_OPS-slave1:60020-0-Writer-1] wal.HLogSplitter: Creating writer path=hdfs://master:54310/hbase/data/default/usertable/1dc0aa61f0e3f43a53375118eb48213f/recovered.edits/0000000000000000098.temp region=1dc0aa61f0e3f43a53375118eb48213f
2014-07-01 15:31:18,270 DEBUG [RS_LOG_REPLAY_OPS-slave1:60020-0-Writer-0] wal.HLogSplitter: Creating writer path=hdfs://master:54310/hbase/data/default/usertable/93253e37c284f3174309fede3f32339f/recovered.edits/0000000000000000101.temp region=93253e37c284f3174309fede3f32339f
2014-07-01 15:31:18,285 DEBUG [RS_LOG_REPLAY_OPS-slave1:60020-0-Writer-2] wal.HLogSplitter: Creating writer path=hdfs://master:54310/hbase/data/default/usertable/bb79ae773f71289e0745068f68a0abd9/recovered.edits/0000000000000000089.temp region=bb79ae773f71289e0745068f68a0abd9
2014-07-01 15:31:18,330 DEBUG [RS_LOG_REPLAY_OPS-slave1:60020-0-Writer-1] wal.HLogSplitter: Creating writer path=hdfs://master:54310/hbase/data/default/usertable/99b73df170b514f6773e742a495216c4/recovered.edits/0000000000000000090.temp region=99b73df170b514f6773e742a495216c4
2014-07-01 15:31:18,718 INFO  [RS_LOG_REPLAY_OPS-slave1:60020-0] wal.HLogSplitter: Finishing writing output logs and closing down.
2014-07-01 15:31:18,718 INFO  [RS_LOG_REPLAY_OPS-slave1:60020-0] wal.HLogSplitter: Waiting for split writer threads to finish
2014-07-01 15:31:18,735 INFO  [RS_LOG_REPLAY_OPS-slave1:60020-0] wal.HLogSplitter: Split writers finished
2014-07-01 15:31:18,735 DEBUG [RS_LOG_REPLAY_OPS-slave1:60020-0] wal.HLogSplitter: Submitting close of hdfs://master:54310/hbase/data/default/usertable/122e92eabfab38cb15a9965b21db25c5/recovered.edits/0000000000000000095.temp
2014-07-01 15:31:18,735 DEBUG [RS_LOG_REPLAY_OPS-slave1:60020-0] wal.HLogSplitter: Submitting close of hdfs://master:54310/hbase/data/default/usertable/1dc0aa61f0e3f43a53375118eb48213f/recovered.edits/0000000000000000098.temp
2014-07-01 15:31:18,736 DEBUG [split-log-closeStream-1] wal.HLogSplitter: Closing hdfs://master:54310/hbase/data/default/usertable/122e92eabfab38cb15a9965b21db25c5/recovered.edits/0000000000000000095.temp
2014-07-01 15:31:18,736 DEBUG [RS_LOG_REPLAY_OPS-slave1:60020-0] wal.HLogSplitter: Submitting close of hdfs://master:54310/hbase/data/default/usertable/52290d336af5a689eef6040bafbe6b15/recovered.edits/0000000000000000094.temp
2014-07-01 15:31:18,736 DEBUG [split-log-closeStream-2] wal.HLogSplitter: Closing hdfs://master:54310/hbase/data/default/usertable/1dc0aa61f0e3f43a53375118eb48213f/recovered.edits/0000000000000000098.temp
2014-07-01 15:31:18,736 DEBUG [RS_LOG_REPLAY_OPS-slave1:60020-0] wal.HLogSplitter: Submitting close of hdfs://master:54310/hbase/data/default/usertable/667c1f2c34fe37d48ba25a789d8aba98/recovered.edits/0000000000000000091.temp
2014-07-01 15:31:18,736 DEBUG [RS_LOG_REPLAY_OPS-slave1:60020-0] wal.HLogSplitter: Submitting close of hdfs://master:54310/hbase/data/default/usertable/93253e37c284f3174309fede3f32339f/recovered.edits/0000000000000000101.temp
2014-07-01 15:31:18,736 DEBUG [RS_LOG_REPLAY_OPS-slave1:60020-0] wal.HLogSplitter: Submitting close of hdfs://master:54310/hbase/data/default/usertable/99b73df170b514f6773e742a495216c4/recovered.edits/0000000000000000090.temp
2014-07-01 15:31:18,736 DEBUG [split-log-closeStream-3] wal.HLogSplitter: Closing hdfs://master:54310/hbase/data/default/usertable/52290d336af5a689eef6040bafbe6b15/recovered.edits/0000000000000000094.temp
2014-07-01 15:31:18,736 DEBUG [RS_LOG_REPLAY_OPS-slave1:60020-0] wal.HLogSplitter: Submitting close of hdfs://master:54310/hbase/data/default/usertable/9cef9e7d34f96f1d9b111be50040a208/recovered.edits/0000000000000000108.temp
2014-07-01 15:31:18,736 DEBUG [RS_LOG_REPLAY_OPS-slave1:60020-0] wal.HLogSplitter: Submitting close of hdfs://master:54310/hbase/data/default/usertable/bb79ae773f71289e0745068f68a0abd9/recovered.edits/0000000000000000089.temp
2014-07-01 15:31:18,736 DEBUG [RS_LOG_REPLAY_OPS-slave1:60020-0] wal.HLogSplitter: Submitting close of hdfs://master:54310/hbase/data/default/usertable/c72fc9d54f680bcd54e9fa888600346b/recovered.edits/0000000000000000104.temp
2014-07-01 15:31:18,742 INFO  [split-log-closeStream-2] wal.HLogSplitter: Closed wap hdfs://master:54310/hbase/data/default/usertable/1dc0aa61f0e3f43a53375118eb48213f/recovered.edits/0000000000000000098.temp (wrote 11 edits in 189ms)
2014-07-01 15:31:18,744 INFO  [split-log-closeStream-3] wal.HLogSplitter: Closed wap hdfs://master:54310/hbase/data/default/usertable/52290d336af5a689eef6040bafbe6b15/recovered.edits/0000000000000000094.temp (wrote 14 edits in 154ms)
2014-07-01 15:31:18,744 INFO  [split-log-closeStream-1] wal.HLogSplitter: Closed wap hdfs://master:54310/hbase/data/default/usertable/122e92eabfab38cb15a9965b21db25c5/recovered.edits/0000000000000000095.temp (wrote 14 edits in 146ms)
2014-07-01 15:31:18,750 DEBUG [split-log-closeStream-2] wal.HLogSplitter: Rename hdfs://master:54310/hbase/data/default/usertable/1dc0aa61f0e3f43a53375118eb48213f/recovered.edits/0000000000000000098.temp to hdfs://master:54310/hbase/data/default/usertable/1dc0aa61f0e3f43a53375118eb48213f/recovered.edits/0000000000000000108
2014-07-01 15:31:18,750 DEBUG [split-log-closeStream-2] wal.HLogSplitter: Closing hdfs://master:54310/hbase/data/default/usertable/667c1f2c34fe37d48ba25a789d8aba98/recovered.edits/0000000000000000091.temp
2014-07-01 15:31:18,750 DEBUG [split-log-closeStream-3] wal.HLogSplitter: Rename hdfs://master:54310/hbase/data/default/usertable/52290d336af5a689eef6040bafbe6b15/recovered.edits/0000000000000000094.temp to hdfs://master:54310/hbase/data/default/usertable/52290d336af5a689eef6040bafbe6b15/recovered.edits/0000000000000000107
2014-07-01 15:31:18,751 DEBUG [split-log-closeStream-3] wal.HLogSplitter: Closing hdfs://master:54310/hbase/data/default/usertable/93253e37c284f3174309fede3f32339f/recovered.edits/0000000000000000101.temp
2014-07-01 15:31:18,753 DEBUG [split-log-closeStream-1] wal.HLogSplitter: Rename hdfs://master:54310/hbase/data/default/usertable/122e92eabfab38cb15a9965b21db25c5/recovered.edits/0000000000000000095.temp to hdfs://master:54310/hbase/data/default/usertable/122e92eabfab38cb15a9965b21db25c5/recovered.edits/0000000000000000108
2014-07-01 15:31:18,753 DEBUG [split-log-closeStream-1] wal.HLogSplitter: Closing hdfs://master:54310/hbase/data/default/usertable/99b73df170b514f6773e742a495216c4/recovered.edits/0000000000000000090.temp
2014-07-01 15:31:18,756 INFO  [split-log-closeStream-2] wal.HLogSplitter: Closed wap hdfs://master:54310/hbase/data/default/usertable/667c1f2c34fe37d48ba25a789d8aba98/recovered.edits/0000000000000000091.temp (wrote 10 edits in 85ms)
2014-07-01 15:31:18,758 INFO  [split-log-closeStream-1] wal.HLogSplitter: Closed wap hdfs://master:54310/hbase/data/default/usertable/99b73df170b514f6773e742a495216c4/recovered.edits/0000000000000000090.temp (wrote 9 edits in 124ms)
2014-07-01 15:31:18,758 INFO  [split-log-closeStream-3] wal.HLogSplitter: Closed wap hdfs://master:54310/hbase/data/default/usertable/93253e37c284f3174309fede3f32339f/recovered.edits/0000000000000000101.temp (wrote 10 edits in 149ms)
2014-07-01 15:31:18,761 DEBUG [split-log-closeStream-2] wal.HLogSplitter: Rename hdfs://master:54310/hbase/data/default/usertable/667c1f2c34fe37d48ba25a789d8aba98/recovered.edits/0000000000000000091.temp to hdfs://master:54310/hbase/data/default/usertable/667c1f2c34fe37d48ba25a789d8aba98/recovered.edits/0000000000000000100
2014-07-01 15:31:18,761 DEBUG [split-log-closeStream-2] wal.HLogSplitter: Closing hdfs://master:54310/hbase/data/default/usertable/9cef9e7d34f96f1d9b111be50040a208/recovered.edits/0000000000000000108.temp
2014-07-01 15:31:18,762 DEBUG [split-log-closeStream-1] wal.HLogSplitter: Rename hdfs://master:54310/hbase/data/default/usertable/99b73df170b514f6773e742a495216c4/recovered.edits/0000000000000000090.temp to hdfs://master:54310/hbase/data/default/usertable/99b73df170b514f6773e742a495216c4/recovered.edits/0000000000000000098
2014-07-01 15:31:18,762 DEBUG [split-log-closeStream-1] wal.HLogSplitter: Closing hdfs://master:54310/hbase/data/default/usertable/bb79ae773f71289e0745068f68a0abd9/recovered.edits/0000000000000000089.temp
2014-07-01 15:31:18,764 DEBUG [split-log-closeStream-3] wal.HLogSplitter: Rename hdfs://master:54310/hbase/data/default/usertable/93253e37c284f3174309fede3f32339f/recovered.edits/0000000000000000101.temp to hdfs://master:54310/hbase/data/default/usertable/93253e37c284f3174309fede3f32339f/recovered.edits/0000000000000000110
2014-07-01 15:31:18,764 DEBUG [split-log-closeStream-3] wal.HLogSplitter: Closing hdfs://master:54310/hbase/data/default/usertable/c72fc9d54f680bcd54e9fa888600346b/recovered.edits/0000000000000000104.temp
2014-07-01 15:31:18,767 INFO  [split-log-closeStream-2] wal.HLogSplitter: Closed wap hdfs://master:54310/hbase/data/default/usertable/9cef9e7d34f96f1d9b111be50040a208/recovered.edits/0000000000000000108.temp (wrote 3 edits in 57ms)
2014-07-01 15:31:18,768 INFO  [split-log-closeStream-1] wal.HLogSplitter: Closed wap hdfs://master:54310/hbase/data/default/usertable/bb79ae773f71289e0745068f68a0abd9/recovered.edits/0000000000000000089.temp (wrote 9 edits in 56ms)
2014-07-01 15:31:18,769 INFO  [split-log-closeStream-3] wal.HLogSplitter: Closed wap hdfs://master:54310/hbase/data/default/usertable/c72fc9d54f680bcd54e9fa888600346b/recovered.edits/0000000000000000104.temp (wrote 7 edits in 105ms)
2014-07-01 15:31:18,778 DEBUG [split-log-closeStream-2] wal.HLogSplitter: Rename hdfs://master:54310/hbase/data/default/usertable/9cef9e7d34f96f1d9b111be50040a208/recovered.edits/0000000000000000108.temp to hdfs://master:54310/hbase/data/default/usertable/9cef9e7d34f96f1d9b111be50040a208/recovered.edits/0000000000000000110
2014-07-01 15:31:18,781 DEBUG [split-log-closeStream-1] wal.HLogSplitter: Rename hdfs://master:54310/hbase/data/default/usertable/bb79ae773f71289e0745068f68a0abd9/recovered.edits/0000000000000000089.temp to hdfs://master:54310/hbase/data/default/usertable/bb79ae773f71289e0745068f68a0abd9/recovered.edits/0000000000000000097
2014-07-01 15:31:18,819 DEBUG [split-log-closeStream-3] wal.HLogSplitter: Rename hdfs://master:54310/hbase/data/default/usertable/c72fc9d54f680bcd54e9fa888600346b/recovered.edits/0000000000000000104.temp to hdfs://master:54310/hbase/data/default/usertable/c72fc9d54f680bcd54e9fa888600346b/recovered.edits/0000000000000000110
2014-07-01 15:31:18,823 INFO  [RS_LOG_REPLAY_OPS-slave1:60020-0] wal.HLogSplitter: Processed 87 edits across 9 regions; log file=hdfs://master:54310/hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1404225100003-splitting/sceplus-vm48.almaden.ibm.com%2C60020%2C1404225100003.1404228312119 is corrupted = false progress failed = false
2014-07-01 15:31:18,829 INFO  [RS_LOG_REPLAY_OPS-slave1:60020-0] handler.HLogSplitterHandler: successfully transitioned task /hbase/splitWAL/WALs%2Fsceplus-vm48.almaden.ibm.com%2C60020%2C1404225100003-splitting%2Fsceplus-vm48.almaden.ibm.com%252C60020%252C1404225100003.1404228312119 to final state DONE slave1,60020,1404253868846
2014-07-01 15:31:18,829 INFO  [RS_LOG_REPLAY_OPS-slave1:60020-0] handler.HLogSplitterHandler: worker slave1,60020,1404253868846 done with task /hbase/splitWAL/WALs%2Fsceplus-vm48.almaden.ibm.com%2C60020%2C1404225100003-splitting%2Fsceplus-vm48.almaden.ibm.com%252C60020%252C1404225100003.1404228312119 in 737ms
2014-07-01 15:31:18,961 INFO  [SplitLogWorker-slave1,60020,1404253868846] regionserver.SplitLogWorker: worker slave1,60020,1404253868846 acquired task /hbase/splitWAL/WALs%2Fsceplus-vm48.almaden.ibm.com%2C60020%2C1404225100003-splitting%2Fsceplus-vm48.almaden.ibm.com%252C60020%252C1404225100003.1404228315796
2014-07-01 15:31:18,987 INFO  [RS_LOG_REPLAY_OPS-slave1:60020-1] wal.HLogSplitter: Splitting hlog: hdfs://master:54310/hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1404225100003-splitting/sceplus-vm48.almaden.ibm.com%2C60020%2C1404225100003.1404228315796, length=67108864
2014-07-01 15:31:18,987 INFO  [RS_LOG_REPLAY_OPS-slave1:60020-1] wal.HLogSplitter: DistributedLogReplay = false
2014-07-01 15:31:18,992 INFO  [RS_LOG_REPLAY_OPS-slave1:60020-1] util.FSHDFSUtils: Recovering lease on dfs file hdfs://master:54310/hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1404225100003-splitting/sceplus-vm48.almaden.ibm.com%2C60020%2C1404225100003.1404228315796
2014-07-01 15:31:18,994 INFO  [RS_LOG_REPLAY_OPS-slave1:60020-1] util.FSHDFSUtils: recoverLease=true, attempt=0 on file=hdfs://master:54310/hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1404225100003-splitting/sceplus-vm48.almaden.ibm.com%2C60020%2C1404225100003.1404228315796 after 2ms
2014-07-01 15:31:19,004 DEBUG [RS_LOG_REPLAY_OPS-slave1:60020-1-Writer-0] wal.HLogSplitter: Writer thread Thread[RS_LOG_REPLAY_OPS-slave1:60020-1-Writer-0,5,main]: starting
2014-07-01 15:31:19,004 DEBUG [RS_LOG_REPLAY_OPS-slave1:60020-1-Writer-1] wal.HLogSplitter: Writer thread Thread[RS_LOG_REPLAY_OPS-slave1:60020-1-Writer-1,5,main]: starting
2014-07-01 15:31:19,004 DEBUG [RS_LOG_REPLAY_OPS-slave1:60020-1-Writer-2] wal.HLogSplitter: Writer thread Thread[RS_LOG_REPLAY_OPS-slave1:60020-1-Writer-2,5,main]: starting
2014-07-01 15:31:19,031 DEBUG [RS_LOG_REPLAY_OPS-slave1:60020-1-Writer-2] wal.HLogSplitter: Creating writer path=hdfs://master:54310/hbase/data/default/usertable/bb79ae773f71289e0745068f68a0abd9/recovered.edits/0000000000000000098.temp region=bb79ae773f71289e0745068f68a0abd9
2014-07-01 15:31:19,041 DEBUG [RS_LOG_REPLAY_OPS-slave1:60020-1-Writer-0] wal.HLogSplitter: Creating writer path=hdfs://master:54310/hbase/data/default/usertable/667c1f2c34fe37d48ba25a789d8aba98/recovered.edits/0000000000000000101.temp region=667c1f2c34fe37d48ba25a789d8aba98
2014-07-01 15:31:19,047 DEBUG [RS_LOG_REPLAY_OPS-slave1:60020-1-Writer-1] wal.HLogSplitter: Creating writer path=hdfs://master:54310/hbase/data/default/usertable/1dc0aa61f0e3f43a53375118eb48213f/recovered.edits/0000000000000000109.temp region=1dc0aa61f0e3f43a53375118eb48213f
2014-07-01 15:31:19,067 DEBUG [RS_LOG_REPLAY_OPS-slave1:60020-1-Writer-2] wal.HLogSplitter: Creating writer path=hdfs://master:54310/hbase/data/default/usertable/99b73df170b514f6773e742a495216c4/recovered.edits/0000000000000000099.temp region=99b73df170b514f6773e742a495216c4
2014-07-01 15:31:19,068 DEBUG [RS_LOG_REPLAY_OPS-slave1:60020-1-Writer-1] wal.HLogSplitter: Creating writer path=hdfs://master:54310/hbase/data/default/usertable/52290d336af5a689eef6040bafbe6b15/recovered.edits/0000000000000000108.temp region=52290d336af5a689eef6040bafbe6b15
2014-07-01 15:31:19,147 DEBUG [RS_LOG_REPLAY_OPS-slave1:60020-1-Writer-0] wal.HLogSplitter: Creating writer path=hdfs://master:54310/hbase/data/default/usertable/122e92eabfab38cb15a9965b21db25c5/recovered.edits/0000000000000000109.temp region=122e92eabfab38cb15a9965b21db25c5
2014-07-01 15:31:19,285 DEBUG [RS_LOG_REPLAY_OPS-slave1:60020-1-Writer-1] wal.HLogSplitter: Creating writer path=hdfs://master:54310/hbase/data/default/usertable/9cef9e7d34f96f1d9b111be50040a208/recovered.edits/0000000000000000111.temp region=9cef9e7d34f96f1d9b111be50040a208
2014-07-01 15:31:19,300 DEBUG [RS_LOG_REPLAY_OPS-slave1:60020-1-Writer-0] wal.HLogSplitter: Creating writer path=hdfs://master:54310/hbase/data/default/usertable/c72fc9d54f680bcd54e9fa888600346b/recovered.edits/0000000000000000111.temp region=c72fc9d54f680bcd54e9fa888600346b
2014-07-01 15:31:19,306 DEBUG [RS_LOG_REPLAY_OPS-slave1:60020-1-Writer-2] wal.HLogSplitter: Creating writer path=hdfs://master:54310/hbase/data/default/usertable/93253e37c284f3174309fede3f32339f/recovered.edits/0000000000000000111.temp region=93253e37c284f3174309fede3f32339f
2014-07-01 15:31:19,499 ERROR [RS_LOG_REPLAY_OPS-slave1:60020-1] codec.BaseDecoder: Partial cell read caused by EOF: java.io.IOException: Premature EOF from inputStream
2014-07-01 15:31:19,499 INFO  [RS_LOG_REPLAY_OPS-slave1:60020-1] wal.HLogSplitter: Finishing writing output logs and closing down.
2014-07-01 15:31:19,499 INFO  [RS_LOG_REPLAY_OPS-slave1:60020-1] wal.HLogSplitter: Waiting for split writer threads to finish
2014-07-01 15:31:19,504 INFO  [RS_LOG_REPLAY_OPS-slave1:60020-1] wal.HLogSplitter: Split writers finished
2014-07-01 15:31:19,504 DEBUG [RS_LOG_REPLAY_OPS-slave1:60020-1] wal.HLogSplitter: Submitting close of hdfs://master:54310/hbase/data/default/usertable/122e92eabfab38cb15a9965b21db25c5/recovered.edits/0000000000000000109.temp
2014-07-01 15:31:19,504 DEBUG [RS_LOG_REPLAY_OPS-slave1:60020-1] wal.HLogSplitter: Submitting close of hdfs://master:54310/hbase/data/default/usertable/1dc0aa61f0e3f43a53375118eb48213f/recovered.edits/0000000000000000109.temp
2014-07-01 15:31:19,504 DEBUG [split-log-closeStream-1] wal.HLogSplitter: Closing hdfs://master:54310/hbase/data/default/usertable/122e92eabfab38cb15a9965b21db25c5/recovered.edits/0000000000000000109.temp
2014-07-01 15:31:19,505 DEBUG [RS_LOG_REPLAY_OPS-slave1:60020-1] wal.HLogSplitter: Submitting close of hdfs://master:54310/hbase/data/default/usertable/52290d336af5a689eef6040bafbe6b15/recovered.edits/0000000000000000108.temp
2014-07-01 15:31:19,505 DEBUG [split-log-closeStream-2] wal.HLogSplitter: Closing hdfs://master:54310/hbase/data/default/usertable/1dc0aa61f0e3f43a53375118eb48213f/recovered.edits/0000000000000000109.temp
2014-07-01 15:31:19,505 DEBUG [RS_LOG_REPLAY_OPS-slave1:60020-1] wal.HLogSplitter: Submitting close of hdfs://master:54310/hbase/data/default/usertable/667c1f2c34fe37d48ba25a789d8aba98/recovered.edits/0000000000000000101.temp
2014-07-01 15:31:19,505 DEBUG [split-log-closeStream-3] wal.HLogSplitter: Closing hdfs://master:54310/hbase/data/default/usertable/52290d336af5a689eef6040bafbe6b15/recovered.edits/0000000000000000108.temp
2014-07-01 15:31:19,505 DEBUG [RS_LOG_REPLAY_OPS-slave1:60020-1] wal.HLogSplitter: Submitting close of hdfs://master:54310/hbase/data/default/usertable/93253e37c284f3174309fede3f32339f/recovered.edits/0000000000000000111.temp
2014-07-01 15:31:19,505 DEBUG [RS_LOG_REPLAY_OPS-slave1:60020-1] wal.HLogSplitter: Submitting close of hdfs://master:54310/hbase/data/default/usertable/99b73df170b514f6773e742a495216c4/recovered.edits/0000000000000000099.temp
2014-07-01 15:31:19,505 DEBUG [RS_LOG_REPLAY_OPS-slave1:60020-1] wal.HLogSplitter: Submitting close of hdfs://master:54310/hbase/data/default/usertable/9cef9e7d34f96f1d9b111be50040a208/recovered.edits/0000000000000000111.temp
2014-07-01 15:31:19,505 DEBUG [RS_LOG_REPLAY_OPS-slave1:60020-1] wal.HLogSplitter: Submitting close of hdfs://master:54310/hbase/data/default/usertable/bb79ae773f71289e0745068f68a0abd9/recovered.edits/0000000000000000098.temp
2014-07-01 15:31:19,505 DEBUG [RS_LOG_REPLAY_OPS-slave1:60020-1] wal.HLogSplitter: Submitting close of hdfs://master:54310/hbase/data/default/usertable/c72fc9d54f680bcd54e9fa888600346b/recovered.edits/0000000000000000111.temp
2014-07-01 15:31:19,510 INFO  [split-log-closeStream-1] wal.HLogSplitter: Closed wap hdfs://master:54310/hbase/data/default/usertable/122e92eabfab38cb15a9965b21db25c5/recovered.edits/0000000000000000109.temp (wrote 5 edits in 59ms)
2014-07-01 15:31:19,511 INFO  [split-log-closeStream-2] wal.HLogSplitter: Closed wap hdfs://master:54310/hbase/data/default/usertable/1dc0aa61f0e3f43a53375118eb48213f/recovered.edits/0000000000000000109.temp (wrote 7 edits in 74ms)
2014-07-01 15:31:19,512 INFO  [split-log-closeStream-3] wal.HLogSplitter: Closed wap hdfs://master:54310/hbase/data/default/usertable/52290d336af5a689eef6040bafbe6b15/recovered.edits/0000000000000000108.temp (wrote 6 edits in 91ms)
2014-07-01 15:31:19,516 DEBUG [split-log-closeStream-1] wal.HLogSplitter: Rename hdfs://master:54310/hbase/data/default/usertable/122e92eabfab38cb15a9965b21db25c5/recovered.edits/0000000000000000109.temp to hdfs://master:54310/hbase/data/default/usertable/122e92eabfab38cb15a9965b21db25c5/recovered.edits/0000000000000000113
2014-07-01 15:31:19,516 DEBUG [split-log-closeStream-1] wal.HLogSplitter: Closing hdfs://master:54310/hbase/data/default/usertable/667c1f2c34fe37d48ba25a789d8aba98/recovered.edits/0000000000000000101.temp
2014-07-01 15:31:19,517 DEBUG [split-log-closeStream-2] wal.HLogSplitter: Rename hdfs://master:54310/hbase/data/default/usertable/1dc0aa61f0e3f43a53375118eb48213f/recovered.edits/0000000000000000109.temp to hdfs://master:54310/hbase/data/default/usertable/1dc0aa61f0e3f43a53375118eb48213f/recovered.edits/0000000000000000115
2014-07-01 15:31:19,517 DEBUG [split-log-closeStream-2] wal.HLogSplitter: Closing hdfs://master:54310/hbase/data/default/usertable/93253e37c284f3174309fede3f32339f/recovered.edits/0000000000000000111.temp
2014-07-01 15:31:19,520 DEBUG [split-log-closeStream-3] wal.HLogSplitter: Rename hdfs://master:54310/hbase/data/default/usertable/52290d336af5a689eef6040bafbe6b15/recovered.edits/0000000000000000108.temp to hdfs://master:54310/hbase/data/default/usertable/52290d336af5a689eef6040bafbe6b15/recovered.edits/0000000000000000113
2014-07-01 15:31:19,520 DEBUG [split-log-closeStream-3] wal.HLogSplitter: Closing hdfs://master:54310/hbase/data/default/usertable/99b73df170b514f6773e742a495216c4/recovered.edits/0000000000000000099.temp
2014-07-01 15:31:19,521 INFO  [split-log-closeStream-1] wal.HLogSplitter: Closed wap hdfs://master:54310/hbase/data/default/usertable/667c1f2c34fe37d48ba25a789d8aba98/recovered.edits/0000000000000000101.temp (wrote 12 edits in 129ms)
2014-07-01 15:31:19,523 INFO  [split-log-closeStream-2] wal.HLogSplitter: Closed wap hdfs://master:54310/hbase/data/default/usertable/93253e37c284f3174309fede3f32339f/recovered.edits/0000000000000000111.temp (wrote 7 edits in 77ms)
2014-07-01 15:31:19,524 INFO  [split-log-closeStream-3] wal.HLogSplitter: Closed wap hdfs://master:54310/hbase/data/default/usertable/99b73df170b514f6773e742a495216c4/recovered.edits/0000000000000000099.temp (wrote 14 edits in 131ms)
2014-07-01 15:31:19,527 DEBUG [split-log-closeStream-1] wal.HLogSplitter: Rename hdfs://master:54310/hbase/data/default/usertable/667c1f2c34fe37d48ba25a789d8aba98/recovered.edits/0000000000000000101.temp to hdfs://master:54310/hbase/data/default/usertable/667c1f2c34fe37d48ba25a789d8aba98/recovered.edits/0000000000000000112
2014-07-01 15:31:19,527 DEBUG [split-log-closeStream-1] wal.HLogSplitter: Closing hdfs://master:54310/hbase/data/default/usertable/9cef9e7d34f96f1d9b111be50040a208/recovered.edits/0000000000000000111.temp
2014-07-01 15:31:19,528 DEBUG [split-log-closeStream-2] wal.HLogSplitter: Rename hdfs://master:54310/hbase/data/default/usertable/93253e37c284f3174309fede3f32339f/recovered.edits/0000000000000000111.temp to hdfs://master:54310/hbase/data/default/usertable/93253e37c284f3174309fede3f32339f/recovered.edits/0000000000000000117
2014-07-01 15:31:19,528 DEBUG [split-log-closeStream-2] wal.HLogSplitter: Closing hdfs://master:54310/hbase/data/default/usertable/bb79ae773f71289e0745068f68a0abd9/recovered.edits/0000000000000000098.temp
2014-07-01 15:31:19,531 DEBUG [split-log-closeStream-3] wal.HLogSplitter: Rename hdfs://master:54310/hbase/data/default/usertable/99b73df170b514f6773e742a495216c4/recovered.edits/0000000000000000099.temp to hdfs://master:54310/hbase/data/default/usertable/99b73df170b514f6773e742a495216c4/recovered.edits/0000000000000000112
2014-07-01 15:31:19,531 DEBUG [split-log-closeStream-3] wal.HLogSplitter: Closing hdfs://master:54310/hbase/data/default/usertable/c72fc9d54f680bcd54e9fa888600346b/recovered.edits/0000000000000000111.temp
2014-07-01 15:31:19,532 INFO  [split-log-closeStream-1] wal.HLogSplitter: Closed wap hdfs://master:54310/hbase/data/default/usertable/9cef9e7d34f96f1d9b111be50040a208/recovered.edits/0000000000000000111.temp (wrote 11 edits in 97ms)
2014-07-01 15:31:19,533 INFO  [split-log-closeStream-2] wal.HLogSplitter: Closed wap hdfs://master:54310/hbase/data/default/usertable/bb79ae773f71289e0745068f68a0abd9/recovered.edits/0000000000000000098.temp (wrote 15 edits in 67ms)
2014-07-01 15:31:19,535 INFO  [split-log-closeStream-3] wal.HLogSplitter: Closed wap hdfs://master:54310/hbase/data/default/usertable/c72fc9d54f680bcd54e9fa888600346b/recovered.edits/0000000000000000111.temp (wrote 8 edits in 82ms)
2014-07-01 15:31:19,537 DEBUG [split-log-closeStream-1] wal.HLogSplitter: Rename hdfs://master:54310/hbase/data/default/usertable/9cef9e7d34f96f1d9b111be50040a208/recovered.edits/0000000000000000111.temp to hdfs://master:54310/hbase/data/default/usertable/9cef9e7d34f96f1d9b111be50040a208/recovered.edits/0000000000000000121
2014-07-01 15:31:19,538 DEBUG [split-log-closeStream-2] wal.HLogSplitter: Rename hdfs://master:54310/hbase/data/default/usertable/bb79ae773f71289e0745068f68a0abd9/recovered.edits/0000000000000000098.temp to hdfs://master:54310/hbase/data/default/usertable/bb79ae773f71289e0745068f68a0abd9/recovered.edits/0000000000000000112
2014-07-01 15:31:19,575 DEBUG [split-log-closeStream-3] wal.HLogSplitter: Rename hdfs://master:54310/hbase/data/default/usertable/c72fc9d54f680bcd54e9fa888600346b/recovered.edits/0000000000000000111.temp to hdfs://master:54310/hbase/data/default/usertable/c72fc9d54f680bcd54e9fa888600346b/recovered.edits/0000000000000000118
2014-07-01 15:31:19,575 INFO  [RS_LOG_REPLAY_OPS-slave1:60020-1] wal.HLogSplitter: Processed 85 edits across 9 regions; log file=hdfs://master:54310/hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1404225100003-splitting/sceplus-vm48.almaden.ibm.com%2C60020%2C1404225100003.1404228315796 is corrupted = false progress failed = false
2014-07-01 15:31:19,579 INFO  [RS_LOG_REPLAY_OPS-slave1:60020-1] handler.HLogSplitterHandler: successfully transitioned task /hbase/splitWAL/WALs%2Fsceplus-vm48.almaden.ibm.com%2C60020%2C1404225100003-splitting%2Fsceplus-vm48.almaden.ibm.com%252C60020%252C1404225100003.1404228315796 to final state DONE slave1,60020,1404253868846
2014-07-01 15:31:19,580 INFO  [RS_LOG_REPLAY_OPS-slave1:60020-1] handler.HLogSplitterHandler: worker slave1,60020,1404253868846 done with task /hbase/splitWAL/WALs%2Fsceplus-vm48.almaden.ibm.com%2C60020%2C1404225100003-splitting%2Fsceplus-vm48.almaden.ibm.com%252C60020%252C1404225100003.1404228315796 in 618ms
2014-07-01 15:31:19,674 INFO  [Priority.RpcServer.handler=0,port=60020] regionserver.HRegionServer: Open usertable,,1404228265525.9cef9e7d34f96f1d9b111be50040a208.
2014-07-01 15:31:19,780 INFO  [Priority.RpcServer.handler=0,port=60020] regionserver.HRegionServer: Open usertable,user3,1404228265526.93253e37c284f3174309fede3f32339f.
2014-07-01 15:31:19,780 DEBUG [RS_OPEN_REGION-slave1:60020-0] zookeeper.ZKAssign: regionserver:60020-0x146f40d8cbe0002, quorum=sceplus-vm49.almaden.ibm.com:2181,sceplus-vm48.almaden.ibm.com:2181, baseZNode=/hbase Transitioning 9cef9e7d34f96f1d9b111be50040a208 from M_ZK_REGION_OFFLINE to RS_ZK_REGION_OPENING
2014-07-01 15:31:19,780 INFO  [Priority.RpcServer.handler=0,port=60020] regionserver.HRegionServer: Open usertable,user5,1404228265526.122e92eabfab38cb15a9965b21db25c5.
2014-07-01 15:31:19,782 DEBUG [RS_OPEN_REGION-slave1:60020-1] zookeeper.ZKAssign: regionserver:60020-0x146f40d8cbe0002, quorum=sceplus-vm49.almaden.ibm.com:2181,sceplus-vm48.almaden.ibm.com:2181, baseZNode=/hbase Transitioning 93253e37c284f3174309fede3f32339f from M_ZK_REGION_OFFLINE to RS_ZK_REGION_OPENING
2014-07-01 15:31:19,782 INFO  [Priority.RpcServer.handler=0,port=60020] regionserver.HRegionServer: Open usertable,user7,1404228265526.667c1f2c34fe37d48ba25a789d8aba98.
2014-07-01 15:31:19,783 INFO  [Priority.RpcServer.handler=0,port=60020] regionserver.HRegionServer: Open usertable,user9,1404228265526.bb79ae773f71289e0745068f68a0abd9.
2014-07-01 15:31:19,783 DEBUG [RS_OPEN_REGION-slave1:60020-2] zookeeper.ZKAssign: regionserver:60020-0x146f40d8cbe0002, quorum=sceplus-vm49.almaden.ibm.com:2181,sceplus-vm48.almaden.ibm.com:2181, baseZNode=/hbase Transitioning 122e92eabfab38cb15a9965b21db25c5 from M_ZK_REGION_OFFLINE to RS_ZK_REGION_OPENING
2014-07-01 15:31:19,799 DEBUG [RS_OPEN_REGION-slave1:60020-0] zookeeper.ZKAssign: regionserver:60020-0x146f40d8cbe0002, quorum=sceplus-vm49.almaden.ibm.com:2181,sceplus-vm48.almaden.ibm.com:2181, baseZNode=/hbase Transitioned node 9cef9e7d34f96f1d9b111be50040a208 from M_ZK_REGION_OFFLINE to RS_ZK_REGION_OPENING
2014-07-01 15:31:19,799 DEBUG [RS_OPEN_REGION-slave1:60020-1] zookeeper.ZKAssign: regionserver:60020-0x146f40d8cbe0002, quorum=sceplus-vm49.almaden.ibm.com:2181,sceplus-vm48.almaden.ibm.com:2181, baseZNode=/hbase Transitioned node 93253e37c284f3174309fede3f32339f from M_ZK_REGION_OFFLINE to RS_ZK_REGION_OPENING
2014-07-01 15:31:19,800 DEBUG [RS_OPEN_REGION-slave1:60020-2] zookeeper.ZKAssign: regionserver:60020-0x146f40d8cbe0002, quorum=sceplus-vm49.almaden.ibm.com:2181,sceplus-vm48.almaden.ibm.com:2181, baseZNode=/hbase Transitioned node 122e92eabfab38cb15a9965b21db25c5 from M_ZK_REGION_OFFLINE to RS_ZK_REGION_OPENING
2014-07-01 15:31:19,800 DEBUG [RS_OPEN_REGION-slave1:60020-1] regionserver.HRegion: Opening region: {ENCODED => 93253e37c284f3174309fede3f32339f, NAME => 'usertable,user3,1404228265526.93253e37c284f3174309fede3f32339f.', STARTKEY => 'user3', ENDKEY => 'user4'}
2014-07-01 15:31:19,800 DEBUG [RS_OPEN_REGION-slave1:60020-2] regionserver.HRegion: Opening region: {ENCODED => 122e92eabfab38cb15a9965b21db25c5, NAME => 'usertable,user5,1404228265526.122e92eabfab38cb15a9965b21db25c5.', STARTKEY => 'user5', ENDKEY => 'user6'}
2014-07-01 15:31:19,800 DEBUG [RS_OPEN_REGION-slave1:60020-0] regionserver.HRegion: Opening region: {ENCODED => 9cef9e7d34f96f1d9b111be50040a208, NAME => 'usertable,,1404228265525.9cef9e7d34f96f1d9b111be50040a208.', STARTKEY => '', ENDKEY => 'user2'}
2014-07-01 15:31:19,819 INFO  [RS_OPEN_REGION-slave1:60020-2] impl.MetricsSourceAdapter: MBean for source RegionServer,sub=Regions registered.
2014-07-01 15:31:19,819 DEBUG [RS_OPEN_REGION-slave1:60020-2] regionserver.MetricsRegionSourceImpl: Creating new MetricsRegionSourceImpl for table usertable 122e92eabfab38cb15a9965b21db25c5
2014-07-01 15:31:19,820 DEBUG [RS_OPEN_REGION-slave1:60020-1] regionserver.MetricsRegionSourceImpl: Creating new MetricsRegionSourceImpl for table usertable 93253e37c284f3174309fede3f32339f
2014-07-01 15:31:19,820 DEBUG [RS_OPEN_REGION-slave1:60020-0] regionserver.MetricsRegionSourceImpl: Creating new MetricsRegionSourceImpl for table usertable 9cef9e7d34f96f1d9b111be50040a208
2014-07-01 15:31:19,820 DEBUG [RS_OPEN_REGION-slave1:60020-1] regionserver.HRegion: Instantiated usertable,user3,1404228265526.93253e37c284f3174309fede3f32339f.
2014-07-01 15:31:19,820 DEBUG [RS_OPEN_REGION-slave1:60020-0] regionserver.HRegion: Instantiated usertable,,1404228265525.9cef9e7d34f96f1d9b111be50040a208.
2014-07-01 15:31:19,820 DEBUG [RS_OPEN_REGION-slave1:60020-2] regionserver.HRegion: Instantiated usertable,user5,1404228265526.122e92eabfab38cb15a9965b21db25c5.
2014-07-01 15:31:19,834 INFO  [RS_OPEN_REGION-slave1:60020-1] util.NativeCodeLoader: Loaded the native-hadoop library
2014-07-01 15:31:19,836 INFO  [RS_OPEN_REGION-slave1:60020-1] zlib.ZlibFactory: Successfully loaded & initialized native-zlib library
2014-07-01 15:31:19,839 INFO  [RS_OPEN_REGION-slave1:60020-1] compress.CodecPool: Got brand-new compressor
2014-07-01 15:31:19,839 INFO  [RS_OPEN_REGION-slave1:60020-0] compress.CodecPool: Got brand-new compressor
2014-07-01 15:31:19,839 INFO  [RS_OPEN_REGION-slave1:60020-2] compress.CodecPool: Got brand-new compressor
2014-07-01 15:31:19,920 INFO  [StoreOpener-9cef9e7d34f96f1d9b111be50040a208-1] compactions.CompactionConfiguration: size [268435456, 9223372036854775807); files [3, 10); ratio 1.200000; off-peak ratio 5.000000; throttle point 5368709120; delete expired; major period 0, major jitter 0.500000
2014-07-01 15:31:19,920 INFO  [StoreOpener-122e92eabfab38cb15a9965b21db25c5-1] compactions.CompactionConfiguration: size [268435456, 9223372036854775807); files [3, 10); ratio 1.200000; off-peak ratio 5.000000; throttle point 5368709120; delete expired; major period 0, major jitter 0.500000
2014-07-01 15:31:19,926 INFO  [StoreOpener-93253e37c284f3174309fede3f32339f-1] compactions.CompactionConfiguration: size [268435456, 9223372036854775807); files [3, 10); ratio 1.200000; off-peak ratio 5.000000; throttle point 5368709120; delete expired; major period 0, major jitter 0.500000
2014-07-01 15:31:19,949 INFO  [StoreOpener-9cef9e7d34f96f1d9b111be50040a208-1] util.ChecksumType: Checksum using org.apache.hadoop.util.PureJavaCrc32
2014-07-01 15:31:20,020 DEBUG [RS_OPEN_REGION-slave1:60020-2] regionserver.HRegion: Found 12 recovered edits file(s) under hdfs://master:54310/hbase/data/default/usertable/122e92eabfab38cb15a9965b21db25c5
2014-07-01 15:31:20,023 DEBUG [RS_OPEN_REGION-slave1:60020-0] regionserver.HRegion: Found 12 recovered edits file(s) under hdfs://master:54310/hbase/data/default/usertable/9cef9e7d34f96f1d9b111be50040a208
2014-07-01 15:31:20,027 DEBUG [RS_OPEN_REGION-slave1:60020-1] regionserver.HRegion: Found 12 recovered edits file(s) under hdfs://master:54310/hbase/data/default/usertable/93253e37c284f3174309fede3f32339f
2014-07-01 15:31:20,037 INFO  [RS_OPEN_REGION-slave1:60020-2] regionserver.HRegion: Replaying edits from hdfs://master:54310/hbase/data/default/usertable/122e92eabfab38cb15a9965b21db25c5/recovered.edits/0000000000000000003
2014-07-01 15:31:20,039 INFO  [RS_OPEN_REGION-slave1:60020-0] regionserver.HRegion: Replaying edits from hdfs://master:54310/hbase/data/default/usertable/9cef9e7d34f96f1d9b111be50040a208/recovered.edits/0000000000000000006
2014-07-01 15:31:20,042 INFO  [RS_OPEN_REGION-slave1:60020-1] regionserver.HRegion: Replaying edits from hdfs://master:54310/hbase/data/default/usertable/93253e37c284f3174309fede3f32339f/recovered.edits/0000000000000000004
2014-07-01 15:31:20,346 DEBUG [RS_OPEN_REGION-slave1:60020-2] zookeeper.ZKAssign: regionserver:60020-0x146f40d8cbe0002, quorum=sceplus-vm49.almaden.ibm.com:2181,sceplus-vm48.almaden.ibm.com:2181, baseZNode=/hbase Attempting to retransition opening state of node 122e92eabfab38cb15a9965b21db25c5
2014-07-01 15:31:20,347 DEBUG [RS_OPEN_REGION-slave1:60020-2] regionserver.HRegion: Applied 11800, skipped 0, firstSequenceidInLog=2, maxSequenceidInLog=3, path=hdfs://master:54310/hbase/data/default/usertable/122e92eabfab38cb15a9965b21db25c5/recovered.edits/0000000000000000003
2014-07-01 15:31:20,361 INFO  [RS_OPEN_REGION-slave1:60020-2] regionserver.HRegion: Replaying edits from hdfs://master:54310/hbase/data/default/usertable/122e92eabfab38cb15a9965b21db25c5/recovered.edits/0000000000000000014
2014-07-01 15:31:20,385 DEBUG [RS_OPEN_REGION-slave1:60020-1] zookeeper.ZKAssign: regionserver:60020-0x146f40d8cbe0002, quorum=sceplus-vm49.almaden.ibm.com:2181,sceplus-vm48.almaden.ibm.com:2181, baseZNode=/hbase Attempting to retransition opening state of node 93253e37c284f3174309fede3f32339f
2014-07-01 15:31:20,386 DEBUG [RS_OPEN_REGION-slave1:60020-1] regionserver.HRegion: Applied 16900, skipped 0, firstSequenceidInLog=2, maxSequenceidInLog=4, path=hdfs://master:54310/hbase/data/default/usertable/93253e37c284f3174309fede3f32339f/recovered.edits/0000000000000000004
2014-07-01 15:31:20,391 INFO  [RS_OPEN_REGION-slave1:60020-1] regionserver.HRegion: Replaying edits from hdfs://master:54310/hbase/data/default/usertable/93253e37c284f3174309fede3f32339f/recovered.edits/0000000000000000016
2014-07-01 15:31:20,460 DEBUG [RS_OPEN_REGION-slave1:60020-0] zookeeper.ZKAssign: regionserver:60020-0x146f40d8cbe0002, quorum=sceplus-vm49.almaden.ibm.com:2181,sceplus-vm48.almaden.ibm.com:2181, baseZNode=/hbase Attempting to retransition opening state of node 9cef9e7d34f96f1d9b111be50040a208
2014-07-01 15:31:20,462 DEBUG [RS_OPEN_REGION-slave1:60020-0] regionserver.HRegion: Applied 27820, skipped 0, firstSequenceidInLog=2, maxSequenceidInLog=6, path=hdfs://master:54310/hbase/data/default/usertable/9cef9e7d34f96f1d9b111be50040a208/recovered.edits/0000000000000000006
2014-07-01 15:31:20,467 INFO  [RS_OPEN_REGION-slave1:60020-0] regionserver.HRegion: Replaying edits from hdfs://master:54310/hbase/data/default/usertable/9cef9e7d34f96f1d9b111be50040a208/recovered.edits/0000000000000000021
2014-07-01 15:31:20,746 DEBUG [RS_OPEN_REGION-slave1:60020-2] zookeeper.ZKAssign: regionserver:60020-0x146f40d8cbe0002, quorum=sceplus-vm49.almaden.ibm.com:2181,sceplus-vm48.almaden.ibm.com:2181, baseZNode=/hbase Attempting to retransition opening state of node 122e92eabfab38cb15a9965b21db25c5
2014-07-01 15:31:20,748 DEBUG [RS_OPEN_REGION-slave1:60020-2] regionserver.HRegion: Applied 62390, skipped 0, firstSequenceidInLog=4, maxSequenceidInLog=14, path=hdfs://master:54310/hbase/data/default/usertable/122e92eabfab38cb15a9965b21db25c5/recovered.edits/0000000000000000014
2014-07-01 15:31:20,757 INFO  [RS_OPEN_REGION-slave1:60020-2] regionserver.HRegion: Replaying edits from hdfs://master:54310/hbase/data/default/usertable/122e92eabfab38cb15a9965b21db25c5/recovered.edits/0000000000000000025
2014-07-01 15:31:20,788 DEBUG [RS_OPEN_REGION-slave1:60020-1] zookeeper.ZKAssign: regionserver:60020-0x146f40d8cbe0002, quorum=sceplus-vm49.almaden.ibm.com:2181,sceplus-vm48.almaden.ibm.com:2181, baseZNode=/hbase Attempting to retransition opening state of node 93253e37c284f3174309fede3f32339f
2014-07-01 15:31:20,790 DEBUG [RS_OPEN_REGION-slave1:60020-1] regionserver.HRegion: Applied 67300, skipped 0, firstSequenceidInLog=5, maxSequenceidInLog=16, path=hdfs://master:54310/hbase/data/default/usertable/93253e37c284f3174309fede3f32339f/recovered.edits/0000000000000000016
2014-07-01 15:31:20,793 INFO  [RS_OPEN_REGION-slave1:60020-1] regionserver.HRegion: Replaying edits from hdfs://master:54310/hbase/data/default/usertable/93253e37c284f3174309fede3f32339f/recovered.edits/0000000000000000029
2014-07-01 15:31:21,014 DEBUG [RS_OPEN_REGION-slave1:60020-0] zookeeper.ZKAssign: regionserver:60020-0x146f40d8cbe0002, quorum=sceplus-vm49.almaden.ibm.com:2181,sceplus-vm48.almaden.ibm.com:2181, baseZNode=/hbase Attempting to retransition opening state of node 9cef9e7d34f96f1d9b111be50040a208
2014-07-01 15:31:21,016 DEBUG [RS_OPEN_REGION-slave1:60020-0] regionserver.HRegion: Applied 85550, skipped 0, firstSequenceidInLog=7, maxSequenceidInLog=21, path=hdfs://master:54310/hbase/data/default/usertable/9cef9e7d34f96f1d9b111be50040a208/recovered.edits/0000000000000000021
2014-07-01 15:31:21,019 INFO  [RS_OPEN_REGION-slave1:60020-0] regionserver.HRegion: Replaying edits from hdfs://master:54310/hbase/data/default/usertable/9cef9e7d34f96f1d9b111be50040a208/recovered.edits/0000000000000000034
2014-07-01 15:31:21,159 DEBUG [RS_OPEN_REGION-slave1:60020-2] zookeeper.ZKAssign: regionserver:60020-0x146f40d8cbe0002, quorum=sceplus-vm49.almaden.ibm.com:2181,sceplus-vm48.almaden.ibm.com:2181, baseZNode=/hbase Attempting to retransition opening state of node 122e92eabfab38cb15a9965b21db25c5
2014-07-01 15:31:21,162 DEBUG [RS_OPEN_REGION-slave1:60020-2] regionserver.HRegion: Applied 61870, skipped 0, firstSequenceidInLog=15, maxSequenceidInLog=25, path=hdfs://master:54310/hbase/data/default/usertable/122e92eabfab38cb15a9965b21db25c5/recovered.edits/0000000000000000025
2014-07-01 15:31:21,164 INFO  [RS_OPEN_REGION-slave1:60020-2] regionserver.HRegion: Replaying edits from hdfs://master:54310/hbase/data/default/usertable/122e92eabfab38cb15a9965b21db25c5/recovered.edits/0000000000000000035
2014-07-01 15:31:21,400 DEBUG [RS_OPEN_REGION-slave1:60020-1] zookeeper.ZKAssign: regionserver:60020-0x146f40d8cbe0002, quorum=sceplus-vm49.almaden.ibm.com:2181,sceplus-vm48.almaden.ibm.com:2181, baseZNode=/hbase Attempting to retransition opening state of node 93253e37c284f3174309fede3f32339f
2014-07-01 15:31:21,402 DEBUG [RS_OPEN_REGION-slave1:60020-1] regionserver.HRegion: Applied 74210, skipped 0, firstSequenceidInLog=17, maxSequenceidInLog=29, path=hdfs://master:54310/hbase/data/default/usertable/93253e37c284f3174309fede3f32339f/recovered.edits/0000000000000000029
2014-07-01 15:31:21,404 INFO  [RS_OPEN_REGION-slave1:60020-1] regionserver.HRegion: Replaying edits from hdfs://master:54310/hbase/data/default/usertable/93253e37c284f3174309fede3f32339f/recovered.edits/0000000000000000038
2014-07-01 15:31:21,567 DEBUG [RS_OPEN_REGION-slave1:60020-0] zookeeper.ZKAssign: regionserver:60020-0x146f40d8cbe0002, quorum=sceplus-vm49.almaden.ibm.com:2181,sceplus-vm48.almaden.ibm.com:2181, baseZNode=/hbase Attempting to retransition opening state of node 9cef9e7d34f96f1d9b111be50040a208
2014-07-01 15:31:21,570 DEBUG [RS_OPEN_REGION-slave1:60020-0] regionserver.HRegion: Applied 74400, skipped 0, firstSequenceidInLog=22, maxSequenceidInLog=34, path=hdfs://master:54310/hbase/data/default/usertable/9cef9e7d34f96f1d9b111be50040a208/recovered.edits/0000000000000000034
2014-07-01 15:31:21,574 INFO  [RS_OPEN_REGION-slave1:60020-0] regionserver.HRegion: Replaying edits from hdfs://master:54310/hbase/data/default/usertable/9cef9e7d34f96f1d9b111be50040a208/recovered.edits/0000000000000000042
2014-07-01 15:31:21,713 DEBUG [RS_OPEN_REGION-slave1:60020-2] zookeeper.ZKAssign: regionserver:60020-0x146f40d8cbe0002, quorum=sceplus-vm49.almaden.ibm.com:2181,sceplus-vm48.almaden.ibm.com:2181, baseZNode=/hbase Attempting to retransition opening state of node 122e92eabfab38cb15a9965b21db25c5
2014-07-01 15:31:21,715 DEBUG [RS_OPEN_REGION-slave1:60020-2] regionserver.HRegion: Applied 56770, skipped 0, firstSequenceidInLog=26, maxSequenceidInLog=35, path=hdfs://master:54310/hbase/data/default/usertable/122e92eabfab38cb15a9965b21db25c5/recovered.edits/0000000000000000035
2014-07-01 15:31:21,717 INFO  [RS_OPEN_REGION-slave1:60020-2] regionserver.HRegion: Replaying edits from hdfs://master:54310/hbase/data/default/usertable/122e92eabfab38cb15a9965b21db25c5/recovered.edits/0000000000000000043
2014-07-01 15:31:21,840 DEBUG [RS_OPEN_REGION-slave1:60020-1] zookeeper.ZKAssign: regionserver:60020-0x146f40d8cbe0002, quorum=sceplus-vm49.almaden.ibm.com:2181,sceplus-vm48.almaden.ibm.com:2181, baseZNode=/hbase Attempting to retransition opening state of node 93253e37c284f3174309fede3f32339f
2014-07-01 15:31:21,841 DEBUG [RS_OPEN_REGION-slave1:60020-1] regionserver.HRegion: Applied 50750, skipped 0, firstSequenceidInLog=30, maxSequenceidInLog=38, path=hdfs://master:54310/hbase/data/default/usertable/93253e37c284f3174309fede3f32339f/recovered.edits/0000000000000000038
2014-07-01 15:31:21,844 INFO  [RS_OPEN_REGION-slave1:60020-1] regionserver.HRegion: Replaying edits from hdfs://master:54310/hbase/data/default/usertable/93253e37c284f3174309fede3f32339f/recovered.edits/0000000000000000046
2014-07-01 15:31:21,930 DEBUG [RS_OPEN_REGION-slave1:60020-0] zookeeper.ZKAssign: regionserver:60020-0x146f40d8cbe0002, quorum=sceplus-vm49.almaden.ibm.com:2181,sceplus-vm48.almaden.ibm.com:2181, baseZNode=/hbase Attempting to retransition opening state of node 9cef9e7d34f96f1d9b111be50040a208
2014-07-01 15:31:21,932 DEBUG [RS_OPEN_REGION-slave1:60020-0] regionserver.HRegion: Applied 45060, skipped 0, firstSequenceidInLog=35, maxSequenceidInLog=42, path=hdfs://master:54310/hbase/data/default/usertable/9cef9e7d34f96f1d9b111be50040a208/recovered.edits/0000000000000000042
2014-07-01 15:31:21,935 INFO  [RS_OPEN_REGION-slave1:60020-0] regionserver.HRegion: Replaying edits from hdfs://master:54310/hbase/data/default/usertable/9cef9e7d34f96f1d9b111be50040a208/recovered.edits/0000000000000000049
2014-07-01 15:31:22,077 DEBUG [RS_OPEN_REGION-slave1:60020-2] zookeeper.ZKAssign: regionserver:60020-0x146f40d8cbe0002, quorum=sceplus-vm49.almaden.ibm.com:2181,sceplus-vm48.almaden.ibm.com:2181, baseZNode=/hbase Attempting to retransition opening state of node 122e92eabfab38cb15a9965b21db25c5
2014-07-01 15:31:22,079 DEBUG [RS_OPEN_REGION-slave1:60020-2] regionserver.HRegion: Applied 44860, skipped 0, firstSequenceidInLog=36, maxSequenceidInLog=43, path=hdfs://master:54310/hbase/data/default/usertable/122e92eabfab38cb15a9965b21db25c5/recovered.edits/0000000000000000043
2014-07-01 15:31:22,081 INFO  [RS_OPEN_REGION-slave1:60020-2] regionserver.HRegion: Replaying edits from hdfs://master:54310/hbase/data/default/usertable/122e92eabfab38cb15a9965b21db25c5/recovered.edits/0000000000000000053
2014-07-01 15:31:22,226 DEBUG [RS_OPEN_REGION-slave1:60020-1] zookeeper.ZKAssign: regionserver:60020-0x146f40d8cbe0002, quorum=sceplus-vm49.almaden.ibm.com:2181,sceplus-vm48.almaden.ibm.com:2181, baseZNode=/hbase Attempting to retransition opening state of node 93253e37c284f3174309fede3f32339f
2014-07-01 15:31:22,228 DEBUG [RS_OPEN_REGION-slave1:60020-1] regionserver.HRegion: Applied 45020, skipped 0, firstSequenceidInLog=39, maxSequenceidInLog=46, path=hdfs://master:54310/hbase/data/default/usertable/93253e37c284f3174309fede3f32339f/recovered.edits/0000000000000000046
2014-07-01 15:31:22,232 INFO  [RS_OPEN_REGION-slave1:60020-1] regionserver.HRegion: Replaying edits from hdfs://master:54310/hbase/data/default/usertable/93253e37c284f3174309fede3f32339f/recovered.edits/0000000000000000059
2014-07-01 15:31:22,257 DEBUG [RS_OPEN_REGION-slave1:60020-0] zookeeper.ZKAssign: regionserver:60020-0x146f40d8cbe0002, quorum=sceplus-vm49.almaden.ibm.com:2181,sceplus-vm48.almaden.ibm.com:2181, baseZNode=/hbase Attempting to retransition opening state of node 9cef9e7d34f96f1d9b111be50040a208
2014-07-01 15:31:22,259 DEBUG [RS_OPEN_REGION-slave1:60020-0] regionserver.HRegion: Applied 39640, skipped 0, firstSequenceidInLog=43, maxSequenceidInLog=49, path=hdfs://master:54310/hbase/data/default/usertable/9cef9e7d34f96f1d9b111be50040a208/recovered.edits/0000000000000000049
2014-07-01 15:31:22,262 INFO  [RS_OPEN_REGION-slave1:60020-0] regionserver.HRegion: Replaying edits from hdfs://master:54310/hbase/data/default/usertable/9cef9e7d34f96f1d9b111be50040a208/recovered.edits/0000000000000000063
2014-07-01 15:31:22,594 DEBUG [RS_OPEN_REGION-slave1:60020-2] zookeeper.ZKAssign: regionserver:60020-0x146f40d8cbe0002, quorum=sceplus-vm49.almaden.ibm.com:2181,sceplus-vm48.almaden.ibm.com:2181, baseZNode=/hbase Attempting to retransition opening state of node 122e92eabfab38cb15a9965b21db25c5
2014-07-01 15:31:22,596 DEBUG [RS_OPEN_REGION-slave1:60020-2] regionserver.HRegion: Applied 57090, skipped 0, firstSequenceidInLog=44, maxSequenceidInLog=53, path=hdfs://master:54310/hbase/data/default/usertable/122e92eabfab38cb15a9965b21db25c5/recovered.edits/0000000000000000053
2014-07-01 15:31:22,600 INFO  [RS_OPEN_REGION-slave1:60020-2] regionserver.HRegion: Replaying edits from hdfs://master:54310/hbase/data/default/usertable/122e92eabfab38cb15a9965b21db25c5/recovered.edits/0000000000000000065
2014-07-01 15:31:23,051 DEBUG [RS_OPEN_REGION-slave1:60020-1] zookeeper.ZKAssign: regionserver:60020-0x146f40d8cbe0002, quorum=sceplus-vm49.almaden.ibm.com:2181,sceplus-vm48.almaden.ibm.com:2181, baseZNode=/hbase Attempting to retransition opening state of node 93253e37c284f3174309fede3f32339f
2014-07-01 15:31:23,053 DEBUG [RS_OPEN_REGION-slave1:60020-1] regionserver.HRegion: Applied 74390, skipped 0, firstSequenceidInLog=47, maxSequenceidInLog=59, path=hdfs://master:54310/hbase/data/default/usertable/93253e37c284f3174309fede3f32339f/recovered.edits/0000000000000000059
2014-07-01 15:31:23,055 INFO  [RS_OPEN_REGION-slave1:60020-1] regionserver.HRegion: Replaying edits from hdfs://master:54310/hbase/data/default/usertable/93253e37c284f3174309fede3f32339f/recovered.edits/0000000000000000067
2014-07-01 15:31:23,091 DEBUG [RS_OPEN_REGION-slave1:60020-0] zookeeper.ZKAssign: regionserver:60020-0x146f40d8cbe0002, quorum=sceplus-vm49.almaden.ibm.com:2181,sceplus-vm48.almaden.ibm.com:2181, baseZNode=/hbase Attempting to retransition opening state of node 9cef9e7d34f96f1d9b111be50040a208
2014-07-01 15:31:23,093 DEBUG [RS_OPEN_REGION-slave1:60020-0] regionserver.HRegion: Applied 78660, skipped 0, firstSequenceidInLog=50, maxSequenceidInLog=63, path=hdfs://master:54310/hbase/data/default/usertable/9cef9e7d34f96f1d9b111be50040a208/recovered.edits/0000000000000000063
2014-07-01 15:31:23,095 INFO  [RS_OPEN_REGION-slave1:60020-0] regionserver.HRegion: Replaying edits from hdfs://master:54310/hbase/data/default/usertable/9cef9e7d34f96f1d9b111be50040a208/recovered.edits/0000000000000000071
2014-07-01 15:31:23,307 DEBUG [RS_OPEN_REGION-slave1:60020-2] zookeeper.ZKAssign: regionserver:60020-0x146f40d8cbe0002, quorum=sceplus-vm49.almaden.ibm.com:2181,sceplus-vm48.almaden.ibm.com:2181, baseZNode=/hbase Attempting to retransition opening state of node 122e92eabfab38cb15a9965b21db25c5
2014-07-01 15:31:23,309 DEBUG [RS_OPEN_REGION-slave1:60020-2] regionserver.HRegion: Applied 68140, skipped 0, firstSequenceidInLog=54, maxSequenceidInLog=65, path=hdfs://master:54310/hbase/data/default/usertable/122e92eabfab38cb15a9965b21db25c5/recovered.edits/0000000000000000065
2014-07-01 15:31:23,312 INFO  [RS_OPEN_REGION-slave1:60020-2] regionserver.HRegion: Replaying edits from hdfs://master:54310/hbase/data/default/usertable/122e92eabfab38cb15a9965b21db25c5/recovered.edits/0000000000000000077
2014-07-01 15:31:23,431 DEBUG [RS_OPEN_REGION-slave1:60020-1] zookeeper.ZKAssign: regionserver:60020-0x146f40d8cbe0002, quorum=sceplus-vm49.almaden.ibm.com:2181,sceplus-vm48.almaden.ibm.com:2181, baseZNode=/hbase Attempting to retransition opening state of node 93253e37c284f3174309fede3f32339f
2014-07-01 15:31:23,433 DEBUG [RS_OPEN_REGION-slave1:60020-1] regionserver.HRegion: Applied 44510, skipped 0, firstSequenceidInLog=60, maxSequenceidInLog=67, path=hdfs://master:54310/hbase/data/default/usertable/93253e37c284f3174309fede3f32339f/recovered.edits/0000000000000000067
2014-07-01 15:31:23,436 INFO  [RS_OPEN_REGION-slave1:60020-1] regionserver.HRegion: Replaying edits from hdfs://master:54310/hbase/data/default/usertable/93253e37c284f3174309fede3f32339f/recovered.edits/0000000000000000080
2014-07-01 15:31:23,485 DEBUG [RS_OPEN_REGION-slave1:60020-0] zookeeper.ZKAssign: regionserver:60020-0x146f40d8cbe0002, quorum=sceplus-vm49.almaden.ibm.com:2181,sceplus-vm48.almaden.ibm.com:2181, baseZNode=/hbase Attempting to retransition opening state of node 9cef9e7d34f96f1d9b111be50040a208
2014-07-01 15:31:23,486 DEBUG [RS_OPEN_REGION-slave1:60020-0] regionserver.HRegion: Applied 45190, skipped 0, firstSequenceidInLog=64, maxSequenceidInLog=71, path=hdfs://master:54310/hbase/data/default/usertable/9cef9e7d34f96f1d9b111be50040a208/recovered.edits/0000000000000000071
2014-07-01 15:31:23,488 INFO  [RS_OPEN_REGION-slave1:60020-0] regionserver.HRegion: Replaying edits from hdfs://master:54310/hbase/data/default/usertable/9cef9e7d34f96f1d9b111be50040a208/recovered.edits/0000000000000000085
2014-07-01 15:31:23,904 DEBUG [RS_OPEN_REGION-slave1:60020-2] zookeeper.ZKAssign: regionserver:60020-0x146f40d8cbe0002, quorum=sceplus-vm49.almaden.ibm.com:2181,sceplus-vm48.almaden.ibm.com:2181, baseZNode=/hbase Attempting to retransition opening state of node 122e92eabfab38cb15a9965b21db25c5
2014-07-01 15:31:23,905 DEBUG [RS_OPEN_REGION-slave1:60020-2] regionserver.HRegion: Applied 67990, skipped 0, firstSequenceidInLog=66, maxSequenceidInLog=77, path=hdfs://master:54310/hbase/data/default/usertable/122e92eabfab38cb15a9965b21db25c5/recovered.edits/0000000000000000077
2014-07-01 15:31:23,908 INFO  [RS_OPEN_REGION-slave1:60020-2] regionserver.HRegion: Replaying edits from hdfs://master:54310/hbase/data/default/usertable/122e92eabfab38cb15a9965b21db25c5/recovered.edits/0000000000000000085
2014-07-01 15:31:24,135 DEBUG [RS_OPEN_REGION-slave1:60020-1] zookeeper.ZKAssign: regionserver:60020-0x146f40d8cbe0002, quorum=sceplus-vm49.almaden.ibm.com:2181,sceplus-vm48.almaden.ibm.com:2181, baseZNode=/hbase Attempting to retransition opening state of node 93253e37c284f3174309fede3f32339f
2014-07-01 15:31:24,136 DEBUG [RS_OPEN_REGION-slave1:60020-1] regionserver.HRegion: Applied 73650, skipped 0, firstSequenceidInLog=68, maxSequenceidInLog=80, path=hdfs://master:54310/hbase/data/default/usertable/93253e37c284f3174309fede3f32339f/recovered.edits/0000000000000000080
2014-07-01 15:31:24,139 INFO  [RS_OPEN_REGION-slave1:60020-1] regionserver.HRegion: Replaying edits from hdfs://master:54310/hbase/data/default/usertable/93253e37c284f3174309fede3f32339f/recovered.edits/0000000000000000087
2014-07-01 15:31:24,205 DEBUG [RS_OPEN_REGION-slave1:60020-0] zookeeper.ZKAssign: regionserver:60020-0x146f40d8cbe0002, quorum=sceplus-vm49.almaden.ibm.com:2181,sceplus-vm48.almaden.ibm.com:2181, baseZNode=/hbase Attempting to retransition opening state of node 9cef9e7d34f96f1d9b111be50040a208
2014-07-01 15:31:24,207 DEBUG [RS_OPEN_REGION-slave1:60020-0] regionserver.HRegion: Applied 79150, skipped 0, firstSequenceidInLog=72, maxSequenceidInLog=85, path=hdfs://master:54310/hbase/data/default/usertable/9cef9e7d34f96f1d9b111be50040a208/recovered.edits/0000000000000000085
2014-07-01 15:31:24,210 INFO  [RS_OPEN_REGION-slave1:60020-0] regionserver.HRegion: Replaying edits from hdfs://master:54310/hbase/data/default/usertable/9cef9e7d34f96f1d9b111be50040a208/recovered.edits/0000000000000000090
2014-07-01 15:31:24,383 DEBUG [RS_OPEN_REGION-slave1:60020-2] zookeeper.ZKAssign: regionserver:60020-0x146f40d8cbe0002, quorum=sceplus-vm49.almaden.ibm.com:2181,sceplus-vm48.almaden.ibm.com:2181, baseZNode=/hbase Attempting to retransition opening state of node 122e92eabfab38cb15a9965b21db25c5
2014-07-01 15:31:24,384 DEBUG [RS_OPEN_REGION-slave1:60020-2] regionserver.HRegion: Applied 44690, skipped 0, firstSequenceidInLog=78, maxSequenceidInLog=85, path=hdfs://master:54310/hbase/data/default/usertable/122e92eabfab38cb15a9965b21db25c5/recovered.edits/0000000000000000085
2014-07-01 15:31:24,388 INFO  [RS_OPEN_REGION-slave1:60020-2] regionserver.HRegion: Replaying edits from hdfs://master:54310/hbase/data/default/usertable/122e92eabfab38cb15a9965b21db25c5/recovered.edits/0000000000000000094
2014-07-01 15:31:24,392 DEBUG [RS_OPEN_REGION-slave1:60020-0] zookeeper.ZKAssign: regionserver:60020-0x146f40d8cbe0002, quorum=sceplus-vm49.almaden.ibm.com:2181,sceplus-vm48.almaden.ibm.com:2181, baseZNode=/hbase Attempting to retransition opening state of node 9cef9e7d34f96f1d9b111be50040a208
2014-07-01 15:31:24,393 DEBUG [RS_OPEN_REGION-slave1:60020-0] regionserver.HRegion: Applied 28020, skipped 0, firstSequenceidInLog=86, maxSequenceidInLog=90, path=hdfs://master:54310/hbase/data/default/usertable/9cef9e7d34f96f1d9b111be50040a208/recovered.edits/0000000000000000090
2014-07-01 15:31:24,395 DEBUG [RS_OPEN_REGION-slave1:60020-1] zookeeper.ZKAssign: regionserver:60020-0x146f40d8cbe0002, quorum=sceplus-vm49.almaden.ibm.com:2181,sceplus-vm48.almaden.ibm.com:2181, baseZNode=/hbase Attempting to retransition opening state of node 93253e37c284f3174309fede3f32339f
2014-07-01 15:31:24,396 INFO  [RS_OPEN_REGION-slave1:60020-0] regionserver.HRegion: Replaying edits from hdfs://master:54310/hbase/data/default/usertable/9cef9e7d34f96f1d9b111be50040a208/recovered.edits/0000000000000000107
2014-07-01 15:31:24,396 DEBUG [RS_OPEN_REGION-slave1:60020-1] regionserver.HRegion: Applied 39460, skipped 0, firstSequenceidInLog=81, maxSequenceidInLog=87, path=hdfs://master:54310/hbase/data/default/usertable/93253e37c284f3174309fede3f32339f/recovered.edits/0000000000000000087
2014-07-01 15:31:24,517 INFO  [RS_OPEN_REGION-slave1:60020-1] regionserver.HRegion: Replaying edits from hdfs://master:54310/hbase/data/default/usertable/93253e37c284f3174309fede3f32339f/recovered.edits/0000000000000000100
2014-07-01 15:31:24,947 DEBUG [RS_OPEN_REGION-slave1:60020-2] zookeeper.ZKAssign: regionserver:60020-0x146f40d8cbe0002, quorum=sceplus-vm49.almaden.ibm.com:2181,sceplus-vm48.almaden.ibm.com:2181, baseZNode=/hbase Attempting to retransition opening state of node 122e92eabfab38cb15a9965b21db25c5
2014-07-01 15:31:24,948 DEBUG [RS_OPEN_REGION-slave1:60020-2] regionserver.HRegion: Applied 50520, skipped 0, firstSequenceidInLog=86, maxSequenceidInLog=94, path=hdfs://master:54310/hbase/data/default/usertable/122e92eabfab38cb15a9965b21db25c5/recovered.edits/0000000000000000094
2014-07-01 15:31:24,951 INFO  [RS_OPEN_REGION-slave1:60020-2] regionserver.HRegion: Replaying edits from hdfs://master:54310/hbase/data/default/usertable/122e92eabfab38cb15a9965b21db25c5/recovered.edits/0000000000000000108
2014-07-01 15:31:25,127 DEBUG [RS_OPEN_REGION-slave1:60020-1] zookeeper.ZKAssign: regionserver:60020-0x146f40d8cbe0002, quorum=sceplus-vm49.almaden.ibm.com:2181,sceplus-vm48.almaden.ibm.com:2181, baseZNode=/hbase Attempting to retransition opening state of node 93253e37c284f3174309fede3f32339f
2014-07-01 15:31:25,129 DEBUG [RS_OPEN_REGION-slave1:60020-1] regionserver.HRegion: Applied 73790, skipped 0, firstSequenceidInLog=88, maxSequenceidInLog=100, path=hdfs://master:54310/hbase/data/default/usertable/93253e37c284f3174309fede3f32339f/recovered.edits/0000000000000000100
2014-07-01 15:31:25,131 INFO  [RS_OPEN_REGION-slave1:60020-1] regionserver.HRegion: Replaying edits from hdfs://master:54310/hbase/data/default/usertable/93253e37c284f3174309fede3f32339f/recovered.edits/0000000000000000110
2014-07-01 15:31:25,312 DEBUG [RS_OPEN_REGION-slave1:60020-0] zookeeper.ZKAssign: regionserver:60020-0x146f40d8cbe0002, quorum=sceplus-vm49.almaden.ibm.com:2181,sceplus-vm48.almaden.ibm.com:2181, baseZNode=/hbase Attempting to retransition opening state of node 9cef9e7d34f96f1d9b111be50040a208
2014-07-01 15:31:25,313 DEBUG [RS_OPEN_REGION-slave1:60020-0] regionserver.HRegion: Applied 96010, skipped 0, firstSequenceidInLog=91, maxSequenceidInLog=107, path=hdfs://master:54310/hbase/data/default/usertable/9cef9e7d34f96f1d9b111be50040a208/recovered.edits/0000000000000000107
2014-07-01 15:31:25,317 INFO  [RS_OPEN_REGION-slave1:60020-0] regionserver.HRegion: Replaying edits from hdfs://master:54310/hbase/data/default/usertable/9cef9e7d34f96f1d9b111be50040a208/recovered.edits/0000000000000000110
2014-07-01 15:31:25,437 DEBUG [RS_OPEN_REGION-slave1:60020-0] zookeeper.ZKAssign: regionserver:60020-0x146f40d8cbe0002, quorum=sceplus-vm49.almaden.ibm.com:2181,sceplus-vm48.almaden.ibm.com:2181, baseZNode=/hbase Attempting to retransition opening state of node 9cef9e7d34f96f1d9b111be50040a208
2014-07-01 15:31:25,439 DEBUG [RS_OPEN_REGION-slave1:60020-0] regionserver.HRegion: Applied 17610, skipped 0, firstSequenceidInLog=108, maxSequenceidInLog=110, path=hdfs://master:54310/hbase/data/default/usertable/9cef9e7d34f96f1d9b111be50040a208/recovered.edits/0000000000000000110
2014-07-01 15:31:25,441 INFO  [RS_OPEN_REGION-slave1:60020-0] regionserver.HRegion: Replaying edits from hdfs://master:54310/hbase/data/default/usertable/9cef9e7d34f96f1d9b111be50040a208/recovered.edits/0000000000000000121
2014-07-01 15:31:25,782 DEBUG [RS_OPEN_REGION-slave1:60020-1] zookeeper.ZKAssign: regionserver:60020-0x146f40d8cbe0002, quorum=sceplus-vm49.almaden.ibm.com:2181,sceplus-vm48.almaden.ibm.com:2181, baseZNode=/hbase Attempting to retransition opening state of node 93253e37c284f3174309fede3f32339f
2014-07-01 15:31:25,784 DEBUG [RS_OPEN_REGION-slave1:60020-1] regionserver.HRegion: Applied 56060, skipped 0, firstSequenceidInLog=101, maxSequenceidInLog=110, path=hdfs://master:54310/hbase/data/default/usertable/93253e37c284f3174309fede3f32339f/recovered.edits/0000000000000000110
2014-07-01 15:31:25,786 INFO  [RS_OPEN_REGION-slave1:60020-1] regionserver.HRegion: Replaying edits from hdfs://master:54310/hbase/data/default/usertable/93253e37c284f3174309fede3f32339f/recovered.edits/0000000000000000117
2014-07-01 15:31:25,822 DEBUG [RS_OPEN_REGION-slave1:60020-2] zookeeper.ZKAssign: regionserver:60020-0x146f40d8cbe0002, quorum=sceplus-vm49.almaden.ibm.com:2181,sceplus-vm48.almaden.ibm.com:2181, baseZNode=/hbase Attempting to retransition opening state of node 122e92eabfab38cb15a9965b21db25c5
2014-07-01 15:31:25,823 DEBUG [RS_OPEN_REGION-slave1:60020-2] regionserver.HRegion: Applied 79730, skipped 0, firstSequenceidInLog=95, maxSequenceidInLog=108, path=hdfs://master:54310/hbase/data/default/usertable/122e92eabfab38cb15a9965b21db25c5/recovered.edits/0000000000000000108
2014-07-01 15:31:25,826 INFO  [RS_OPEN_REGION-slave1:60020-2] regionserver.HRegion: Replaying edits from hdfs://master:54310/hbase/data/default/usertable/122e92eabfab38cb15a9965b21db25c5/recovered.edits/0000000000000000113
2014-07-01 15:31:26,026 DEBUG [RS_OPEN_REGION-slave1:60020-0] zookeeper.ZKAssign: regionserver:60020-0x146f40d8cbe0002, quorum=sceplus-vm49.almaden.ibm.com:2181,sceplus-vm48.almaden.ibm.com:2181, baseZNode=/hbase Attempting to retransition opening state of node 9cef9e7d34f96f1d9b111be50040a208
2014-07-01 15:31:26,028 DEBUG [RS_OPEN_REGION-slave1:60020-0] regionserver.HRegion: Applied 62420, skipped 0, firstSequenceidInLog=111, maxSequenceidInLog=121, path=hdfs://master:54310/hbase/data/default/usertable/9cef9e7d34f96f1d9b111be50040a208/recovered.edits/0000000000000000121
2014-07-01 15:31:26,029 DEBUG [RS_OPEN_REGION-slave1:60020-0] regionserver.HRegion: Started memstore flush for usertable,,1404228265525.9cef9e7d34f96f1d9b111be50040a208., current region memstore size 186.6m; wal is null, using passed sequenceid=121
2014-07-01 15:31:26,029 DEBUG [RS_OPEN_REGION-slave1:60020-2] zookeeper.ZKAssign: regionserver:60020-0x146f40d8cbe0002, quorum=sceplus-vm49.almaden.ibm.com:2181,sceplus-vm48.almaden.ibm.com:2181, baseZNode=/hbase Attempting to retransition opening state of node 122e92eabfab38cb15a9965b21db25c5
2014-07-01 15:31:26,030 DEBUG [RS_OPEN_REGION-slave1:60020-2] regionserver.HRegion: Applied 28100, skipped 0, firstSequenceidInLog=109, maxSequenceidInLog=113, path=hdfs://master:54310/hbase/data/default/usertable/122e92eabfab38cb15a9965b21db25c5/recovered.edits/0000000000000000113
2014-07-01 15:31:26,031 DEBUG [RS_OPEN_REGION-slave1:60020-2] regionserver.HRegion: Started memstore flush for usertable,user5,1404228265526.122e92eabfab38cb15a9965b21db25c5., current region memstore size 174.1m; wal is null, using passed sequenceid=113
2014-07-01 15:31:26,059 DEBUG [RS_OPEN_REGION-slave1:60020-1] zookeeper.ZKAssign: regionserver:60020-0x146f40d8cbe0002, quorum=sceplus-vm49.almaden.ibm.com:2181,sceplus-vm48.almaden.ibm.com:2181, baseZNode=/hbase Attempting to retransition opening state of node 93253e37c284f3174309fede3f32339f
2014-07-01 15:31:26,060 DEBUG [RS_OPEN_REGION-slave1:60020-1] regionserver.HRegion: Applied 40530, skipped 0, firstSequenceidInLog=111, maxSequenceidInLog=117, path=hdfs://master:54310/hbase/data/default/usertable/93253e37c284f3174309fede3f32339f/recovered.edits/0000000000000000117
2014-07-01 15:31:26,060 DEBUG [RS_OPEN_REGION-slave1:60020-1] regionserver.HRegion: Started memstore flush for usertable,user3,1404228265526.93253e37c284f3174309fede3f32339f., current region memstore size 180.3m; wal is null, using passed sequenceid=117
2014-07-01 15:31:26,157 DEBUG [RS_OPEN_REGION-slave1:60020-0] util.FSUtils: DFS Client does not support most favored nodes create; using default create
2014-07-01 15:31:26,157 DEBUG [RS_OPEN_REGION-slave1:60020-2] util.FSUtils: DFS Client does not support most favored nodes create; using default create
2014-07-01 15:31:26,166 DEBUG [RS_OPEN_REGION-slave1:60020-1] util.FSUtils: DFS Client does not support most favored nodes create; using default create
2014-07-01 15:31:26,169 INFO  [RS_OPEN_REGION-slave1:60020-1] compress.CodecPool: Got brand-new compressor
2014-07-01 15:31:26,169 INFO  [RS_OPEN_REGION-slave1:60020-1] compress.CodecPool: Got brand-new compressor
2014-07-01 15:31:26,169 INFO  [RS_OPEN_REGION-slave1:60020-2] compress.CodecPool: Got brand-new compressor
2014-07-01 15:31:31,351 INFO  [RS_OPEN_REGION-slave1:60020-2] regionserver.DefaultStoreFlusher: Flushed, sequenceid=113, memsize=174.1m, hasBloomFilter=true, into tmp file hdfs://master:54310/hbase/data/default/usertable/122e92eabfab38cb15a9965b21db25c5/.tmp/d4c796f1737f4cae8358c27df888ec15
2014-07-01 15:31:31,389 INFO  [RS_OPEN_REGION-slave1:60020-2] compress.CodecPool: Got brand-new decompressor
2014-07-01 15:31:31,417 DEBUG [RS_OPEN_REGION-slave1:60020-2] regionserver.HRegionFileSystem: Committing store file hdfs://master:54310/hbase/data/default/usertable/122e92eabfab38cb15a9965b21db25c5/.tmp/d4c796f1737f4cae8358c27df888ec15 as hdfs://master:54310/hbase/data/default/usertable/122e92eabfab38cb15a9965b21db25c5/family/d4c796f1737f4cae8358c27df888ec15
2014-07-01 15:31:31,441 INFO  [RS_OPEN_REGION-slave1:60020-2] regionserver.HStore: Added hdfs://master:54310/hbase/data/default/usertable/122e92eabfab38cb15a9965b21db25c5/family/d4c796f1737f4cae8358c27df888ec15, entries=633950, sequenceid=113, filesize=45.2m
2014-07-01 15:31:31,442 INFO  [RS_OPEN_REGION-slave1:60020-2] regionserver.HRegion: Finished memstore flush of ~174.1m/182571440, currentsize=0.0/0 for region usertable,user5,1404228265526.122e92eabfab38cb15a9965b21db25c5. in 5411ms, sequenceid=113, compaction requested=false; wal=null
2014-07-01 15:31:31,447 DEBUG [RS_OPEN_REGION-slave1:60020-2] regionserver.HRegion: Deleted recovered.edits file=hdfs://master:54310/hbase/data/default/usertable/122e92eabfab38cb15a9965b21db25c5/recovered.edits/0000000000000000003
2014-07-01 15:31:31,451 DEBUG [RS_OPEN_REGION-slave1:60020-2] regionserver.HRegion: Deleted recovered.edits file=hdfs://master:54310/hbase/data/default/usertable/122e92eabfab38cb15a9965b21db25c5/recovered.edits/0000000000000000014
2014-07-01 15:31:31,453 DEBUG [RS_OPEN_REGION-slave1:60020-2] regionserver.HRegion: Deleted recovered.edits file=hdfs://master:54310/hbase/data/default/usertable/122e92eabfab38cb15a9965b21db25c5/recovered.edits/0000000000000000025
2014-07-01 15:31:31,455 DEBUG [RS_OPEN_REGION-slave1:60020-2] regionserver.HRegion: Deleted recovered.edits file=hdfs://master:54310/hbase/data/default/usertable/122e92eabfab38cb15a9965b21db25c5/recovered.edits/0000000000000000035
2014-07-01 15:31:31,459 DEBUG [RS_OPEN_REGION-slave1:60020-2] regionserver.HRegion: Deleted recovered.edits file=hdfs://master:54310/hbase/data/default/usertable/122e92eabfab38cb15a9965b21db25c5/recovered.edits/0000000000000000043
2014-07-01 15:31:31,465 DEBUG [RS_OPEN_REGION-slave1:60020-2] regionserver.HRegion: Deleted recovered.edits file=hdfs://master:54310/hbase/data/default/usertable/122e92eabfab38cb15a9965b21db25c5/recovered.edits/0000000000000000053
2014-07-01 15:31:31,470 DEBUG [RS_OPEN_REGION-slave1:60020-2] regionserver.HRegion: Deleted recovered.edits file=hdfs://master:54310/hbase/data/default/usertable/122e92eabfab38cb15a9965b21db25c5/recovered.edits/0000000000000000065
2014-07-01 15:31:31,475 DEBUG [RS_OPEN_REGION-slave1:60020-2] regionserver.HRegion: Deleted recovered.edits file=hdfs://master:54310/hbase/data/default/usertable/122e92eabfab38cb15a9965b21db25c5/recovered.edits/0000000000000000077
2014-07-01 15:31:31,477 DEBUG [RS_OPEN_REGION-slave1:60020-2] regionserver.HRegion: Deleted recovered.edits file=hdfs://master:54310/hbase/data/default/usertable/122e92eabfab38cb15a9965b21db25c5/recovered.edits/0000000000000000085
2014-07-01 15:31:31,479 DEBUG [RS_OPEN_REGION-slave1:60020-2] regionserver.HRegion: Deleted recovered.edits file=hdfs://master:54310/hbase/data/default/usertable/122e92eabfab38cb15a9965b21db25c5/recovered.edits/0000000000000000094
2014-07-01 15:31:31,482 DEBUG [RS_OPEN_REGION-slave1:60020-2] regionserver.HRegion: Deleted recovered.edits file=hdfs://master:54310/hbase/data/default/usertable/122e92eabfab38cb15a9965b21db25c5/recovered.edits/0000000000000000108
2014-07-01 15:31:31,484 DEBUG [RS_OPEN_REGION-slave1:60020-2] regionserver.HRegion: Deleted recovered.edits file=hdfs://master:54310/hbase/data/default/usertable/122e92eabfab38cb15a9965b21db25c5/recovered.edits/0000000000000000113
2014-07-01 15:31:31,493 INFO  [RS_OPEN_REGION-slave1:60020-2] regionserver.HRegion: Onlined 122e92eabfab38cb15a9965b21db25c5; next sequenceid=114
2014-07-01 15:31:31,493 DEBUG [RS_OPEN_REGION-slave1:60020-2] zookeeper.ZKAssign: regionserver:60020-0x146f40d8cbe0002, quorum=sceplus-vm49.almaden.ibm.com:2181,sceplus-vm48.almaden.ibm.com:2181, baseZNode=/hbase Attempting to retransition opening state of node 122e92eabfab38cb15a9965b21db25c5
2014-07-01 15:31:31,496 INFO  [PostOpenDeployTasks:122e92eabfab38cb15a9965b21db25c5] regionserver.HRegionServer: Post open deploy tasks for region=usertable,user5,1404228265526.122e92eabfab38cb15a9965b21db25c5.
2014-07-01 15:31:31,571 INFO  [RS_OPEN_REGION-slave1:60020-1] regionserver.DefaultStoreFlusher: Flushed, sequenceid=117, memsize=180.3m, hasBloomFilter=true, into tmp file hdfs://master:54310/hbase/data/default/usertable/93253e37c284f3174309fede3f32339f/.tmp/80c506b485354d5981ae697240b0d0d7
2014-07-01 15:31:31,595 DEBUG [RS_OPEN_REGION-slave1:60020-1] regionserver.HRegionFileSystem: Committing store file hdfs://master:54310/hbase/data/default/usertable/93253e37c284f3174309fede3f32339f/.tmp/80c506b485354d5981ae697240b0d0d7 as hdfs://master:54310/hbase/data/default/usertable/93253e37c284f3174309fede3f32339f/family/80c506b485354d5981ae697240b0d0d7
2014-07-01 15:31:31,601 INFO  [PostOpenDeployTasks:122e92eabfab38cb15a9965b21db25c5] catalog.MetaEditor: Updated row usertable,user5,1404228265526.122e92eabfab38cb15a9965b21db25c5. with server=slave1,60020,1404253868846
2014-07-01 15:31:31,602 INFO  [PostOpenDeployTasks:122e92eabfab38cb15a9965b21db25c5] regionserver.HRegionServer: Finished post open deploy task for usertable,user5,1404228265526.122e92eabfab38cb15a9965b21db25c5.
2014-07-01 15:31:31,602 DEBUG [RS_OPEN_REGION-slave1:60020-2] zookeeper.ZKAssign: regionserver:60020-0x146f40d8cbe0002, quorum=sceplus-vm49.almaden.ibm.com:2181,sceplus-vm48.almaden.ibm.com:2181, baseZNode=/hbase Transitioning 122e92eabfab38cb15a9965b21db25c5 from RS_ZK_REGION_OPENING to RS_ZK_REGION_OPENED
2014-07-01 15:31:31,606 DEBUG [RS_OPEN_REGION-slave1:60020-2] zookeeper.ZKAssign: regionserver:60020-0x146f40d8cbe0002, quorum=sceplus-vm49.almaden.ibm.com:2181,sceplus-vm48.almaden.ibm.com:2181, baseZNode=/hbase Transitioned node 122e92eabfab38cb15a9965b21db25c5 from RS_ZK_REGION_OPENING to RS_ZK_REGION_OPENED
2014-07-01 15:31:31,606 DEBUG [RS_OPEN_REGION-slave1:60020-2] handler.OpenRegionHandler: Transitioned 122e92eabfab38cb15a9965b21db25c5 to OPENED in zk on slave1,60020,1404253868846
2014-07-01 15:31:31,607 DEBUG [RS_OPEN_REGION-slave1:60020-2] handler.OpenRegionHandler: Opened usertable,user5,1404228265526.122e92eabfab38cb15a9965b21db25c5. on slave1,60020,1404253868846
2014-07-01 15:31:31,607 DEBUG [RS_OPEN_REGION-slave1:60020-2] zookeeper.ZKAssign: regionserver:60020-0x146f40d8cbe0002, quorum=sceplus-vm49.almaden.ibm.com:2181,sceplus-vm48.almaden.ibm.com:2181, baseZNode=/hbase Transitioning 667c1f2c34fe37d48ba25a789d8aba98 from M_ZK_REGION_OFFLINE to RS_ZK_REGION_OPENING
2014-07-01 15:31:31,612 DEBUG [RS_OPEN_REGION-slave1:60020-2] zookeeper.ZKAssign: regionserver:60020-0x146f40d8cbe0002, quorum=sceplus-vm49.almaden.ibm.com:2181,sceplus-vm48.almaden.ibm.com:2181, baseZNode=/hbase Transitioned node 667c1f2c34fe37d48ba25a789d8aba98 from M_ZK_REGION_OFFLINE to RS_ZK_REGION_OPENING
2014-07-01 15:31:31,613 DEBUG [RS_OPEN_REGION-slave1:60020-2] regionserver.HRegion: Opening region: {ENCODED => 667c1f2c34fe37d48ba25a789d8aba98, NAME => 'usertable,user7,1404228265526.667c1f2c34fe37d48ba25a789d8aba98.', STARTKEY => 'user7', ENDKEY => 'user8'}
2014-07-01 15:31:31,614 INFO  [RS_OPEN_REGION-slave1:60020-1] regionserver.HStore: Added hdfs://master:54310/hbase/data/default/usertable/93253e37c284f3174309fede3f32339f/family/80c506b485354d5981ae697240b0d0d7, entries=656570, sequenceid=117, filesize=46.8m
2014-07-01 15:31:31,615 INFO  [RS_OPEN_REGION-slave1:60020-1] regionserver.HRegion: Finished memstore flush of ~180.3m/189086160, currentsize=0.0/0 for region usertable,user3,1404228265526.93253e37c284f3174309fede3f32339f. in 5554ms, sequenceid=117, compaction requested=false; wal=null
2014-07-01 15:31:31,615 DEBUG [RS_OPEN_REGION-slave1:60020-2] regionserver.MetricsRegionSourceImpl: Creating new MetricsRegionSourceImpl for table usertable 667c1f2c34fe37d48ba25a789d8aba98
2014-07-01 15:31:31,615 DEBUG [RS_OPEN_REGION-slave1:60020-2] regionserver.HRegion: Instantiated usertable,user7,1404228265526.667c1f2c34fe37d48ba25a789d8aba98.
2014-07-01 15:31:31,618 DEBUG [RS_OPEN_REGION-slave1:60020-1] regionserver.HRegion: Deleted recovered.edits file=hdfs://master:54310/hbase/data/default/usertable/93253e37c284f3174309fede3f32339f/recovered.edits/0000000000000000004
2014-07-01 15:31:31,626 DEBUG [RS_OPEN_REGION-slave1:60020-1] regionserver.HRegion: Deleted recovered.edits file=hdfs://master:54310/hbase/data/default/usertable/93253e37c284f3174309fede3f32339f/recovered.edits/0000000000000000016
2014-07-01 15:31:31,632 DEBUG [RS_OPEN_REGION-slave1:60020-1] regionserver.HRegion: Deleted recovered.edits file=hdfs://master:54310/hbase/data/default/usertable/93253e37c284f3174309fede3f32339f/recovered.edits/0000000000000000029
2014-07-01 15:31:31,639 INFO  [StoreOpener-667c1f2c34fe37d48ba25a789d8aba98-1] compactions.CompactionConfiguration: size [268435456, 9223372036854775807); files [3, 10); ratio 1.200000; off-peak ratio 5.000000; throttle point 5368709120; delete expired; major period 0, major jitter 0.500000
2014-07-01 15:31:31,639 DEBUG [RS_OPEN_REGION-slave1:60020-1] regionserver.HRegion: Deleted recovered.edits file=hdfs://master:54310/hbase/data/default/usertable/93253e37c284f3174309fede3f32339f/recovered.edits/0000000000000000038
2014-07-01 15:31:31,642 DEBUG [RS_OPEN_REGION-slave1:60020-1] regionserver.HRegion: Deleted recovered.edits file=hdfs://master:54310/hbase/data/default/usertable/93253e37c284f3174309fede3f32339f/recovered.edits/0000000000000000046
2014-07-01 15:31:31,646 DEBUG [RS_OPEN_REGION-slave1:60020-1] regionserver.HRegion: Deleted recovered.edits file=hdfs://master:54310/hbase/data/default/usertable/93253e37c284f3174309fede3f32339f/recovered.edits/0000000000000000059
2014-07-01 15:31:31,652 DEBUG [RS_OPEN_REGION-slave1:60020-1] regionserver.HRegion: Deleted recovered.edits file=hdfs://master:54310/hbase/data/default/usertable/93253e37c284f3174309fede3f32339f/recovered.edits/0000000000000000067
2014-07-01 15:31:31,658 DEBUG [RS_OPEN_REGION-slave1:60020-1] regionserver.HRegion: Deleted recovered.edits file=hdfs://master:54310/hbase/data/default/usertable/93253e37c284f3174309fede3f32339f/recovered.edits/0000000000000000080
2014-07-01 15:31:31,662 DEBUG [RS_OPEN_REGION-slave1:60020-1] regionserver.HRegion: Deleted recovered.edits file=hdfs://master:54310/hbase/data/default/usertable/93253e37c284f3174309fede3f32339f/recovered.edits/0000000000000000087
2014-07-01 15:31:31,667 DEBUG [RS_OPEN_REGION-slave1:60020-1] regionserver.HRegion: Deleted recovered.edits file=hdfs://master:54310/hbase/data/default/usertable/93253e37c284f3174309fede3f32339f/recovered.edits/0000000000000000100
2014-07-01 15:31:31,672 DEBUG [RS_OPEN_REGION-slave1:60020-1] regionserver.HRegion: Deleted recovered.edits file=hdfs://master:54310/hbase/data/default/usertable/93253e37c284f3174309fede3f32339f/recovered.edits/0000000000000000110
2014-07-01 15:31:31,674 DEBUG [RS_OPEN_REGION-slave1:60020-1] regionserver.HRegion: Deleted recovered.edits file=hdfs://master:54310/hbase/data/default/usertable/93253e37c284f3174309fede3f32339f/recovered.edits/0000000000000000117
2014-07-01 15:31:31,682 INFO  [RS_OPEN_REGION-slave1:60020-1] regionserver.HRegion: Onlined 93253e37c284f3174309fede3f32339f; next sequenceid=118
2014-07-01 15:31:31,682 DEBUG [RS_OPEN_REGION-slave1:60020-1] zookeeper.ZKAssign: regionserver:60020-0x146f40d8cbe0002, quorum=sceplus-vm49.almaden.ibm.com:2181,sceplus-vm48.almaden.ibm.com:2181, baseZNode=/hbase Attempting to retransition opening state of node 93253e37c284f3174309fede3f32339f
2014-07-01 15:31:31,683 INFO  [PostOpenDeployTasks:93253e37c284f3174309fede3f32339f] regionserver.HRegionServer: Post open deploy tasks for region=usertable,user3,1404228265526.93253e37c284f3174309fede3f32339f.
2014-07-01 15:31:31,694 INFO  [PostOpenDeployTasks:93253e37c284f3174309fede3f32339f] catalog.MetaEditor: Updated row usertable,user3,1404228265526.93253e37c284f3174309fede3f32339f. with server=slave1,60020,1404253868846
2014-07-01 15:31:31,694 INFO  [PostOpenDeployTasks:93253e37c284f3174309fede3f32339f] regionserver.HRegionServer: Finished post open deploy task for usertable,user3,1404228265526.93253e37c284f3174309fede3f32339f.
2014-07-01 15:31:31,695 DEBUG [RS_OPEN_REGION-slave1:60020-1] zookeeper.ZKAssign: regionserver:60020-0x146f40d8cbe0002, quorum=sceplus-vm49.almaden.ibm.com:2181,sceplus-vm48.almaden.ibm.com:2181, baseZNode=/hbase Transitioning 93253e37c284f3174309fede3f32339f from RS_ZK_REGION_OPENING to RS_ZK_REGION_OPENED
2014-07-01 15:31:31,698 DEBUG [RS_OPEN_REGION-slave1:60020-2] regionserver.HRegion: Found 12 recovered edits file(s) under hdfs://master:54310/hbase/data/default/usertable/667c1f2c34fe37d48ba25a789d8aba98
2014-07-01 15:31:31,698 DEBUG [RS_OPEN_REGION-slave1:60020-1] zookeeper.ZKAssign: regionserver:60020-0x146f40d8cbe0002, quorum=sceplus-vm49.almaden.ibm.com:2181,sceplus-vm48.almaden.ibm.com:2181, baseZNode=/hbase Transitioned node 93253e37c284f3174309fede3f32339f from RS_ZK_REGION_OPENING to RS_ZK_REGION_OPENED
2014-07-01 15:31:31,698 DEBUG [RS_OPEN_REGION-slave1:60020-1] handler.OpenRegionHandler: Transitioned 93253e37c284f3174309fede3f32339f to OPENED in zk on slave1,60020,1404253868846
2014-07-01 15:31:31,700 DEBUG [RS_OPEN_REGION-slave1:60020-1] handler.OpenRegionHandler: Opened usertable,user3,1404228265526.93253e37c284f3174309fede3f32339f. on slave1,60020,1404253868846
2014-07-01 15:31:31,701 DEBUG [RS_OPEN_REGION-slave1:60020-1] zookeeper.ZKAssign: regionserver:60020-0x146f40d8cbe0002, quorum=sceplus-vm49.almaden.ibm.com:2181,sceplus-vm48.almaden.ibm.com:2181, baseZNode=/hbase Transitioning bb79ae773f71289e0745068f68a0abd9 from M_ZK_REGION_OFFLINE to RS_ZK_REGION_OPENING
2014-07-01 15:31:31,702 INFO  [RS_OPEN_REGION-slave1:60020-2] regionserver.HRegion: Replaying edits from hdfs://master:54310/hbase/data/default/usertable/667c1f2c34fe37d48ba25a789d8aba98/recovered.edits/0000000000000000002
2014-07-01 15:31:31,703 INFO  [RS_OPEN_REGION-slave1:60020-0] regionserver.DefaultStoreFlusher: Flushed, sequenceid=121, memsize=186.6m, hasBloomFilter=true, into tmp file hdfs://master:54310/hbase/data/default/usertable/9cef9e7d34f96f1d9b111be50040a208/.tmp/1a3ddd0bf6eb49c094e7bfe4686943b0
2014-07-01 15:31:31,707 DEBUG [RS_OPEN_REGION-slave1:60020-1] zookeeper.ZKAssign: regionserver:60020-0x146f40d8cbe0002, quorum=sceplus-vm49.almaden.ibm.com:2181,sceplus-vm48.almaden.ibm.com:2181, baseZNode=/hbase Transitioned node bb79ae773f71289e0745068f68a0abd9 from M_ZK_REGION_OFFLINE to RS_ZK_REGION_OPENING
2014-07-01 15:31:31,708 DEBUG [RS_OPEN_REGION-slave1:60020-1] regionserver.HRegion: Opening region: {ENCODED => bb79ae773f71289e0745068f68a0abd9, NAME => 'usertable,user9,1404228265526.bb79ae773f71289e0745068f68a0abd9.', STARTKEY => 'user9', ENDKEY => ''}
2014-07-01 15:31:31,709 DEBUG [RS_OPEN_REGION-slave1:60020-1] regionserver.MetricsRegionSourceImpl: Creating new MetricsRegionSourceImpl for table usertable bb79ae773f71289e0745068f68a0abd9
2014-07-01 15:31:31,709 DEBUG [RS_OPEN_REGION-slave1:60020-1] regionserver.HRegion: Instantiated usertable,user9,1404228265526.bb79ae773f71289e0745068f68a0abd9.
2014-07-01 15:31:31,726 INFO  [StoreOpener-bb79ae773f71289e0745068f68a0abd9-1] compactions.CompactionConfiguration: size [268435456, 9223372036854775807); files [3, 10); ratio 1.200000; off-peak ratio 5.000000; throttle point 5368709120; delete expired; major period 0, major jitter 0.500000
2014-07-01 15:31:31,737 DEBUG [RS_OPEN_REGION-slave1:60020-0] regionserver.HRegionFileSystem: Committing store file hdfs://master:54310/hbase/data/default/usertable/9cef9e7d34f96f1d9b111be50040a208/.tmp/1a3ddd0bf6eb49c094e7bfe4686943b0 as hdfs://master:54310/hbase/data/default/usertable/9cef9e7d34f96f1d9b111be50040a208/family/1a3ddd0bf6eb49c094e7bfe4686943b0
2014-07-01 15:31:31,753 DEBUG [RS_OPEN_REGION-slave1:60020-2] zookeeper.ZKAssign: regionserver:60020-0x146f40d8cbe0002, quorum=sceplus-vm49.almaden.ibm.com:2181,sceplus-vm48.almaden.ibm.com:2181, baseZNode=/hbase Attempting to retransition opening state of node 667c1f2c34fe37d48ba25a789d8aba98
2014-07-01 15:31:31,754 DEBUG [RS_OPEN_REGION-slave1:60020-2] regionserver.HRegion: Applied 5460, skipped 0, firstSequenceidInLog=2, maxSequenceidInLog=2, path=hdfs://master:54310/hbase/data/default/usertable/667c1f2c34fe37d48ba25a789d8aba98/recovered.edits/0000000000000000002
2014-07-01 15:31:31,771 INFO  [RS_OPEN_REGION-slave1:60020-2] regionserver.HRegion: Replaying edits from hdfs://master:54310/hbase/data/default/usertable/667c1f2c34fe37d48ba25a789d8aba98/recovered.edits/0000000000000000011
2014-07-01 15:31:31,785 INFO  [RS_OPEN_REGION-slave1:60020-0] regionserver.HStore: Added hdfs://master:54310/hbase/data/default/usertable/9cef9e7d34f96f1d9b111be50040a208/family/1a3ddd0bf6eb49c094e7bfe4686943b0, entries=679530, sequenceid=121, filesize=48.4m
2014-07-01 15:31:31,785 INFO  [RS_OPEN_REGION-slave1:60020-0] regionserver.HRegion: Finished memstore flush of ~186.6m/195700080, currentsize=0.0/0 for region usertable,,1404228265525.9cef9e7d34f96f1d9b111be50040a208. in 5757ms, sequenceid=121, compaction requested=false; wal=null
2014-07-01 15:31:31,792 DEBUG [RS_OPEN_REGION-slave1:60020-0] regionserver.HRegion: Deleted recovered.edits file=hdfs://master:54310/hbase/data/default/usertable/9cef9e7d34f96f1d9b111be50040a208/recovered.edits/0000000000000000006
2014-07-01 15:31:31,801 DEBUG [RS_OPEN_REGION-slave1:60020-0] regionserver.HRegion: Deleted recovered.edits file=hdfs://master:54310/hbase/data/default/usertable/9cef9e7d34f96f1d9b111be50040a208/recovered.edits/0000000000000000021
2014-07-01 15:31:31,809 DEBUG [RS_OPEN_REGION-slave1:60020-0] regionserver.HRegion: Deleted recovered.edits file=hdfs://master:54310/hbase/data/default/usertable/9cef9e7d34f96f1d9b111be50040a208/recovered.edits/0000000000000000034
2014-07-01 15:31:31,817 DEBUG [RS_OPEN_REGION-slave1:60020-0] regionserver.HRegion: Deleted recovered.edits file=hdfs://master:54310/hbase/data/default/usertable/9cef9e7d34f96f1d9b111be50040a208/recovered.edits/0000000000000000042
2014-07-01 15:31:31,825 DEBUG [RS_OPEN_REGION-slave1:60020-0] regionserver.HRegion: Deleted recovered.edits file=hdfs://master:54310/hbase/data/default/usertable/9cef9e7d34f96f1d9b111be50040a208/recovered.edits/0000000000000000049
2014-07-01 15:31:31,831 DEBUG [RS_OPEN_REGION-slave1:60020-0] regionserver.HRegion: Deleted recovered.edits file=hdfs://master:54310/hbase/data/default/usertable/9cef9e7d34f96f1d9b111be50040a208/recovered.edits/0000000000000000063
2014-07-01 15:31:31,833 DEBUG [RS_OPEN_REGION-slave1:60020-1] regionserver.HRegion: Found 11 recovered edits file(s) under hdfs://master:54310/hbase/data/default/usertable/bb79ae773f71289e0745068f68a0abd9
2014-07-01 15:31:31,834 DEBUG [RS_OPEN_REGION-slave1:60020-0] regionserver.HRegion: Deleted recovered.edits file=hdfs://master:54310/hbase/data/default/usertable/9cef9e7d34f96f1d9b111be50040a208/recovered.edits/0000000000000000071
2014-07-01 15:31:31,840 DEBUG [RS_OPEN_REGION-slave1:60020-0] regionserver.HRegion: Deleted recovered.edits file=hdfs://master:54310/hbase/data/default/usertable/9cef9e7d34f96f1d9b111be50040a208/recovered.edits/0000000000000000085
2014-07-01 15:31:31,841 INFO  [RS_OPEN_REGION-slave1:60020-1] regionserver.HRegion: Replaying edits from hdfs://master:54310/hbase/data/default/usertable/bb79ae773f71289e0745068f68a0abd9/recovered.edits/0000000000000000008
2014-07-01 15:31:31,845 DEBUG [RS_OPEN_REGION-slave1:60020-0] regionserver.HRegion: Deleted recovered.edits file=hdfs://master:54310/hbase/data/default/usertable/9cef9e7d34f96f1d9b111be50040a208/recovered.edits/0000000000000000090
2014-07-01 15:31:31,851 DEBUG [RS_OPEN_REGION-slave1:60020-0] regionserver.HRegion: Deleted recovered.edits file=hdfs://master:54310/hbase/data/default/usertable/9cef9e7d34f96f1d9b111be50040a208/recovered.edits/0000000000000000107
2014-07-01 15:31:31,892 DEBUG [RS_OPEN_REGION-slave1:60020-0] regionserver.HRegion: Deleted recovered.edits file=hdfs://master:54310/hbase/data/default/usertable/9cef9e7d34f96f1d9b111be50040a208/recovered.edits/0000000000000000110
2014-07-01 15:31:31,901 DEBUG [RS_OPEN_REGION-slave1:60020-0] regionserver.HRegion: Deleted recovered.edits file=hdfs://master:54310/hbase/data/default/usertable/9cef9e7d34f96f1d9b111be50040a208/recovered.edits/0000000000000000121
2014-07-01 15:31:31,910 INFO  [RS_OPEN_REGION-slave1:60020-0] regionserver.HRegion: Onlined 9cef9e7d34f96f1d9b111be50040a208; next sequenceid=122
2014-07-01 15:31:31,910 DEBUG [RS_OPEN_REGION-slave1:60020-0] zookeeper.ZKAssign: regionserver:60020-0x146f40d8cbe0002, quorum=sceplus-vm49.almaden.ibm.com:2181,sceplus-vm48.almaden.ibm.com:2181, baseZNode=/hbase Attempting to retransition opening state of node 9cef9e7d34f96f1d9b111be50040a208
2014-07-01 15:31:31,912 INFO  [PostOpenDeployTasks:9cef9e7d34f96f1d9b111be50040a208] regionserver.HRegionServer: Post open deploy tasks for region=usertable,,1404228265525.9cef9e7d34f96f1d9b111be50040a208.
2014-07-01 15:31:31,919 DEBUG [RS_OPEN_REGION-slave1:60020-1] zookeeper.ZKAssign: regionserver:60020-0x146f40d8cbe0002, quorum=sceplus-vm49.almaden.ibm.com:2181,sceplus-vm48.almaden.ibm.com:2181, baseZNode=/hbase Attempting to retransition opening state of node bb79ae773f71289e0745068f68a0abd9
2014-07-01 15:31:31,920 DEBUG [RS_OPEN_REGION-slave1:60020-1] regionserver.HRegion: Applied 11490, skipped 0, firstSequenceidInLog=2, maxSequenceidInLog=8, path=hdfs://master:54310/hbase/data/default/usertable/bb79ae773f71289e0745068f68a0abd9/recovered.edits/0000000000000000008
2014-07-01 15:31:31,926 INFO  [RS_OPEN_REGION-slave1:60020-1] regionserver.HRegion: Replaying edits from hdfs://master:54310/hbase/data/default/usertable/bb79ae773f71289e0745068f68a0abd9/recovered.edits/0000000000000000016
2014-07-01 15:31:31,989 DEBUG [RS_OPEN_REGION-slave1:60020-1] zookeeper.ZKAssign: regionserver:60020-0x146f40d8cbe0002, quorum=sceplus-vm49.almaden.ibm.com:2181,sceplus-vm48.almaden.ibm.com:2181, baseZNode=/hbase Attempting to retransition opening state of node bb79ae773f71289e0745068f68a0abd9
2014-07-01 15:31:31,990 DEBUG [RS_OPEN_REGION-slave1:60020-1] regionserver.HRegion: Applied 13150, skipped 0, firstSequenceidInLog=9, maxSequenceidInLog=16, path=hdfs://master:54310/hbase/data/default/usertable/bb79ae773f71289e0745068f68a0abd9/recovered.edits/0000000000000000016
2014-07-01 15:31:31,998 INFO  [RS_OPEN_REGION-slave1:60020-1] regionserver.HRegion: Replaying edits from hdfs://master:54310/hbase/data/default/usertable/bb79ae773f71289e0745068f68a0abd9/recovered.edits/0000000000000000029
2014-07-01 15:31:32,001 DEBUG [RS_OPEN_REGION-slave1:60020-2] zookeeper.ZKAssign: regionserver:60020-0x146f40d8cbe0002, quorum=sceplus-vm49.almaden.ibm.com:2181,sceplus-vm48.almaden.ibm.com:2181, baseZNode=/hbase Attempting to retransition opening state of node 667c1f2c34fe37d48ba25a789d8aba98
2014-07-01 15:31:32,003 DEBUG [RS_OPEN_REGION-slave1:60020-2] regionserver.HRegion: Applied 50780, skipped 0, firstSequenceidInLog=3, maxSequenceidInLog=11, path=hdfs://master:54310/hbase/data/default/usertable/667c1f2c34fe37d48ba25a789d8aba98/recovered.edits/0000000000000000011
2014-07-01 15:31:32,011 INFO  [RS_OPEN_REGION-slave1:60020-2] regionserver.HRegion: Replaying edits from hdfs://master:54310/hbase/data/default/usertable/667c1f2c34fe37d48ba25a789d8aba98/recovered.edits/0000000000000000019
2014-07-01 15:31:32,098 INFO  [PostOpenDeployTasks:9cef9e7d34f96f1d9b111be50040a208] catalog.MetaEditor: Updated row usertable,,1404228265525.9cef9e7d34f96f1d9b111be50040a208. with server=slave1,60020,1404253868846
2014-07-01 15:31:32,098 INFO  [PostOpenDeployTasks:9cef9e7d34f96f1d9b111be50040a208] regionserver.HRegionServer: Finished post open deploy task for usertable,,1404228265525.9cef9e7d34f96f1d9b111be50040a208.
2014-07-01 15:31:32,099 DEBUG [RS_OPEN_REGION-slave1:60020-0] zookeeper.ZKAssign: regionserver:60020-0x146f40d8cbe0002, quorum=sceplus-vm49.almaden.ibm.com:2181,sceplus-vm48.almaden.ibm.com:2181, baseZNode=/hbase Transitioning 9cef9e7d34f96f1d9b111be50040a208 from RS_ZK_REGION_OPENING to RS_ZK_REGION_OPENED
2014-07-01 15:31:32,104 DEBUG [RS_OPEN_REGION-slave1:60020-0] zookeeper.ZKAssign: regionserver:60020-0x146f40d8cbe0002, quorum=sceplus-vm49.almaden.ibm.com:2181,sceplus-vm48.almaden.ibm.com:2181, baseZNode=/hbase Transitioned node 9cef9e7d34f96f1d9b111be50040a208 from RS_ZK_REGION_OPENING to RS_ZK_REGION_OPENED
2014-07-01 15:31:32,104 DEBUG [RS_OPEN_REGION-slave1:60020-0] handler.OpenRegionHandler: Transitioned 9cef9e7d34f96f1d9b111be50040a208 to OPENED in zk on slave1,60020,1404253868846
2014-07-01 15:31:32,104 DEBUG [RS_OPEN_REGION-slave1:60020-0] handler.OpenRegionHandler: Opened usertable,,1404228265525.9cef9e7d34f96f1d9b111be50040a208. on slave1,60020,1404253868846
2014-07-01 15:31:32,107 DEBUG [RS_OPEN_REGION-slave1:60020-1] zookeeper.ZKAssign: regionserver:60020-0x146f40d8cbe0002, quorum=sceplus-vm49.almaden.ibm.com:2181,sceplus-vm48.almaden.ibm.com:2181, baseZNode=/hbase Attempting to retransition opening state of node bb79ae773f71289e0745068f68a0abd9
2014-07-01 15:31:32,108 DEBUG [RS_OPEN_REGION-slave1:60020-1] regionserver.HRegion: Applied 23150, skipped 0, firstSequenceidInLog=17, maxSequenceidInLog=29, path=hdfs://master:54310/hbase/data/default/usertable/bb79ae773f71289e0745068f68a0abd9/recovered.edits/0000000000000000029
2014-07-01 15:31:32,176 INFO  [RS_OPEN_REGION-slave1:60020-1] regionserver.HRegion: Replaying edits from hdfs://master:54310/hbase/data/default/usertable/bb79ae773f71289e0745068f68a0abd9/recovered.edits/0000000000000000043
2014-07-01 15:31:32,296 DEBUG [RS_OPEN_REGION-slave1:60020-1] zookeeper.ZKAssign: regionserver:60020-0x146f40d8cbe0002, quorum=sceplus-vm49.almaden.ibm.com:2181,sceplus-vm48.almaden.ibm.com:2181, baseZNode=/hbase Attempting to retransition opening state of node bb79ae773f71289e0745068f68a0abd9
2014-07-01 15:31:32,298 DEBUG [RS_OPEN_REGION-slave1:60020-1] regionserver.HRegion: Applied 23700, skipped 0, firstSequenceidInLog=30, maxSequenceidInLog=43, path=hdfs://master:54310/hbase/data/default/usertable/bb79ae773f71289e0745068f68a0abd9/recovered.edits/0000000000000000043
2014-07-01 15:31:32,300 DEBUG [RS_OPEN_REGION-slave1:60020-2] zookeeper.ZKAssign: regionserver:60020-0x146f40d8cbe0002, quorum=sceplus-vm49.almaden.ibm.com:2181,sceplus-vm48.almaden.ibm.com:2181, baseZNode=/hbase Attempting to retransition opening state of node 667c1f2c34fe37d48ba25a789d8aba98
2014-07-01 15:31:32,301 DEBUG [RS_OPEN_REGION-slave1:60020-2] regionserver.HRegion: Applied 45060, skipped 0, firstSequenceidInLog=12, maxSequenceidInLog=19, path=hdfs://master:54310/hbase/data/default/usertable/667c1f2c34fe37d48ba25a789d8aba98/recovered.edits/0000000000000000019
2014-07-01 15:31:32,303 INFO  [RS_OPEN_REGION-slave1:60020-1] regionserver.HRegion: Replaying edits from hdfs://master:54310/hbase/data/default/usertable/bb79ae773f71289e0745068f68a0abd9/recovered.edits/0000000000000000046
2014-07-01 15:31:32,307 INFO  [RS_OPEN_REGION-slave1:60020-2] regionserver.HRegion: Replaying edits from hdfs://master:54310/hbase/data/default/usertable/667c1f2c34fe37d48ba25a789d8aba98/recovered.edits/0000000000000000033
2014-07-01 15:31:32,341 DEBUG [RS_OPEN_REGION-slave1:60020-1] zookeeper.ZKAssign: regionserver:60020-0x146f40d8cbe0002, quorum=sceplus-vm49.almaden.ibm.com:2181,sceplus-vm48.almaden.ibm.com:2181, baseZNode=/hbase Attempting to retransition opening state of node bb79ae773f71289e0745068f68a0abd9
2014-07-01 15:31:32,343 DEBUG [RS_OPEN_REGION-slave1:60020-1] regionserver.HRegion: Applied 5230, skipped 0, firstSequenceidInLog=44, maxSequenceidInLog=46, path=hdfs://master:54310/hbase/data/default/usertable/bb79ae773f71289e0745068f68a0abd9/recovered.edits/0000000000000000046
2014-07-01 15:31:32,350 INFO  [RS_OPEN_REGION-slave1:60020-1] regionserver.HRegion: Replaying edits from hdfs://master:54310/hbase/data/default/usertable/bb79ae773f71289e0745068f68a0abd9/recovered.edits/0000000000000000059
2014-07-01 15:31:32,474 DEBUG [RS_OPEN_REGION-slave1:60020-1] zookeeper.ZKAssign: regionserver:60020-0x146f40d8cbe0002, quorum=sceplus-vm49.almaden.ibm.com:2181,sceplus-vm48.almaden.ibm.com:2181, baseZNode=/hbase Attempting to retransition opening state of node bb79ae773f71289e0745068f68a0abd9
2014-07-01 15:31:32,476 DEBUG [RS_OPEN_REGION-slave1:60020-1] regionserver.HRegion: Applied 21860, skipped 0, firstSequenceidInLog=47, maxSequenceidInLog=59, path=hdfs://master:54310/hbase/data/default/usertable/bb79ae773f71289e0745068f68a0abd9/recovered.edits/0000000000000000059
2014-07-01 15:31:32,479 INFO  [RS_OPEN_REGION-slave1:60020-1] regionserver.HRegion: Replaying edits from hdfs://master:54310/hbase/data/default/usertable/bb79ae773f71289e0745068f68a0abd9/recovered.edits/0000000000000000067
2014-07-01 15:31:32,646 DEBUG [RS_OPEN_REGION-slave1:60020-1] zookeeper.ZKAssign: regionserver:60020-0x146f40d8cbe0002, quorum=sceplus-vm49.almaden.ibm.com:2181,sceplus-vm48.almaden.ibm.com:2181, baseZNode=/hbase Attempting to retransition opening state of node bb79ae773f71289e0745068f68a0abd9
2014-07-01 15:31:32,648 DEBUG [RS_OPEN_REGION-slave1:60020-1] regionserver.HRegion: Applied 13660, skipped 0, firstSequenceidInLog=60, maxSequenceidInLog=67, path=hdfs://master:54310/hbase/data/default/usertable/bb79ae773f71289e0745068f68a0abd9/recovered.edits/0000000000000000067
2014-07-01 15:31:32,652 INFO  [RS_OPEN_REGION-slave1:60020-1] regionserver.HRegion: Replaying edits from hdfs://master:54310/hbase/data/default/usertable/bb79ae773f71289e0745068f68a0abd9/recovered.edits/0000000000000000077
2014-07-01 15:31:32,748 DEBUG [RS_OPEN_REGION-slave1:60020-1] zookeeper.ZKAssign: regionserver:60020-0x146f40d8cbe0002, quorum=sceplus-vm49.almaden.ibm.com:2181,sceplus-vm48.almaden.ibm.com:2181, baseZNode=/hbase Attempting to retransition opening state of node bb79ae773f71289e0745068f68a0abd9
2014-07-01 15:31:32,750 DEBUG [RS_OPEN_REGION-slave1:60020-1] regionserver.HRegion: Applied 17340, skipped 0, firstSequenceidInLog=68, maxSequenceidInLog=77, path=hdfs://master:54310/hbase/data/default/usertable/bb79ae773f71289e0745068f68a0abd9/recovered.edits/0000000000000000077
2014-07-01 15:31:32,753 INFO  [RS_OPEN_REGION-slave1:60020-1] regionserver.HRegion: Replaying edits from hdfs://master:54310/hbase/data/default/usertable/bb79ae773f71289e0745068f68a0abd9/recovered.edits/0000000000000000088
2014-07-01 15:31:32,795 DEBUG [RS_OPEN_REGION-slave1:60020-2] zookeeper.ZKAssign: regionserver:60020-0x146f40d8cbe0002, quorum=sceplus-vm49.almaden.ibm.com:2181,sceplus-vm48.almaden.ibm.com:2181, baseZNode=/hbase Attempting to retransition opening state of node 667c1f2c34fe37d48ba25a789d8aba98
2014-07-01 15:31:32,797 DEBUG [RS_OPEN_REGION-slave1:60020-2] regionserver.HRegion: Applied 78830, skipped 0, firstSequenceidInLog=20, maxSequenceidInLog=33, path=hdfs://master:54310/hbase/data/default/usertable/667c1f2c34fe37d48ba25a789d8aba98/recovered.edits/0000000000000000033
2014-07-01 15:31:32,800 INFO  [RS_OPEN_REGION-slave1:60020-2] regionserver.HRegion: Replaying edits from hdfs://master:54310/hbase/data/default/usertable/667c1f2c34fe37d48ba25a789d8aba98/recovered.edits/0000000000000000043
2014-07-01 15:31:32,852 DEBUG [RS_OPEN_REGION-slave1:60020-1] zookeeper.ZKAssign: regionserver:60020-0x146f40d8cbe0002, quorum=sceplus-vm49.almaden.ibm.com:2181,sceplus-vm48.almaden.ibm.com:2181, baseZNode=/hbase Attempting to retransition opening state of node bb79ae773f71289e0745068f68a0abd9
2014-07-01 15:31:32,853 DEBUG [RS_OPEN_REGION-slave1:60020-1] regionserver.HRegion: Applied 18500, skipped 0, firstSequenceidInLog=78, maxSequenceidInLog=88, path=hdfs://master:54310/hbase/data/default/usertable/bb79ae773f71289e0745068f68a0abd9/recovered.edits/0000000000000000088
2014-07-01 15:31:32,856 INFO  [RS_OPEN_REGION-slave1:60020-1] regionserver.HRegion: Replaying edits from hdfs://master:54310/hbase/data/default/usertable/bb79ae773f71289e0745068f68a0abd9/recovered.edits/0000000000000000097
2014-07-01 15:31:32,956 DEBUG [RS_OPEN_REGION-slave1:60020-1] zookeeper.ZKAssign: regionserver:60020-0x146f40d8cbe0002, quorum=sceplus-vm49.almaden.ibm.com:2181,sceplus-vm48.almaden.ibm.com:2181, baseZNode=/hbase Attempting to retransition opening state of node bb79ae773f71289e0745068f68a0abd9
2014-07-01 15:31:32,957 DEBUG [RS_OPEN_REGION-slave1:60020-1] regionserver.HRegion: Applied 15100, skipped 0, firstSequenceidInLog=89, maxSequenceidInLog=97, path=hdfs://master:54310/hbase/data/default/usertable/bb79ae773f71289e0745068f68a0abd9/recovered.edits/0000000000000000097
2014-07-01 15:31:32,961 INFO  [RS_OPEN_REGION-slave1:60020-1] regionserver.HRegion: Replaying edits from hdfs://master:54310/hbase/data/default/usertable/bb79ae773f71289e0745068f68a0abd9/recovered.edits/0000000000000000112
2014-07-01 15:31:33,215 DEBUG [RS_OPEN_REGION-slave1:60020-2] zookeeper.ZKAssign: regionserver:60020-0x146f40d8cbe0002, quorum=sceplus-vm49.almaden.ibm.com:2181,sceplus-vm48.almaden.ibm.com:2181, baseZNode=/hbase Attempting to retransition opening state of node 667c1f2c34fe37d48ba25a789d8aba98
2014-07-01 15:31:33,216 DEBUG [RS_OPEN_REGION-slave1:60020-2] regionserver.HRegion: Applied 57310, skipped 0, firstSequenceidInLog=34, maxSequenceidInLog=43, path=hdfs://master:54310/hbase/data/default/usertable/667c1f2c34fe37d48ba25a789d8aba98/recovered.edits/0000000000000000043
2014-07-01 15:31:33,218 DEBUG [RS_OPEN_REGION-slave1:60020-1] zookeeper.ZKAssign: regionserver:60020-0x146f40d8cbe0002, quorum=sceplus-vm49.almaden.ibm.com:2181,sceplus-vm48.almaden.ibm.com:2181, baseZNode=/hbase Attempting to retransition opening state of node bb79ae773f71289e0745068f68a0abd9
2014-07-01 15:31:33,219 DEBUG [RS_OPEN_REGION-slave1:60020-1] regionserver.HRegion: Applied 25730, skipped 0, firstSequenceidInLog=98, maxSequenceidInLog=112, path=hdfs://master:54310/hbase/data/default/usertable/bb79ae773f71289e0745068f68a0abd9/recovered.edits/0000000000000000112
2014-07-01 15:31:33,219 INFO  [RS_OPEN_REGION-slave1:60020-2] regionserver.HRegion: Replaying edits from hdfs://master:54310/hbase/data/default/usertable/667c1f2c34fe37d48ba25a789d8aba98/recovered.edits/0000000000000000050
2014-07-01 15:31:33,219 DEBUG [RS_OPEN_REGION-slave1:60020-1] regionserver.HRegion: Started memstore flush for usertable,user9,1404228265526.bb79ae773f71289e0745068f68a0abd9., current region memstore size 51.9m; wal is null, using passed sequenceid=112
2014-07-01 15:31:33,236 DEBUG [RS_OPEN_REGION-slave1:60020-1] util.FSUtils: DFS Client does not support most favored nodes create; using default create
2014-07-01 15:31:33,955 DEBUG [RS_OPEN_REGION-slave1:60020-2] zookeeper.ZKAssign: regionserver:60020-0x146f40d8cbe0002, quorum=sceplus-vm49.almaden.ibm.com:2181,sceplus-vm48.almaden.ibm.com:2181, baseZNode=/hbase Attempting to retransition opening state of node 667c1f2c34fe37d48ba25a789d8aba98
2014-07-01 15:31:33,957 DEBUG [RS_OPEN_REGION-slave1:60020-2] regionserver.HRegion: Applied 39610, skipped 0, firstSequenceidInLog=44, maxSequenceidInLog=50, path=hdfs://master:54310/hbase/data/default/usertable/667c1f2c34fe37d48ba25a789d8aba98/recovered.edits/0000000000000000050
2014-07-01 15:31:33,961 INFO  [RS_OPEN_REGION-slave1:60020-2] regionserver.HRegion: Replaying edits from hdfs://master:54310/hbase/data/default/usertable/667c1f2c34fe37d48ba25a789d8aba98/recovered.edits/0000000000000000063
2014-07-01 15:31:35,015 DEBUG [RS_OPEN_REGION-slave1:60020-2] zookeeper.ZKAssign: regionserver:60020-0x146f40d8cbe0002, quorum=sceplus-vm49.almaden.ibm.com:2181,sceplus-vm48.almaden.ibm.com:2181, baseZNode=/hbase Attempting to retransition opening state of node 667c1f2c34fe37d48ba25a789d8aba98
2014-07-01 15:31:35,017 DEBUG [RS_OPEN_REGION-slave1:60020-2] regionserver.HRegion: Applied 73650, skipped 0, firstSequenceidInLog=51, maxSequenceidInLog=63, path=hdfs://master:54310/hbase/data/default/usertable/667c1f2c34fe37d48ba25a789d8aba98/recovered.edits/0000000000000000063
2014-07-01 15:31:35,023 INFO  [RS_OPEN_REGION-slave1:60020-2] regionserver.HRegion: Replaying edits from hdfs://master:54310/hbase/data/default/usertable/667c1f2c34fe37d48ba25a789d8aba98/recovered.edits/0000000000000000069
2014-07-01 15:31:35,225 DEBUG [RS_OPEN_REGION-slave1:60020-2] zookeeper.ZKAssign: regionserver:60020-0x146f40d8cbe0002, quorum=sceplus-vm49.almaden.ibm.com:2181,sceplus-vm48.almaden.ibm.com:2181, baseZNode=/hbase Attempting to retransition opening state of node 667c1f2c34fe37d48ba25a789d8aba98
2014-07-01 15:31:35,227 DEBUG [RS_OPEN_REGION-slave1:60020-2] regionserver.HRegion: Applied 33890, skipped 0, firstSequenceidInLog=64, maxSequenceidInLog=69, path=hdfs://master:54310/hbase/data/default/usertable/667c1f2c34fe37d48ba25a789d8aba98/recovered.edits/0000000000000000069
2014-07-01 15:31:35,231 INFO  [RS_OPEN_REGION-slave1:60020-2] regionserver.HRegion: Replaying edits from hdfs://master:54310/hbase/data/default/usertable/667c1f2c34fe37d48ba25a789d8aba98/recovered.edits/0000000000000000081
2014-07-01 15:31:35,597 INFO  [RS_OPEN_REGION-slave1:60020-1] regionserver.DefaultStoreFlusher: Flushed, sequenceid=112, memsize=51.9m, hasBloomFilter=true, into tmp file hdfs://master:54310/hbase/data/default/usertable/bb79ae773f71289e0745068f68a0abd9/.tmp/9a37cf2d20d042d4b5429677f7f034fa
2014-07-01 15:31:35,636 DEBUG [RS_OPEN_REGION-slave1:60020-1] regionserver.HRegionFileSystem: Committing store file hdfs://master:54310/hbase/data/default/usertable/bb79ae773f71289e0745068f68a0abd9/.tmp/9a37cf2d20d042d4b5429677f7f034fa as hdfs://master:54310/hbase/data/default/usertable/bb79ae773f71289e0745068f68a0abd9/family/9a37cf2d20d042d4b5429677f7f034fa
2014-07-01 15:31:35,673 INFO  [RS_OPEN_REGION-slave1:60020-1] regionserver.HStore: Added hdfs://master:54310/hbase/data/default/usertable/bb79ae773f71289e0745068f68a0abd9/family/9a37cf2d20d042d4b5429677f7f034fa, entries=188910, sequenceid=112, filesize=13.5m
2014-07-01 15:31:35,674 INFO  [RS_OPEN_REGION-slave1:60020-1] regionserver.HRegion: Finished memstore flush of ~51.9m/54400240, currentsize=0.0/0 for region usertable,user9,1404228265526.bb79ae773f71289e0745068f68a0abd9. in 2454ms, sequenceid=112, compaction requested=false; wal=null
2014-07-01 15:31:35,677 DEBUG [RS_OPEN_REGION-slave1:60020-1] regionserver.HRegion: Deleted recovered.edits file=hdfs://master:54310/hbase/data/default/usertable/bb79ae773f71289e0745068f68a0abd9/recovered.edits/0000000000000000008
2014-07-01 15:31:35,679 DEBUG [RS_OPEN_REGION-slave1:60020-1] regionserver.HRegion: Deleted recovered.edits file=hdfs://master:54310/hbase/data/default/usertable/bb79ae773f71289e0745068f68a0abd9/recovered.edits/0000000000000000016
2014-07-01 15:31:35,681 DEBUG [RS_OPEN_REGION-slave1:60020-1] regionserver.HRegion: Deleted recovered.edits file=hdfs://master:54310/hbase/data/default/usertable/bb79ae773f71289e0745068f68a0abd9/recovered.edits/0000000000000000029
2014-07-01 15:31:35,685 DEBUG [RS_OPEN_REGION-slave1:60020-1] regionserver.HRegion: Deleted recovered.edits file=hdfs://master:54310/hbase/data/default/usertable/bb79ae773f71289e0745068f68a0abd9/recovered.edits/0000000000000000043
2014-07-01 15:31:35,689 DEBUG [RS_OPEN_REGION-slave1:60020-1] regionserver.HRegion: Deleted recovered.edits file=hdfs://master:54310/hbase/data/default/usertable/bb79ae773f71289e0745068f68a0abd9/recovered.edits/0000000000000000046
2014-07-01 15:31:35,691 DEBUG [RS_OPEN_REGION-slave1:60020-1] regionserver.HRegion: Deleted recovered.edits file=hdfs://master:54310/hbase/data/default/usertable/bb79ae773f71289e0745068f68a0abd9/recovered.edits/0000000000000000059
2014-07-01 15:31:35,693 DEBUG [RS_OPEN_REGION-slave1:60020-2] zookeeper.ZKAssign: regionserver:60020-0x146f40d8cbe0002, quorum=sceplus-vm49.almaden.ibm.com:2181,sceplus-vm48.almaden.ibm.com:2181, baseZNode=/hbase Attempting to retransition opening state of node 667c1f2c34fe37d48ba25a789d8aba98
2014-07-01 15:31:35,695 DEBUG [RS_OPEN_REGION-slave1:60020-2] regionserver.HRegion: Applied 68200, skipped 0, firstSequenceidInLog=70, maxSequenceidInLog=81, path=hdfs://master:54310/hbase/data/default/usertable/667c1f2c34fe37d48ba25a789d8aba98/recovered.edits/0000000000000000081
2014-07-01 15:31:35,696 DEBUG [RS_OPEN_REGION-slave1:60020-1] regionserver.HRegion: Deleted recovered.edits file=hdfs://master:54310/hbase/data/default/usertable/bb79ae773f71289e0745068f68a0abd9/recovered.edits/0000000000000000067
2014-07-01 15:31:35,706 DEBUG [RS_OPEN_REGION-slave1:60020-1] regionserver.HRegion: Deleted recovered.edits file=hdfs://master:54310/hbase/data/default/usertable/bb79ae773f71289e0745068f68a0abd9/recovered.edits/0000000000000000077
2014-07-01 15:31:35,710 INFO  [RS_OPEN_REGION-slave1:60020-2] regionserver.HRegion: Replaying edits from hdfs://master:54310/hbase/data/default/usertable/667c1f2c34fe37d48ba25a789d8aba98/recovered.edits/0000000000000000090
2014-07-01 15:31:35,712 DEBUG [RS_OPEN_REGION-slave1:60020-1] regionserver.HRegion: Deleted recovered.edits file=hdfs://master:54310/hbase/data/default/usertable/bb79ae773f71289e0745068f68a0abd9/recovered.edits/0000000000000000088
2014-07-01 15:31:35,723 DEBUG [RS_OPEN_REGION-slave1:60020-1] regionserver.HRegion: Deleted recovered.edits file=hdfs://master:54310/hbase/data/default/usertable/bb79ae773f71289e0745068f68a0abd9/recovered.edits/0000000000000000097
2014-07-01 15:31:35,731 DEBUG [RS_OPEN_REGION-slave1:60020-1] regionserver.HRegion: Deleted recovered.edits file=hdfs://master:54310/hbase/data/default/usertable/bb79ae773f71289e0745068f68a0abd9/recovered.edits/0000000000000000112
2014-07-01 15:31:35,745 INFO  [RS_OPEN_REGION-slave1:60020-1] regionserver.HRegion: Onlined bb79ae773f71289e0745068f68a0abd9; next sequenceid=113
2014-07-01 15:31:35,745 DEBUG [RS_OPEN_REGION-slave1:60020-1] zookeeper.ZKAssign: regionserver:60020-0x146f40d8cbe0002, quorum=sceplus-vm49.almaden.ibm.com:2181,sceplus-vm48.almaden.ibm.com:2181, baseZNode=/hbase Attempting to retransition opening state of node bb79ae773f71289e0745068f68a0abd9
2014-07-01 15:31:35,747 INFO  [PostOpenDeployTasks:bb79ae773f71289e0745068f68a0abd9] regionserver.HRegionServer: Post open deploy tasks for region=usertable,user9,1404228265526.bb79ae773f71289e0745068f68a0abd9.
2014-07-01 15:31:35,765 INFO  [PostOpenDeployTasks:bb79ae773f71289e0745068f68a0abd9] catalog.MetaEditor: Updated row usertable,user9,1404228265526.bb79ae773f71289e0745068f68a0abd9. with server=slave1,60020,1404253868846
2014-07-01 15:31:35,765 INFO  [PostOpenDeployTasks:bb79ae773f71289e0745068f68a0abd9] regionserver.HRegionServer: Finished post open deploy task for usertable,user9,1404228265526.bb79ae773f71289e0745068f68a0abd9.
2014-07-01 15:31:35,767 DEBUG [RS_OPEN_REGION-slave1:60020-1] zookeeper.ZKAssign: regionserver:60020-0x146f40d8cbe0002, quorum=sceplus-vm49.almaden.ibm.com:2181,sceplus-vm48.almaden.ibm.com:2181, baseZNode=/hbase Transitioning bb79ae773f71289e0745068f68a0abd9 from RS_ZK_REGION_OPENING to RS_ZK_REGION_OPENED
2014-07-01 15:31:35,775 DEBUG [RS_OPEN_REGION-slave1:60020-1] zookeeper.ZKAssign: regionserver:60020-0x146f40d8cbe0002, quorum=sceplus-vm49.almaden.ibm.com:2181,sceplus-vm48.almaden.ibm.com:2181, baseZNode=/hbase Transitioned node bb79ae773f71289e0745068f68a0abd9 from RS_ZK_REGION_OPENING to RS_ZK_REGION_OPENED
2014-07-01 15:31:35,776 DEBUG [RS_OPEN_REGION-slave1:60020-1] handler.OpenRegionHandler: Transitioned bb79ae773f71289e0745068f68a0abd9 to OPENED in zk on slave1,60020,1404253868846
2014-07-01 15:31:35,776 DEBUG [RS_OPEN_REGION-slave1:60020-1] handler.OpenRegionHandler: Opened usertable,user9,1404228265526.bb79ae773f71289e0745068f68a0abd9. on slave1,60020,1404253868846
2014-07-01 15:31:36,016 DEBUG [RS_OPEN_REGION-slave1:60020-2] zookeeper.ZKAssign: regionserver:60020-0x146f40d8cbe0002, quorum=sceplus-vm49.almaden.ibm.com:2181,sceplus-vm48.almaden.ibm.com:2181, baseZNode=/hbase Attempting to retransition opening state of node 667c1f2c34fe37d48ba25a789d8aba98
2014-07-01 15:31:36,018 DEBUG [RS_OPEN_REGION-slave1:60020-2] regionserver.HRegion: Applied 50670, skipped 0, firstSequenceidInLog=82, maxSequenceidInLog=90, path=hdfs://master:54310/hbase/data/default/usertable/667c1f2c34fe37d48ba25a789d8aba98/recovered.edits/0000000000000000090
2014-07-01 15:31:36,021 INFO  [RS_OPEN_REGION-slave1:60020-2] regionserver.HRegion: Replaying edits from hdfs://master:54310/hbase/data/default/usertable/667c1f2c34fe37d48ba25a789d8aba98/recovered.edits/0000000000000000100
2014-07-01 15:31:36,429 DEBUG [RS_OPEN_REGION-slave1:60020-2] zookeeper.ZKAssign: regionserver:60020-0x146f40d8cbe0002, quorum=sceplus-vm49.almaden.ibm.com:2181,sceplus-vm48.almaden.ibm.com:2181, baseZNode=/hbase Attempting to retransition opening state of node 667c1f2c34fe37d48ba25a789d8aba98
2014-07-01 15:31:36,432 DEBUG [RS_OPEN_REGION-slave1:60020-2] regionserver.HRegion: Applied 56580, skipped 0, firstSequenceidInLog=91, maxSequenceidInLog=100, path=hdfs://master:54310/hbase/data/default/usertable/667c1f2c34fe37d48ba25a789d8aba98/recovered.edits/0000000000000000100
2014-07-01 15:31:36,440 INFO  [RS_OPEN_REGION-slave1:60020-2] regionserver.HRegion: Replaying edits from hdfs://master:54310/hbase/data/default/usertable/667c1f2c34fe37d48ba25a789d8aba98/recovered.edits/0000000000000000112
2014-07-01 15:31:36,857 DEBUG [RS_OPEN_REGION-slave1:60020-2] zookeeper.ZKAssign: regionserver:60020-0x146f40d8cbe0002, quorum=sceplus-vm49.almaden.ibm.com:2181,sceplus-vm48.almaden.ibm.com:2181, baseZNode=/hbase Attempting to retransition opening state of node 667c1f2c34fe37d48ba25a789d8aba98
2014-07-01 15:31:36,858 DEBUG [RS_OPEN_REGION-slave1:60020-2] regionserver.HRegion: Applied 68140, skipped 0, firstSequenceidInLog=101, maxSequenceidInLog=112, path=hdfs://master:54310/hbase/data/default/usertable/667c1f2c34fe37d48ba25a789d8aba98/recovered.edits/0000000000000000112
2014-07-01 15:31:36,859 DEBUG [RS_OPEN_REGION-slave1:60020-2] regionserver.HRegion: Started memstore flush for usertable,user7,1404228265526.667c1f2c34fe37d48ba25a789d8aba98., current region memstore size 172.5m; wal is null, using passed sequenceid=112
2014-07-01 15:31:36,927 DEBUG [RS_OPEN_REGION-slave1:60020-2] util.FSUtils: DFS Client does not support most favored nodes create; using default create
2014-07-01 15:31:38,911 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-01 15:31:38,934 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 91 synced till here 85
2014-07-01 15:31:38,955 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1404253868846/slave1%2C60020%2C1404253868846.1404253870211 with entries=91, filesize=68.9m; new WAL /hbase/WALs/slave1,60020,1404253868846/slave1%2C60020%2C1404253868846.1404253898912
2014-07-01 15:31:41,472 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-01 15:31:41,488 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 178 synced till here 171
2014-07-01 15:31:41,683 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1404253868846/slave1%2C60020%2C1404253868846.1404253898912 with entries=87, filesize=70.4m; new WAL /hbase/WALs/slave1,60020,1404253868846/slave1%2C60020%2C1404253868846.1404253901472
2014-07-01 15:31:43,197 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-01 15:31:43,214 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 283 synced till here 273
2014-07-01 15:31:43,247 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1404253868846/slave1%2C60020%2C1404253868846.1404253901472 with entries=105, filesize=63.8m; new WAL /hbase/WALs/slave1,60020,1404253868846/slave1%2C60020%2C1404253868846.1404253903198
2014-07-01 15:31:45,491 INFO  [RS_OPEN_REGION-slave1:60020-2] regionserver.DefaultStoreFlusher: Flushed, sequenceid=112, memsize=172.5m, hasBloomFilter=true, into tmp file hdfs://master:54310/hbase/data/default/usertable/667c1f2c34fe37d48ba25a789d8aba98/.tmp/9fdb368501fa45e18be2f611d85417a8
2014-07-01 15:31:45,509 DEBUG [RS_OPEN_REGION-slave1:60020-2] regionserver.HRegionFileSystem: Committing store file hdfs://master:54310/hbase/data/default/usertable/667c1f2c34fe37d48ba25a789d8aba98/.tmp/9fdb368501fa45e18be2f611d85417a8 as hdfs://master:54310/hbase/data/default/usertable/667c1f2c34fe37d48ba25a789d8aba98/family/9fdb368501fa45e18be2f611d85417a8
2014-07-01 15:31:45,520 INFO  [RS_OPEN_REGION-slave1:60020-2] regionserver.HStore: Added hdfs://master:54310/hbase/data/default/usertable/667c1f2c34fe37d48ba25a789d8aba98/family/9fdb368501fa45e18be2f611d85417a8, entries=628180, sequenceid=112, filesize=44.8m
2014-07-01 15:31:45,521 INFO  [RS_OPEN_REGION-slave1:60020-2] regionserver.HRegion: Finished memstore flush of ~172.5m/180912320, currentsize=0.0/0 for region usertable,user7,1404228265526.667c1f2c34fe37d48ba25a789d8aba98. in 8662ms, sequenceid=112, compaction requested=false; wal=null
2014-07-01 15:31:45,523 DEBUG [RS_OPEN_REGION-slave1:60020-2] regionserver.HRegion: Deleted recovered.edits file=hdfs://master:54310/hbase/data/default/usertable/667c1f2c34fe37d48ba25a789d8aba98/recovered.edits/0000000000000000002
2014-07-01 15:31:45,527 DEBUG [RS_OPEN_REGION-slave1:60020-2] regionserver.HRegion: Deleted recovered.edits file=hdfs://master:54310/hbase/data/default/usertable/667c1f2c34fe37d48ba25a789d8aba98/recovered.edits/0000000000000000011
2014-07-01 15:31:45,528 DEBUG [RS_OPEN_REGION-slave1:60020-2] regionserver.HRegion: Deleted recovered.edits file=hdfs://master:54310/hbase/data/default/usertable/667c1f2c34fe37d48ba25a789d8aba98/recovered.edits/0000000000000000019
2014-07-01 15:31:45,530 DEBUG [RS_OPEN_REGION-slave1:60020-2] regionserver.HRegion: Deleted recovered.edits file=hdfs://master:54310/hbase/data/default/usertable/667c1f2c34fe37d48ba25a789d8aba98/recovered.edits/0000000000000000033
2014-07-01 15:31:45,532 DEBUG [RS_OPEN_REGION-slave1:60020-2] regionserver.HRegion: Deleted recovered.edits file=hdfs://master:54310/hbase/data/default/usertable/667c1f2c34fe37d48ba25a789d8aba98/recovered.edits/0000000000000000043
2014-07-01 15:31:45,533 DEBUG [RS_OPEN_REGION-slave1:60020-2] regionserver.HRegion: Deleted recovered.edits file=hdfs://master:54310/hbase/data/default/usertable/667c1f2c34fe37d48ba25a789d8aba98/recovered.edits/0000000000000000050
2014-07-01 15:31:45,535 DEBUG [RS_OPEN_REGION-slave1:60020-2] regionserver.HRegion: Deleted recovered.edits file=hdfs://master:54310/hbase/data/default/usertable/667c1f2c34fe37d48ba25a789d8aba98/recovered.edits/0000000000000000063
2014-07-01 15:31:45,537 DEBUG [RS_OPEN_REGION-slave1:60020-2] regionserver.HRegion: Deleted recovered.edits file=hdfs://master:54310/hbase/data/default/usertable/667c1f2c34fe37d48ba25a789d8aba98/recovered.edits/0000000000000000069
2014-07-01 15:31:45,538 DEBUG [RS_OPEN_REGION-slave1:60020-2] regionserver.HRegion: Deleted recovered.edits file=hdfs://master:54310/hbase/data/default/usertable/667c1f2c34fe37d48ba25a789d8aba98/recovered.edits/0000000000000000081
2014-07-01 15:31:45,540 DEBUG [RS_OPEN_REGION-slave1:60020-2] regionserver.HRegion: Deleted recovered.edits file=hdfs://master:54310/hbase/data/default/usertable/667c1f2c34fe37d48ba25a789d8aba98/recovered.edits/0000000000000000090
2014-07-01 15:31:45,541 DEBUG [RS_OPEN_REGION-slave1:60020-2] regionserver.HRegion: Deleted recovered.edits file=hdfs://master:54310/hbase/data/default/usertable/667c1f2c34fe37d48ba25a789d8aba98/recovered.edits/0000000000000000100
2014-07-01 15:31:45,551 DEBUG [RS_OPEN_REGION-slave1:60020-2] regionserver.HRegion: Deleted recovered.edits file=hdfs://master:54310/hbase/data/default/usertable/667c1f2c34fe37d48ba25a789d8aba98/recovered.edits/0000000000000000112
2014-07-01 15:31:45,559 INFO  [RS_OPEN_REGION-slave1:60020-2] regionserver.HRegion: Onlined 667c1f2c34fe37d48ba25a789d8aba98; next sequenceid=113
2014-07-01 15:31:45,559 DEBUG [RS_OPEN_REGION-slave1:60020-2] zookeeper.ZKAssign: regionserver:60020-0x146f40d8cbe0002, quorum=sceplus-vm49.almaden.ibm.com:2181,sceplus-vm48.almaden.ibm.com:2181, baseZNode=/hbase Attempting to retransition opening state of node 667c1f2c34fe37d48ba25a789d8aba98
2014-07-01 15:31:45,561 INFO  [PostOpenDeployTasks:667c1f2c34fe37d48ba25a789d8aba98] regionserver.HRegionServer: Post open deploy tasks for region=usertable,user7,1404228265526.667c1f2c34fe37d48ba25a789d8aba98.
2014-07-01 15:31:45,604 INFO  [PostOpenDeployTasks:667c1f2c34fe37d48ba25a789d8aba98] catalog.MetaEditor: Updated row usertable,user7,1404228265526.667c1f2c34fe37d48ba25a789d8aba98. with server=slave1,60020,1404253868846
2014-07-01 15:31:45,604 INFO  [PostOpenDeployTasks:667c1f2c34fe37d48ba25a789d8aba98] regionserver.HRegionServer: Finished post open deploy task for usertable,user7,1404228265526.667c1f2c34fe37d48ba25a789d8aba98.
2014-07-01 15:31:45,605 DEBUG [RS_OPEN_REGION-slave1:60020-2] zookeeper.ZKAssign: regionserver:60020-0x146f40d8cbe0002, quorum=sceplus-vm49.almaden.ibm.com:2181,sceplus-vm48.almaden.ibm.com:2181, baseZNode=/hbase Transitioning 667c1f2c34fe37d48ba25a789d8aba98 from RS_ZK_REGION_OPENING to RS_ZK_REGION_OPENED
2014-07-01 15:31:45,611 DEBUG [RS_OPEN_REGION-slave1:60020-2] zookeeper.ZKAssign: regionserver:60020-0x146f40d8cbe0002, quorum=sceplus-vm49.almaden.ibm.com:2181,sceplus-vm48.almaden.ibm.com:2181, baseZNode=/hbase Transitioned node 667c1f2c34fe37d48ba25a789d8aba98 from RS_ZK_REGION_OPENING to RS_ZK_REGION_OPENED
2014-07-01 15:31:45,612 DEBUG [RS_OPEN_REGION-slave1:60020-2] handler.OpenRegionHandler: Transitioned 667c1f2c34fe37d48ba25a789d8aba98 to OPENED in zk on slave1,60020,1404253868846
2014-07-01 15:31:45,612 DEBUG [RS_OPEN_REGION-slave1:60020-2] handler.OpenRegionHandler: Opened usertable,user7,1404228265526.667c1f2c34fe37d48ba25a789d8aba98. on slave1,60020,1404253868846
2014-07-01 15:31:45,812 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-01 15:31:45,946 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1404253868846/slave1%2C60020%2C1404253868846.1404253903198 with entries=113, filesize=61.3m; new WAL /hbase/WALs/slave1,60020,1404253868846/slave1%2C60020%2C1404253868846.1404253905812
2014-07-01 15:31:48,429 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-01 15:31:48,458 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1404253868846/slave1%2C60020%2C1404253868846.1404253905812 with entries=79, filesize=62.0m; new WAL /hbase/WALs/slave1,60020,1404253868846/slave1%2C60020%2C1404253868846.1404253908430
2014-07-01 15:31:50,363 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-01 15:31:50,513 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 568 synced till here 556
2014-07-01 15:31:50,720 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1404253868846/slave1%2C60020%2C1404253868846.1404253908430 with entries=93, filesize=73.6m; new WAL /hbase/WALs/slave1,60020,1404253868846/slave1%2C60020%2C1404253868846.1404253910363
2014-07-01 15:31:52,913 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-01 15:31:53,781 DEBUG [RpcServer.handler=3,port=60020] regionserver.HRegion: Flush requested on usertable,user3,1404228265526.93253e37c284f3174309fede3f32339f.
2014-07-01 15:31:53,788 DEBUG [MemStoreFlusher.1] regionserver.HRegion: Started memstore flush for usertable,user3,1404228265526.93253e37c284f3174309fede3f32339f., current region memstore size 261.4m
2014-07-01 15:31:53,939 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 713 synced till here 696
2014-07-01 15:31:54,108 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1404253868846/slave1%2C60020%2C1404253868846.1404253910363 with entries=145, filesize=111.0m; new WAL /hbase/WALs/slave1,60020,1404253868846/slave1%2C60020%2C1404253868846.1404253912914
2014-07-01 15:31:54,246 DEBUG [MemStoreFlusher.1] util.FSUtils: DFS Client does not support most favored nodes create; using default create
2014-07-01 15:31:55,081 DEBUG [RpcServer.handler=45,port=60020] regionserver.HRegion: Flush requested on usertable,user5,1404228265526.122e92eabfab38cb15a9965b21db25c5.
2014-07-01 15:31:55,081 DEBUG [MemStoreFlusher.0] regionserver.HRegion: Started memstore flush for usertable,user5,1404228265526.122e92eabfab38cb15a9965b21db25c5., current region memstore size 260.4m
2014-07-01 15:31:56,410 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-01 15:31:56,773 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 828 synced till here 803
2014-07-01 15:31:57,353 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1404253868846/slave1%2C60020%2C1404253868846.1404253912914 with entries=115, filesize=86.3m; new WAL /hbase/WALs/slave1,60020,1404253868846/slave1%2C60020%2C1404253868846.1404253916410
2014-07-01 15:31:57,665 DEBUG [MemStoreFlusher.0] util.FSUtils: DFS Client does not support most favored nodes create; using default create
2014-07-01 15:32:00,066 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-01 15:32:00,198 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 958 synced till here 928
2014-07-01 15:32:01,399 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1404253868846/slave1%2C60020%2C1404253868846.1404253916410 with entries=130, filesize=88.6m; new WAL /hbase/WALs/slave1,60020,1404253868846/slave1%2C60020%2C1404253868846.1404253920067
2014-07-01 15:32:02,249 DEBUG [RpcServer.handler=9,port=60020] regionserver.HRegion: Flush requested on usertable,user7,1404228265526.667c1f2c34fe37d48ba25a789d8aba98.
2014-07-01 15:32:04,284 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-01 15:32:04,504 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 1079 synced till here 1046
2014-07-01 15:32:05,426 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1404253868846/slave1%2C60020%2C1404253868846.1404253920067 with entries=121, filesize=85.8m; new WAL /hbase/WALs/slave1,60020,1404253868846/slave1%2C60020%2C1404253868846.1404253924285
2014-07-01 15:32:08,415 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-01 15:32:08,440 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 1197 synced till here 1175
2014-07-01 15:32:09,120 DEBUG [RpcServer.handler=42,port=60020] regionserver.HRegion: Flush requested on usertable,,1404228265525.9cef9e7d34f96f1d9b111be50040a208.
2014-07-01 15:32:09,147 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1404253868846/slave1%2C60020%2C1404253868846.1404253924285 with entries=118, filesize=86.5m; new WAL /hbase/WALs/slave1,60020,1404253868846/slave1%2C60020%2C1404253868846.1404253928416
2014-07-01 15:32:12,047 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-01 15:32:12,179 INFO  [MemStoreFlusher.1] regionserver.DefaultStoreFlusher: Flushed, sequenceid=281, memsize=163.5m, hasBloomFilter=true, into tmp file hdfs://master:54310/hbase/data/default/usertable/93253e37c284f3174309fede3f32339f/.tmp/dc4f955fdaeb4e22b5a5efc1dbc4a6de
2014-07-01 15:32:12,191 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 1320 synced till here 1299
2014-07-01 15:32:12,409 DEBUG [MemStoreFlusher.1] regionserver.HRegionFileSystem: Committing store file hdfs://master:54310/hbase/data/default/usertable/93253e37c284f3174309fede3f32339f/.tmp/dc4f955fdaeb4e22b5a5efc1dbc4a6de as hdfs://master:54310/hbase/data/default/usertable/93253e37c284f3174309fede3f32339f/family/dc4f955fdaeb4e22b5a5efc1dbc4a6de
2014-07-01 15:32:12,428 INFO  [MemStoreFlusher.1] regionserver.HStore: Added hdfs://master:54310/hbase/data/default/usertable/93253e37c284f3174309fede3f32339f/family/dc4f955fdaeb4e22b5a5efc1dbc4a6de, entries=595160, sequenceid=281, filesize=42.4m
2014-07-01 15:32:12,443 INFO  [MemStoreFlusher.1] regionserver.HRegion: Finished memstore flush of ~261.4m/274129440, currentsize=204.4m/214290320 for region usertable,user3,1404228265526.93253e37c284f3174309fede3f32339f. in 18655ms, sequenceid=281, compaction requested=false
2014-07-01 15:32:12,444 DEBUG [MemStoreFlusher.1] regionserver.HRegion: Started memstore flush for usertable,user7,1404228265526.667c1f2c34fe37d48ba25a789d8aba98., current region memstore size 380.2m
2014-07-01 15:32:12,582 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1404253868846/slave1%2C60020%2C1404253868846.1404253928416 with entries=123, filesize=74.7m; new WAL /hbase/WALs/slave1,60020,1404253868846/slave1%2C60020%2C1404253868846.1404253932048
2014-07-01 15:32:14,444 DEBUG [MemStoreFlusher.1] util.FSUtils: DFS Client does not support most favored nodes create; using default create
2014-07-01 15:32:14,647 INFO  [MemStoreFlusher.0] regionserver.DefaultStoreFlusher: Flushed, sequenceid=284, memsize=169.8m, hasBloomFilter=true, into tmp file hdfs://master:54310/hbase/data/default/usertable/122e92eabfab38cb15a9965b21db25c5/.tmp/87033878504c46e29934056168d8d614
2014-07-01 15:32:14,677 DEBUG [MemStoreFlusher.0] regionserver.HRegionFileSystem: Committing store file hdfs://master:54310/hbase/data/default/usertable/122e92eabfab38cb15a9965b21db25c5/.tmp/87033878504c46e29934056168d8d614 as hdfs://master:54310/hbase/data/default/usertable/122e92eabfab38cb15a9965b21db25c5/family/87033878504c46e29934056168d8d614
2014-07-01 15:32:14,693 INFO  [MemStoreFlusher.0] regionserver.HStore: Added hdfs://master:54310/hbase/data/default/usertable/122e92eabfab38cb15a9965b21db25c5/family/87033878504c46e29934056168d8d614, entries=618220, sequenceid=284, filesize=44.1m
2014-07-01 15:32:14,694 INFO  [MemStoreFlusher.0] regionserver.HRegion: Finished memstore flush of ~269.2m/282311360, currentsize=197.7m/207343680 for region usertable,user5,1404228265526.122e92eabfab38cb15a9965b21db25c5. in 19613ms, sequenceid=284, compaction requested=false
2014-07-01 15:32:14,694 DEBUG [MemStoreFlusher.0] regionserver.HRegion: Started memstore flush for usertable,,1404228265525.9cef9e7d34f96f1d9b111be50040a208., current region memstore size 295.0m
2014-07-01 15:32:14,910 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-01 15:32:14,981 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 1443 synced till here 1423
2014-07-01 15:32:15,367 DEBUG [RpcServer.handler=49,port=60020] regionserver.HRegion: Flush requested on usertable,user3,1404228265526.93253e37c284f3174309fede3f32339f.
2014-07-01 15:32:15,384 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1404253868846/slave1%2C60020%2C1404253868846.1404253932048 with entries=123, filesize=91.7m; new WAL /hbase/WALs/slave1,60020,1404253868846/slave1%2C60020%2C1404253868846.1404253934911
2014-07-01 15:32:16,085 DEBUG [MemStoreFlusher.0] util.FSUtils: DFS Client does not support most favored nodes create; using default create
2014-07-01 15:32:17,391 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-01 15:32:17,486 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 1575 synced till here 1537
2014-07-01 15:32:17,816 DEBUG [RpcServer.handler=37,port=60020] regionserver.HRegion: Flush requested on usertable,user5,1404228265526.122e92eabfab38cb15a9965b21db25c5.
2014-07-01 15:32:18,159 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1404253868846/slave1%2C60020%2C1404253868846.1404253934911 with entries=132, filesize=93.6m; new WAL /hbase/WALs/slave1,60020,1404253868846/slave1%2C60020%2C1404253868846.1404253937391
2014-07-01 15:32:19,675 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-01 15:32:19,888 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 1697 synced till here 1681
2014-07-01 15:32:19,933 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1404253868846/slave1%2C60020%2C1404253868846.1404253937391 with entries=122, filesize=78.1m; new WAL /hbase/WALs/slave1,60020,1404253868846/slave1%2C60020%2C1404253868846.1404253939675
2014-07-01 15:32:22,564 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-01 15:32:23,675 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 1887 synced till here 1840
2014-07-01 15:32:24,812 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1404253868846/slave1%2C60020%2C1404253868846.1404253939675 with entries=190, filesize=135.1m; new WAL /hbase/WALs/slave1,60020,1404253868846/slave1%2C60020%2C1404253868846.1404253942565
2014-07-01 15:32:27,543 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-01 15:32:27,883 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 1997 synced till here 1977
2014-07-01 15:32:29,404 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1404253868846/slave1%2C60020%2C1404253868846.1404253942565 with entries=110, filesize=75.7m; new WAL /hbase/WALs/slave1,60020,1404253868846/slave1%2C60020%2C1404253868846.1404253947543
2014-07-01 15:32:31,767 INFO  [MemStoreFlusher.0] regionserver.DefaultStoreFlusher: Flushed, sequenceid=380, memsize=178.4m, hasBloomFilter=true, into tmp file hdfs://master:54310/hbase/data/default/usertable/9cef9e7d34f96f1d9b111be50040a208/.tmp/d503bdfe9b5742faa7fd875ddeb4e5ea
2014-07-01 15:32:31,783 DEBUG [MemStoreFlusher.0] regionserver.HRegionFileSystem: Committing store file hdfs://master:54310/hbase/data/default/usertable/9cef9e7d34f96f1d9b111be50040a208/.tmp/d503bdfe9b5742faa7fd875ddeb4e5ea as hdfs://master:54310/hbase/data/default/usertable/9cef9e7d34f96f1d9b111be50040a208/family/d503bdfe9b5742faa7fd875ddeb4e5ea
2014-07-01 15:32:31,800 INFO  [MemStoreFlusher.0] regionserver.HStore: Added hdfs://master:54310/hbase/data/default/usertable/9cef9e7d34f96f1d9b111be50040a208/family/d503bdfe9b5742faa7fd875ddeb4e5ea, entries=649670, sequenceid=380, filesize=46.3m
2014-07-01 15:32:31,801 INFO  [MemStoreFlusher.0] regionserver.HRegion: Finished memstore flush of ~296.6m/311050160, currentsize=74.4m/78063360 for region usertable,,1404228265525.9cef9e7d34f96f1d9b111be50040a208. in 17107ms, sequenceid=380, compaction requested=false
2014-07-01 15:32:31,802 DEBUG [MemStoreFlusher.0] regionserver.HRegion: Started memstore flush for usertable,user3,1404228265526.93253e37c284f3174309fede3f32339f., current region memstore size 473.3m
2014-07-01 15:32:31,946 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-01 15:32:32,356 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 2122 synced till here 2117
2014-07-01 15:32:32,393 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1404253868846/slave1%2C60020%2C1404253868846.1404253947543 with entries=125, filesize=86.9m; new WAL /hbase/WALs/slave1,60020,1404253868846/slave1%2C60020%2C1404253868846.1404253951947
2014-07-01 15:32:33,464 DEBUG [MemStoreFlusher.0] util.FSUtils: DFS Client does not support most favored nodes create; using default create
2014-07-01 15:32:33,683 INFO  [MemStoreFlusher.1] regionserver.DefaultStoreFlusher: Flushed, sequenceid=364, memsize=180.5m, hasBloomFilter=true, into tmp file hdfs://master:54310/hbase/data/default/usertable/667c1f2c34fe37d48ba25a789d8aba98/.tmp/df03e3e8f7f94f0abbc1add81984086b
2014-07-01 15:32:33,698 DEBUG [MemStoreFlusher.1] regionserver.HRegionFileSystem: Committing store file hdfs://master:54310/hbase/data/default/usertable/667c1f2c34fe37d48ba25a789d8aba98/.tmp/df03e3e8f7f94f0abbc1add81984086b as hdfs://master:54310/hbase/data/default/usertable/667c1f2c34fe37d48ba25a789d8aba98/family/df03e3e8f7f94f0abbc1add81984086b
2014-07-01 15:32:33,711 INFO  [MemStoreFlusher.1] regionserver.HStore: Added hdfs://master:54310/hbase/data/default/usertable/667c1f2c34fe37d48ba25a789d8aba98/family/df03e3e8f7f94f0abbc1add81984086b, entries=657170, sequenceid=364, filesize=46.8m
2014-07-01 15:32:33,712 INFO  [MemStoreFlusher.1] regionserver.HRegion: Finished memstore flush of ~392.5m/411615120, currentsize=288.2m/302204560 for region usertable,user7,1404228265526.667c1f2c34fe37d48ba25a789d8aba98. in 21267ms, sequenceid=364, compaction requested=false
2014-07-01 15:32:33,712 DEBUG [MemStoreFlusher.1] regionserver.HRegion: Started memstore flush for usertable,user5,1404228265526.122e92eabfab38cb15a9965b21db25c5., current region memstore size 481.9m
2014-07-01 15:32:33,859 DEBUG [RpcServer.handler=20,port=60020] regionserver.HRegion: Flush requested on usertable,user7,1404228265526.667c1f2c34fe37d48ba25a789d8aba98.
2014-07-01 15:32:34,258 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-01 15:32:34,346 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 2251 synced till here 2219
2014-07-01 15:32:35,116 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1404253868846/slave1%2C60020%2C1404253868846.1404253951947 with entries=129, filesize=85.2m; new WAL /hbase/WALs/slave1,60020,1404253868846/slave1%2C60020%2C1404253868846.1404253954259
2014-07-01 15:32:35,135 DEBUG [MemStoreFlusher.1] util.FSUtils: DFS Client does not support most favored nodes create; using default create
2014-07-01 15:32:37,313 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-01 15:32:37,485 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 2368 synced till here 2343
2014-07-01 15:32:38,391 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1404253868846/slave1%2C60020%2C1404253868846.1404253954259 with entries=117, filesize=86.8m; new WAL /hbase/WALs/slave1,60020,1404253868846/slave1%2C60020%2C1404253868846.1404253957314
2014-07-01 15:32:41,943 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-01 15:32:41,998 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 2518 synced till here 2492
2014-07-01 15:32:42,468 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1404253868846/slave1%2C60020%2C1404253868846.1404253957314 with entries=150, filesize=91.7m; new WAL /hbase/WALs/slave1,60020,1404253868846/slave1%2C60020%2C1404253868846.1404253961944
2014-07-01 15:32:44,268 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-01 15:32:44,298 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 2636 synced till here 2633
2014-07-01 15:32:44,370 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1404253868846/slave1%2C60020%2C1404253868846.1404253961944 with entries=118, filesize=63.2m; new WAL /hbase/WALs/slave1,60020,1404253868846/slave1%2C60020%2C1404253868846.1404253964269
2014-07-01 15:32:44,785 INFO  [MemStoreFlusher.0] regionserver.DefaultStoreFlusher: Flushed, sequenceid=604, memsize=72.1m, hasBloomFilter=true, into tmp file hdfs://master:54310/hbase/data/default/usertable/93253e37c284f3174309fede3f32339f/.tmp/e1dbcb59d2c144168bb6d9e50869a573
2014-07-01 15:32:44,809 DEBUG [MemStoreFlusher.0] regionserver.HRegionFileSystem: Committing store file hdfs://master:54310/hbase/data/default/usertable/93253e37c284f3174309fede3f32339f/.tmp/e1dbcb59d2c144168bb6d9e50869a573 as hdfs://master:54310/hbase/data/default/usertable/93253e37c284f3174309fede3f32339f/family/e1dbcb59d2c144168bb6d9e50869a573
2014-07-01 15:32:44,852 INFO  [MemStoreFlusher.0] regionserver.HStore: Added hdfs://master:54310/hbase/data/default/usertable/93253e37c284f3174309fede3f32339f/family/e1dbcb59d2c144168bb6d9e50869a573, entries=262370, sequenceid=604, filesize=18.7m
2014-07-01 15:32:44,852 INFO  [MemStoreFlusher.0] regionserver.HRegion: Finished memstore flush of ~488.0m/511676880, currentsize=141.6m/148484560 for region usertable,user3,1404228265526.93253e37c284f3174309fede3f32339f. in 13050ms, sequenceid=604, compaction requested=true
2014-07-01 15:32:44,856 DEBUG [MemStoreFlusher.0] regionserver.CompactSplitThread: Small Compaction requested: system; Because: MemStoreFlusher.0; compaction_queue=(0:0), split_queue=0, merge_queue=0
2014-07-01 15:32:44,857 DEBUG [MemStoreFlusher.0] regionserver.HRegion: Started memstore flush for usertable,user7,1404228265526.667c1f2c34fe37d48ba25a789d8aba98., current region memstore size 444.3m
2014-07-01 15:32:44,857 DEBUG [regionserver60020-smallCompactions-1404253964856] compactions.RatioBasedCompactionPolicy: Selecting compaction from 3 store files, 0 compacting, 3 eligible, 20 blocking
2014-07-01 15:32:44,860 DEBUG [regionserver60020-smallCompactions-1404253964856] compactions.ExploringCompactionPolicy: Exploring compaction algorithm has selected 3 files of size 113122623 starting at candidate #0 after considering 1 permutations with 1 in ratio
2014-07-01 15:32:44,861 DEBUG [regionserver60020-smallCompactions-1404253964856] regionserver.HStore: 93253e37c284f3174309fede3f32339f - family: Initiating major compaction
2014-07-01 15:32:44,862 INFO  [regionserver60020-smallCompactions-1404253964856] regionserver.HRegion: Starting compaction on family in region usertable,user3,1404228265526.93253e37c284f3174309fede3f32339f.
2014-07-01 15:32:44,862 INFO  [regionserver60020-smallCompactions-1404253964856] regionserver.HStore: Starting compaction of 3 file(s) in family of usertable,user3,1404228265526.93253e37c284f3174309fede3f32339f. into tmpdir=hdfs://master:54310/hbase/data/default/usertable/93253e37c284f3174309fede3f32339f/.tmp, totalSize=107.9m
2014-07-01 15:32:44,863 DEBUG [regionserver60020-smallCompactions-1404253964856] compactions.Compactor: Compacting hdfs://master:54310/hbase/data/default/usertable/93253e37c284f3174309fede3f32339f/family/80c506b485354d5981ae697240b0d0d7, keycount=65657, bloomtype=ROW, size=46.8m, encoding=NONE, seqNum=117, earliestPutTs=1404228281603
2014-07-01 15:32:44,863 DEBUG [regionserver60020-smallCompactions-1404253964856] compactions.Compactor: Compacting hdfs://master:54310/hbase/data/default/usertable/93253e37c284f3174309fede3f32339f/family/dc4f955fdaeb4e22b5a5efc1dbc4a6de, keycount=59516, bloomtype=ROW, size=42.4m, encoding=NONE, seqNum=281, earliestPutTs=1404253893431
2014-07-01 15:32:44,864 DEBUG [regionserver60020-smallCompactions-1404253964856] compactions.Compactor: Compacting hdfs://master:54310/hbase/data/default/usertable/93253e37c284f3174309fede3f32339f/family/e1dbcb59d2c144168bb6d9e50869a573, keycount=26237, bloomtype=ROW, size=18.7m, encoding=NONE, seqNum=604, earliestPutTs=1404253943312
2014-07-01 15:32:44,927 DEBUG [regionserver60020-smallCompactions-1404253964856] util.FSUtils: DFS Client does not support most favored nodes create; using default create
2014-07-01 15:32:45,235 DEBUG [MemStoreFlusher.0] util.FSUtils: DFS Client does not support most favored nodes create; using default create
2014-07-01 15:32:45,329 INFO  [MemStoreFlusher.1] regionserver.DefaultStoreFlusher: Flushed, sequenceid=608, memsize=72.2m, hasBloomFilter=true, into tmp file hdfs://master:54310/hbase/data/default/usertable/122e92eabfab38cb15a9965b21db25c5/.tmp/af57772534c148489b0d6e49769f471e
2014-07-01 15:32:45,345 DEBUG [MemStoreFlusher.1] regionserver.HRegionFileSystem: Committing store file hdfs://master:54310/hbase/data/default/usertable/122e92eabfab38cb15a9965b21db25c5/.tmp/af57772534c148489b0d6e49769f471e as hdfs://master:54310/hbase/data/default/usertable/122e92eabfab38cb15a9965b21db25c5/family/af57772534c148489b0d6e49769f471e
2014-07-01 15:32:45,358 INFO  [MemStoreFlusher.1] compress.CodecPool: Got brand-new decompressor
2014-07-01 15:32:45,360 INFO  [MemStoreFlusher.1] regionserver.HStore: Added hdfs://master:54310/hbase/data/default/usertable/122e92eabfab38cb15a9965b21db25c5/family/af57772534c148489b0d6e49769f471e, entries=263020, sequenceid=608, filesize=18.7m
2014-07-01 15:32:45,362 INFO  [MemStoreFlusher.1] regionserver.HRegion: Finished memstore flush of ~493.6m/517619280, currentsize=131.7m/138049760 for region usertable,user5,1404228265526.122e92eabfab38cb15a9965b21db25c5. in 11650ms, sequenceid=608, compaction requested=true
2014-07-01 15:32:45,363 DEBUG [MemStoreFlusher.1] regionserver.CompactSplitThread: Small Compaction requested: system; Because: MemStoreFlusher.1; compaction_queue=(0:1), split_queue=0, merge_queue=0
2014-07-01 15:32:49,538 INFO  [MemStoreFlusher.0] regionserver.DefaultStoreFlusher: Flushed, sequenceid=656, memsize=73.7m, hasBloomFilter=true, into tmp file hdfs://master:54310/hbase/data/default/usertable/667c1f2c34fe37d48ba25a789d8aba98/.tmp/beb66310e7124c9c873f543ef061e1c7
2014-07-01 15:32:49,557 DEBUG [MemStoreFlusher.0] regionserver.HRegionFileSystem: Committing store file hdfs://master:54310/hbase/data/default/usertable/667c1f2c34fe37d48ba25a789d8aba98/.tmp/beb66310e7124c9c873f543ef061e1c7 as hdfs://master:54310/hbase/data/default/usertable/667c1f2c34fe37d48ba25a789d8aba98/family/beb66310e7124c9c873f543ef061e1c7
2014-07-01 15:32:49,570 INFO  [MemStoreFlusher.0] regionserver.HStore: Added hdfs://master:54310/hbase/data/default/usertable/667c1f2c34fe37d48ba25a789d8aba98/family/beb66310e7124c9c873f543ef061e1c7, entries=268180, sequenceid=656, filesize=19.1m
2014-07-01 15:32:49,571 INFO  [MemStoreFlusher.0] regionserver.HRegion: Finished memstore flush of ~444.3m/465887520, currentsize=12.7m/13270960 for region usertable,user7,1404228265526.667c1f2c34fe37d48ba25a789d8aba98. in 4713ms, sequenceid=656, compaction requested=true
2014-07-01 15:32:49,571 DEBUG [MemStoreFlusher.0] regionserver.CompactSplitThread: Small Compaction requested: system; Because: MemStoreFlusher.0; compaction_queue=(0:2), split_queue=0, merge_queue=0
2014-07-01 15:32:49,668 DEBUG [RpcServer.handler=6,port=60020] regionserver.HRegion: Flush requested on usertable,user9,1404228265526.bb79ae773f71289e0745068f68a0abd9.
2014-07-01 15:32:49,669 DEBUG [MemStoreFlusher.1] regionserver.HRegion: Started memstore flush for usertable,user9,1404228265526.bb79ae773f71289e0745068f68a0abd9., current region memstore size 256.4m
2014-07-01 15:32:50,363 DEBUG [MemStoreFlusher.1] util.FSUtils: DFS Client does not support most favored nodes create; using default create
2014-07-01 15:32:50,806 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-01 15:32:50,957 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 2910 synced till here 2897
2014-07-01 15:32:51,233 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1404253868846/slave1%2C60020%2C1404253868846.1404253964269 with entries=274, filesize=70.6m; new WAL /hbase/WALs/slave1,60020,1404253868846/slave1%2C60020%2C1404253868846.1404253970806
2014-07-01 15:32:54,223 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-01 15:32:54,365 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 3032 synced till here 3010
2014-07-01 15:32:54,807 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1404253868846/slave1%2C60020%2C1404253868846.1404253970806 with entries=122, filesize=89.0m; new WAL /hbase/WALs/slave1,60020,1404253868846/slave1%2C60020%2C1404253868846.1404253974223
2014-07-01 15:32:56,930 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-01 15:32:57,057 DEBUG [RpcServer.handler=7,port=60020] regionserver.HRegion: Flush requested on usertable,user3,1404228265526.93253e37c284f3174309fede3f32339f.
2014-07-01 15:32:57,058 DEBUG [MemStoreFlusher.0] regionserver.HRegion: Started memstore flush for usertable,user3,1404228265526.93253e37c284f3174309fede3f32339f., current region memstore size 257.1m
2014-07-01 15:32:57,220 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 3146 synced till here 3123
2014-07-01 15:32:57,437 DEBUG [MemStoreFlusher.0] util.FSUtils: DFS Client does not support most favored nodes create; using default create
2014-07-01 15:32:57,449 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1404253868846/slave1%2C60020%2C1404253868846.1404253974223 with entries=114, filesize=86.3m; new WAL /hbase/WALs/slave1,60020,1404253868846/slave1%2C60020%2C1404253868846.1404253976930
2014-07-01 15:32:57,649 INFO  [MemStoreFlusher.1] regionserver.DefaultStoreFlusher: Flushed, sequenceid=713, memsize=57.0m, hasBloomFilter=true, into tmp file hdfs://master:54310/hbase/data/default/usertable/bb79ae773f71289e0745068f68a0abd9/.tmp/ced32cffb61d4e6989176c141cc0ee27
2014-07-01 15:32:57,664 DEBUG [MemStoreFlusher.1] regionserver.HRegionFileSystem: Committing store file hdfs://master:54310/hbase/data/default/usertable/bb79ae773f71289e0745068f68a0abd9/.tmp/ced32cffb61d4e6989176c141cc0ee27 as hdfs://master:54310/hbase/data/default/usertable/bb79ae773f71289e0745068f68a0abd9/family/ced32cffb61d4e6989176c141cc0ee27
2014-07-01 15:32:57,677 INFO  [MemStoreFlusher.1] regionserver.HStore: Added hdfs://master:54310/hbase/data/default/usertable/bb79ae773f71289e0745068f68a0abd9/family/ced32cffb61d4e6989176c141cc0ee27, entries=207640, sequenceid=713, filesize=14.8m
2014-07-01 15:32:57,678 INFO  [MemStoreFlusher.1] regionserver.HRegion: Finished memstore flush of ~258.0m/270506880, currentsize=25.4m/26603520 for region usertable,user9,1404228265526.bb79ae773f71289e0745068f68a0abd9. in 8009ms, sequenceid=713, compaction requested=false
2014-07-01 15:32:57,987 DEBUG [RpcServer.handler=5,port=60020] regionserver.HRegion: Flush requested on usertable,user5,1404228265526.122e92eabfab38cb15a9965b21db25c5.
2014-07-01 15:32:57,987 DEBUG [MemStoreFlusher.1] regionserver.HRegion: Started memstore flush for usertable,user5,1404228265526.122e92eabfab38cb15a9965b21db25c5., current region memstore size 257.3m
2014-07-01 15:32:58,243 DEBUG [MemStoreFlusher.1] util.FSUtils: DFS Client does not support most favored nodes create; using default create
2014-07-01 15:32:59,811 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-01 15:32:59,828 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 3241 synced till here 3234
2014-07-01 15:32:59,860 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1404253868846/slave1%2C60020%2C1404253868846.1404253976930 with entries=95, filesize=63.3m; new WAL /hbase/WALs/slave1,60020,1404253868846/slave1%2C60020%2C1404253868846.1404253979812
2014-07-01 15:32:59,861 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1404253868846/slave1%2C60020%2C1404253868846.1404253870211
2014-07-01 15:32:59,861 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1404253868846/slave1%2C60020%2C1404253868846.1404253898912
2014-07-01 15:32:59,861 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1404253868846/slave1%2C60020%2C1404253868846.1404253901472
2014-07-01 15:32:59,861 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1404253868846/slave1%2C60020%2C1404253868846.1404253903198
2014-07-01 15:32:59,861 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1404253868846/slave1%2C60020%2C1404253868846.1404253905812
2014-07-01 15:32:59,861 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1404253868846/slave1%2C60020%2C1404253868846.1404253908430
2014-07-01 15:32:59,861 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1404253868846/slave1%2C60020%2C1404253868846.1404253910363
2014-07-01 15:32:59,861 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1404253868846/slave1%2C60020%2C1404253868846.1404253912914
2014-07-01 15:32:59,861 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1404253868846/slave1%2C60020%2C1404253868846.1404253916410
2014-07-01 15:32:59,861 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1404253868846/slave1%2C60020%2C1404253868846.1404253920067
2014-07-01 15:32:59,862 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1404253868846/slave1%2C60020%2C1404253868846.1404253924285
2014-07-01 15:32:59,862 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1404253868846/slave1%2C60020%2C1404253868846.1404253928416
2014-07-01 15:33:02,659 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-01 15:33:02,774 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 3345 synced till here 3330
2014-07-01 15:33:02,881 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1404253868846/slave1%2C60020%2C1404253868846.1404253979812 with entries=104, filesize=72.7m; new WAL /hbase/WALs/slave1,60020,1404253868846/slave1%2C60020%2C1404253868846.1404253982659
2014-07-01 15:33:05,594 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-01 15:33:05,736 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 3439 synced till here 3429
2014-07-01 15:33:05,926 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1404253868846/slave1%2C60020%2C1404253868846.1404253982659 with entries=94, filesize=71.1m; new WAL /hbase/WALs/slave1,60020,1404253868846/slave1%2C60020%2C1404253868846.1404253985595
2014-07-01 15:33:06,283 INFO  [MemStoreFlusher.0] regionserver.DefaultStoreFlusher: Flushed, sequenceid=817, memsize=103.3m, hasBloomFilter=true, into tmp file hdfs://master:54310/hbase/data/default/usertable/93253e37c284f3174309fede3f32339f/.tmp/c983ea1b694d4ca0a196965f3913a212
2014-07-01 15:33:06,412 DEBUG [MemStoreFlusher.0] regionserver.HRegionFileSystem: Committing store file hdfs://master:54310/hbase/data/default/usertable/93253e37c284f3174309fede3f32339f/.tmp/c983ea1b694d4ca0a196965f3913a212 as hdfs://master:54310/hbase/data/default/usertable/93253e37c284f3174309fede3f32339f/family/c983ea1b694d4ca0a196965f3913a212
2014-07-01 15:33:06,462 INFO  [MemStoreFlusher.0] regionserver.HStore: Added hdfs://master:54310/hbase/data/default/usertable/93253e37c284f3174309fede3f32339f/family/c983ea1b694d4ca0a196965f3913a212, entries=376260, sequenceid=817, filesize=26.8m
2014-07-01 15:33:06,462 INFO  [MemStoreFlusher.0] regionserver.HRegion: Finished memstore flush of ~262.2m/274968240, currentsize=112.2m/117669920 for region usertable,user3,1404228265526.93253e37c284f3174309fede3f32339f. in 9404ms, sequenceid=817, compaction requested=false
2014-07-01 15:33:07,033 INFO  [MemStoreFlusher.1] regionserver.DefaultStoreFlusher: Flushed, sequenceid=815, memsize=104.0m, hasBloomFilter=true, into tmp file hdfs://master:54310/hbase/data/default/usertable/122e92eabfab38cb15a9965b21db25c5/.tmp/30f4fb84d20f45e5b3cff6270774c600
2014-07-01 15:33:07,170 DEBUG [MemStoreFlusher.1] regionserver.HRegionFileSystem: Committing store file hdfs://master:54310/hbase/data/default/usertable/122e92eabfab38cb15a9965b21db25c5/.tmp/30f4fb84d20f45e5b3cff6270774c600 as hdfs://master:54310/hbase/data/default/usertable/122e92eabfab38cb15a9965b21db25c5/family/30f4fb84d20f45e5b3cff6270774c600
2014-07-01 15:33:07,186 INFO  [MemStoreFlusher.1] regionserver.HStore: Added hdfs://master:54310/hbase/data/default/usertable/122e92eabfab38cb15a9965b21db25c5/family/30f4fb84d20f45e5b3cff6270774c600, entries=378580, sequenceid=815, filesize=27.0m
2014-07-01 15:33:07,187 INFO  [MemStoreFlusher.1] regionserver.HRegion: Finished memstore flush of ~257.3m/269820000, currentsize=97.8m/102577440 for region usertable,user5,1404228265526.122e92eabfab38cb15a9965b21db25c5. in 9200ms, sequenceid=815, compaction requested=true
2014-07-01 15:33:07,187 DEBUG [MemStoreFlusher.1] regionserver.CompactSplitThread: Small Compaction requested: system; Because: MemStoreFlusher.1; compaction_queue=(0:3), split_queue=0, merge_queue=0
2014-07-01 15:33:08,023 DEBUG [RpcServer.handler=22,port=60020] regionserver.HRegion: Flush requested on usertable,,1404228265525.9cef9e7d34f96f1d9b111be50040a208.
2014-07-01 15:33:08,027 DEBUG [MemStoreFlusher.0] regionserver.HRegion: Started memstore flush for usertable,,1404228265525.9cef9e7d34f96f1d9b111be50040a208., current region memstore size 260.3m
2014-07-01 15:33:08,373 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-01 15:33:08,514 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 3547 synced till here 3524
2014-07-01 15:33:08,969 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1404253868846/slave1%2C60020%2C1404253868846.1404253985595 with entries=108, filesize=80.0m; new WAL /hbase/WALs/slave1,60020,1404253868846/slave1%2C60020%2C1404253868846.1404253988374
2014-07-01 15:33:09,300 DEBUG [MemStoreFlusher.0] util.FSUtils: DFS Client does not support most favored nodes create; using default create
2014-07-01 15:33:10,983 DEBUG [RpcServer.handler=15,port=60020] regionserver.HRegion: Flush requested on usertable,user7,1404228265526.667c1f2c34fe37d48ba25a789d8aba98.
2014-07-01 15:33:10,985 DEBUG [MemStoreFlusher.1] regionserver.HRegion: Started memstore flush for usertable,user7,1404228265526.667c1f2c34fe37d48ba25a789d8aba98., current region memstore size 262.9m
2014-07-01 15:33:12,037 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-01 15:33:12,084 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 3656 synced till here 3637
2014-07-01 15:33:12,374 DEBUG [MemStoreFlusher.1] util.FSUtils: DFS Client does not support most favored nodes create; using default create
2014-07-01 15:33:12,534 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1404253868846/slave1%2C60020%2C1404253868846.1404253988374 with entries=109, filesize=80.9m; new WAL /hbase/WALs/slave1,60020,1404253868846/slave1%2C60020%2C1404253868846.1404253992038
2014-07-01 15:33:12,875 DEBUG [regionserver60020-smallCompactions-1404253964856] regionserver.HRegionFileSystem: Committing store file hdfs://master:54310/hbase/data/default/usertable/93253e37c284f3174309fede3f32339f/.tmp/8fe3051444564c758ac8530e9b7243bf as hdfs://master:54310/hbase/data/default/usertable/93253e37c284f3174309fede3f32339f/family/8fe3051444564c758ac8530e9b7243bf
2014-07-01 15:33:13,460 DEBUG [regionserver60020-smallCompactions-1404253964856] regionserver.HStore: Removing store files after compaction...
2014-07-01 15:33:13,591 DEBUG [regionserver60020-smallCompactions-1404253964856] backup.HFileArchiver: Finished archiving from class org.apache.hadoop.hbase.backup.HFileArchiver$FileableStoreFile, file:hdfs://master:54310/hbase/data/default/usertable/93253e37c284f3174309fede3f32339f/family/80c506b485354d5981ae697240b0d0d7, to hdfs://master:54310/hbase/archive/data/default/usertable/93253e37c284f3174309fede3f32339f/family/80c506b485354d5981ae697240b0d0d7
2014-07-01 15:33:13,594 DEBUG [regionserver60020-smallCompactions-1404253964856] backup.HFileArchiver: Finished archiving from class org.apache.hadoop.hbase.backup.HFileArchiver$FileableStoreFile, file:hdfs://master:54310/hbase/data/default/usertable/93253e37c284f3174309fede3f32339f/family/dc4f955fdaeb4e22b5a5efc1dbc4a6de, to hdfs://master:54310/hbase/archive/data/default/usertable/93253e37c284f3174309fede3f32339f/family/dc4f955fdaeb4e22b5a5efc1dbc4a6de
2014-07-01 15:33:13,600 DEBUG [regionserver60020-smallCompactions-1404253964856] backup.HFileArchiver: Finished archiving from class org.apache.hadoop.hbase.backup.HFileArchiver$FileableStoreFile, file:hdfs://master:54310/hbase/data/default/usertable/93253e37c284f3174309fede3f32339f/family/e1dbcb59d2c144168bb6d9e50869a573, to hdfs://master:54310/hbase/archive/data/default/usertable/93253e37c284f3174309fede3f32339f/family/e1dbcb59d2c144168bb6d9e50869a573
2014-07-01 15:33:13,601 INFO  [regionserver60020-smallCompactions-1404253964856] regionserver.HStore: Completed major compaction of 3 file(s) in family of usertable,user3,1404228265526.93253e37c284f3174309fede3f32339f. into 8fe3051444564c758ac8530e9b7243bf(size=91.5m), total size for store is 118.3m. This selection was in queue for 0sec, and took 28sec to execute.
2014-07-01 15:33:13,605 INFO  [regionserver60020-smallCompactions-1404253964856] regionserver.CompactSplitThread: Completed compaction: Request = regionName=usertable,user3,1404228265526.93253e37c284f3174309fede3f32339f., storeName=family, fileCount=3, fileSize=107.9m, priority=17, time=74076250332219; duration=28sec
2014-07-01 15:33:13,605 DEBUG [regionserver60020-smallCompactions-1404253964856] regionserver.CompactSplitThread: CompactSplitThread Status: compaction_queue=(0:3), split_queue=0, merge_queue=0
2014-07-01 15:33:13,606 DEBUG [regionserver60020-smallCompactions-1404253964856] compactions.RatioBasedCompactionPolicy: Selecting compaction from 4 store files, 0 compacting, 4 eligible, 20 blocking
2014-07-01 15:33:13,606 DEBUG [regionserver60020-smallCompactions-1404253964856] compactions.ExploringCompactionPolicy: Exploring compaction algorithm has selected 4 files of size 141503575 starting at candidate #0 after considering 3 permutations with 3 in ratio
2014-07-01 15:33:13,606 DEBUG [regionserver60020-smallCompactions-1404253964856] regionserver.HStore: 122e92eabfab38cb15a9965b21db25c5 - family: Initiating major compaction
2014-07-01 15:33:13,606 INFO  [regionserver60020-smallCompactions-1404253964856] regionserver.HRegion: Starting compaction on family in region usertable,user5,1404228265526.122e92eabfab38cb15a9965b21db25c5.
2014-07-01 15:33:13,607 INFO  [regionserver60020-smallCompactions-1404253964856] regionserver.HStore: Starting compaction of 4 file(s) in family of usertable,user5,1404228265526.122e92eabfab38cb15a9965b21db25c5. into tmpdir=hdfs://master:54310/hbase/data/default/usertable/122e92eabfab38cb15a9965b21db25c5/.tmp, totalSize=134.9m
2014-07-01 15:33:13,607 DEBUG [regionserver60020-smallCompactions-1404253964856] compactions.Compactor: Compacting hdfs://master:54310/hbase/data/default/usertable/122e92eabfab38cb15a9965b21db25c5/family/d4c796f1737f4cae8358c27df888ec15, keycount=63395, bloomtype=ROW, size=45.2m, encoding=NONE, seqNum=113, earliestPutTs=1404228281724
2014-07-01 15:33:13,607 DEBUG [regionserver60020-smallCompactions-1404253964856] compactions.Compactor: Compacting hdfs://master:54310/hbase/data/default/usertable/122e92eabfab38cb15a9965b21db25c5/family/87033878504c46e29934056168d8d614, keycount=61822, bloomtype=ROW, size=44.1m, encoding=NONE, seqNum=284, earliestPutTs=1404253894091
2014-07-01 15:33:13,608 DEBUG [regionserver60020-smallCompactions-1404253964856] compactions.Compactor: Compacting hdfs://master:54310/hbase/data/default/usertable/122e92eabfab38cb15a9965b21db25c5/family/af57772534c148489b0d6e49769f471e, keycount=26302, bloomtype=ROW, size=18.7m, encoding=NONE, seqNum=608, earliestPutTs=1404253947219
2014-07-01 15:33:13,608 DEBUG [regionserver60020-smallCompactions-1404253964856] compactions.Compactor: Compacting hdfs://master:54310/hbase/data/default/usertable/122e92eabfab38cb15a9965b21db25c5/family/30f4fb84d20f45e5b3cff6270774c600, keycount=37858, bloomtype=ROW, size=27.0m, encoding=NONE, seqNum=815, earliestPutTs=1404253962830
2014-07-01 15:33:13,676 DEBUG [regionserver60020-smallCompactions-1404253964856] util.FSUtils: DFS Client does not support most favored nodes create; using default create
2014-07-01 15:33:15,024 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-01 15:33:15,137 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 3761 synced till here 3752
2014-07-01 15:33:15,166 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1404253868846/slave1%2C60020%2C1404253868846.1404253992038 with entries=105, filesize=71.8m; new WAL /hbase/WALs/slave1,60020,1404253868846/slave1%2C60020%2C1404253868846.1404253995025
2014-07-01 15:33:17,846 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-01 15:33:17,895 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 3870 synced till here 3858
2014-07-01 15:33:18,063 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1404253868846/slave1%2C60020%2C1404253868846.1404253995025 with entries=109, filesize=69.6m; new WAL /hbase/WALs/slave1,60020,1404253868846/slave1%2C60020%2C1404253868846.1404253997846
2014-07-01 15:33:19,088 DEBUG [RpcServer.handler=1,port=60020] regionserver.HRegion: Flush requested on usertable,user3,1404228265526.93253e37c284f3174309fede3f32339f.
2014-07-01 15:33:20,571 DEBUG [RpcServer.handler=7,port=60020] regionserver.HRegion: Flush requested on usertable,user5,1404228265526.122e92eabfab38cb15a9965b21db25c5.
2014-07-01 15:33:20,714 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-01 15:33:21,219 INFO  [MemStoreFlusher.0] regionserver.DefaultStoreFlusher: Flushed, sequenceid=653, memsize=147.5m, hasBloomFilter=true, into tmp file hdfs://master:54310/hbase/data/default/usertable/9cef9e7d34f96f1d9b111be50040a208/.tmp/b6459b2efea04af1894b4d4fb1f62251
2014-07-01 15:33:21,235 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 3991 synced till here 3986
2014-07-01 15:33:21,237 DEBUG [MemStoreFlusher.0] regionserver.HRegionFileSystem: Committing store file hdfs://master:54310/hbase/data/default/usertable/9cef9e7d34f96f1d9b111be50040a208/.tmp/b6459b2efea04af1894b4d4fb1f62251 as hdfs://master:54310/hbase/data/default/usertable/9cef9e7d34f96f1d9b111be50040a208/family/b6459b2efea04af1894b4d4fb1f62251
2014-07-01 15:33:21,248 INFO  [MemStoreFlusher.0] regionserver.HStore: Added hdfs://master:54310/hbase/data/default/usertable/9cef9e7d34f96f1d9b111be50040a208/family/b6459b2efea04af1894b4d4fb1f62251, entries=537070, sequenceid=653, filesize=38.3m
2014-07-01 15:33:21,249 INFO  [MemStoreFlusher.0] regionserver.HRegion: Finished memstore flush of ~261.8m/274486160, currentsize=52.1m/54629520 for region usertable,,1404228265525.9cef9e7d34f96f1d9b111be50040a208. in 13221ms, sequenceid=653, compaction requested=true
2014-07-01 15:33:21,249 DEBUG [MemStoreFlusher.0] regionserver.CompactSplitThread: Small Compaction requested: system; Because: MemStoreFlusher.0; compaction_queue=(0:3), split_queue=0, merge_queue=0
2014-07-01 15:33:21,249 DEBUG [MemStoreFlusher.0] regionserver.HRegion: Started memstore flush for usertable,user3,1404228265526.93253e37c284f3174309fede3f32339f., current region memstore size 287.0m
2014-07-01 15:33:21,307 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1404253868846/slave1%2C60020%2C1404253868846.1404253997846 with entries=121, filesize=80.8m; new WAL /hbase/WALs/slave1,60020,1404253868846/slave1%2C60020%2C1404253868846.1404254000714
2014-07-01 15:33:21,308 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1404253868846/slave1%2C60020%2C1404253868846.1404253932048
2014-07-01 15:33:21,308 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1404253868846/slave1%2C60020%2C1404253868846.1404253934911
2014-07-01 15:33:21,308 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1404253868846/slave1%2C60020%2C1404253868846.1404253937391
2014-07-01 15:33:21,308 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1404253868846/slave1%2C60020%2C1404253868846.1404253939675
2014-07-01 15:33:21,308 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1404253868846/slave1%2C60020%2C1404253868846.1404253942565
2014-07-01 15:33:21,308 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1404253868846/slave1%2C60020%2C1404253868846.1404253947543
2014-07-01 15:33:21,308 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1404253868846/slave1%2C60020%2C1404253868846.1404253951947
2014-07-01 15:33:21,308 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1404253868846/slave1%2C60020%2C1404253868846.1404253954259
2014-07-01 15:33:21,308 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1404253868846/slave1%2C60020%2C1404253868846.1404253957314
2014-07-01 15:33:21,308 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1404253868846/slave1%2C60020%2C1404253868846.1404253961944
2014-07-01 15:33:22,018 DEBUG [MemStoreFlusher.0] util.FSUtils: DFS Client does not support most favored nodes create; using default create
2014-07-01 15:33:24,059 INFO  [MemStoreFlusher.1] regionserver.DefaultStoreFlusher: Flushed, sequenceid=863, memsize=149.3m, hasBloomFilter=true, into tmp file hdfs://master:54310/hbase/data/default/usertable/667c1f2c34fe37d48ba25a789d8aba98/.tmp/ef6af2585e694a298b461d3e71b415a2
2014-07-01 15:33:24,087 DEBUG [MemStoreFlusher.1] regionserver.HRegionFileSystem: Committing store file hdfs://master:54310/hbase/data/default/usertable/667c1f2c34fe37d48ba25a789d8aba98/.tmp/ef6af2585e694a298b461d3e71b415a2 as hdfs://master:54310/hbase/data/default/usertable/667c1f2c34fe37d48ba25a789d8aba98/family/ef6af2585e694a298b461d3e71b415a2
2014-07-01 15:33:24,192 INFO  [MemStoreFlusher.1] regionserver.HStore: Added hdfs://master:54310/hbase/data/default/usertable/667c1f2c34fe37d48ba25a789d8aba98/family/ef6af2585e694a298b461d3e71b415a2, entries=543590, sequenceid=863, filesize=38.7m
2014-07-01 15:33:24,192 INFO  [MemStoreFlusher.1] regionserver.HRegion: Finished memstore flush of ~269.6m/282722320, currentsize=157.0m/164643120 for region usertable,user7,1404228265526.667c1f2c34fe37d48ba25a789d8aba98. in 13207ms, sequenceid=863, compaction requested=true
2014-07-01 15:33:24,192 DEBUG [MemStoreFlusher.1] regionserver.CompactSplitThread: Small Compaction requested: system; Because: MemStoreFlusher.1; compaction_queue=(0:4), split_queue=0, merge_queue=0
2014-07-01 15:33:24,194 DEBUG [MemStoreFlusher.1] regionserver.HRegion: Started memstore flush for usertable,user5,1404228265526.122e92eabfab38cb15a9965b21db25c5., current region memstore size 300.4m
2014-07-01 15:33:24,420 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-01 15:33:24,546 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 4086 synced till here 4077
2014-07-01 15:33:25,066 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1404253868846/slave1%2C60020%2C1404253868846.1404254000714 with entries=95, filesize=75.9m; new WAL /hbase/WALs/slave1,60020,1404253868846/slave1%2C60020%2C1404253868846.1404254004421
2014-07-01 15:33:25,357 DEBUG [MemStoreFlusher.1] util.FSUtils: DFS Client does not support most favored nodes create; using default create
2014-07-01 15:33:27,342 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-01 15:33:27,536 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 4173 synced till here 4165
2014-07-01 15:33:27,770 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1404253868846/slave1%2C60020%2C1404253868846.1404254004421 with entries=87, filesize=67.5m; new WAL /hbase/WALs/slave1,60020,1404253868846/slave1%2C60020%2C1404253868846.1404254007342
2014-07-01 15:33:30,681 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-01 15:33:30,805 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 4283 synced till here 4257
2014-07-01 15:33:31,171 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1404253868846/slave1%2C60020%2C1404253868846.1404254007342 with entries=110, filesize=93.5m; new WAL /hbase/WALs/slave1,60020,1404253868846/slave1%2C60020%2C1404253868846.1404254010682
2014-07-01 15:33:32,080 DEBUG [RpcServer.handler=23,port=60020] regionserver.HRegion: Flush requested on usertable,user7,1404228265526.667c1f2c34fe37d48ba25a789d8aba98.
2014-07-01 15:33:33,086 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-01 15:33:33,165 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 4393 synced till here 4374
2014-07-01 15:33:33,276 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1404253868846/slave1%2C60020%2C1404253868846.1404254010682 with entries=110, filesize=77.5m; new WAL /hbase/WALs/slave1,60020,1404253868846/slave1%2C60020%2C1404253868846.1404254013086
2014-07-01 15:33:35,895 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-01 15:33:36,015 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 4491 synced till here 4482
2014-07-01 15:33:36,232 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1404253868846/slave1%2C60020%2C1404253868846.1404254013086 with entries=98, filesize=78.0m; new WAL /hbase/WALs/slave1,60020,1404253868846/slave1%2C60020%2C1404253868846.1404254015896
2014-07-01 15:33:36,953 INFO  [MemStoreFlusher.0] regionserver.DefaultStoreFlusher: Flushed, sequenceid=989, memsize=159.6m, hasBloomFilter=true, into tmp file hdfs://master:54310/hbase/data/default/usertable/93253e37c284f3174309fede3f32339f/.tmp/d98c2ed64b8e4dbd915634a5b65edc0c
2014-07-01 15:33:36,974 DEBUG [MemStoreFlusher.0] regionserver.HRegionFileSystem: Committing store file hdfs://master:54310/hbase/data/default/usertable/93253e37c284f3174309fede3f32339f/.tmp/d98c2ed64b8e4dbd915634a5b65edc0c as hdfs://master:54310/hbase/data/default/usertable/93253e37c284f3174309fede3f32339f/family/d98c2ed64b8e4dbd915634a5b65edc0c
2014-07-01 15:33:37,068 INFO  [MemStoreFlusher.0] regionserver.HStore: Added hdfs://master:54310/hbase/data/default/usertable/93253e37c284f3174309fede3f32339f/family/d98c2ed64b8e4dbd915634a5b65edc0c, entries=580960, sequenceid=989, filesize=41.4m
2014-07-01 15:33:37,069 INFO  [MemStoreFlusher.0] regionserver.HRegion: Finished memstore flush of ~290.5m/304615120, currentsize=209.0m/219199360 for region usertable,user3,1404228265526.93253e37c284f3174309fede3f32339f. in 15820ms, sequenceid=989, compaction requested=true
2014-07-01 15:33:37,069 DEBUG [MemStoreFlusher.0] regionserver.CompactSplitThread: Small Compaction requested: system; Because: MemStoreFlusher.0; compaction_queue=(0:5), split_queue=0, merge_queue=0
2014-07-01 15:33:37,070 DEBUG [MemStoreFlusher.0] regionserver.HRegion: Started memstore flush for usertable,user7,1404228265526.667c1f2c34fe37d48ba25a789d8aba98., current region memstore size 342.1m
2014-07-01 15:33:38,070 DEBUG [MemStoreFlusher.0] util.FSUtils: DFS Client does not support most favored nodes create; using default create
2014-07-01 15:33:38,680 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-01 15:33:39,381 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 4631 synced till here 4622
2014-07-01 15:33:39,431 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1404253868846/slave1%2C60020%2C1404253868846.1404254015896 with entries=140, filesize=95.1m; new WAL /hbase/WALs/slave1,60020,1404253868846/slave1%2C60020%2C1404253868846.1404254018680
2014-07-01 15:33:40,736 DEBUG [RpcServer.handler=14,port=60020] regionserver.HRegion: Flush requested on usertable,user3,1404228265526.93253e37c284f3174309fede3f32339f.
2014-07-01 15:33:41,124 INFO  [MemStoreFlusher.1] regionserver.DefaultStoreFlusher: Flushed, sequenceid=1000, memsize=171.5m, hasBloomFilter=true, into tmp file hdfs://master:54310/hbase/data/default/usertable/122e92eabfab38cb15a9965b21db25c5/.tmp/364186ae5d724fa9a969ef43c4fd8fde
2014-07-01 15:33:41,147 DEBUG [MemStoreFlusher.1] regionserver.HRegionFileSystem: Committing store file hdfs://master:54310/hbase/data/default/usertable/122e92eabfab38cb15a9965b21db25c5/.tmp/364186ae5d724fa9a969ef43c4fd8fde as hdfs://master:54310/hbase/data/default/usertable/122e92eabfab38cb15a9965b21db25c5/family/364186ae5d724fa9a969ef43c4fd8fde
2014-07-01 15:33:41,167 INFO  [MemStoreFlusher.1] regionserver.HStore: Added hdfs://master:54310/hbase/data/default/usertable/122e92eabfab38cb15a9965b21db25c5/family/364186ae5d724fa9a969ef43c4fd8fde, entries=624560, sequenceid=1000, filesize=44.5m
2014-07-01 15:33:41,168 INFO  [MemStoreFlusher.1] regionserver.HRegion: Finished memstore flush of ~312.4m/327564720, currentsize=227.3m/238373760 for region usertable,user5,1404228265526.122e92eabfab38cb15a9965b21db25c5. in 16973ms, sequenceid=1000, compaction requested=false
2014-07-01 15:33:41,168 DEBUG [MemStoreFlusher.1] regionserver.HRegion: Started memstore flush for usertable,user3,1404228265526.93253e37c284f3174309fede3f32339f., current region memstore size 261.5m
2014-07-01 15:33:41,191 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-01 15:33:41,300 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 4737 synced till here 4734
2014-07-01 15:33:41,548 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1404253868846/slave1%2C60020%2C1404253868846.1404254018680 with entries=106, filesize=64.5m; new WAL /hbase/WALs/slave1,60020,1404253868846/slave1%2C60020%2C1404253868846.1404254021192
2014-07-01 15:33:41,761 DEBUG [MemStoreFlusher.1] util.FSUtils: DFS Client does not support most favored nodes create; using default create
2014-07-01 15:33:43,472 DEBUG [RpcServer.handler=0,port=60020] regionserver.HRegion: Flush requested on usertable,user5,1404228265526.122e92eabfab38cb15a9965b21db25c5.
2014-07-01 15:33:43,624 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-01 15:33:43,652 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 4836 synced till here 4828
2014-07-01 15:33:44,119 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1404253868846/slave1%2C60020%2C1404253868846.1404254021192 with entries=99, filesize=67.4m; new WAL /hbase/WALs/slave1,60020,1404253868846/slave1%2C60020%2C1404253868846.1404254023624
2014-07-01 15:33:46,134 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-01 15:33:46,280 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 4941 synced till here 4931
2014-07-01 15:33:46,377 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1404253868846/slave1%2C60020%2C1404253868846.1404254023624 with entries=105, filesize=67.4m; new WAL /hbase/WALs/slave1,60020,1404253868846/slave1%2C60020%2C1404253868846.1404254026134
2014-07-01 15:33:48,249 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-01 15:33:48,278 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 5039 synced till here 5030
2014-07-01 15:33:48,485 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1404253868846/slave1%2C60020%2C1404253868846.1404254026134 with entries=98, filesize=66.6m; new WAL /hbase/WALs/slave1,60020,1404253868846/slave1%2C60020%2C1404253868846.1404254028251
2014-07-01 15:33:50,491 DEBUG [RpcServer.handler=11,port=60020] regionserver.HRegion: Flush requested on usertable,,1404228265525.9cef9e7d34f96f1d9b111be50040a208.
2014-07-01 15:33:50,927 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-01 15:33:51,217 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 5144 synced till here 5128
2014-07-01 15:33:51,865 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1404253868846/slave1%2C60020%2C1404253868846.1404254028251 with entries=105, filesize=75.6m; new WAL /hbase/WALs/slave1,60020,1404253868846/slave1%2C60020%2C1404253868846.1404254030927
2014-07-01 15:33:53,574 DEBUG [regionserver60020-smallCompactions-1404253964856] regionserver.HRegionFileSystem: Committing store file hdfs://master:54310/hbase/data/default/usertable/122e92eabfab38cb15a9965b21db25c5/.tmp/37d47aff33684f1191cc991aab1081e2 as hdfs://master:54310/hbase/data/default/usertable/122e92eabfab38cb15a9965b21db25c5/family/37d47aff33684f1191cc991aab1081e2
2014-07-01 15:33:54,563 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-01 15:33:54,565 DEBUG [regionserver60020-smallCompactions-1404253964856] regionserver.HStore: Removing store files after compaction...
2014-07-01 15:33:54,756 DEBUG [regionserver60020-smallCompactions-1404253964856] backup.HFileArchiver: Finished archiving from class org.apache.hadoop.hbase.backup.HFileArchiver$FileableStoreFile, file:hdfs://master:54310/hbase/data/default/usertable/122e92eabfab38cb15a9965b21db25c5/family/d4c796f1737f4cae8358c27df888ec15, to hdfs://master:54310/hbase/archive/data/default/usertable/122e92eabfab38cb15a9965b21db25c5/family/d4c796f1737f4cae8358c27df888ec15
2014-07-01 15:33:54,763 DEBUG [regionserver60020-smallCompactions-1404253964856] backup.HFileArchiver: Finished archiving from class org.apache.hadoop.hbase.backup.HFileArchiver$FileableStoreFile, file:hdfs://master:54310/hbase/data/default/usertable/122e92eabfab38cb15a9965b21db25c5/family/87033878504c46e29934056168d8d614, to hdfs://master:54310/hbase/archive/data/default/usertable/122e92eabfab38cb15a9965b21db25c5/family/87033878504c46e29934056168d8d614
2014-07-01 15:33:54,766 DEBUG [regionserver60020-smallCompactions-1404253964856] backup.HFileArchiver: Finished archiving from class org.apache.hadoop.hbase.backup.HFileArchiver$FileableStoreFile, file:hdfs://master:54310/hbase/data/default/usertable/122e92eabfab38cb15a9965b21db25c5/family/af57772534c148489b0d6e49769f471e, to hdfs://master:54310/hbase/archive/data/default/usertable/122e92eabfab38cb15a9965b21db25c5/family/af57772534c148489b0d6e49769f471e
2014-07-01 15:33:54,897 DEBUG [regionserver60020-smallCompactions-1404253964856] backup.HFileArchiver: Finished archiving from class org.apache.hadoop.hbase.backup.HFileArchiver$FileableStoreFile, file:hdfs://master:54310/hbase/data/default/usertable/122e92eabfab38cb15a9965b21db25c5/family/30f4fb84d20f45e5b3cff6270774c600, to hdfs://master:54310/hbase/archive/data/default/usertable/122e92eabfab38cb15a9965b21db25c5/family/30f4fb84d20f45e5b3cff6270774c600
2014-07-01 15:33:54,898 INFO  [regionserver60020-smallCompactions-1404253964856] regionserver.HStore: Completed major compaction of 4 file(s) in family of usertable,user5,1404228265526.122e92eabfab38cb15a9965b21db25c5. into 37d47aff33684f1191cc991aab1081e2(size=99.6m), total size for store is 144.1m. This selection was in queue for 0sec, and took 41sec to execute.
2014-07-01 15:33:54,898 INFO  [regionserver60020-smallCompactions-1404253964856] regionserver.CompactSplitThread: Completed compaction: Request = regionName=usertable,user5,1404228265526.122e92eabfab38cb15a9965b21db25c5., storeName=family, fileCount=4, fileSize=134.9m, priority=16, time=74104995166574; duration=41sec
2014-07-01 15:33:54,899 DEBUG [regionserver60020-smallCompactions-1404253964856] regionserver.CompactSplitThread: CompactSplitThread Status: compaction_queue=(0:5), split_queue=0, merge_queue=0
2014-07-01 15:33:54,899 DEBUG [regionserver60020-smallCompactions-1404253964856] compactions.RatioBasedCompactionPolicy: Selecting compaction from 4 store files, 0 compacting, 4 eligible, 20 blocking
2014-07-01 15:33:54,899 DEBUG [regionserver60020-smallCompactions-1404253964856] compactions.ExploringCompactionPolicy: Exploring compaction algorithm has selected 4 files of size 156659287 starting at candidate #0 after considering 3 permutations with 3 in ratio
2014-07-01 15:33:54,900 DEBUG [regionserver60020-smallCompactions-1404253964856] regionserver.HStore: 667c1f2c34fe37d48ba25a789d8aba98 - family: Initiating major compaction
2014-07-01 15:33:54,900 INFO  [regionserver60020-smallCompactions-1404253964856] regionserver.HRegion: Starting compaction on family in region usertable,user7,1404228265526.667c1f2c34fe37d48ba25a789d8aba98.
2014-07-01 15:33:54,901 INFO  [regionserver60020-smallCompactions-1404253964856] regionserver.HStore: Starting compaction of 4 file(s) in family of usertable,user7,1404228265526.667c1f2c34fe37d48ba25a789d8aba98. into tmpdir=hdfs://master:54310/hbase/data/default/usertable/667c1f2c34fe37d48ba25a789d8aba98/.tmp, totalSize=149.4m
2014-07-01 15:33:54,901 DEBUG [regionserver60020-smallCompactions-1404253964856] compactions.Compactor: Compacting hdfs://master:54310/hbase/data/default/usertable/667c1f2c34fe37d48ba25a789d8aba98/family/9fdb368501fa45e18be2f611d85417a8, keycount=62818, bloomtype=ROW, size=44.8m, encoding=NONE, seqNum=112, earliestPutTs=1404228281826
2014-07-01 15:33:54,901 DEBUG [regionserver60020-smallCompactions-1404253964856] compactions.Compactor: Compacting hdfs://master:54310/hbase/data/default/usertable/667c1f2c34fe37d48ba25a789d8aba98/family/df03e3e8f7f94f0abbc1add81984086b, keycount=65717, bloomtype=ROW, size=46.8m, encoding=NONE, seqNum=364, earliestPutTs=1404253905678
2014-07-01 15:33:54,902 DEBUG [regionserver60020-smallCompactions-1404253964856] compactions.Compactor: Compacting hdfs://master:54310/hbase/data/default/usertable/667c1f2c34fe37d48ba25a789d8aba98/family/beb66310e7124c9c873f543ef061e1c7, keycount=26818, bloomtype=ROW, size=19.1m, encoding=NONE, seqNum=656, earliestPutTs=1404253958383
2014-07-01 15:33:54,902 DEBUG [regionserver60020-smallCompactions-1404253964856] compactions.Compactor: Compacting hdfs://master:54310/hbase/data/default/usertable/667c1f2c34fe37d48ba25a789d8aba98/family/ef6af2585e694a298b461d3e71b415a2, keycount=54359, bloomtype=ROW, size=38.7m, encoding=NONE, seqNum=863, earliestPutTs=1404253964998
2014-07-01 15:33:54,981 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 5266 synced till here 5232
2014-07-01 15:33:55,067 DEBUG [regionserver60020-smallCompactions-1404253964856] util.FSUtils: DFS Client does not support most favored nodes create; using default create
2014-07-01 15:33:55,954 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1404253868846/slave1%2C60020%2C1404253868846.1404254030927 with entries=122, filesize=99.5m; new WAL /hbase/WALs/slave1,60020,1404253868846/slave1%2C60020%2C1404253868846.1404254034564
2014-07-01 15:33:56,735 INFO  [MemStoreFlusher.1] regionserver.DefaultStoreFlusher: Flushed, sequenceid=1142, memsize=132.9m, hasBloomFilter=true, into tmp file hdfs://master:54310/hbase/data/default/usertable/93253e37c284f3174309fede3f32339f/.tmp/058c1de8eaf345c9b851a25088ad3042
2014-07-01 15:33:56,756 DEBUG [MemStoreFlusher.1] regionserver.HRegionFileSystem: Committing store file hdfs://master:54310/hbase/data/default/usertable/93253e37c284f3174309fede3f32339f/.tmp/058c1de8eaf345c9b851a25088ad3042 as hdfs://master:54310/hbase/data/default/usertable/93253e37c284f3174309fede3f32339f/family/058c1de8eaf345c9b851a25088ad3042
2014-07-01 15:33:56,769 INFO  [MemStoreFlusher.1] regionserver.HStore: Added hdfs://master:54310/hbase/data/default/usertable/93253e37c284f3174309fede3f32339f/family/058c1de8eaf345c9b851a25088ad3042, entries=484070, sequenceid=1142, filesize=34.5m
2014-07-01 15:33:56,769 INFO  [MemStoreFlusher.1] regionserver.HRegion: Finished memstore flush of ~263.2m/276017200, currentsize=195.3m/204785280 for region usertable,user3,1404228265526.93253e37c284f3174309fede3f32339f. in 15601ms, sequenceid=1142, compaction requested=true
2014-07-01 15:33:56,769 DEBUG [MemStoreFlusher.1] regionserver.CompactSplitThread: Small Compaction requested: system; Because: MemStoreFlusher.1; compaction_queue=(0:5), split_queue=0, merge_queue=0
2014-07-01 15:33:56,769 DEBUG [MemStoreFlusher.1] regionserver.HRegion: Started memstore flush for usertable,user5,1404228265526.122e92eabfab38cb15a9965b21db25c5., current region memstore size 410.0m
2014-07-01 15:33:57,102 INFO  [MemStoreFlusher.0] regionserver.DefaultStoreFlusher: Flushed, sequenceid=1065, memsize=141.7m, hasBloomFilter=true, into tmp file hdfs://master:54310/hbase/data/default/usertable/667c1f2c34fe37d48ba25a789d8aba98/.tmp/448a712f5be34ade9a16c57c0c793359
2014-07-01 15:33:57,245 DEBUG [MemStoreFlusher.0] regionserver.HRegionFileSystem: Committing store file hdfs://master:54310/hbase/data/default/usertable/667c1f2c34fe37d48ba25a789d8aba98/.tmp/448a712f5be34ade9a16c57c0c793359 as hdfs://master:54310/hbase/data/default/usertable/667c1f2c34fe37d48ba25a789d8aba98/family/448a712f5be34ade9a16c57c0c793359
2014-07-01 15:33:57,261 INFO  [MemStoreFlusher.0] regionserver.HStore: Added hdfs://master:54310/hbase/data/default/usertable/667c1f2c34fe37d48ba25a789d8aba98/family/448a712f5be34ade9a16c57c0c793359, entries=515930, sequenceid=1065, filesize=36.8m
2014-07-01 15:33:57,279 INFO  [MemStoreFlusher.0] regionserver.HRegion: Finished memstore flush of ~349.0m/365985920, currentsize=237.4m/248926240 for region usertable,user7,1404228265526.667c1f2c34fe37d48ba25a789d8aba98. in 20209ms, sequenceid=1065, compaction requested=false
2014-07-01 15:33:57,280 DEBUG [MemStoreFlusher.0] regionserver.HRegion: Started memstore flush for usertable,,1404228265525.9cef9e7d34f96f1d9b111be50040a208., current region memstore size 289.9m
2014-07-01 15:33:57,719 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-01 15:33:57,907 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 5375 synced till here 5349
2014-07-01 15:33:58,389 DEBUG [MemStoreFlusher.1] util.FSUtils: DFS Client does not support most favored nodes create; using default create
2014-07-01 15:33:58,390 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1404253868846/slave1%2C60020%2C1404253868846.1404254034564 with entries=109, filesize=87.0m; new WAL /hbase/WALs/slave1,60020,1404253868846/slave1%2C60020%2C1404253868846.1404254037720
2014-07-01 15:33:58,514 DEBUG [RpcServer.handler=26,port=60020] regionserver.HRegion: Flush requested on usertable,user7,1404228265526.667c1f2c34fe37d48ba25a789d8aba98.
2014-07-01 15:33:58,866 DEBUG [MemStoreFlusher.0] util.FSUtils: DFS Client does not support most favored nodes create; using default create
2014-07-01 15:34:00,015 DEBUG [RpcServer.handler=49,port=60020] regionserver.HRegion: Flush requested on usertable,user9,1404228265526.bb79ae773f71289e0745068f68a0abd9.
2014-07-01 15:34:00,379 DEBUG [RpcServer.handler=20,port=60020] regionserver.HRegion: Flush requested on usertable,user3,1404228265526.93253e37c284f3174309fede3f32339f.
2014-07-01 15:34:01,750 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-01 15:34:02,004 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 5503 synced till here 5479
2014-07-01 15:34:02,349 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1404253868846/slave1%2C60020%2C1404253868846.1404254037720 with entries=128, filesize=102.5m; new WAL /hbase/WALs/slave1,60020,1404253868846/slave1%2C60020%2C1404253868846.1404254041750
2014-07-01 15:34:05,279 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-01 15:34:05,450 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 5600 synced till here 5579
2014-07-01 15:34:05,902 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1404253868846/slave1%2C60020%2C1404253868846.1404254041750 with entries=97, filesize=80.7m; new WAL /hbase/WALs/slave1,60020,1404253868846/slave1%2C60020%2C1404253868846.1404254045279
2014-07-01 15:34:08,418 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-01 15:34:08,675 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 5712 synced till here 5684
2014-07-01 15:34:09,770 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1404253868846/slave1%2C60020%2C1404253868846.1404254045279 with entries=112, filesize=90.3m; new WAL /hbase/WALs/slave1,60020,1404253868846/slave1%2C60020%2C1404253868846.1404254048419
2014-07-01 15:34:13,031 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-01 15:34:13,308 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 5829 synced till here 5809
2014-07-01 15:34:14,422 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1404253868846/slave1%2C60020%2C1404253868846.1404254048419 with entries=117, filesize=96.2m; new WAL /hbase/WALs/slave1,60020,1404253868846/slave1%2C60020%2C1404253868846.1404254053031
2014-07-01 15:34:16,870 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-01 15:34:17,021 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 5940 synced till here 5927
2014-07-01 15:34:17,239 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1404253868846/slave1%2C60020%2C1404253868846.1404254053031 with entries=111, filesize=77.9m; new WAL /hbase/WALs/slave1,60020,1404253868846/slave1%2C60020%2C1404253868846.1404254056870
2014-07-01 15:34:18,645 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-01 15:34:18,659 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 6046 synced till here 6036
2014-07-01 15:34:18,700 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1404253868846/slave1%2C60020%2C1404253868846.1404254056870 with entries=106, filesize=65.0m; new WAL /hbase/WALs/slave1,60020,1404253868846/slave1%2C60020%2C1404253868846.1404254058646
2014-07-01 15:34:21,645 INFO  [MemStoreFlusher.0] regionserver.DefaultStoreFlusher: Flushed, sequenceid=1021, memsize=264.4m, hasBloomFilter=true, into tmp file hdfs://master:54310/hbase/data/default/usertable/9cef9e7d34f96f1d9b111be50040a208/.tmp/242fc66525a74fc5bc085539024c616a
2014-07-01 15:34:21,658 DEBUG [MemStoreFlusher.0] regionserver.HRegionFileSystem: Committing store file hdfs://master:54310/hbase/data/default/usertable/9cef9e7d34f96f1d9b111be50040a208/.tmp/242fc66525a74fc5bc085539024c616a as hdfs://master:54310/hbase/data/default/usertable/9cef9e7d34f96f1d9b111be50040a208/family/242fc66525a74fc5bc085539024c616a
2014-07-01 15:34:21,672 INFO  [MemStoreFlusher.0] regionserver.HStore: Added hdfs://master:54310/hbase/data/default/usertable/9cef9e7d34f96f1d9b111be50040a208/family/242fc66525a74fc5bc085539024c616a, entries=962700, sequenceid=1021, filesize=68.6m
2014-07-01 15:34:21,673 INFO  [MemStoreFlusher.0] regionserver.HRegion: Finished memstore flush of ~293.3m/307581760, currentsize=93.5m/98064560 for region usertable,,1404228265525.9cef9e7d34f96f1d9b111be50040a208. in 24393ms, sequenceid=1021, compaction requested=true
2014-07-01 15:34:21,673 DEBUG [MemStoreFlusher.0] regionserver.CompactSplitThread: Small Compaction requested: system; Because: MemStoreFlusher.0; compaction_queue=(0:6), split_queue=0, merge_queue=0
2014-07-01 15:34:21,673 DEBUG [MemStoreFlusher.0] regionserver.HRegion: Started memstore flush for usertable,user7,1404228265526.667c1f2c34fe37d48ba25a789d8aba98., current region memstore size 527.0m
2014-07-01 15:34:22,081 INFO  [MemStoreFlusher.1] regionserver.DefaultStoreFlusher: Flushed, sequenceid=1249, memsize=251.7m, hasBloomFilter=true, into tmp file hdfs://master:54310/hbase/data/default/usertable/122e92eabfab38cb15a9965b21db25c5/.tmp/7905079665664e82b3df5ceb29d6d35c
2014-07-01 15:34:22,105 DEBUG [MemStoreFlusher.1] regionserver.HRegionFileSystem: Committing store file hdfs://master:54310/hbase/data/default/usertable/122e92eabfab38cb15a9965b21db25c5/.tmp/7905079665664e82b3df5ceb29d6d35c as hdfs://master:54310/hbase/data/default/usertable/122e92eabfab38cb15a9965b21db25c5/family/7905079665664e82b3df5ceb29d6d35c
2014-07-01 15:34:22,125 INFO  [MemStoreFlusher.1] regionserver.HStore: Added hdfs://master:54310/hbase/data/default/usertable/122e92eabfab38cb15a9965b21db25c5/family/7905079665664e82b3df5ceb29d6d35c, entries=916400, sequenceid=1249, filesize=65.3m
2014-07-01 15:34:22,125 INFO  [MemStoreFlusher.1] regionserver.HRegion: Finished memstore flush of ~428.9m/449784320, currentsize=271.1m/284321040 for region usertable,user5,1404228265526.122e92eabfab38cb15a9965b21db25c5. in 25356ms, sequenceid=1249, compaction requested=true
2014-07-01 15:34:22,125 DEBUG [MemStoreFlusher.1] regionserver.CompactSplitThread: Small Compaction requested: system; Because: MemStoreFlusher.1; compaction_queue=(0:7), split_queue=0, merge_queue=0
2014-07-01 15:34:22,126 DEBUG [MemStoreFlusher.1] regionserver.HRegion: Started memstore flush for usertable,user9,1404228265526.bb79ae773f71289e0745068f68a0abd9., current region memstore size 338.4m
2014-07-01 15:34:22,203 DEBUG [MemStoreFlusher.0] util.FSUtils: DFS Client does not support most favored nodes create; using default create
2014-07-01 15:34:22,522 DEBUG [MemStoreFlusher.1] util.FSUtils: DFS Client does not support most favored nodes create; using default create
2014-07-01 15:34:26,675 DEBUG [RpcServer.handler=29,port=60020] regionserver.HRegion: Flush requested on usertable,user5,1404228265526.122e92eabfab38cb15a9965b21db25c5.
2014-07-01 15:34:27,718 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-01 15:34:27,735 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 6135 synced till here 6131
2014-07-01 15:34:27,754 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1404253868846/slave1%2C60020%2C1404253868846.1404254058646 with entries=89, filesize=63.8m; new WAL /hbase/WALs/slave1,60020,1404253868846/slave1%2C60020%2C1404253868846.1404254067718
2014-07-01 15:34:29,352 INFO  [MemStoreFlusher.1] regionserver.DefaultStoreFlusher: Flushed, sequenceid=1370, memsize=118.4m, hasBloomFilter=true, into tmp file hdfs://master:54310/hbase/data/default/usertable/bb79ae773f71289e0745068f68a0abd9/.tmp/c4a2b963ccb947d08390f8e9467a4f58
2014-07-01 15:34:29,421 DEBUG [MemStoreFlusher.1] regionserver.HRegionFileSystem: Committing store file hdfs://master:54310/hbase/data/default/usertable/bb79ae773f71289e0745068f68a0abd9/.tmp/c4a2b963ccb947d08390f8e9467a4f58 as hdfs://master:54310/hbase/data/default/usertable/bb79ae773f71289e0745068f68a0abd9/family/c4a2b963ccb947d08390f8e9467a4f58
2014-07-01 15:34:29,431 INFO  [MemStoreFlusher.1] regionserver.HStore: Added hdfs://master:54310/hbase/data/default/usertable/bb79ae773f71289e0745068f68a0abd9/family/c4a2b963ccb947d08390f8e9467a4f58, entries=431040, sequenceid=1370, filesize=30.7m
2014-07-01 15:34:29,432 INFO  [MemStoreFlusher.1] regionserver.HRegion: Finished memstore flush of ~338.4m/354846160, currentsize=11.6m/12132000 for region usertable,user9,1404228265526.bb79ae773f71289e0745068f68a0abd9. in 7306ms, sequenceid=1370, compaction requested=true
2014-07-01 15:34:29,432 DEBUG [MemStoreFlusher.1] regionserver.CompactSplitThread: Small Compaction requested: system; Because: MemStoreFlusher.1; compaction_queue=(0:8), split_queue=0, merge_queue=0
2014-07-01 15:34:29,432 DEBUG [MemStoreFlusher.1] regionserver.HRegion: Started memstore flush for usertable,user3,1404228265526.93253e37c284f3174309fede3f32339f., current region memstore size 523.4m
2014-07-01 15:34:30,081 DEBUG [MemStoreFlusher.1] util.FSUtils: DFS Client does not support most favored nodes create; using default create
2014-07-01 15:34:30,395 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-01 15:34:30,531 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1404253868846/slave1%2C60020%2C1404253868846.1404254067718 with entries=88, filesize=61.1m; new WAL /hbase/WALs/slave1,60020,1404253868846/slave1%2C60020%2C1404253868846.1404254070395
2014-07-01 15:34:30,532 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1404253868846/slave1%2C60020%2C1404253868846.1404253964269
2014-07-01 15:34:30,532 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1404253868846/slave1%2C60020%2C1404253868846.1404253970806
2014-07-01 15:34:30,532 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1404253868846/slave1%2C60020%2C1404253868846.1404253974223
2014-07-01 15:34:30,532 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1404253868846/slave1%2C60020%2C1404253868846.1404253976930
2014-07-01 15:34:30,532 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1404253868846/slave1%2C60020%2C1404253868846.1404253979812
2014-07-01 15:34:30,532 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1404253868846/slave1%2C60020%2C1404253868846.1404253982659
2014-07-01 15:34:30,532 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1404253868846/slave1%2C60020%2C1404253868846.1404253985595
2014-07-01 15:34:30,532 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1404253868846/slave1%2C60020%2C1404253868846.1404253988374
2014-07-01 15:34:30,532 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1404253868846/slave1%2C60020%2C1404253868846.1404253992038
2014-07-01 15:34:30,532 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1404253868846/slave1%2C60020%2C1404253868846.1404253995025
2014-07-01 15:34:30,532 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1404253868846/slave1%2C60020%2C1404253868846.1404253997846
2014-07-01 15:34:30,532 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1404253868846/slave1%2C60020%2C1404253868846.1404254000714
2014-07-01 15:34:30,533 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1404253868846/slave1%2C60020%2C1404253868846.1404254004421
2014-07-01 15:34:30,533 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1404253868846/slave1%2C60020%2C1404253868846.1404254007342
2014-07-01 15:34:30,533 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1404253868846/slave1%2C60020%2C1404253868846.1404254010682
2014-07-01 15:34:30,533 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1404253868846/slave1%2C60020%2C1404253868846.1404254013086
2014-07-01 15:34:32,263 DEBUG [regionserver60020-smallCompactions-1404253964856] regionserver.HRegionFileSystem: Committing store file hdfs://master:54310/hbase/data/default/usertable/667c1f2c34fe37d48ba25a789d8aba98/.tmp/dde913fa18fc4787ba86eb9aa2d45ab4 as hdfs://master:54310/hbase/data/default/usertable/667c1f2c34fe37d48ba25a789d8aba98/family/dde913fa18fc4787ba86eb9aa2d45ab4
2014-07-01 15:34:32,355 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-01 15:34:32,388 DEBUG [regionserver60020-smallCompactions-1404253964856] regionserver.HStore: Removing store files after compaction...
2014-07-01 15:34:32,599 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1404253868846/slave1%2C60020%2C1404253868846.1404254070395 with entries=99, filesize=63.2m; new WAL /hbase/WALs/slave1,60020,1404253868846/slave1%2C60020%2C1404253868846.1404254072356
2014-07-01 15:34:32,792 DEBUG [regionserver60020-smallCompactions-1404253964856] backup.HFileArchiver: Finished archiving from class org.apache.hadoop.hbase.backup.HFileArchiver$FileableStoreFile, file:hdfs://master:54310/hbase/data/default/usertable/667c1f2c34fe37d48ba25a789d8aba98/family/9fdb368501fa45e18be2f611d85417a8, to hdfs://master:54310/hbase/archive/data/default/usertable/667c1f2c34fe37d48ba25a789d8aba98/family/9fdb368501fa45e18be2f611d85417a8
2014-07-01 15:34:33,031 DEBUG [regionserver60020-smallCompactions-1404253964856] backup.HFileArchiver: Finished archiving from class org.apache.hadoop.hbase.backup.HFileArchiver$FileableStoreFile, file:hdfs://master:54310/hbase/data/default/usertable/667c1f2c34fe37d48ba25a789d8aba98/family/df03e3e8f7f94f0abbc1add81984086b, to hdfs://master:54310/hbase/archive/data/default/usertable/667c1f2c34fe37d48ba25a789d8aba98/family/df03e3e8f7f94f0abbc1add81984086b
2014-07-01 15:34:33,077 DEBUG [regionserver60020-smallCompactions-1404253964856] backup.HFileArchiver: Finished archiving from class org.apache.hadoop.hbase.backup.HFileArchiver$FileableStoreFile, file:hdfs://master:54310/hbase/data/default/usertable/667c1f2c34fe37d48ba25a789d8aba98/family/beb66310e7124c9c873f543ef061e1c7, to hdfs://master:54310/hbase/archive/data/default/usertable/667c1f2c34fe37d48ba25a789d8aba98/family/beb66310e7124c9c873f543ef061e1c7
2014-07-01 15:34:33,237 DEBUG [regionserver60020-smallCompactions-1404253964856] backup.HFileArchiver: Finished archiving from class org.apache.hadoop.hbase.backup.HFileArchiver$FileableStoreFile, file:hdfs://master:54310/hbase/data/default/usertable/667c1f2c34fe37d48ba25a789d8aba98/family/ef6af2585e694a298b461d3e71b415a2, to hdfs://master:54310/hbase/archive/data/default/usertable/667c1f2c34fe37d48ba25a789d8aba98/family/ef6af2585e694a298b461d3e71b415a2
2014-07-01 15:34:33,237 INFO  [regionserver60020-smallCompactions-1404253964856] regionserver.HStore: Completed major compaction of 4 file(s) in family of usertable,user7,1404228265526.667c1f2c34fe37d48ba25a789d8aba98. into dde913fa18fc4787ba86eb9aa2d45ab4(size=115.6m), total size for store is 152.4m. This selection was in queue for 0sec, and took 38sec to execute.
2014-07-01 15:34:33,238 INFO  [regionserver60020-smallCompactions-1404253964856] regionserver.CompactSplitThread: Completed compaction: Request = regionName=usertable,user7,1404228265526.667c1f2c34fe37d48ba25a789d8aba98., storeName=family, fileCount=4, fileSize=149.4m, priority=16, time=74146288755200; duration=38sec
2014-07-01 15:34:33,238 DEBUG [regionserver60020-smallCompactions-1404253964856] regionserver.CompactSplitThread: CompactSplitThread Status: compaction_queue=(0:8), split_queue=0, merge_queue=0
2014-07-01 15:34:33,238 DEBUG [regionserver60020-smallCompactions-1404253964856] compactions.RatioBasedCompactionPolicy: Selecting compaction from 4 store files, 0 compacting, 4 eligible, 20 blocking
2014-07-01 15:34:33,238 DEBUG [regionserver60020-smallCompactions-1404253964856] compactions.ExploringCompactionPolicy: Exploring compaction algorithm has selected 4 files of size 203675113 starting at candidate #0 after considering 3 permutations with 3 in ratio
2014-07-01 15:34:33,238 DEBUG [regionserver60020-smallCompactions-1404253964856] regionserver.HStore: 93253e37c284f3174309fede3f32339f - family: Initiating major compaction
2014-07-01 15:34:33,239 INFO  [regionserver60020-smallCompactions-1404253964856] regionserver.HRegion: Starting compaction on family in region usertable,user3,1404228265526.93253e37c284f3174309fede3f32339f.
2014-07-01 15:34:33,239 INFO  [regionserver60020-smallCompactions-1404253964856] regionserver.HStore: Starting compaction of 4 file(s) in family of usertable,user3,1404228265526.93253e37c284f3174309fede3f32339f. into tmpdir=hdfs://master:54310/hbase/data/default/usertable/93253e37c284f3174309fede3f32339f/.tmp, totalSize=194.2m
2014-07-01 15:34:33,239 DEBUG [regionserver60020-smallCompactions-1404253964856] compactions.Compactor: Compacting hdfs://master:54310/hbase/data/default/usertable/93253e37c284f3174309fede3f32339f/family/8fe3051444564c758ac8530e9b7243bf, keycount=128346, bloomtype=ROW, size=91.5m, encoding=NONE, seqNum=604, earliestPutTs=1404228281603
2014-07-01 15:34:33,239 DEBUG [regionserver60020-smallCompactions-1404253964856] compactions.Compactor: Compacting hdfs://master:54310/hbase/data/default/usertable/93253e37c284f3174309fede3f32339f/family/c983ea1b694d4ca0a196965f3913a212, keycount=37626, bloomtype=ROW, size=26.8m, encoding=NONE, seqNum=817, earliestPutTs=1404253961976
2014-07-01 15:34:33,239 DEBUG [regionserver60020-smallCompactions-1404253964856] compactions.Compactor: Compacting hdfs://master:54310/hbase/data/default/usertable/93253e37c284f3174309fede3f32339f/family/d98c2ed64b8e4dbd915634a5b65edc0c, keycount=58096, bloomtype=ROW, size=41.4m, encoding=NONE, seqNum=989, earliestPutTs=1404253977066
2014-07-01 15:34:33,240 DEBUG [regionserver60020-smallCompactions-1404253964856] compactions.Compactor: Compacting hdfs://master:54310/hbase/data/default/usertable/93253e37c284f3174309fede3f32339f/family/058c1de8eaf345c9b851a25088ad3042, keycount=48407, bloomtype=ROW, size=34.5m, encoding=NONE, seqNum=1142, earliestPutTs=1404254002754
2014-07-01 15:34:33,424 DEBUG [regionserver60020-smallCompactions-1404253964856] util.FSUtils: DFS Client does not support most favored nodes create; using default create
2014-07-01 15:34:34,963 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-01 15:34:34,993 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 6426 synced till here 6419
2014-07-01 15:34:35,321 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1404253868846/slave1%2C60020%2C1404253868846.1404254072356 with entries=104, filesize=67.1m; new WAL /hbase/WALs/slave1,60020,1404253868846/slave1%2C60020%2C1404253868846.1404254074964
2014-07-01 15:34:36,195 INFO  [MemStoreFlusher.0] regionserver.DefaultStoreFlusher: Flushed, sequenceid=1371, memsize=196.8m, hasBloomFilter=true, into tmp file hdfs://master:54310/hbase/data/default/usertable/667c1f2c34fe37d48ba25a789d8aba98/.tmp/5604109a72ba430fa33c345ec291f6cc
2014-07-01 15:34:36,212 DEBUG [MemStoreFlusher.0] regionserver.HRegionFileSystem: Committing store file hdfs://master:54310/hbase/data/default/usertable/667c1f2c34fe37d48ba25a789d8aba98/.tmp/5604109a72ba430fa33c345ec291f6cc as hdfs://master:54310/hbase/data/default/usertable/667c1f2c34fe37d48ba25a789d8aba98/family/5604109a72ba430fa33c345ec291f6cc
2014-07-01 15:34:36,225 INFO  [MemStoreFlusher.0] regionserver.HStore: Added hdfs://master:54310/hbase/data/default/usertable/667c1f2c34fe37d48ba25a789d8aba98/family/5604109a72ba430fa33c345ec291f6cc, entries=716600, sequenceid=1371, filesize=51.1m
2014-07-01 15:34:36,225 INFO  [MemStoreFlusher.0] regionserver.HRegion: Finished memstore flush of ~527.0m/552563120, currentsize=126.0m/132150160 for region usertable,user7,1404228265526.667c1f2c34fe37d48ba25a789d8aba98. in 14552ms, sequenceid=1371, compaction requested=true
2014-07-01 15:34:36,225 DEBUG [MemStoreFlusher.0] regionserver.CompactSplitThread: Small Compaction requested: system; Because: MemStoreFlusher.0; compaction_queue=(0:8), split_queue=0, merge_queue=0
2014-07-01 15:34:36,226 DEBUG [MemStoreFlusher.0] regionserver.HRegion: Started memstore flush for usertable,user5,1404228265526.122e92eabfab38cb15a9965b21db25c5., current region memstore size 402.4m
2014-07-01 15:34:36,960 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-01 15:34:37,064 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 6525 synced till here 6512
2014-07-01 15:34:37,283 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1404253868846/slave1%2C60020%2C1404253868846.1404254074964 with entries=99, filesize=70.1m; new WAL /hbase/WALs/slave1,60020,1404253868846/slave1%2C60020%2C1404253868846.1404254076960
2014-07-01 15:34:37,284 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1404253868846/slave1%2C60020%2C1404253868846.1404254015896
2014-07-01 15:34:37,284 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1404253868846/slave1%2C60020%2C1404253868846.1404254018680
2014-07-01 15:34:37,287 DEBUG [MemStoreFlusher.0] util.FSUtils: DFS Client does not support most favored nodes create; using default create
2014-07-01 15:34:39,129 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-01 15:34:39,161 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 6625 synced till here 6617
2014-07-01 15:34:39,392 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1404253868846/slave1%2C60020%2C1404253868846.1404254076960 with entries=100, filesize=72.1m; new WAL /hbase/WALs/slave1,60020,1404253868846/slave1%2C60020%2C1404253868846.1404254079130
2014-07-01 15:34:42,645 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-01 15:34:42,673 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 6728 synced till here 6712
2014-07-01 15:34:43,053 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1404253868846/slave1%2C60020%2C1404253868846.1404254079130 with entries=103, filesize=79.4m; new WAL /hbase/WALs/slave1,60020,1404253868846/slave1%2C60020%2C1404253868846.1404254082646
2014-07-01 15:34:44,954 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-01 15:34:44,999 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 6843 synced till here 6815
2014-07-01 15:34:45,244 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1404253868846/slave1%2C60020%2C1404253868846.1404254082646 with entries=115, filesize=78.5m; new WAL /hbase/WALs/slave1,60020,1404253868846/slave1%2C60020%2C1404253868846.1404254084954
2014-07-01 15:34:45,855 DEBUG [RpcServer.handler=4,port=60020] regionserver.HRegion: Flush requested on usertable,user7,1404228265526.667c1f2c34fe37d48ba25a789d8aba98.
2014-07-01 15:34:47,505 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-01 15:34:50,334 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 6953 synced till here 6928
2014-07-01 15:34:50,406 INFO  [JvmPauseMonitor] util.JvmPauseMonitor: Detected pause in JVM or host machine (eg GC): pause of approximately 2183ms
GC pool 'ParNew' had collection(s): count=3 time=2461ms
2014-07-01 15:34:50,914 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1404253868846/slave1%2C60020%2C1404253868846.1404254084954 with entries=110, filesize=80.9m; new WAL /hbase/WALs/slave1,60020,1404253868846/slave1%2C60020%2C1404253868846.1404254087506
2014-07-01 15:34:53,367 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-01 15:34:53,723 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 7070 synced till here 7043
2014-07-01 15:34:54,115 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1404253868846/slave1%2C60020%2C1404253868846.1404254087506 with entries=117, filesize=94.3m; new WAL /hbase/WALs/slave1,60020,1404253868846/slave1%2C60020%2C1404253868846.1404254093368
2014-07-01 15:34:54,567 INFO  [MemStoreFlusher.1] regionserver.DefaultStoreFlusher: Flushed, sequenceid=1448, memsize=169.4m, hasBloomFilter=true, into tmp file hdfs://master:54310/hbase/data/default/usertable/93253e37c284f3174309fede3f32339f/.tmp/a2f5c30d99ef4611ac44a48e6c238e5e
2014-07-01 15:34:54,598 DEBUG [MemStoreFlusher.1] regionserver.HRegionFileSystem: Committing store file hdfs://master:54310/hbase/data/default/usertable/93253e37c284f3174309fede3f32339f/.tmp/a2f5c30d99ef4611ac44a48e6c238e5e as hdfs://master:54310/hbase/data/default/usertable/93253e37c284f3174309fede3f32339f/family/a2f5c30d99ef4611ac44a48e6c238e5e
2014-07-01 15:34:54,613 INFO  [MemStoreFlusher.1] regionserver.HStore: Added hdfs://master:54310/hbase/data/default/usertable/93253e37c284f3174309fede3f32339f/family/a2f5c30d99ef4611ac44a48e6c238e5e, entries=616870, sequenceid=1448, filesize=44.0m
2014-07-01 15:34:54,622 INFO  [MemStoreFlusher.1] regionserver.HRegion: Finished memstore flush of ~523.4m/548830480, currentsize=338.9m/355390640 for region usertable,user3,1404228265526.93253e37c284f3174309fede3f32339f. in 25190ms, sequenceid=1448, compaction requested=false
2014-07-01 15:34:54,736 DEBUG [MemStoreFlusher.1] regionserver.HRegion: Started memstore flush for usertable,user7,1404228265526.667c1f2c34fe37d48ba25a789d8aba98., current region memstore size 357.8m
2014-07-01 15:34:54,799 DEBUG [RpcServer.handler=15,port=60020] regionserver.HRegion: Flush requested on usertable,user3,1404228265526.93253e37c284f3174309fede3f32339f.
2014-07-01 15:34:55,289 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-01 15:34:55,373 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 7167 synced till here 7164
2014-07-01 15:34:55,495 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1404253868846/slave1%2C60020%2C1404253868846.1404254093368 with entries=97, filesize=69.3m; new WAL /hbase/WALs/slave1,60020,1404253868846/slave1%2C60020%2C1404253868846.1404254095290
2014-07-01 15:34:55,495 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1404253868846/slave1%2C60020%2C1404253868846.1404254021192
2014-07-01 15:34:55,495 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1404253868846/slave1%2C60020%2C1404253868846.1404254023624
2014-07-01 15:34:55,495 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1404253868846/slave1%2C60020%2C1404253868846.1404254026134
2014-07-01 15:34:55,495 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1404253868846/slave1%2C60020%2C1404253868846.1404254028251
2014-07-01 15:34:55,495 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1404253868846/slave1%2C60020%2C1404253868846.1404254030927
2014-07-01 15:34:55,903 DEBUG [MemStoreFlusher.1] util.FSUtils: DFS Client does not support most favored nodes create; using default create
2014-07-01 15:34:56,731 INFO  [MemStoreFlusher.0] regionserver.DefaultStoreFlusher: Flushed, sequenceid=1488, memsize=143.1m, hasBloomFilter=true, into tmp file hdfs://master:54310/hbase/data/default/usertable/122e92eabfab38cb15a9965b21db25c5/.tmp/5102e47fde4f44ad99e428f06749b5b0
2014-07-01 15:34:56,758 DEBUG [MemStoreFlusher.0] regionserver.HRegionFileSystem: Committing store file hdfs://master:54310/hbase/data/default/usertable/122e92eabfab38cb15a9965b21db25c5/.tmp/5102e47fde4f44ad99e428f06749b5b0 as hdfs://master:54310/hbase/data/default/usertable/122e92eabfab38cb15a9965b21db25c5/family/5102e47fde4f44ad99e428f06749b5b0
2014-07-01 15:34:56,923 INFO  [MemStoreFlusher.0] regionserver.HStore: Added hdfs://master:54310/hbase/data/default/usertable/122e92eabfab38cb15a9965b21db25c5/family/5102e47fde4f44ad99e428f06749b5b0, entries=521210, sequenceid=1488, filesize=37.1m
2014-07-01 15:34:56,924 INFO  [MemStoreFlusher.0] regionserver.HRegion: Finished memstore flush of ~409.5m/429355680, currentsize=275.4m/288758240 for region usertable,user5,1404228265526.122e92eabfab38cb15a9965b21db25c5. in 20699ms, sequenceid=1488, compaction requested=true
2014-07-01 15:34:56,924 DEBUG [MemStoreFlusher.0] regionserver.CompactSplitThread: Small Compaction requested: system; Because: MemStoreFlusher.0; compaction_queue=(0:9), split_queue=0, merge_queue=0
2014-07-01 15:34:56,924 DEBUG [MemStoreFlusher.0] regionserver.HRegion: Started memstore flush for usertable,user3,1404228265526.93253e37c284f3174309fede3f32339f., current region memstore size 364.3m
2014-07-01 15:34:56,969 DEBUG [RpcServer.handler=20,port=60020] regionserver.HRegion: Flush requested on usertable,user5,1404228265526.122e92eabfab38cb15a9965b21db25c5.
2014-07-01 15:34:57,914 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-01 15:34:58,027 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 7266 synced till here 7255
2014-07-01 15:34:58,187 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1404253868846/slave1%2C60020%2C1404253868846.1404254095290 with entries=99, filesize=75.8m; new WAL /hbase/WALs/slave1,60020,1404253868846/slave1%2C60020%2C1404253868846.1404254097915
2014-07-01 15:34:58,584 DEBUG [MemStoreFlusher.0] util.FSUtils: DFS Client does not support most favored nodes create; using default create
2014-07-01 15:35:00,014 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-01 15:35:00,044 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 7358 synced till here 7355
2014-07-01 15:35:00,189 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1404253868846/slave1%2C60020%2C1404253868846.1404254097915 with entries=92, filesize=63.2m; new WAL /hbase/WALs/slave1,60020,1404253868846/slave1%2C60020%2C1404253868846.1404254100015
2014-07-01 15:35:02,621 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-01 15:35:03,158 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 7448 synced till here 7441
2014-07-01 15:35:03,535 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1404253868846/slave1%2C60020%2C1404253868846.1404254100015 with entries=90, filesize=70.1m; new WAL /hbase/WALs/slave1,60020,1404253868846/slave1%2C60020%2C1404253868846.1404254102621
2014-07-01 15:35:04,132 DEBUG [RpcServer.handler=42,port=60020] regionserver.HRegion: Flush requested on usertable,,1404228265525.9cef9e7d34f96f1d9b111be50040a208.
2014-07-01 15:35:08,430 INFO  [JvmPauseMonitor] util.JvmPauseMonitor: Detected pause in JVM or host machine (eg GC): pause of approximately 1887ms
GC pool 'ParNew' had collection(s): count=1 time=2304ms
2014-07-01 15:35:08,960 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-01 15:35:09,416 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 7559 synced till here 7524
2014-07-01 15:35:10,664 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1404253868846/slave1%2C60020%2C1404253868846.1404254102621 with entries=111, filesize=92.7m; new WAL /hbase/WALs/slave1,60020,1404253868846/slave1%2C60020%2C1404253868846.1404254108961
2014-07-01 15:35:13,708 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-01 15:35:13,916 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 7671 synced till here 7637
2014-07-01 15:35:14,448 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1404253868846/slave1%2C60020%2C1404253868846.1404254108961 with entries=112, filesize=92.6m; new WAL /hbase/WALs/slave1,60020,1404253868846/slave1%2C60020%2C1404253868846.1404254113708
2014-07-01 15:35:15,965 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-01 15:35:16,314 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 7782 synced till here 7758
2014-07-01 15:35:16,559 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1404253868846/slave1%2C60020%2C1404253868846.1404254113708 with entries=111, filesize=78.2m; new WAL /hbase/WALs/slave1,60020,1404253868846/slave1%2C60020%2C1404253868846.1404254115965
2014-07-01 15:35:18,765 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-01 15:35:19,241 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 7913 synced till here 7899
2014-07-01 15:35:19,533 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1404253868846/slave1%2C60020%2C1404253868846.1404254115965 with entries=131, filesize=99.7m; new WAL /hbase/WALs/slave1,60020,1404253868846/slave1%2C60020%2C1404253868846.1404254118765
2014-07-01 15:35:21,580 INFO  [MemStoreFlusher.0] regionserver.DefaultStoreFlusher: Flushed, sequenceid=1669, memsize=148.8m, hasBloomFilter=true, into tmp file hdfs://master:54310/hbase/data/default/usertable/93253e37c284f3174309fede3f32339f/.tmp/f674b967455d4849b06b82ffdb29ea0e
2014-07-01 15:35:21,594 DEBUG [MemStoreFlusher.0] regionserver.HRegionFileSystem: Committing store file hdfs://master:54310/hbase/data/default/usertable/93253e37c284f3174309fede3f32339f/.tmp/f674b967455d4849b06b82ffdb29ea0e as hdfs://master:54310/hbase/data/default/usertable/93253e37c284f3174309fede3f32339f/family/f674b967455d4849b06b82ffdb29ea0e
2014-07-01 15:35:21,595 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-01 15:35:21,718 INFO  [MemStoreFlusher.0] regionserver.HStore: Added hdfs://master:54310/hbase/data/default/usertable/93253e37c284f3174309fede3f32339f/family/f674b967455d4849b06b82ffdb29ea0e, entries=541810, sequenceid=1669, filesize=38.6m
2014-07-01 15:35:21,725 INFO  [MemStoreFlusher.0] regionserver.HRegion: Finished memstore flush of ~374.5m/392691440, currentsize=296.4m/310832480 for region usertable,user3,1404228265526.93253e37c284f3174309fede3f32339f. in 24801ms, sequenceid=1669, compaction requested=false
2014-07-01 15:35:21,726 DEBUG [MemStoreFlusher.0] regionserver.HRegion: Started memstore flush for usertable,user5,1404228265526.122e92eabfab38cb15a9965b21db25c5., current region memstore size 584.2m
2014-07-01 15:35:21,736 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 8044 synced till here 8009
2014-07-01 15:35:21,945 DEBUG [RpcServer.handler=7,port=60020] regionserver.HRegion: Flush requested on usertable,user3,1404228265526.93253e37c284f3174309fede3f32339f.
2014-07-01 15:35:22,283 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1404253868846/slave1%2C60020%2C1404253868846.1404254118765 with entries=131, filesize=101.0m; new WAL /hbase/WALs/slave1,60020,1404253868846/slave1%2C60020%2C1404253868846.1404254121595
2014-07-01 15:35:23,705 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-01 15:35:23,722 INFO  [MemStoreFlusher.1] regionserver.DefaultStoreFlusher: Flushed, sequenceid=1594, memsize=191.2m, hasBloomFilter=true, into tmp file hdfs://master:54310/hbase/data/default/usertable/667c1f2c34fe37d48ba25a789d8aba98/.tmp/72abfd2537aa4422a8b06003dfff6e47
2014-07-01 15:35:23,750 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 8150 synced till here 8135
2014-07-01 15:35:23,803 DEBUG [MemStoreFlusher.1] regionserver.HRegionFileSystem: Committing store file hdfs://master:54310/hbase/data/default/usertable/667c1f2c34fe37d48ba25a789d8aba98/.tmp/72abfd2537aa4422a8b06003dfff6e47 as hdfs://master:54310/hbase/data/default/usertable/667c1f2c34fe37d48ba25a789d8aba98/family/72abfd2537aa4422a8b06003dfff6e47
2014-07-01 15:35:23,817 INFO  [MemStoreFlusher.1] regionserver.HStore: Added hdfs://master:54310/hbase/data/default/usertable/667c1f2c34fe37d48ba25a789d8aba98/family/72abfd2537aa4422a8b06003dfff6e47, entries=696020, sequenceid=1594, filesize=49.6m
2014-07-01 15:35:23,818 INFO  [MemStoreFlusher.1] regionserver.HRegion: Finished memstore flush of ~380.0m/398501680, currentsize=371.6m/389694640 for region usertable,user7,1404228265526.667c1f2c34fe37d48ba25a789d8aba98. in 29081ms, sequenceid=1594, compaction requested=true
2014-07-01 15:35:23,818 DEBUG [MemStoreFlusher.1] regionserver.CompactSplitThread: Small Compaction requested: system; Because: MemStoreFlusher.1; compaction_queue=(0:10), split_queue=0, merge_queue=0
2014-07-01 15:35:23,818 DEBUG [MemStoreFlusher.1] regionserver.HRegion: Started memstore flush for usertable,,1404228265525.9cef9e7d34f96f1d9b111be50040a208., current region memstore size 387.2m
2014-07-01 15:35:23,820 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1404253868846/slave1%2C60020%2C1404253868846.1404254121595 with entries=106, filesize=70.3m; new WAL /hbase/WALs/slave1,60020,1404253868846/slave1%2C60020%2C1404253868846.1404254123706
2014-07-01 15:35:23,992 DEBUG [RpcServer.handler=23,port=60020] regionserver.HRegion: Flush requested on usertable,user7,1404228265526.667c1f2c34fe37d48ba25a789d8aba98.
2014-07-01 15:35:24,242 DEBUG [MemStoreFlusher.0] util.FSUtils: DFS Client does not support most favored nodes create; using default create
2014-07-01 15:35:24,606 DEBUG [MemStoreFlusher.1] util.FSUtils: DFS Client does not support most favored nodes create; using default create
2014-07-01 15:35:27,148 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-01 15:35:27,180 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 8257 synced till here 8255
2014-07-01 15:35:27,477 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1404253868846/slave1%2C60020%2C1404253868846.1404254123706 with entries=107, filesize=62.8m; new WAL /hbase/WALs/slave1,60020,1404253868846/slave1%2C60020%2C1404253868846.1404254127148
2014-07-01 15:35:29,775 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-01 15:35:29,807 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 8360 synced till here 8342
2014-07-01 15:35:30,040 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1404253868846/slave1%2C60020%2C1404253868846.1404254127148 with entries=103, filesize=78.7m; new WAL /hbase/WALs/slave1,60020,1404253868846/slave1%2C60020%2C1404253868846.1404254129776
2014-07-01 15:35:31,124 DEBUG [RpcServer.handler=21,port=60020] regionserver.HRegion: Flush requested on usertable,user9,1404228265526.bb79ae773f71289e0745068f68a0abd9.
2014-07-01 15:35:31,859 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-01 15:35:31,883 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 8478 synced till here 8471
2014-07-01 15:35:31,951 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1404253868846/slave1%2C60020%2C1404253868846.1404254129776 with entries=118, filesize=64.4m; new WAL /hbase/WALs/slave1,60020,1404253868846/slave1%2C60020%2C1404253868846.1404254131859
2014-07-01 15:35:33,975 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-01 15:35:33,996 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 8578 synced till here 8577
2014-07-01 15:35:34,019 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1404253868846/slave1%2C60020%2C1404253868846.1404254131859 with entries=100, filesize=62.9m; new WAL /hbase/WALs/slave1,60020,1404253868846/slave1%2C60020%2C1404253868846.1404254133975
2014-07-01 15:35:36,258 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-01 15:35:36,294 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 8674 synced till here 8666
2014-07-01 15:35:36,512 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1404253868846/slave1%2C60020%2C1404253868846.1404254133975 with entries=96, filesize=65.9m; new WAL /hbase/WALs/slave1,60020,1404253868846/slave1%2C60020%2C1404253868846.1404254136259
2014-07-01 15:35:39,646 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-01 15:35:39,845 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 8767 synced till here 8755
2014-07-01 15:35:40,044 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1404253868846/slave1%2C60020%2C1404253868846.1404254136259 with entries=93, filesize=69.0m; new WAL /hbase/WALs/slave1,60020,1404253868846/slave1%2C60020%2C1404253868846.1404254139647
2014-07-01 15:35:43,083 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-01 15:35:43,412 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 8875 synced till here 8856
2014-07-01 15:35:43,918 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1404253868846/slave1%2C60020%2C1404253868846.1404254139647 with entries=108, filesize=82.2m; new WAL /hbase/WALs/slave1,60020,1404253868846/slave1%2C60020%2C1404253868846.1404254143084
2014-07-01 15:35:45,908 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-01 15:35:46,063 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 8973 synced till here 8955
2014-07-01 15:35:46,784 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1404253868846/slave1%2C60020%2C1404253868846.1404254143084 with entries=98, filesize=72.7m; new WAL /hbase/WALs/slave1,60020,1404253868846/slave1%2C60020%2C1404253868846.1404254145909
2014-07-01 15:35:47,929 DEBUG [regionserver60020-smallCompactions-1404253964856] regionserver.HRegionFileSystem: Committing store file hdfs://master:54310/hbase/data/default/usertable/93253e37c284f3174309fede3f32339f/.tmp/0f786ade400f4409a3ed4e0f9a493553 as hdfs://master:54310/hbase/data/default/usertable/93253e37c284f3174309fede3f32339f/family/0f786ade400f4409a3ed4e0f9a493553
2014-07-01 15:35:47,948 INFO  [MemStoreFlusher.1] regionserver.DefaultStoreFlusher: Flushed, sequenceid=1420, memsize=211.9m, hasBloomFilter=true, into tmp file hdfs://master:54310/hbase/data/default/usertable/9cef9e7d34f96f1d9b111be50040a208/.tmp/e12a9b0b94c546e18a602bad19ea7347
2014-07-01 15:35:48,038 DEBUG [MemStoreFlusher.1] regionserver.HRegionFileSystem: Committing store file hdfs://master:54310/hbase/data/default/usertable/9cef9e7d34f96f1d9b111be50040a208/.tmp/e12a9b0b94c546e18a602bad19ea7347 as hdfs://master:54310/hbase/data/default/usertable/9cef9e7d34f96f1d9b111be50040a208/family/e12a9b0b94c546e18a602bad19ea7347
2014-07-01 15:35:48,058 INFO  [MemStoreFlusher.1] regionserver.HStore: Added hdfs://master:54310/hbase/data/default/usertable/9cef9e7d34f96f1d9b111be50040a208/family/e12a9b0b94c546e18a602bad19ea7347, entries=771400, sequenceid=1420, filesize=55.0m
2014-07-01 15:35:48,058 INFO  [MemStoreFlusher.1] regionserver.HRegion: Finished memstore flush of ~387.2m/405994720, currentsize=130.3m/136612800 for region usertable,,1404228265525.9cef9e7d34f96f1d9b111be50040a208. in 24240ms, sequenceid=1420, compaction requested=true
2014-07-01 15:35:48,058 DEBUG [MemStoreFlusher.1] regionserver.CompactSplitThread: Small Compaction requested: system; Because: MemStoreFlusher.1; compaction_queue=(0:11), split_queue=0, merge_queue=0
2014-07-01 15:35:48,059 DEBUG [MemStoreFlusher.1] regionserver.HRegion: Started memstore flush for usertable,user3,1404228265526.93253e37c284f3174309fede3f32339f., current region memstore size 617.3m
2014-07-01 15:35:48,061 DEBUG [regionserver60020-smallCompactions-1404253964856] regionserver.HStore: Removing store files after compaction...
2014-07-01 15:35:48,069 DEBUG [regionserver60020-smallCompactions-1404253964856] backup.HFileArchiver: Finished archiving from class org.apache.hadoop.hbase.backup.HFileArchiver$FileableStoreFile, file:hdfs://master:54310/hbase/data/default/usertable/93253e37c284f3174309fede3f32339f/family/8fe3051444564c758ac8530e9b7243bf, to hdfs://master:54310/hbase/archive/data/default/usertable/93253e37c284f3174309fede3f32339f/family/8fe3051444564c758ac8530e9b7243bf
2014-07-01 15:35:48,072 DEBUG [regionserver60020-smallCompactions-1404253964856] backup.HFileArchiver: Finished archiving from class org.apache.hadoop.hbase.backup.HFileArchiver$FileableStoreFile, file:hdfs://master:54310/hbase/data/default/usertable/93253e37c284f3174309fede3f32339f/family/c983ea1b694d4ca0a196965f3913a212, to hdfs://master:54310/hbase/archive/data/default/usertable/93253e37c284f3174309fede3f32339f/family/c983ea1b694d4ca0a196965f3913a212
2014-07-01 15:35:48,076 DEBUG [regionserver60020-smallCompactions-1404253964856] backup.HFileArchiver: Finished archiving from class org.apache.hadoop.hbase.backup.HFileArchiver$FileableStoreFile, file:hdfs://master:54310/hbase/data/default/usertable/93253e37c284f3174309fede3f32339f/family/d98c2ed64b8e4dbd915634a5b65edc0c, to hdfs://master:54310/hbase/archive/data/default/usertable/93253e37c284f3174309fede3f32339f/family/d98c2ed64b8e4dbd915634a5b65edc0c
2014-07-01 15:35:48,081 DEBUG [regionserver60020-smallCompactions-1404253964856] backup.HFileArchiver: Finished archiving from class org.apache.hadoop.hbase.backup.HFileArchiver$FileableStoreFile, file:hdfs://master:54310/hbase/data/default/usertable/93253e37c284f3174309fede3f32339f/family/058c1de8eaf345c9b851a25088ad3042, to hdfs://master:54310/hbase/archive/data/default/usertable/93253e37c284f3174309fede3f32339f/family/058c1de8eaf345c9b851a25088ad3042
2014-07-01 15:35:48,082 INFO  [regionserver60020-smallCompactions-1404253964856] regionserver.HStore: Completed major compaction of 4 file(s) in family of usertable,user3,1404228265526.93253e37c284f3174309fede3f32339f. into 0f786ade400f4409a3ed4e0f9a493553(size=151.8m), total size for store is 234.5m. This selection was in queue for 0sec, and took 1mins, 14sec to execute.
2014-07-01 15:35:48,082 INFO  [regionserver60020-smallCompactions-1404253964856] regionserver.CompactSplitThread: Completed compaction: Request = regionName=usertable,user3,1404228265526.93253e37c284f3174309fede3f32339f., storeName=family, fileCount=4, fileSize=194.2m, priority=16, time=74184627561441; duration=1mins, 14sec
2014-07-01 15:35:48,082 DEBUG [regionserver60020-smallCompactions-1404253964856] regionserver.CompactSplitThread: CompactSplitThread Status: compaction_queue=(0:11), split_queue=0, merge_queue=0
2014-07-01 15:35:48,082 DEBUG [regionserver60020-smallCompactions-1404253964856] compactions.RatioBasedCompactionPolicy: Selecting compaction from 5 store files, 0 compacting, 5 eligible, 20 blocking
2014-07-01 15:35:48,083 DEBUG [regionserver60020-smallCompactions-1404253964856] compactions.ExploringCompactionPolicy: Exploring compaction algorithm has selected 5 files of size 269093251 starting at candidate #0 after considering 6 permutations with 6 in ratio
2014-07-01 15:35:48,083 DEBUG [regionserver60020-smallCompactions-1404253964856] regionserver.HStore: 9cef9e7d34f96f1d9b111be50040a208 - family: Initiating major compaction
2014-07-01 15:35:48,083 INFO  [regionserver60020-smallCompactions-1404253964856] regionserver.HRegion: Starting compaction on family in region usertable,,1404228265525.9cef9e7d34f96f1d9b111be50040a208.
2014-07-01 15:35:48,083 INFO  [regionserver60020-smallCompactions-1404253964856] regionserver.HStore: Starting compaction of 5 file(s) in family of usertable,,1404228265525.9cef9e7d34f96f1d9b111be50040a208. into tmpdir=hdfs://master:54310/hbase/data/default/usertable/9cef9e7d34f96f1d9b111be50040a208/.tmp, totalSize=256.6m
2014-07-01 15:35:48,084 DEBUG [regionserver60020-smallCompactions-1404253964856] compactions.Compactor: Compacting hdfs://master:54310/hbase/data/default/usertable/9cef9e7d34f96f1d9b111be50040a208/family/1a3ddd0bf6eb49c094e7bfe4686943b0, keycount=67953, bloomtype=ROW, size=48.4m, encoding=NONE, seqNum=121, earliestPutTs=1404228281510
2014-07-01 15:35:48,084 DEBUG [regionserver60020-smallCompactions-1404253964856] compactions.Compactor: Compacting hdfs://master:54310/hbase/data/default/usertable/9cef9e7d34f96f1d9b111be50040a208/family/d503bdfe9b5742faa7fd875ddeb4e5ea, keycount=64967, bloomtype=ROW, size=46.3m, encoding=NONE, seqNum=380, earliestPutTs=1404253897956
2014-07-01 15:35:48,084 DEBUG [regionserver60020-smallCompactions-1404253964856] compactions.Compactor: Compacting hdfs://master:54310/hbase/data/default/usertable/9cef9e7d34f96f1d9b111be50040a208/family/b6459b2efea04af1894b4d4fb1f62251, keycount=53707, bloomtype=ROW, size=38.3m, encoding=NONE, seqNum=653, earliestPutTs=1404253956409
2014-07-01 15:35:48,084 DEBUG [regionserver60020-smallCompactions-1404253964856] compactions.Compactor: Compacting hdfs://master:54310/hbase/data/default/usertable/9cef9e7d34f96f1d9b111be50040a208/family/242fc66525a74fc5bc085539024c616a, keycount=96270, bloomtype=ROW, size=68.6m, encoding=NONE, seqNum=1021, earliestPutTs=1404253988153
2014-07-01 15:35:48,084 DEBUG [regionserver60020-smallCompactions-1404253964856] compactions.Compactor: Compacting hdfs://master:54310/hbase/data/default/usertable/9cef9e7d34f96f1d9b111be50040a208/family/e12a9b0b94c546e18a602bad19ea7347, keycount=77140, bloomtype=ROW, size=55.0m, encoding=NONE, seqNum=1420, earliestPutTs=1404254047512
2014-07-01 15:35:48,278 DEBUG [regionserver60020-smallCompactions-1404253964856] util.FSUtils: DFS Client does not support most favored nodes create; using default create
2014-07-01 15:35:48,319 INFO  [MemStoreFlusher.0] regionserver.DefaultStoreFlusher: Flushed, sequenceid=1839, memsize=139.2m, hasBloomFilter=true, into tmp file hdfs://master:54310/hbase/data/default/usertable/122e92eabfab38cb15a9965b21db25c5/.tmp/ad6abf67212d44faa242d52d31aecc9c
2014-07-01 15:35:48,451 DEBUG [MemStoreFlusher.0] regionserver.HRegionFileSystem: Committing store file hdfs://master:54310/hbase/data/default/usertable/122e92eabfab38cb15a9965b21db25c5/.tmp/ad6abf67212d44faa242d52d31aecc9c as hdfs://master:54310/hbase/data/default/usertable/122e92eabfab38cb15a9965b21db25c5/family/ad6abf67212d44faa242d52d31aecc9c
2014-07-01 15:35:48,468 INFO  [MemStoreFlusher.0] regionserver.HStore: Added hdfs://master:54310/hbase/data/default/usertable/122e92eabfab38cb15a9965b21db25c5/family/ad6abf67212d44faa242d52d31aecc9c, entries=506710, sequenceid=1839, filesize=36.1m
2014-07-01 15:35:48,469 INFO  [MemStoreFlusher.0] regionserver.HRegion: Finished memstore flush of ~592.7m/621477600, currentsize=322.0m/337655440 for region usertable,user5,1404228265526.122e92eabfab38cb15a9965b21db25c5. in 26742ms, sequenceid=1839, compaction requested=true
2014-07-01 15:35:48,469 DEBUG [MemStoreFlusher.0] regionserver.CompactSplitThread: Small Compaction requested: system; Because: MemStoreFlusher.0; compaction_queue=(0:11), split_queue=0, merge_queue=0
2014-07-01 15:35:48,469 DEBUG [MemStoreFlusher.0] regionserver.HRegion: Started memstore flush for usertable,user7,1404228265526.667c1f2c34fe37d48ba25a789d8aba98., current region memstore size 670.3m
2014-07-01 15:35:48,470 DEBUG [RpcServer.handler=38,port=60020] regionserver.HRegion: Flush requested on usertable,user5,1404228265526.122e92eabfab38cb15a9965b21db25c5.
2014-07-01 15:35:48,874 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-01 15:35:48,897 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 9070 synced till here 9061
2014-07-01 15:35:49,150 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1404253868846/slave1%2C60020%2C1404253868846.1404254145909 with entries=97, filesize=68.8m; new WAL /hbase/WALs/slave1,60020,1404253868846/slave1%2C60020%2C1404253868846.1404254148874
2014-07-01 15:35:49,150 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1404253868846/slave1%2C60020%2C1404253868846.1404254034564
2014-07-01 15:35:49,150 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1404253868846/slave1%2C60020%2C1404253868846.1404254037720
2014-07-01 15:35:49,150 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1404253868846/slave1%2C60020%2C1404253868846.1404254041750
2014-07-01 15:35:49,151 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1404253868846/slave1%2C60020%2C1404253868846.1404254045279
2014-07-01 15:35:49,151 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1404253868846/slave1%2C60020%2C1404253868846.1404254048419
2014-07-01 15:35:49,151 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1404253868846/slave1%2C60020%2C1404253868846.1404254053031
2014-07-01 15:35:49,151 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1404253868846/slave1%2C60020%2C1404253868846.1404254056870
2014-07-01 15:35:50,997 DEBUG [MemStoreFlusher.1] util.FSUtils: DFS Client does not support most favored nodes create; using default create
2014-07-01 15:35:51,739 DEBUG [MemStoreFlusher.0] util.FSUtils: DFS Client does not support most favored nodes create; using default create
2014-07-01 15:35:51,932 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-01 15:35:52,091 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 9156 synced till here 9155
2014-07-01 15:35:52,109 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1404253868846/slave1%2C60020%2C1404253868846.1404254148874 with entries=86, filesize=62.9m; new WAL /hbase/WALs/slave1,60020,1404253868846/slave1%2C60020%2C1404253868846.1404254151933
2014-07-01 15:35:54,905 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-01 15:35:55,185 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 9253 synced till here 9242
2014-07-01 15:35:55,969 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1404253868846/slave1%2C60020%2C1404253868846.1404254151933 with entries=97, filesize=71.1m; new WAL /hbase/WALs/slave1,60020,1404253868846/slave1%2C60020%2C1404253868846.1404254154906
2014-07-01 15:36:23,323 DEBUG [LruStats #0] hfile.LruBlockCache: Total=4.28 MB, free=3.95 GB, max=3.96 GB, blocks=1, accesses=23363, hits=3181, hitRatio=13.61%, , cachingAccesses=3183, cachingHits=3180, cachingHitsRatio=99.90%, evictions=0, evicted=2, evictedPerRun=Infinity
2014-07-01 15:36:23,323 WARN  [regionserver60020.periodicFlusher] util.Sleeper: We slept 32125ms instead of 10000ms, this is likely due to a long garbage collecting pause and it's usually bad, see http://hbase.apache.org/book.html#trouble.rs.runtime.zkexpired
2014-07-01 15:36:23,323 WARN  [regionserver60020] util.Sleeper: We slept 25606ms instead of 3000ms, this is likely due to a long garbage collecting pause and it's usually bad, see http://hbase.apache.org/book.html#trouble.rs.runtime.zkexpired
2014-07-01 15:36:23,323 WARN  [regionserver60020.compactionChecker] util.Sleeper: We slept 32125ms instead of 10000ms, this is likely due to a long garbage collecting pause and it's usually bad, see http://hbase.apache.org/book.html#trouble.rs.runtime.zkexpired
2014-07-01 15:36:23,324 WARN  [JvmPauseMonitor] util.JvmPauseMonitor: Detected pause in JVM or host machine (eg GC): pause of approximately 22713ms
GC pool 'ParNew' had collection(s): count=1 time=1ms
GC pool 'ConcurrentMarkSweep' had collection(s): count=1 time=23147ms
2014-07-01 15:36:23,381 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-01 15:36:23,427 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 9360 synced till here 9339
2014-07-01 15:36:23,587 WARN  [RpcServer.handler=3,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":28858,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.58:57483","starttimems":1404254154535,"queuetimems":0,"class":"HRegionServer","responsesize":12768,"method":"Multi"}
2014-07-01 15:36:23,588 WARN  [RpcServer.handler=3,port=60020] ipc.RpcServer: RpcServer.respondercallId: 6869 service: ClientService methodName: Multi size: 2.2m connection: 9.1.143.58:57483: output error
2014-07-01 15:36:23,793 WARN  [RpcServer.handler=3,port=60020] ipc.RpcServer: RpcServer.handler=3,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-07-01 15:36:23,793 WARN  [RpcServer.handler=30,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":28887,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.58:57483","starttimems":1404254154573,"queuetimems":1,"class":"HRegionServer","responsesize":15027,"method":"Multi"}
2014-07-01 15:36:23,794 WARN  [RpcServer.handler=30,port=60020] ipc.RpcServer: RpcServer.respondercallId: 6866 service: ClientService methodName: Multi size: 2.6m connection: 9.1.143.58:57483: output error
2014-07-01 15:36:23,794 WARN  [RpcServer.handler=30,port=60020] ipc.RpcServer: RpcServer.handler=30,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-07-01 15:36:24,188 WARN  [RpcServer.handler=45,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":24321,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.58:57483","starttimems":1404254159073,"queuetimems":119,"class":"HRegionServer","responsesize":4138,"method":"Multi"}
2014-07-01 15:36:24,189 WARN  [RpcServer.handler=45,port=60020] ipc.RpcServer: RpcServer.respondercallId: 6928 service: ClientService methodName: Multi size: 760.6k connection: 9.1.143.58:57483: output error
2014-07-01 15:36:24,189 WARN  [RpcServer.handler=9,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":28799,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.58:57483","starttimems":1404254154595,"queuetimems":1,"class":"HRegionServer","responsesize":15058,"method":"Multi"}
2014-07-01 15:36:24,189 WARN  [RpcServer.handler=8,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":24447,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.58:57483","starttimems":1404254158947,"queuetimems":1,"class":"HRegionServer","responsesize":4019,"method":"Multi"}
2014-07-01 15:36:24,189 WARN  [RpcServer.handler=9,port=60020] ipc.RpcServer: RpcServer.respondercallId: 6864 service: ClientService methodName: Multi size: 2.6m connection: 9.1.143.58:57483: output error
2014-07-01 15:36:24,189 WARN  [RpcServer.handler=9,port=60020] ipc.RpcServer: RpcServer.handler=9,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-07-01 15:36:24,189 WARN  [RpcServer.handler=16,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":28578,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.58:57483","starttimems":1404254154882,"queuetimems":0,"class":"HRegionServer","responsesize":14895,"method":"Multi"}
2014-07-01 15:36:24,190 WARN  [RpcServer.handler=8,port=60020] ipc.RpcServer: RpcServer.respondercallId: 6929 service: ClientService methodName: Multi size: 739.3k connection: 9.1.143.58:57483: output error
2014-07-01 15:36:24,190 WARN  [RpcServer.handler=8,port=60020] ipc.RpcServer: RpcServer.handler=8,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-07-01 15:36:24,190 WARN  [RpcServer.handler=16,port=60020] ipc.RpcServer: RpcServer.respondercallId: 6859 service: ClientService methodName: Multi size: 2.6m connection: 9.1.143.58:57483: output error
2014-07-01 15:36:24,190 WARN  [RpcServer.handler=16,port=60020] ipc.RpcServer: RpcServer.handler=16,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-07-01 15:36:24,189 WARN  [RpcServer.handler=45,port=60020] ipc.RpcServer: RpcServer.handler=45,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-07-01 15:36:25,163 WARN  [RpcServer.handler=23,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":25575,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.58:57483","starttimems":1404254159587,"queuetimems":1,"class":"HRegionServer","responsesize":3998,"method":"Multi"}
2014-07-01 15:36:25,163 WARN  [RpcServer.handler=23,port=60020] ipc.RpcServer: RpcServer.respondercallId: 6922 service: ClientService methodName: Multi size: 735.6k connection: 9.1.143.58:57483: output error
2014-07-01 15:36:25,163 WARN  [RpcServer.handler=23,port=60020] ipc.RpcServer: RpcServer.handler=23,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-07-01 15:36:25,163 WARN  [RpcServer.handler=31,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":25567,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.58:57483","starttimems":1404254159595,"queuetimems":0,"class":"HRegionServer","responsesize":3865,"method":"Multi"}
2014-07-01 15:36:25,163 WARN  [RpcServer.handler=31,port=60020] ipc.RpcServer: RpcServer.respondercallId: 6921 service: ClientService methodName: Multi size: 711.8k connection: 9.1.143.58:57483: output error
2014-07-01 15:36:25,163 WARN  [RpcServer.handler=31,port=60020] ipc.RpcServer: RpcServer.handler=31,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-07-01 15:36:25,386 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1404253868846/slave1%2C60020%2C1404253868846.1404254154906 with entries=107, filesize=92.6m; new WAL /hbase/WALs/slave1,60020,1404253868846/slave1%2C60020%2C1404253868846.1404254183381
2014-07-01 15:36:25,753 WARN  [RpcServer.handler=26,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":25611,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.58:57483","starttimems":1404254160141,"queuetimems":0,"class":"HRegionServer","responsesize":10781,"method":"Multi"}
2014-07-01 15:36:25,753 WARN  [RpcServer.handler=26,port=60020] ipc.RpcServer: RpcServer.respondercallId: 6949 service: ClientService methodName: Multi size: 1.9m connection: 9.1.143.58:57483: output error
2014-07-01 15:36:25,754 WARN  [RpcServer.handler=26,port=60020] ipc.RpcServer: RpcServer.handler=26,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-07-01 15:36:26,282 WARN  [RpcServer.handler=32,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":31368,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.58:57483","starttimems":1404254154912,"queuetimems":1,"class":"HRegionServer","responsesize":14628,"method":"Multi"}
2014-07-01 15:36:26,282 WARN  [RpcServer.handler=12,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":31091,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.58:57483","starttimems":1404254155190,"queuetimems":0,"class":"HRegionServer","responsesize":14605,"method":"Multi"}
2014-07-01 15:36:26,284 WARN  [RpcServer.handler=32,port=60020] ipc.RpcServer: RpcServer.respondercallId: 6858 service: ClientService methodName: Multi size: 2.6m connection: 9.1.143.58:57483: output error
2014-07-01 15:36:26,284 WARN  [RpcServer.handler=32,port=60020] ipc.RpcServer: RpcServer.handler=32,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-07-01 15:36:26,284 WARN  [RpcServer.handler=12,port=60020] ipc.RpcServer: RpcServer.respondercallId: 6854 service: ClientService methodName: Multi size: 2.6m connection: 9.1.143.58:57483: output error
2014-07-01 15:36:26,284 WARN  [RpcServer.handler=12,port=60020] ipc.RpcServer: RpcServer.handler=12,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-07-01 15:36:26,296 WARN  [RpcServer.handler=5,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":30711,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.58:57483","starttimems":1404254155585,"queuetimems":0,"class":"HRegionServer","responsesize":14975,"method":"Multi"}
2014-07-01 15:36:26,297 WARN  [RpcServer.handler=5,port=60020] ipc.RpcServer: RpcServer.respondercallId: 6885 service: ClientService methodName: Multi size: 2.6m connection: 9.1.143.58:57483: output error
2014-07-01 15:36:26,297 WARN  [RpcServer.handler=5,port=60020] ipc.RpcServer: RpcServer.handler=5,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-07-01 15:36:26,305 WARN  [RpcServer.handler=6,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":30291,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.58:57483","starttimems":1404254156007,"queuetimems":0,"class":"HRegionServer","responsesize":11276,"method":"Multi"}
2014-07-01 15:36:26,306 WARN  [RpcServer.handler=11,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":30855,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.58:57483","starttimems":1404254155450,"queuetimems":1,"class":"HRegionServer","responsesize":14887,"method":"Multi"}
2014-07-01 15:36:26,307 WARN  [RpcServer.handler=6,port=60020] ipc.RpcServer: RpcServer.respondercallId: 6880 service: ClientService methodName: Multi size: 2.0m connection: 9.1.143.58:57483: output error
2014-07-01 15:36:26,307 WARN  [RpcServer.handler=6,port=60020] ipc.RpcServer: RpcServer.handler=6,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-07-01 15:36:26,307 WARN  [RpcServer.handler=11,port=60020] ipc.RpcServer: RpcServer.respondercallId: 6892 service: ClientService methodName: Multi size: 2.6m connection: 9.1.143.58:57483: output error
2014-07-01 15:36:26,307 WARN  [RpcServer.handler=11,port=60020] ipc.RpcServer: RpcServer.handler=11,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-07-01 15:36:26,312 WARN  [RpcServer.handler=17,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":30831,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.58:57483","starttimems":1404254155481,"queuetimems":0,"class":"HRegionServer","responsesize":12671,"method":"Multi"}
2014-07-01 15:36:26,313 WARN  [RpcServer.handler=17,port=60020] ipc.RpcServer: RpcServer.respondercallId: 6889 service: ClientService methodName: Multi size: 2.2m connection: 9.1.143.58:57483: output error
2014-07-01 15:36:26,313 WARN  [RpcServer.handler=17,port=60020] ipc.RpcServer: RpcServer.handler=17,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-07-01 15:36:26,318 WARN  [RpcServer.handler=42,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":29735,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.58:57483","starttimems":1404254156580,"queuetimems":0,"class":"HRegionServer","responsesize":14654,"method":"Multi"}
2014-07-01 15:36:26,318 WARN  [RpcServer.handler=10,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":30327,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.58:57483","starttimems":1404254155987,"queuetimems":1,"class":"HRegionServer","responsesize":14865,"method":"Multi"}
2014-07-01 15:36:26,318 WARN  [RpcServer.handler=42,port=60020] ipc.RpcServer: RpcServer.respondercallId: 6876 service: ClientService methodName: Multi size: 2.6m connection: 9.1.143.58:57483: output error
2014-07-01 15:36:26,318 WARN  [RpcServer.handler=42,port=60020] ipc.RpcServer: RpcServer.handler=42,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-07-01 15:36:26,318 WARN  [RpcServer.handler=10,port=60020] ipc.RpcServer: RpcServer.respondercallId: 6882 service: ClientService methodName: Multi size: 2.6m connection: 9.1.143.58:57483: output error
2014-07-01 15:36:26,319 WARN  [RpcServer.handler=10,port=60020] ipc.RpcServer: RpcServer.handler=10,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-07-01 15:36:29,366 INFO  [JvmPauseMonitor] util.JvmPauseMonitor: Detected pause in JVM or host machine (eg GC): pause of approximately 1483ms
GC pool 'ParNew' had collection(s): count=1 time=1667ms
2014-07-01 15:36:29,544 WARN  [RpcServer.handler=24,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":34319,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.58:57483","starttimems":1404254155224,"queuetimems":0,"class":"HRegionServer","responsesize":14976,"method":"Multi"}
2014-07-01 15:36:29,544 WARN  [RpcServer.handler=24,port=60020] ipc.RpcServer: RpcServer.respondercallId: 6853 service: ClientService methodName: Multi size: 2.6m connection: 9.1.143.58:57483: output error
2014-07-01 15:36:29,544 WARN  [RpcServer.handler=24,port=60020] ipc.RpcServer: RpcServer.handler=24,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-07-01 15:36:29,547 WARN  [RpcServer.handler=13,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":32302,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.58:57483","starttimems":1404254157245,"queuetimems":0,"class":"HRegionServer","responsesize":12939,"method":"Multi"}
2014-07-01 15:36:29,548 WARN  [RpcServer.handler=13,port=60020] ipc.RpcServer: RpcServer.respondercallId: 6904 service: ClientService methodName: Multi size: 2.3m connection: 9.1.143.58:57483: output error
2014-07-01 15:36:29,548 WARN  [RpcServer.handler=13,port=60020] ipc.RpcServer: RpcServer.handler=13,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-07-01 15:36:29,551 WARN  [RpcServer.handler=38,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":32992,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.58:57483","starttimems":1404254156559,"queuetimems":0,"class":"HRegionServer","responsesize":14354,"method":"Multi"}
2014-07-01 15:36:29,552 WARN  [RpcServer.handler=38,port=60020] ipc.RpcServer: RpcServer.respondercallId: 6878 service: ClientService methodName: Multi size: 2.5m connection: 9.1.143.58:57483: output error
2014-07-01 15:36:29,552 WARN  [RpcServer.handler=38,port=60020] ipc.RpcServer: RpcServer.handler=38,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-07-01 15:36:29,553 WARN  [RpcServer.handler=35,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":32928,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.58:57483","starttimems":1404254156624,"queuetimems":1,"class":"HRegionServer","responsesize":14777,"method":"Multi"}
2014-07-01 15:36:29,553 WARN  [RpcServer.handler=35,port=60020] ipc.RpcServer: RpcServer.respondercallId: 6905 service: ClientService methodName: Multi size: 2.6m connection: 9.1.143.58:57483: output error
2014-07-01 15:36:29,553 WARN  [RpcServer.handler=35,port=60020] ipc.RpcServer: RpcServer.handler=35,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-07-01 15:36:30,035 WARN  [RpcServer.handler=4,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":31894,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.58:57483","starttimems":1404254158140,"queuetimems":0,"class":"HRegionServer","responsesize":14697,"method":"Multi"}
2014-07-01 15:36:30,035 WARN  [RpcServer.handler=44,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":32290,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.58:57483","starttimems":1404254157744,"queuetimems":1,"class":"HRegionServer","responsesize":14763,"method":"Multi"}
2014-07-01 15:36:30,035 WARN  [RpcServer.handler=7,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":32312,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.58:57483","starttimems":1404254157722,"queuetimems":0,"class":"HRegionServer","responsesize":12472,"method":"Multi"}
2014-07-01 15:36:30,035 WARN  [RpcServer.handler=15,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":32756,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.58:57483","starttimems":1404254157278,"queuetimems":0,"class":"HRegionServer","responsesize":14895,"method":"Multi"}
2014-07-01 15:36:30,036 WARN  [RpcServer.handler=33,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":32322,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.58:57483","starttimems":1404254157713,"queuetimems":12,"class":"HRegionServer","responsesize":12988,"method":"Multi"}
2014-07-01 15:36:30,035 WARN  [RpcServer.handler=4,port=60020] ipc.RpcServer: RpcServer.respondercallId: 6907 service: ClientService methodName: Multi size: 2.6m connection: 9.1.143.58:57483: output error
2014-07-01 15:36:30,036 WARN  [RpcServer.handler=4,port=60020] ipc.RpcServer: RpcServer.handler=4,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-07-01 15:36:30,036 WARN  [RpcServer.handler=33,port=60020] ipc.RpcServer: RpcServer.respondercallId: 6913 service: ClientService methodName: Multi size: 2.3m connection: 9.1.143.58:57483: output error
2014-07-01 15:36:30,036 WARN  [RpcServer.handler=33,port=60020] ipc.RpcServer: RpcServer.handler=33,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-07-01 15:36:30,037 WARN  [RpcServer.handler=15,port=60020] ipc.RpcServer: RpcServer.respondercallId: 6903 service: ClientService methodName: Multi size: 2.6m connection: 9.1.143.58:57483: output error
2014-07-01 15:36:30,037 WARN  [RpcServer.handler=15,port=60020] ipc.RpcServer: RpcServer.handler=15,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-07-01 15:36:30,044 WARN  [RpcServer.handler=7,port=60020] ipc.RpcServer: RpcServer.respondercallId: 6911 service: ClientService methodName: Multi size: 2.2m connection: 9.1.143.58:57483: output error
2014-07-01 15:36:30,044 WARN  [RpcServer.handler=7,port=60020] ipc.RpcServer: RpcServer.handler=7,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-07-01 15:36:30,056 WARN  [RpcServer.handler=44,port=60020] ipc.RpcServer: RpcServer.respondercallId: 6909 service: ClientService methodName: Multi size: 2.6m connection: 9.1.143.58:57483: output error
2014-07-01 15:36:30,056 WARN  [RpcServer.handler=44,port=60020] ipc.RpcServer: RpcServer.handler=44,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-07-01 15:36:30,662 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-01 15:36:30,670 WARN  [RpcServer.handler=46,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":32331,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.58:57483","starttimems":1404254158338,"queuetimems":1,"class":"HRegionServer","responsesize":14865,"method":"Multi"}
2014-07-01 15:36:30,671 WARN  [RpcServer.handler=40,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":32049,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.58:57483","starttimems":1404254158622,"queuetimems":1,"class":"HRegionServer","responsesize":15058,"method":"Multi"}
2014-07-01 15:36:30,670 WARN  [RpcServer.handler=20,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":32315,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.58:57483","starttimems":1404254158355,"queuetimems":0,"class":"HRegionServer","responsesize":14975,"method":"Multi"}
2014-07-01 15:36:30,671 WARN  [RpcServer.handler=28,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":31548,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.58:57483","starttimems":1404254159123,"queuetimems":1,"class":"HRegionServer","responsesize":15027,"method":"Multi"}
2014-07-01 15:36:30,671 WARN  [RpcServer.handler=22,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":31758,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.58:57483","starttimems":1404254158913,"queuetimems":1,"class":"HRegionServer","responsesize":14654,"method":"Multi"}
2014-07-01 15:36:30,672 WARN  [RpcServer.handler=46,port=60020] ipc.RpcServer: RpcServer.respondercallId: 6937 service: ClientService methodName: Multi size: 2.6m connection: 9.1.143.58:57483: output error
2014-07-01 15:36:30,672 WARN  [RpcServer.handler=46,port=60020] ipc.RpcServer: RpcServer.handler=46,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-07-01 15:36:30,673 WARN  [RpcServer.handler=28,port=60020] ipc.RpcServer: RpcServer.respondercallId: 6926 service: ClientService methodName: Multi size: 2.6m connection: 9.1.143.58:57483: output error
2014-07-01 15:36:30,673 WARN  [RpcServer.handler=28,port=60020] ipc.RpcServer: RpcServer.handler=28,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-07-01 15:36:30,673 WARN  [RpcServer.handler=19,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":32297,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.58:57483","starttimems":1404254158375,"queuetimems":1,"class":"HRegionServer","responsesize":14930,"method":"Multi"}
2014-07-01 15:36:30,673 WARN  [RpcServer.handler=39,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":31781,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.58:57483","starttimems":1404254158892,"queuetimems":0,"class":"HRegionServer","responsesize":14887,"method":"Multi"}
2014-07-01 15:36:30,673 WARN  [RpcServer.handler=22,port=60020] ipc.RpcServer: RpcServer.respondercallId: 6931 service: ClientService methodName: Multi size: 2.6m connection: 9.1.143.58:57483: output error
2014-07-01 15:36:30,675 WARN  [RpcServer.handler=22,port=60020] ipc.RpcServer: RpcServer.handler=22,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-07-01 15:36:30,675 WARN  [RpcServer.handler=20,port=60020] ipc.RpcServer: RpcServer.respondercallId: 6936 service: ClientService methodName: Multi size: 2.6m connection: 9.1.143.58:57483: output error
2014-07-01 15:36:30,675 WARN  [RpcServer.handler=20,port=60020] ipc.RpcServer: RpcServer.handler=20,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-07-01 15:36:30,676 WARN  [RpcServer.handler=40,port=60020] ipc.RpcServer: RpcServer.respondercallId: 6934 service: ClientService methodName: Multi size: 2.6m connection: 9.1.143.58:57483: output error
2014-07-01 15:36:30,676 WARN  [RpcServer.handler=40,port=60020] ipc.RpcServer: RpcServer.handler=40,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-07-01 15:36:30,676 WARN  [RpcServer.handler=19,port=60020] ipc.RpcServer: RpcServer.respondercallId: 6935 service: ClientService methodName: Multi size: 2.6m connection: 9.1.143.58:57483: output error
2014-07-01 15:36:30,676 WARN  [RpcServer.handler=19,port=60020] ipc.RpcServer: RpcServer.handler=19,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-07-01 15:36:30,676 WARN  [RpcServer.handler=39,port=60020] ipc.RpcServer: RpcServer.respondercallId: 6932 service: ClientService methodName: Multi size: 2.6m connection: 9.1.143.58:57483: output error
2014-07-01 15:36:30,676 WARN  [RpcServer.handler=39,port=60020] ipc.RpcServer: RpcServer.handler=39,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-07-01 15:36:30,751 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 9481 synced till here 9460
2014-07-01 15:36:30,949 WARN  [RpcServer.handler=41,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":31369,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.58:57483","starttimems":1404254159579,"queuetimems":0,"class":"HRegionServer","responsesize":11276,"method":"Multi"}
2014-07-01 15:36:30,949 WARN  [RpcServer.handler=41,port=60020] ipc.RpcServer: RpcServer.respondercallId: 6923 service: ClientService methodName: Multi size: 2.0m connection: 9.1.143.58:57483: output error
2014-07-01 15:36:30,949 WARN  [RpcServer.handler=41,port=60020] ipc.RpcServer: RpcServer.handler=41,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-07-01 15:36:30,951 WARN  [RpcServer.handler=34,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":31647,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.58:57483","starttimems":1404254159303,"queuetimems":0,"class":"HRegionServer","responsesize":12671,"method":"Multi"}
2014-07-01 15:36:30,951 WARN  [RpcServer.handler=37,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":32009,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.58:57483","starttimems":1404254158941,"queuetimems":1,"class":"HRegionServer","responsesize":14354,"method":"Multi"}
2014-07-01 15:36:30,951 WARN  [RpcServer.handler=34,port=60020] ipc.RpcServer: RpcServer.respondercallId: 6925 service: ClientService methodName: Multi size: 2.2m connection: 9.1.143.58:57483: output error
2014-07-01 15:36:30,951 WARN  [RpcServer.handler=34,port=60020] ipc.RpcServer: RpcServer.handler=34,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-07-01 15:36:30,952 WARN  [RpcServer.handler=49,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":31392,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.58:57483","starttimems":1404254159559,"queuetimems":0,"class":"HRegionServer","responsesize":12768,"method":"Multi"}
2014-07-01 15:36:30,952 WARN  [RpcServer.handler=49,port=60020] ipc.RpcServer: RpcServer.respondercallId: 6924 service: ClientService methodName: Multi size: 2.2m connection: 9.1.143.58:57483: output error
2014-07-01 15:36:30,952 WARN  [RpcServer.handler=49,port=60020] ipc.RpcServer: RpcServer.handler=49,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-07-01 15:36:30,953 WARN  [RpcServer.handler=37,port=60020] ipc.RpcServer: RpcServer.respondercallId: 6930 service: ClientService methodName: Multi size: 2.5m connection: 9.1.143.58:57483: output error
2014-07-01 15:36:30,953 WARN  [RpcServer.handler=37,port=60020] ipc.RpcServer: RpcServer.handler=37,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-07-01 15:36:30,956 WARN  [RpcServer.handler=2,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":31856,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.58:57483","starttimems":1404254159100,"queuetimems":3,"class":"HRegionServer","responsesize":15021,"method":"Multi"}
2014-07-01 15:36:30,957 WARN  [RpcServer.handler=2,port=60020] ipc.RpcServer: RpcServer.respondercallId: 6927 service: ClientService methodName: Multi size: 2.6m connection: 9.1.143.58:57483: output error
2014-07-01 15:36:30,957 WARN  [RpcServer.handler=2,port=60020] ipc.RpcServer: RpcServer.handler=2,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-07-01 15:36:30,983 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1404253868846/slave1%2C60020%2C1404253868846.1404254183381 with entries=121, filesize=94.7m; new WAL /hbase/WALs/slave1,60020,1404253868846/slave1%2C60020%2C1404253868846.1404254190663
2014-07-01 15:36:30,984 INFO  [regionserver60020.logRoller] wal.FSHLog: Too many hlogs: logs=33, maxlogs=32; forcing flush of 1 regions(s): bb79ae773f71289e0745068f68a0abd9
2014-07-01 15:36:31,251 WARN  [RpcServer.handler=21,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":31459,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.58:57483","starttimems":1404254159791,"queuetimems":0,"class":"HRegionServer","responsesize":10909,"method":"Multi"}
2014-07-01 15:36:31,252 WARN  [RpcServer.handler=21,port=60020] ipc.RpcServer: RpcServer.respondercallId: 6915 service: ClientService methodName: Multi size: 1.9m connection: 9.1.143.58:57483: output error
2014-07-01 15:36:31,252 WARN  [RpcServer.handler=21,port=60020] ipc.RpcServer: RpcServer.handler=21,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-07-01 15:36:31,251 WARN  [RpcServer.handler=18,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":31278,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.58:57483","starttimems":1404254159972,"queuetimems":1,"class":"HRegionServer","responsesize":14628,"method":"Multi"}
2014-07-01 15:36:31,252 WARN  [RpcServer.handler=29,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":31131,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.58:57483","starttimems":1404254160120,"queuetimems":0,"class":"HRegionServer","responsesize":14895,"method":"Multi"}
2014-07-01 15:36:31,252 WARN  [RpcServer.handler=0,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":31493,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.58:57483","starttimems":1404254159758,"queuetimems":0,"class":"HRegionServer","responsesize":13428,"method":"Multi"}
2014-07-01 15:36:31,252 WARN  [RpcServer.handler=18,port=60020] ipc.RpcServer: RpcServer.respondercallId: 6952 service: ClientService methodName: Multi size: 2.6m connection: 9.1.143.58:57483: output error
2014-07-01 15:36:31,252 WARN  [RpcServer.handler=47,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":31253,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.58:57483","starttimems":1404254159998,"queuetimems":1,"class":"HRegionServer","responsesize":12988,"method":"Multi"}
2014-07-01 15:36:31,253 WARN  [RpcServer.handler=0,port=60020] ipc.RpcServer: RpcServer.respondercallId: 6917 service: ClientService methodName: Multi size: 2.3m connection: 9.1.143.58:57483: output error
2014-07-01 15:36:31,253 WARN  [RpcServer.handler=0,port=60020] ipc.RpcServer: RpcServer.handler=0,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-07-01 15:36:31,253 WARN  [RpcServer.handler=29,port=60020] ipc.RpcServer: RpcServer.respondercallId: 6950 service: ClientService methodName: Multi size: 2.6m connection: 9.1.143.58:57483: output error
2014-07-01 15:36:31,253 WARN  [RpcServer.handler=29,port=60020] ipc.RpcServer: RpcServer.handler=29,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-07-01 15:36:31,253 WARN  [RpcServer.handler=47,port=60020] ipc.RpcServer: RpcServer.respondercallId: 6951 service: ClientService methodName: Multi size: 2.3m connection: 9.1.143.58:57483: output error
2014-07-01 15:36:31,253 WARN  [RpcServer.handler=47,port=60020] ipc.RpcServer: RpcServer.handler=47,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-07-01 15:36:31,252 WARN  [RpcServer.handler=18,port=60020] ipc.RpcServer: RpcServer.handler=18,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-07-01 15:36:31,522 WARN  [RpcServer.handler=1,port=60020] ipc.RpcServer: RpcServer.respondercallId: 6945 service: ClientService methodName: Multi size: 2.6m connection: 9.1.143.58:57483: output error
2014-07-01 15:36:31,522 WARN  [RpcServer.handler=1,port=60020] ipc.RpcServer: RpcServer.handler=1,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-07-01 15:36:31,523 WARN  [RpcServer.handler=27,port=60020] ipc.RpcServer: RpcServer.respondercallId: 6944 service: ClientService methodName: Multi size: 2.2m connection: 9.1.143.58:57483: output error
2014-07-01 15:36:31,523 WARN  [RpcServer.handler=27,port=60020] ipc.RpcServer: RpcServer.handler=27,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-07-01 15:36:31,524 WARN  [RpcServer.handler=43,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":31356,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.58:57483","starttimems":1404254160167,"queuetimems":0,"class":"HRegionServer","responsesize":14763,"method":"Multi"}
2014-07-01 15:36:31,524 WARN  [RpcServer.handler=43,port=60020] ipc.RpcServer: RpcServer.respondercallId: 6946 service: ClientService methodName: Multi size: 2.6m connection: 9.1.143.58:57483: output error
2014-07-01 15:36:31,524 WARN  [RpcServer.handler=43,port=60020] ipc.RpcServer: RpcServer.handler=43,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-07-01 15:36:37,695 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-01 15:36:37,775 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 9590 synced till here 9578
2014-07-01 15:36:37,914 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1404253868846/slave1%2C60020%2C1404253868846.1404254190663 with entries=109, filesize=77.1m; new WAL /hbase/WALs/slave1,60020,1404253868846/slave1%2C60020%2C1404253868846.1404254197697
2014-07-01 15:36:37,914 INFO  [regionserver60020.logRoller] wal.FSHLog: Too many hlogs: logs=34, maxlogs=32; forcing flush of 1 regions(s): bb79ae773f71289e0745068f68a0abd9
2014-07-01 15:36:38,550 INFO  [MemStoreFlusher.1] regionserver.DefaultStoreFlusher: Flushed, sequenceid=2034, memsize=226.2m, hasBloomFilter=true, into tmp file hdfs://master:54310/hbase/data/default/usertable/93253e37c284f3174309fede3f32339f/.tmp/5294515babe64f0f9f64bceccfbb78aa
2014-07-01 15:36:38,574 DEBUG [MemStoreFlusher.1] regionserver.HRegionFileSystem: Committing store file hdfs://master:54310/hbase/data/default/usertable/93253e37c284f3174309fede3f32339f/.tmp/5294515babe64f0f9f64bceccfbb78aa as hdfs://master:54310/hbase/data/default/usertable/93253e37c284f3174309fede3f32339f/family/5294515babe64f0f9f64bceccfbb78aa
2014-07-01 15:36:38,587 INFO  [MemStoreFlusher.1] regionserver.HStore: Added hdfs://master:54310/hbase/data/default/usertable/93253e37c284f3174309fede3f32339f/family/5294515babe64f0f9f64bceccfbb78aa, entries=823560, sequenceid=2034, filesize=58.7m
2014-07-01 15:36:38,588 INFO  [MemStoreFlusher.1] regionserver.HRegion: Finished memstore flush of ~617.3m/647252160, currentsize=236.0m/247492000 for region usertable,user3,1404228265526.93253e37c284f3174309fede3f32339f. in 50529ms, sequenceid=2034, compaction requested=true
2014-07-01 15:36:38,588 DEBUG [MemStoreFlusher.1] regionserver.CompactSplitThread: Small Compaction requested: system; Because: MemStoreFlusher.1; compaction_queue=(0:12), split_queue=0, merge_queue=0
2014-07-01 15:36:38,588 DEBUG [MemStoreFlusher.1] regionserver.HRegion: Started memstore flush for usertable,user9,1404228265526.bb79ae773f71289e0745068f68a0abd9., current region memstore size 381.7m
2014-07-01 15:36:38,787 INFO  [MemStoreFlusher.0] regionserver.DefaultStoreFlusher: Flushed, sequenceid=1991, memsize=240.6m, hasBloomFilter=true, into tmp file hdfs://master:54310/hbase/data/default/usertable/667c1f2c34fe37d48ba25a789d8aba98/.tmp/92895d11141e499989cdac0102a760ea
2014-07-01 15:36:39,586 DEBUG [MemStoreFlusher.0] regionserver.HRegionFileSystem: Committing store file hdfs://master:54310/hbase/data/default/usertable/667c1f2c34fe37d48ba25a789d8aba98/.tmp/92895d11141e499989cdac0102a760ea as hdfs://master:54310/hbase/data/default/usertable/667c1f2c34fe37d48ba25a789d8aba98/family/92895d11141e499989cdac0102a760ea
2014-07-01 15:36:39,602 INFO  [MemStoreFlusher.0] regionserver.HStore: Added hdfs://master:54310/hbase/data/default/usertable/667c1f2c34fe37d48ba25a789d8aba98/family/92895d11141e499989cdac0102a760ea, entries=876040, sequenceid=1991, filesize=62.4m
2014-07-01 15:36:39,602 INFO  [MemStoreFlusher.0] regionserver.HRegion: Finished memstore flush of ~671.8m/704417280, currentsize=229.6m/240758960 for region usertable,user7,1404228265526.667c1f2c34fe37d48ba25a789d8aba98. in 51133ms, sequenceid=1991, compaction requested=true
2014-07-01 15:36:39,603 DEBUG [MemStoreFlusher.0] regionserver.CompactSplitThread: Small Compaction requested: system; Because: MemStoreFlusher.0; compaction_queue=(0:13), split_queue=0, merge_queue=0
2014-07-01 15:36:39,603 DEBUG [MemStoreFlusher.0] regionserver.HRegion: Started memstore flush for usertable,user5,1404228265526.122e92eabfab38cb15a9965b21db25c5., current region memstore size 555.5m
2014-07-01 15:36:39,614 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-01 15:36:39,634 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 9682 synced till here 9673
2014-07-01 15:36:39,709 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1404253868846/slave1%2C60020%2C1404253868846.1404254197697 with entries=92, filesize=64.4m; new WAL /hbase/WALs/slave1,60020,1404253868846/slave1%2C60020%2C1404253868846.1404254199614
2014-07-01 15:36:39,787 DEBUG [MemStoreFlusher.1] util.FSUtils: DFS Client does not support most favored nodes create; using default create
2014-07-01 15:36:40,131 DEBUG [RpcServer.handler=19,port=60020] regionserver.HRegion: Flush requested on usertable,user3,1404228265526.93253e37c284f3174309fede3f32339f.
2014-07-01 15:36:40,342 DEBUG [MemStoreFlusher.0] util.FSUtils: DFS Client does not support most favored nodes create; using default create
2014-07-01 15:36:40,382 DEBUG [RpcServer.handler=19,port=60020] regionserver.HRegion: Flush requested on usertable,user7,1404228265526.667c1f2c34fe37d48ba25a789d8aba98.
2014-07-01 15:36:40,637 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-01 15:36:40,654 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 9775 synced till here 9773
2014-07-01 15:36:40,717 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1404253868846/slave1%2C60020%2C1404253868846.1404254199614 with entries=93, filesize=64.8m; new WAL /hbase/WALs/slave1,60020,1404253868846/slave1%2C60020%2C1404253868846.1404254200637
2014-07-01 15:36:42,379 DEBUG [RpcServer.handler=40,port=60020] regionserver.HRegion: Flush requested on usertable,,1404228265525.9cef9e7d34f96f1d9b111be50040a208.
2014-07-01 15:36:43,199 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-01 15:36:43,219 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 9866 synced till here 9856
2014-07-01 15:36:44,208 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1404253868846/slave1%2C60020%2C1404253868846.1404254200637 with entries=91, filesize=66.9m; new WAL /hbase/WALs/slave1,60020,1404253868846/slave1%2C60020%2C1404253868846.1404254203199
2014-07-01 15:36:45,142 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-01 15:36:45,173 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 9966 synced till here 9959
2014-07-01 15:36:45,233 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1404253868846/slave1%2C60020%2C1404253868846.1404254203199 with entries=100, filesize=65.0m; new WAL /hbase/WALs/slave1,60020,1404253868846/slave1%2C60020%2C1404253868846.1404254205143
2014-07-01 15:36:46,962 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-01 15:36:47,024 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 10058 synced till here 10056
2014-07-01 15:36:47,089 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1404253868846/slave1%2C60020%2C1404253868846.1404254205143 with entries=92, filesize=65.8m; new WAL /hbase/WALs/slave1,60020,1404253868846/slave1%2C60020%2C1404253868846.1404254206964
2014-07-01 15:36:49,218 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-01 15:36:49,367 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 10192 synced till here 10168
2014-07-01 15:36:50,498 INFO  [MemStoreFlusher.1] regionserver.DefaultStoreFlusher: Flushed, sequenceid=2117, memsize=131.8m, hasBloomFilter=true, into tmp file hdfs://master:54310/hbase/data/default/usertable/bb79ae773f71289e0745068f68a0abd9/.tmp/964a508e7e9441bba2d103f7676241ca
2014-07-01 15:36:50,510 DEBUG [MemStoreFlusher.1] regionserver.HRegionFileSystem: Committing store file hdfs://master:54310/hbase/data/default/usertable/bb79ae773f71289e0745068f68a0abd9/.tmp/964a508e7e9441bba2d103f7676241ca as hdfs://master:54310/hbase/data/default/usertable/bb79ae773f71289e0745068f68a0abd9/family/964a508e7e9441bba2d103f7676241ca
2014-07-01 15:36:50,524 INFO  [MemStoreFlusher.1] regionserver.HStore: Added hdfs://master:54310/hbase/data/default/usertable/bb79ae773f71289e0745068f68a0abd9/family/964a508e7e9441bba2d103f7676241ca, entries=480080, sequenceid=2117, filesize=34.2m
2014-07-01 15:36:50,525 INFO  [MemStoreFlusher.1] regionserver.HRegion: Finished memstore flush of ~383.4m/402062160, currentsize=50.3m/52742160 for region usertable,user9,1404228265526.bb79ae773f71289e0745068f68a0abd9. in 11936ms, sequenceid=2117, compaction requested=true
2014-07-01 15:36:50,525 DEBUG [MemStoreFlusher.1] regionserver.CompactSplitThread: Small Compaction requested: system; Because: MemStoreFlusher.1; compaction_queue=(0:14), split_queue=0, merge_queue=0
2014-07-01 15:36:50,525 DEBUG [MemStoreFlusher.1] regionserver.HRegion: Started memstore flush for usertable,user3,1404228265526.93253e37c284f3174309fede3f32339f., current region memstore size 408.8m
2014-07-01 15:36:50,534 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1404253868846/slave1%2C60020%2C1404253868846.1404254206964 with entries=134, filesize=91.6m; new WAL /hbase/WALs/slave1,60020,1404253868846/slave1%2C60020%2C1404253868846.1404254209218
2014-07-01 15:36:50,534 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1404253868846/slave1%2C60020%2C1404253868846.1404254058646
2014-07-01 15:36:50,534 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1404253868846/slave1%2C60020%2C1404253868846.1404254067718
2014-07-01 15:36:50,534 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1404253868846/slave1%2C60020%2C1404253868846.1404254070395
2014-07-01 15:36:50,535 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1404253868846/slave1%2C60020%2C1404253868846.1404254072356
2014-07-01 15:36:50,535 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1404253868846/slave1%2C60020%2C1404253868846.1404254074964
2014-07-01 15:36:50,535 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1404253868846/slave1%2C60020%2C1404253868846.1404254076960
2014-07-01 15:36:50,535 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1404253868846/slave1%2C60020%2C1404253868846.1404254079130
2014-07-01 15:36:50,535 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1404253868846/slave1%2C60020%2C1404253868846.1404254082646
2014-07-01 15:36:50,535 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1404253868846/slave1%2C60020%2C1404253868846.1404254084954
2014-07-01 15:36:50,535 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1404253868846/slave1%2C60020%2C1404253868846.1404254087506
2014-07-01 15:36:50,535 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1404253868846/slave1%2C60020%2C1404253868846.1404254093368
2014-07-01 15:36:50,535 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1404253868846/slave1%2C60020%2C1404253868846.1404254095290
2014-07-01 15:36:50,535 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1404253868846/slave1%2C60020%2C1404253868846.1404254097915
2014-07-01 15:36:50,535 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1404253868846/slave1%2C60020%2C1404253868846.1404254100015
2014-07-01 15:36:50,535 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1404253868846/slave1%2C60020%2C1404253868846.1404254102621
2014-07-01 15:36:50,535 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1404253868846/slave1%2C60020%2C1404253868846.1404254108961
2014-07-01 15:36:50,535 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1404253868846/slave1%2C60020%2C1404253868846.1404254113708
2014-07-01 15:36:50,535 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1404253868846/slave1%2C60020%2C1404253868846.1404254115965
2014-07-01 15:36:50,536 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1404253868846/slave1%2C60020%2C1404253868846.1404254118765
2014-07-01 15:36:51,086 DEBUG [MemStoreFlusher.1] util.FSUtils: DFS Client does not support most favored nodes create; using default create
2014-07-01 15:36:51,968 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-01 15:36:51,987 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 10282 synced till here 10281
2014-07-01 15:36:52,026 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1404253868846/slave1%2C60020%2C1404253868846.1404254209218 with entries=90, filesize=62.6m; new WAL /hbase/WALs/slave1,60020,1404253868846/slave1%2C60020%2C1404253868846.1404254211969
2014-07-01 15:36:53,462 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-01 15:36:55,881 INFO  [JvmPauseMonitor] util.JvmPauseMonitor: Detected pause in JVM or host machine (eg GC): pause of approximately 2311ms
GC pool 'ParNew' had collection(s): count=1 time=2384ms
2014-07-01 15:36:55,914 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 10378 synced till here 10375
2014-07-01 15:36:55,935 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1404253868846/slave1%2C60020%2C1404253868846.1404254211969 with entries=96, filesize=71.9m; new WAL /hbase/WALs/slave1,60020,1404253868846/slave1%2C60020%2C1404253868846.1404254213463
2014-07-01 15:36:57,211 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-01 15:36:57,238 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1404253868846/slave1%2C60020%2C1404253868846.1404254213463 with entries=86, filesize=62.3m; new WAL /hbase/WALs/slave1,60020,1404253868846/slave1%2C60020%2C1404253868846.1404254217212
2014-07-01 15:37:00,835 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-01 15:37:00,986 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 10596 synced till here 10559
2014-07-01 15:37:01,266 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1404253868846/slave1%2C60020%2C1404253868846.1404254217212 with entries=132, filesize=105.2m; new WAL /hbase/WALs/slave1,60020,1404253868846/slave1%2C60020%2C1404253868846.1404254220836
2014-07-01 15:37:03,635 INFO  [JvmPauseMonitor] util.JvmPauseMonitor: Detected pause in JVM or host machine (eg GC): pause of approximately 1852ms
GC pool 'ParNew' had collection(s): count=1 time=1936ms
2014-07-01 15:37:04,357 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-01 15:37:04,463 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 10708 synced till here 10679
2014-07-01 15:37:06,452 INFO  [JvmPauseMonitor] util.JvmPauseMonitor: Detected pause in JVM or host machine (eg GC): pause of approximately 1815ms
GC pool 'ParNew' had collection(s): count=1 time=1865ms
2014-07-01 15:37:06,709 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1404253868846/slave1%2C60020%2C1404253868846.1404254220836 with entries=112, filesize=92.4m; new WAL /hbase/WALs/slave1,60020,1404253868846/slave1%2C60020%2C1404253868846.1404254224358
2014-07-01 15:37:06,744 INFO  [MemStoreFlusher.0] regionserver.DefaultStoreFlusher: Flushed, sequenceid=2170, memsize=243.5m, hasBloomFilter=true, into tmp file hdfs://master:54310/hbase/data/default/usertable/122e92eabfab38cb15a9965b21db25c5/.tmp/20ca3320de1d4d98b72b768b2445eb90
2014-07-01 15:37:06,779 DEBUG [MemStoreFlusher.0] regionserver.HRegionFileSystem: Committing store file hdfs://master:54310/hbase/data/default/usertable/122e92eabfab38cb15a9965b21db25c5/.tmp/20ca3320de1d4d98b72b768b2445eb90 as hdfs://master:54310/hbase/data/default/usertable/122e92eabfab38cb15a9965b21db25c5/family/20ca3320de1d4d98b72b768b2445eb90
2014-07-01 15:37:06,799 INFO  [MemStoreFlusher.0] regionserver.HStore: Added hdfs://master:54310/hbase/data/default/usertable/122e92eabfab38cb15a9965b21db25c5/family/20ca3320de1d4d98b72b768b2445eb90, entries=886680, sequenceid=2170, filesize=63.2m
2014-07-01 15:37:06,799 INFO  [MemStoreFlusher.0] regionserver.HRegion: Finished memstore flush of ~560.7m/587986000, currentsize=347.5m/364387600 for region usertable,user5,1404228265526.122e92eabfab38cb15a9965b21db25c5. in 27196ms, sequenceid=2170, compaction requested=true
2014-07-01 15:37:06,800 DEBUG [MemStoreFlusher.0] regionserver.CompactSplitThread: Small Compaction requested: system; Because: MemStoreFlusher.0; compaction_queue=(0:15), split_queue=0, merge_queue=0
2014-07-01 15:37:06,800 DEBUG [MemStoreFlusher.0] regionserver.HRegion: Started memstore flush for usertable,user7,1404228265526.667c1f2c34fe37d48ba25a789d8aba98., current region memstore size 564.4m
2014-07-01 15:37:07,119 DEBUG [RpcServer.handler=45,port=60020] regionserver.HRegion: Flush requested on usertable,user5,1404228265526.122e92eabfab38cb15a9965b21db25c5.
2014-07-01 15:37:09,312 INFO  [JvmPauseMonitor] util.JvmPauseMonitor: Detected pause in JVM or host machine (eg GC): pause of approximately 1859ms
GC pool 'ParNew' had collection(s): count=1 time=1943ms
2014-07-01 15:37:09,538 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-01 15:37:09,540 WARN  [RpcServer.handler=32,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":10484,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.58:57750","starttimems":1404254219055,"queuetimems":0,"class":"HRegionServer","responsesize":20390,"method":"Multi"}
2014-07-01 15:37:09,652 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 10843 synced till here 10807
2014-07-01 15:37:09,790 DEBUG [MemStoreFlusher.0] util.FSUtils: DFS Client does not support most favored nodes create; using default create
2014-07-01 15:37:10,212 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1404253868846/slave1%2C60020%2C1404253868846.1404254224358 with entries=135, filesize=92.4m; new WAL /hbase/WALs/slave1,60020,1404253868846/slave1%2C60020%2C1404253868846.1404254229538
2014-07-01 15:37:10,212 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1404253868846/slave1%2C60020%2C1404253868846.1404254121595
2014-07-01 15:37:10,213 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1404253868846/slave1%2C60020%2C1404253868846.1404254143084
2014-07-01 15:37:12,121 INFO  [JvmPauseMonitor] util.JvmPauseMonitor: Detected pause in JVM or host machine (eg GC): pause of approximately 1307ms
GC pool 'ParNew' had collection(s): count=1 time=1482ms
2014-07-01 15:37:12,629 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-01 15:37:12,709 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 10968 synced till here 10937
2014-07-01 15:37:12,951 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1404253868846/slave1%2C60020%2C1404253868846.1404254229538 with entries=125, filesize=100.9m; new WAL /hbase/WALs/slave1,60020,1404253868846/slave1%2C60020%2C1404253868846.1404254232629
2014-07-01 15:37:14,249 INFO  [JvmPauseMonitor] util.JvmPauseMonitor: Detected pause in JVM or host machine (eg GC): pause of approximately 1121ms
GC pool 'ParNew' had collection(s): count=1 time=1243ms
2014-07-01 15:37:15,453 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-01 15:37:17,007 INFO  [JvmPauseMonitor] util.JvmPauseMonitor: Detected pause in JVM or host machine (eg GC): pause of approximately 1256ms
GC pool 'ParNew' had collection(s): count=2 time=1467ms
2014-07-01 15:37:17,008 INFO  [MemStoreFlusher.1] regionserver.DefaultStoreFlusher: Flushed, sequenceid=2278, memsize=204.7m, hasBloomFilter=true, into tmp file hdfs://master:54310/hbase/data/default/usertable/93253e37c284f3174309fede3f32339f/.tmp/847eed712227414fac5ea1ff40f653a1
2014-07-01 15:37:17,028 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 11117 synced till here 11077
2014-07-01 15:37:17,031 DEBUG [MemStoreFlusher.1] regionserver.HRegionFileSystem: Committing store file hdfs://master:54310/hbase/data/default/usertable/93253e37c284f3174309fede3f32339f/.tmp/847eed712227414fac5ea1ff40f653a1 as hdfs://master:54310/hbase/data/default/usertable/93253e37c284f3174309fede3f32339f/family/847eed712227414fac5ea1ff40f653a1
2014-07-01 15:37:17,042 INFO  [MemStoreFlusher.1] regionserver.HStore: Added hdfs://master:54310/hbase/data/default/usertable/93253e37c284f3174309fede3f32339f/family/847eed712227414fac5ea1ff40f653a1, entries=745270, sequenceid=2278, filesize=53.1m
2014-07-01 15:37:17,043 INFO  [MemStoreFlusher.1] regionserver.HRegion: Finished memstore flush of ~413.7m/433838400, currentsize=309.3m/324376800 for region usertable,user3,1404228265526.93253e37c284f3174309fede3f32339f. in 26517ms, sequenceid=2278, compaction requested=true
2014-07-01 15:37:17,043 DEBUG [MemStoreFlusher.1] regionserver.CompactSplitThread: Small Compaction requested: system; Because: MemStoreFlusher.1; compaction_queue=(0:16), split_queue=0, merge_queue=0
2014-07-01 15:37:17,043 DEBUG [MemStoreFlusher.1] regionserver.HRegion: Started memstore flush for usertable,,1404228265525.9cef9e7d34f96f1d9b111be50040a208., current region memstore size 509.2m
2014-07-01 15:37:17,086 DEBUG [RpcServer.handler=28,port=60020] regionserver.HRegion: Flush requested on usertable,user3,1404228265526.93253e37c284f3174309fede3f32339f.
2014-07-01 15:37:17,267 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1404253868846/slave1%2C60020%2C1404253868846.1404254232629 with entries=149, filesize=104.3m; new WAL /hbase/WALs/slave1,60020,1404253868846/slave1%2C60020%2C1404253868846.1404254235454
2014-07-01 15:37:17,729 DEBUG [MemStoreFlusher.1] util.FSUtils: DFS Client does not support most favored nodes create; using default create
2014-07-01 15:37:18,267 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-01 15:37:19,258 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 11238 synced till here 11210
2014-07-01 15:37:19,475 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1404253868846/slave1%2C60020%2C1404253868846.1404254235454 with entries=121, filesize=93.5m; new WAL /hbase/WALs/slave1,60020,1404253868846/slave1%2C60020%2C1404253868846.1404254238268
2014-07-01 15:37:20,150 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-01 15:37:20,174 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 11356 synced till here 11328
2014-07-01 15:37:21,448 INFO  [JvmPauseMonitor] util.JvmPauseMonitor: Detected pause in JVM or host machine (eg GC): pause of approximately 1215ms
GC pool 'ParNew' had collection(s): count=1 time=1245ms
2014-07-01 15:37:21,797 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1404253868846/slave1%2C60020%2C1404253868846.1404254238268 with entries=118, filesize=91.2m; new WAL /hbase/WALs/slave1,60020,1404253868846/slave1%2C60020%2C1404253868846.1404254240151
2014-07-01 15:37:23,594 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-01 15:37:23,619 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 11484 synced till here 11460
2014-07-01 15:37:23,794 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1404253868846/slave1%2C60020%2C1404253868846.1404254240151 with entries=128, filesize=81.7m; new WAL /hbase/WALs/slave1,60020,1404253868846/slave1%2C60020%2C1404253868846.1404254243594
2014-07-01 15:37:25,238 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-01 15:37:25,253 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 11602 synced till here 11601
2014-07-01 15:37:25,267 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1404253868846/slave1%2C60020%2C1404253868846.1404254243594 with entries=118, filesize=61.1m; new WAL /hbase/WALs/slave1,60020,1404253868846/slave1%2C60020%2C1404253868846.1404254245239
2014-07-01 15:37:27,028 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-01 15:37:27,067 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1404253868846/slave1%2C60020%2C1404253868846.1404254245239 with entries=108, filesize=61.3m; new WAL /hbase/WALs/slave1,60020,1404253868846/slave1%2C60020%2C1404253868846.1404254247029
2014-07-01 15:37:28,981 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-01 15:37:29,019 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 11817 synced till here 11816
2014-07-01 15:37:29,029 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1404253868846/slave1%2C60020%2C1404253868846.1404254247029 with entries=107, filesize=61.9m; new WAL /hbase/WALs/slave1,60020,1404253868846/slave1%2C60020%2C1404253868846.1404254248981
2014-07-01 15:37:31,181 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-01 15:37:31,821 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 11953 synced till here 11952
2014-07-01 15:37:31,832 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1404253868846/slave1%2C60020%2C1404253868846.1404254248981 with entries=136, filesize=71.2m; new WAL /hbase/WALs/slave1,60020,1404253868846/slave1%2C60020%2C1404253868846.1404254251182
2014-07-01 15:37:33,009 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-01 15:37:33,513 INFO  [MemStoreFlusher.0] regionserver.DefaultStoreFlusher: Flushed, sequenceid=2341, memsize=287.8m, hasBloomFilter=true, into tmp file hdfs://master:54310/hbase/data/default/usertable/667c1f2c34fe37d48ba25a789d8aba98/.tmp/b50947a863cd4f888cc97a27e8dab5f4
2014-07-01 15:37:33,639 DEBUG [MemStoreFlusher.0] regionserver.HRegionFileSystem: Committing store file hdfs://master:54310/hbase/data/default/usertable/667c1f2c34fe37d48ba25a789d8aba98/.tmp/b50947a863cd4f888cc97a27e8dab5f4 as hdfs://master:54310/hbase/data/default/usertable/667c1f2c34fe37d48ba25a789d8aba98/family/b50947a863cd4f888cc97a27e8dab5f4
2014-07-01 15:37:33,713 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 12066 synced till here 12060
2014-07-01 15:37:34,072 INFO  [MemStoreFlusher.0] regionserver.HStore: Added hdfs://master:54310/hbase/data/default/usertable/667c1f2c34fe37d48ba25a789d8aba98/family/b50947a863cd4f888cc97a27e8dab5f4, entries=1047780, sequenceid=2341, filesize=74.7m
2014-07-01 15:37:34,073 INFO  [MemStoreFlusher.0] regionserver.HRegion: Finished memstore flush of ~586.9m/615366000, currentsize=400.5m/419991040 for region usertable,user7,1404228265526.667c1f2c34fe37d48ba25a789d8aba98. in 27273ms, sequenceid=2341, compaction requested=true
2014-07-01 15:37:34,073 DEBUG [MemStoreFlusher.0] regionserver.CompactSplitThread: Small Compaction requested: system; Because: MemStoreFlusher.0; compaction_queue=(0:17), split_queue=0, merge_queue=0
2014-07-01 15:37:34,074 DEBUG [MemStoreFlusher.0] regionserver.HRegion: Started memstore flush for usertable,user5,1404228265526.122e92eabfab38cb15a9965b21db25c5., current region memstore size 748.8m
2014-07-01 15:37:34,452 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1404253868846/slave1%2C60020%2C1404253868846.1404254251182 with entries=113, filesize=82.2m; new WAL /hbase/WALs/slave1,60020,1404253868846/slave1%2C60020%2C1404253868846.1404254253010
2014-07-01 15:37:34,508 DEBUG [regionserver60020-smallCompactions-1404253964856] regionserver.HRegionFileSystem: Committing store file hdfs://master:54310/hbase/data/default/usertable/9cef9e7d34f96f1d9b111be50040a208/.tmp/582d11d149af4c829654b41f188bc10d as hdfs://master:54310/hbase/data/default/usertable/9cef9e7d34f96f1d9b111be50040a208/family/582d11d149af4c829654b41f188bc10d
2014-07-01 15:37:34,871 DEBUG [RpcServer.handler=21,port=60020] regionserver.HRegion: Flush requested on usertable,user7,1404228265526.667c1f2c34fe37d48ba25a789d8aba98.
2014-07-01 15:37:35,174 DEBUG [regionserver60020-smallCompactions-1404253964856] regionserver.HStore: Removing store files after compaction...
2014-07-01 15:37:36,206 DEBUG [regionserver60020-smallCompactions-1404253964856] backup.HFileArchiver: Finished archiving from class org.apache.hadoop.hbase.backup.HFileArchiver$FileableStoreFile, file:hdfs://master:54310/hbase/data/default/usertable/9cef9e7d34f96f1d9b111be50040a208/family/1a3ddd0bf6eb49c094e7bfe4686943b0, to hdfs://master:54310/hbase/archive/data/default/usertable/9cef9e7d34f96f1d9b111be50040a208/family/1a3ddd0bf6eb49c094e7bfe4686943b0
2014-07-01 15:37:36,209 DEBUG [regionserver60020-smallCompactions-1404253964856] backup.HFileArchiver: Finished archiving from class org.apache.hadoop.hbase.backup.HFileArchiver$FileableStoreFile, file:hdfs://master:54310/hbase/data/default/usertable/9cef9e7d34f96f1d9b111be50040a208/family/d503bdfe9b5742faa7fd875ddeb4e5ea, to hdfs://master:54310/hbase/archive/data/default/usertable/9cef9e7d34f96f1d9b111be50040a208/family/d503bdfe9b5742faa7fd875ddeb4e5ea
2014-07-01 15:37:36,215 DEBUG [regionserver60020-smallCompactions-1404253964856] backup.HFileArchiver: Finished archiving from class org.apache.hadoop.hbase.backup.HFileArchiver$FileableStoreFile, file:hdfs://master:54310/hbase/data/default/usertable/9cef9e7d34f96f1d9b111be50040a208/family/b6459b2efea04af1894b4d4fb1f62251, to hdfs://master:54310/hbase/archive/data/default/usertable/9cef9e7d34f96f1d9b111be50040a208/family/b6459b2efea04af1894b4d4fb1f62251
2014-07-01 15:37:36,219 DEBUG [regionserver60020-smallCompactions-1404253964856] backup.HFileArchiver: Finished archiving from class org.apache.hadoop.hbase.backup.HFileArchiver$FileableStoreFile, file:hdfs://master:54310/hbase/data/default/usertable/9cef9e7d34f96f1d9b111be50040a208/family/242fc66525a74fc5bc085539024c616a, to hdfs://master:54310/hbase/archive/data/default/usertable/9cef9e7d34f96f1d9b111be50040a208/family/242fc66525a74fc5bc085539024c616a
2014-07-01 15:37:36,221 DEBUG [regionserver60020-smallCompactions-1404253964856] backup.HFileArchiver: Finished archiving from class org.apache.hadoop.hbase.backup.HFileArchiver$FileableStoreFile, file:hdfs://master:54310/hbase/data/default/usertable/9cef9e7d34f96f1d9b111be50040a208/family/e12a9b0b94c546e18a602bad19ea7347, to hdfs://master:54310/hbase/archive/data/default/usertable/9cef9e7d34f96f1d9b111be50040a208/family/e12a9b0b94c546e18a602bad19ea7347
2014-07-01 15:37:36,221 INFO  [regionserver60020-smallCompactions-1404253964856] regionserver.HStore: Completed major compaction of 5 file(s) in family of usertable,,1404228265525.9cef9e7d34f96f1d9b111be50040a208. into 582d11d149af4c829654b41f188bc10d(size=240.6m), total size for store is 240.6m. This selection was in queue for 0sec, and took 1mins, 48sec to execute.
2014-07-01 15:37:36,222 INFO  [regionserver60020-smallCompactions-1404253964856] regionserver.CompactSplitThread: Completed compaction: Request = regionName=usertable,,1404228265525.9cef9e7d34f96f1d9b111be50040a208., storeName=family, fileCount=5, fileSize=256.6m, priority=15, time=74259472092562; duration=1mins, 48sec
2014-07-01 15:37:36,222 DEBUG [regionserver60020-smallCompactions-1404253964856] regionserver.CompactSplitThread: CompactSplitThread Status: compaction_queue=(0:17), split_queue=0, merge_queue=0
2014-07-01 15:37:36,222 DEBUG [regionserver60020-smallCompactions-1404253964856] compactions.RatioBasedCompactionPolicy: Selecting compaction from 6 store files, 0 compacting, 6 eligible, 20 blocking
2014-07-01 15:37:36,222 DEBUG [regionserver60020-smallCompactions-1404253964856] compactions.ExploringCompactionPolicy: Exploring compaction algorithm has selected 6 files of size 362702284 starting at candidate #0 after considering 10 permutations with 10 in ratio
2014-07-01 15:37:36,222 DEBUG [regionserver60020-smallCompactions-1404253964856] regionserver.HStore: 122e92eabfab38cb15a9965b21db25c5 - family: Initiating major compaction
2014-07-01 15:37:36,223 INFO  [regionserver60020-smallCompactions-1404253964856] regionserver.HRegion: Starting compaction on family in region usertable,user5,1404228265526.122e92eabfab38cb15a9965b21db25c5.
2014-07-01 15:37:36,223 INFO  [regionserver60020-smallCompactions-1404253964856] regionserver.HStore: Starting compaction of 6 file(s) in family of usertable,user5,1404228265526.122e92eabfab38cb15a9965b21db25c5. into tmpdir=hdfs://master:54310/hbase/data/default/usertable/122e92eabfab38cb15a9965b21db25c5/.tmp, totalSize=345.9m
2014-07-01 15:37:36,223 DEBUG [regionserver60020-smallCompactions-1404253964856] compactions.Compactor: Compacting hdfs://master:54310/hbase/data/default/usertable/122e92eabfab38cb15a9965b21db25c5/family/37d47aff33684f1191cc991aab1081e2, keycount=139629, bloomtype=ROW, size=99.6m, encoding=NONE, seqNum=815, earliestPutTs=1404228281724
2014-07-01 15:37:36,223 DEBUG [regionserver60020-smallCompactions-1404253964856] compactions.Compactor: Compacting hdfs://master:54310/hbase/data/default/usertable/122e92eabfab38cb15a9965b21db25c5/family/364186ae5d724fa9a969ef43c4fd8fde, keycount=62456, bloomtype=ROW, size=44.5m, encoding=NONE, seqNum=1000, earliestPutTs=1404253978510
2014-07-01 15:37:36,223 DEBUG [regionserver60020-smallCompactions-1404253964856] compactions.Compactor: Compacting hdfs://master:54310/hbase/data/default/usertable/122e92eabfab38cb15a9965b21db25c5/family/7905079665664e82b3df5ceb29d6d35c, keycount=91640, bloomtype=ROW, size=65.3m, encoding=NONE, seqNum=1249, earliestPutTs=1404254006092
2014-07-01 15:37:36,224 DEBUG [regionserver60020-smallCompactions-1404253964856] compactions.Compactor: Compacting hdfs://master:54310/hbase/data/default/usertable/122e92eabfab38cb15a9965b21db25c5/family/5102e47fde4f44ad99e428f06749b5b0, keycount=52121, bloomtype=ROW, size=37.1m, encoding=NONE, seqNum=1488, earliestPutTs=1404254056960
2014-07-01 15:37:36,224 DEBUG [regionserver60020-smallCompactions-1404253964856] compactions.Compactor: Compacting hdfs://master:54310/hbase/data/default/usertable/122e92eabfab38cb15a9965b21db25c5/family/ad6abf67212d44faa242d52d31aecc9c, keycount=50671, bloomtype=ROW, size=36.1m, encoding=NONE, seqNum=1839, earliestPutTs=1404254076246
2014-07-01 15:37:36,224 DEBUG [regionserver60020-smallCompactions-1404253964856] compactions.Compactor: Compacting hdfs://master:54310/hbase/data/default/usertable/122e92eabfab38cb15a9965b21db25c5/family/20ca3320de1d4d98b72b768b2445eb90, keycount=88668, bloomtype=ROW, size=63.2m, encoding=NONE, seqNum=2170, earliestPutTs=1404254123222
2014-07-01 15:37:36,234 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-01 15:37:36,258 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 12154 synced till here 12152
2014-07-01 15:37:36,273 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1404253868846/slave1%2C60020%2C1404253868846.1404254253010 with entries=88, filesize=63.4m; new WAL /hbase/WALs/slave1,60020,1404253868846/slave1%2C60020%2C1404253868846.1404254256234
2014-07-01 15:37:36,308 DEBUG [MemStoreFlusher.0] util.FSUtils: DFS Client does not support most favored nodes create; using default create
2014-07-01 15:37:36,309 DEBUG [regionserver60020-smallCompactions-1404253964856] util.FSUtils: DFS Client does not support most favored nodes create; using default create
2014-07-01 15:37:37,956 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-01 15:37:38,005 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 12250 synced till here 12231
2014-07-01 15:37:38,219 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1404253868846/slave1%2C60020%2C1404253868846.1404254256234 with entries=96, filesize=78.2m; new WAL /hbase/WALs/slave1,60020,1404253868846/slave1%2C60020%2C1404253868846.1404254257956
2014-07-01 15:37:40,058 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-01 15:37:40,159 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 12355 synced till here 12331
2014-07-01 15:37:40,200 INFO  [MemStoreFlusher.1] regionserver.DefaultStoreFlusher: Flushed, sequenceid=1965, memsize=344.9m, hasBloomFilter=true, into tmp file hdfs://master:54310/hbase/data/default/usertable/9cef9e7d34f96f1d9b111be50040a208/.tmp/12264dc0884947c1960f156423a73464
2014-07-01 15:37:40,221 DEBUG [MemStoreFlusher.1] regionserver.HRegionFileSystem: Committing store file hdfs://master:54310/hbase/data/default/usertable/9cef9e7d34f96f1d9b111be50040a208/.tmp/12264dc0884947c1960f156423a73464 as hdfs://master:54310/hbase/data/default/usertable/9cef9e7d34f96f1d9b111be50040a208/family/12264dc0884947c1960f156423a73464
2014-07-01 15:37:40,234 INFO  [MemStoreFlusher.1] regionserver.HStore: Added hdfs://master:54310/hbase/data/default/usertable/9cef9e7d34f96f1d9b111be50040a208/family/12264dc0884947c1960f156423a73464, entries=1255950, sequenceid=1965, filesize=89.5m
2014-07-01 15:37:40,234 INFO  [MemStoreFlusher.1] regionserver.HRegion: Finished memstore flush of ~525.9m/551410720, currentsize=258.3m/270828320 for region usertable,,1404228265525.9cef9e7d34f96f1d9b111be50040a208. in 23191ms, sequenceid=1965, compaction requested=false
2014-07-01 15:37:40,235 DEBUG [MemStoreFlusher.1] regionserver.HRegion: Started memstore flush for usertable,user3,1404228265526.93253e37c284f3174309fede3f32339f., current region memstore size 689.3m
2014-07-01 15:37:40,274 DEBUG [RpcServer.handler=31,port=60020] regionserver.HRegion: Flush requested on usertable,,1404228265525.9cef9e7d34f96f1d9b111be50040a208.
2014-07-01 15:37:40,348 DEBUG [RpcServer.handler=17,port=60020] regionserver.HRegion: Flush requested on usertable,user9,1404228265526.bb79ae773f71289e0745068f68a0abd9.
2014-07-01 15:37:40,364 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1404253868846/slave1%2C60020%2C1404253868846.1404254257956 with entries=105, filesize=79.4m; new WAL /hbase/WALs/slave1,60020,1404253868846/slave1%2C60020%2C1404253868846.1404254260059
2014-07-01 15:37:40,364 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1404253868846/slave1%2C60020%2C1404253868846.1404254123706
2014-07-01 15:37:40,365 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1404253868846/slave1%2C60020%2C1404253868846.1404254127148
2014-07-01 15:37:40,365 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1404253868846/slave1%2C60020%2C1404253868846.1404254129776
2014-07-01 15:37:40,365 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1404253868846/slave1%2C60020%2C1404253868846.1404254131859
2014-07-01 15:37:40,365 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1404253868846/slave1%2C60020%2C1404253868846.1404254133975
2014-07-01 15:37:40,366 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1404253868846/slave1%2C60020%2C1404253868846.1404254136259
2014-07-01 15:37:40,366 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1404253868846/slave1%2C60020%2C1404253868846.1404254139647
2014-07-01 15:37:40,366 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1404253868846/slave1%2C60020%2C1404253868846.1404254145909
2014-07-01 15:37:40,366 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1404253868846/slave1%2C60020%2C1404253868846.1404254148874
2014-07-01 15:37:40,366 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1404253868846/slave1%2C60020%2C1404253868846.1404254151933
2014-07-01 15:37:40,367 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1404253868846/slave1%2C60020%2C1404253868846.1404254154906
2014-07-01 15:37:40,367 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1404253868846/slave1%2C60020%2C1404253868846.1404254183381
2014-07-01 15:37:40,368 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1404253868846/slave1%2C60020%2C1404253868846.1404254190663
2014-07-01 15:37:42,455 DEBUG [MemStoreFlusher.1] util.FSUtils: DFS Client does not support most favored nodes create; using default create
2014-07-01 15:37:42,547 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-01 15:37:42,592 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 12487 synced till here 12455
2014-07-01 15:37:42,739 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1404253868846/slave1%2C60020%2C1404253868846.1404254260059 with entries=132, filesize=94.8m; new WAL /hbase/WALs/slave1,60020,1404253868846/slave1%2C60020%2C1404253868846.1404254262548
2014-07-01 15:37:44,536 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-01 15:37:44,591 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 12606 synced till here 12584
2014-07-01 15:37:44,769 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1404253868846/slave1%2C60020%2C1404253868846.1404254262548 with entries=119, filesize=86.9m; new WAL /hbase/WALs/slave1,60020,1404253868846/slave1%2C60020%2C1404253868846.1404254264537
2014-07-01 15:37:46,686 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-01 15:37:46,730 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 12719 synced till here 12694
2014-07-01 15:37:47,018 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1404253868846/slave1%2C60020%2C1404253868846.1404254264537 with entries=113, filesize=83.9m; new WAL /hbase/WALs/slave1,60020,1404253868846/slave1%2C60020%2C1404253868846.1404254266687
2014-07-01 15:37:48,990 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-01 15:37:49,077 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 12830 synced till here 12801
2014-07-01 15:37:50,348 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1404253868846/slave1%2C60020%2C1404253868846.1404254266687 with entries=111, filesize=83.9m; new WAL /hbase/WALs/slave1,60020,1404253868846/slave1%2C60020%2C1404253868846.1404254268990
2014-07-01 15:37:51,360 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-01 15:37:52,953 INFO  [JvmPauseMonitor] util.JvmPauseMonitor: Detected pause in JVM or host machine (eg GC): pause of approximately 1174ms
GC pool 'ParNew' had collection(s): count=1 time=1300ms
2014-07-01 15:37:52,956 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 12948 synced till here 12945
2014-07-01 15:37:53,105 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1404253868846/slave1%2C60020%2C1404253868846.1404254268990 with entries=118, filesize=81.5m; new WAL /hbase/WALs/slave1,60020,1404253868846/slave1%2C60020%2C1404253868846.1404254271361
2014-07-01 15:37:53,929 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-01 15:37:53,954 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 13038 synced till here 13034
2014-07-01 15:37:53,967 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1404253868846/slave1%2C60020%2C1404253868846.1404254271361 with entries=90, filesize=63.5m; new WAL /hbase/WALs/slave1,60020,1404253868846/slave1%2C60020%2C1404253868846.1404254273929
2014-07-01 15:37:56,604 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-01 15:37:56,645 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 13127 synced till here 13125
2014-07-01 15:37:56,701 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1404253868846/slave1%2C60020%2C1404253868846.1404254273929 with entries=89, filesize=65.6m; new WAL /hbase/WALs/slave1,60020,1404253868846/slave1%2C60020%2C1404253868846.1404254276604
2014-07-01 15:37:58,717 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-01 15:37:59,774 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 13229 synced till here 13208
2014-07-01 15:38:00,051 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1404253868846/slave1%2C60020%2C1404253868846.1404254276604 with entries=102, filesize=83.8m; new WAL /hbase/WALs/slave1,60020,1404253868846/slave1%2C60020%2C1404253868846.1404254278717
2014-07-01 15:38:00,052 INFO  [regionserver60020.logRoller] wal.FSHLog: Too many hlogs: logs=33, maxlogs=32; forcing flush of 1 regions(s): bb79ae773f71289e0745068f68a0abd9
2014-07-01 15:38:02,096 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-01 15:38:02,183 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 13347 synced till here 13325
2014-07-01 15:38:02,457 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1404253868846/slave1%2C60020%2C1404253868846.1404254278717 with entries=118, filesize=80.9m; new WAL /hbase/WALs/slave1,60020,1404253868846/slave1%2C60020%2C1404253868846.1404254282096
2014-07-01 15:38:02,458 INFO  [regionserver60020.logRoller] wal.FSHLog: Too many hlogs: logs=34, maxlogs=32; forcing flush of 1 regions(s): bb79ae773f71289e0745068f68a0abd9
2014-07-01 15:38:04,626 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-01 15:38:04,701 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 13476 synced till here 13445
2014-07-01 15:38:04,858 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1404253868846/slave1%2C60020%2C1404253868846.1404254282096 with entries=129, filesize=83.9m; new WAL /hbase/WALs/slave1,60020,1404253868846/slave1%2C60020%2C1404253868846.1404254284627
2014-07-01 15:38:04,859 INFO  [regionserver60020.logRoller] wal.FSHLog: Too many hlogs: logs=35, maxlogs=32; forcing flush of 1 regions(s): bb79ae773f71289e0745068f68a0abd9
2014-07-01 15:38:05,088 INFO  [RpcServer.handler=26,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1404253868846: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-01 15:38:05,089 INFO  [RpcServer.handler=10,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1404253868846: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-01 15:38:05,091 INFO  [RpcServer.handler=28,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1404253868846: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-01 15:38:05,091 INFO  [RpcServer.handler=31,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1404253868846: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-01 15:38:05,092 INFO  [RpcServer.handler=33,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1404253868846: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-01 15:38:05,093 INFO  [RpcServer.handler=27,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1404253868846: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-01 15:38:05,093 INFO  [RpcServer.handler=7,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1404253868846: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-01 15:38:05,093 INFO  [RpcServer.handler=20,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1404253868846: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-01 15:38:05,094 INFO  [RpcServer.handler=44,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1404253868846: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-01 15:38:05,101 INFO  [RpcServer.handler=39,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1404253868846: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-01 15:38:05,119 INFO  [RpcServer.handler=35,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1404253868846: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-01 15:38:05,119 INFO  [RpcServer.handler=5,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1404253868846: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-01 15:38:05,121 INFO  [RpcServer.handler=32,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1404253868846: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-01 15:38:05,272 INFO  [RpcServer.handler=18,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1404253868846: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-01 15:38:05,273 INFO  [RpcServer.handler=24,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1404253868846: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-01 15:38:05,274 INFO  [RpcServer.handler=41,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1404253868846: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-01 15:38:05,275 INFO  [RpcServer.handler=38,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1404253868846: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-01 15:38:05,275 INFO  [RpcServer.handler=42,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1404253868846: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-01 15:38:05,276 INFO  [RpcServer.handler=43,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1404253868846: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-01 15:38:05,276 INFO  [RpcServer.handler=37,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1404253868846: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-01 15:38:05,277 INFO  [RpcServer.handler=9,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1404253868846: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-01 15:38:05,278 INFO  [RpcServer.handler=23,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1404253868846: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-01 15:38:05,278 INFO  [RpcServer.handler=49,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1404253868846: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-01 15:38:05,278 INFO  [RpcServer.handler=21,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1404253868846: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-01 15:38:05,279 INFO  [RpcServer.handler=2,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1404253868846: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-01 15:38:05,279 INFO  [RpcServer.handler=0,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1404253868846: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-01 15:38:05,279 INFO  [RpcServer.handler=22,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1404253868846: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-01 15:38:05,279 INFO  [RpcServer.handler=11,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1404253868846: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-01 15:38:05,280 INFO  [RpcServer.handler=1,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1404253868846: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-01 15:38:05,282 INFO  [RpcServer.handler=25,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1404253868846: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-01 15:38:05,282 INFO  [RpcServer.handler=29,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1404253868846: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-01 15:38:05,282 INFO  [RpcServer.handler=17,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1404253868846: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-01 15:38:05,283 INFO  [RpcServer.handler=34,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1404253868846: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-01 15:38:05,284 INFO  [RpcServer.handler=3,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1404253868846: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-01 15:38:05,354 INFO  [RpcServer.handler=12,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1404253868846: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-01 15:38:05,354 INFO  [RpcServer.handler=16,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1404253868846: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-01 15:38:05,355 INFO  [RpcServer.handler=48,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1404253868846: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-01 15:38:05,356 INFO  [RpcServer.handler=4,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1404253868846: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-01 15:38:05,356 INFO  [RpcServer.handler=40,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1404253868846: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-01 15:38:05,356 INFO  [RpcServer.handler=6,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1404253868846: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-01 15:38:05,357 INFO  [RpcServer.handler=46,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1404253868846: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-01 15:38:05,357 INFO  [RpcServer.handler=47,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1404253868846: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-01 15:38:05,357 INFO  [RpcServer.handler=30,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1404253868846: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-01 15:38:05,358 INFO  [RpcServer.handler=45,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1404253868846: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-01 15:38:05,411 INFO  [RpcServer.handler=15,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1404253868846: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-01 15:38:05,435 INFO  [RpcServer.handler=14,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1404253868846: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-01 15:38:06,315 INFO  [RpcServer.handler=13,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1404253868846: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-01 15:38:06,329 INFO  [RpcServer.handler=36,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1404253868846: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-01 15:38:07,474 INFO  [RpcServer.handler=8,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1404253868846: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-01 15:38:07,565 INFO  [RpcServer.handler=19,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1404253868846: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-01 15:38:07,588 INFO  [MemStoreFlusher.1] regionserver.DefaultStoreFlusher: Flushed, sequenceid=2711, memsize=272.7m, hasBloomFilter=true, into tmp file hdfs://master:54310/hbase/data/default/usertable/93253e37c284f3174309fede3f32339f/.tmp/a1918f1d24b6451792a0de5913dea691
2014-07-01 15:38:07,616 DEBUG [MemStoreFlusher.1] regionserver.HRegionFileSystem: Committing store file hdfs://master:54310/hbase/data/default/usertable/93253e37c284f3174309fede3f32339f/.tmp/a1918f1d24b6451792a0de5913dea691 as hdfs://master:54310/hbase/data/default/usertable/93253e37c284f3174309fede3f32339f/family/a1918f1d24b6451792a0de5913dea691
2014-07-01 15:38:07,633 INFO  [MemStoreFlusher.1] regionserver.HStore: Added hdfs://master:54310/hbase/data/default/usertable/93253e37c284f3174309fede3f32339f/family/a1918f1d24b6451792a0de5913dea691, entries=992870, sequenceid=2711, filesize=70.8m
2014-07-01 15:38:07,634 INFO  [MemStoreFlusher.1] regionserver.HRegion: Finished memstore flush of ~712.6m/747194800, currentsize=433.7m/454743280 for region usertable,user3,1404228265526.93253e37c284f3174309fede3f32339f. in 27398ms, sequenceid=2711, compaction requested=true
2014-07-01 15:38:07,634 DEBUG [MemStoreFlusher.1] regionserver.CompactSplitThread: Small Compaction requested: system; Because: MemStoreFlusher.1; compaction_queue=(0:17), split_queue=0, merge_queue=0
2014-07-01 15:38:07,634 WARN  [RpcServer.handler=19,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 70ms
2014-07-01 15:38:07,635 INFO  [RpcServer.handler=19,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1404253868846
2014-07-01 15:38:07,635 DEBUG [MemStoreFlusher.1] regionserver.HRegion: Started memstore flush for usertable,user7,1404228265526.667c1f2c34fe37d48ba25a789d8aba98., current region memstore size 937.9m
2014-07-01 15:38:07,635 WARN  [RpcServer.handler=8,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 161ms
2014-07-01 15:38:07,635 INFO  [RpcServer.handler=8,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1404253868846
2014-07-01 15:38:07,635 WARN  [RpcServer.handler=36,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 1306ms
2014-07-01 15:38:07,635 INFO  [RpcServer.handler=36,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1404253868846
2014-07-01 15:38:07,635 WARN  [RpcServer.handler=13,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 1321ms
2014-07-01 15:38:07,635 INFO  [RpcServer.handler=13,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1404253868846
2014-07-01 15:38:07,636 WARN  [RpcServer.handler=14,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 2201ms
2014-07-01 15:38:07,636 INFO  [RpcServer.handler=14,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1404253868846
2014-07-01 15:38:07,636 WARN  [RpcServer.handler=15,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 2225ms
2014-07-01 15:38:07,636 INFO  [RpcServer.handler=15,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1404253868846
2014-07-01 15:38:07,637 WARN  [RpcServer.handler=45,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 2279ms
2014-07-01 15:38:07,637 INFO  [RpcServer.handler=45,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1404253868846
2014-07-01 15:38:07,638 WARN  [RpcServer.handler=30,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 2281ms
2014-07-01 15:38:07,638 INFO  [RpcServer.handler=30,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1404253868846
2014-07-01 15:38:07,651 WARN  [RpcServer.handler=47,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 2294ms
2014-07-01 15:38:07,651 INFO  [RpcServer.handler=47,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1404253868846
2014-07-01 15:38:07,651 WARN  [RpcServer.handler=46,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 2295ms
2014-07-01 15:38:07,651 INFO  [RpcServer.handler=46,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1404253868846
2014-07-01 15:38:07,651 WARN  [RpcServer.handler=6,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 2295ms
2014-07-01 15:38:07,652 INFO  [RpcServer.handler=6,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1404253868846
2014-07-01 15:38:07,652 WARN  [RpcServer.handler=40,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 2296ms
2014-07-01 15:38:07,652 INFO  [RpcServer.handler=40,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1404253868846
2014-07-01 15:38:07,659 WARN  [RpcServer.handler=4,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 2303ms
2014-07-01 15:38:07,659 INFO  [RpcServer.handler=4,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1404253868846
2014-07-01 15:38:07,659 WARN  [RpcServer.handler=48,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 2304ms
2014-07-01 15:38:07,659 INFO  [RpcServer.handler=48,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1404253868846
2014-07-01 15:38:07,667 WARN  [RpcServer.handler=16,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 2313ms
2014-07-01 15:38:07,667 INFO  [RpcServer.handler=16,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1404253868846
2014-07-01 15:38:07,675 WARN  [RpcServer.handler=12,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 2321ms
2014-07-01 15:38:07,675 INFO  [RpcServer.handler=12,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1404253868846
2014-07-01 15:38:07,675 WARN  [RpcServer.handler=3,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 2391ms
2014-07-01 15:38:07,675 INFO  [RpcServer.handler=3,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1404253868846
2014-07-01 15:38:07,676 WARN  [RpcServer.handler=34,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 2393ms
2014-07-01 15:38:07,676 INFO  [RpcServer.handler=34,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1404253868846
2014-07-01 15:38:07,676 WARN  [RpcServer.handler=17,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 2394ms
2014-07-01 15:38:07,676 INFO  [RpcServer.handler=17,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1404253868846
2014-07-01 15:38:07,676 WARN  [RpcServer.handler=29,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 2394ms
2014-07-01 15:38:07,677 INFO  [RpcServer.handler=29,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1404253868846
2014-07-01 15:38:07,682 WARN  [RpcServer.handler=25,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 2396ms
2014-07-01 15:38:07,682 INFO  [RpcServer.handler=25,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1404253868846
2014-07-01 15:38:07,682 WARN  [RpcServer.handler=1,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 2402ms
2014-07-01 15:38:07,682 INFO  [RpcServer.handler=1,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1404253868846
2014-07-01 15:38:07,683 WARN  [RpcServer.handler=11,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 2406ms
2014-07-01 15:38:07,683 INFO  [RpcServer.handler=11,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1404253868846
2014-07-01 15:38:07,683 WARN  [RpcServer.handler=22,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 2406ms
2014-07-01 15:38:07,683 INFO  [RpcServer.handler=22,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1404253868846
2014-07-01 15:38:07,684 WARN  [RpcServer.handler=0,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 2406ms
2014-07-01 15:38:07,684 INFO  [RpcServer.handler=0,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1404253868846
2014-07-01 15:38:07,688 WARN  [RpcServer.handler=2,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 2410ms
2014-07-01 15:38:07,688 INFO  [RpcServer.handler=2,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1404253868846
2014-07-01 15:38:07,698 WARN  [RpcServer.handler=21,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 2414ms
2014-07-01 15:38:07,698 INFO  [RpcServer.handler=21,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1404253868846
2014-07-01 15:38:07,698 WARN  [RpcServer.handler=49,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 2422ms
2014-07-01 15:38:07,698 INFO  [RpcServer.handler=49,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1404253868846
2014-07-01 15:38:07,699 WARN  [RpcServer.handler=23,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 2423ms
2014-07-01 15:38:07,699 INFO  [RpcServer.handler=23,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1404253868846
2014-07-01 15:38:07,699 WARN  [RpcServer.handler=9,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 2422ms
2014-07-01 15:38:07,699 INFO  [RpcServer.handler=9,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1404253868846
2014-07-01 15:38:07,707 WARN  [RpcServer.handler=37,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 2431ms
2014-07-01 15:38:07,707 INFO  [RpcServer.handler=37,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1404253868846
2014-07-01 15:38:07,707 WARN  [RpcServer.handler=43,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 2431ms
2014-07-01 15:38:07,707 INFO  [RpcServer.handler=43,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1404253868846
2014-07-01 15:38:07,711 WARN  [RpcServer.handler=42,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 2436ms
2014-07-01 15:38:07,711 INFO  [RpcServer.handler=42,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1404253868846
2014-07-01 15:38:07,711 WARN  [RpcServer.handler=38,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 2437ms
2014-07-01 15:38:07,711 INFO  [RpcServer.handler=38,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1404253868846
2014-07-01 15:38:07,712 WARN  [RpcServer.handler=41,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 2437ms
2014-07-01 15:38:07,712 INFO  [RpcServer.handler=41,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1404253868846
2014-07-01 15:38:07,712 WARN  [RpcServer.handler=24,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 2439ms
2014-07-01 15:38:07,712 INFO  [RpcServer.handler=24,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1404253868846
2014-07-01 15:38:07,712 WARN  [RpcServer.handler=18,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 2440ms
2014-07-01 15:38:07,712 INFO  [RpcServer.handler=18,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1404253868846
2014-07-01 15:38:07,715 WARN  [RpcServer.handler=32,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 2594ms
2014-07-01 15:38:07,715 INFO  [RpcServer.handler=32,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1404253868846
2014-07-01 15:38:07,719 WARN  [RpcServer.handler=5,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 2600ms
2014-07-01 15:38:07,719 INFO  [RpcServer.handler=5,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1404253868846
2014-07-01 15:38:07,719 WARN  [RpcServer.handler=35,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 2600ms
2014-07-01 15:38:07,720 INFO  [RpcServer.handler=35,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1404253868846
2014-07-01 15:38:07,721 WARN  [RpcServer.handler=39,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 2619ms
2014-07-01 15:38:07,721 INFO  [RpcServer.handler=39,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1404253868846
2014-07-01 15:38:07,727 WARN  [RpcServer.handler=44,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 2634ms
2014-07-01 15:38:07,727 INFO  [RpcServer.handler=44,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1404253868846
2014-07-01 15:38:07,727 WARN  [RpcServer.handler=20,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 2634ms
2014-07-01 15:38:07,727 INFO  [RpcServer.handler=20,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1404253868846
2014-07-01 15:38:07,728 WARN  [RpcServer.handler=7,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 2635ms
2014-07-01 15:38:07,728 INFO  [RpcServer.handler=7,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1404253868846
2014-07-01 15:38:07,728 WARN  [RpcServer.handler=27,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 2635ms
2014-07-01 15:38:07,728 INFO  [RpcServer.handler=27,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1404253868846
2014-07-01 15:38:07,728 WARN  [RpcServer.handler=33,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 2636ms
2014-07-01 15:38:07,728 INFO  [RpcServer.handler=33,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1404253868846
2014-07-01 15:38:07,729 WARN  [RpcServer.handler=31,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 2638ms
2014-07-01 15:38:07,729 INFO  [RpcServer.handler=31,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1404253868846
2014-07-01 15:38:07,729 WARN  [RpcServer.handler=28,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 2638ms
2014-07-01 15:38:07,729 INFO  [RpcServer.handler=28,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1404253868846
2014-07-01 15:38:07,731 WARN  [RpcServer.handler=10,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 2642ms
2014-07-01 15:38:07,731 INFO  [RpcServer.handler=10,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1404253868846
2014-07-01 15:38:07,735 WARN  [RpcServer.handler=26,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 2647ms
2014-07-01 15:38:07,735 INFO  [RpcServer.handler=26,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1404253868846
2014-07-01 15:38:08,204 INFO  [MemStoreFlusher.0] regionserver.DefaultStoreFlusher: Flushed, sequenceid=2624, memsize=321.8m, hasBloomFilter=true, into tmp file hdfs://master:54310/hbase/data/default/usertable/122e92eabfab38cb15a9965b21db25c5/.tmp/b5325e7ef6d54c65869ab9b110690b41
2014-07-01 15:38:08,220 DEBUG [MemStoreFlusher.0] regionserver.HRegionFileSystem: Committing store file hdfs://master:54310/hbase/data/default/usertable/122e92eabfab38cb15a9965b21db25c5/.tmp/b5325e7ef6d54c65869ab9b110690b41 as hdfs://master:54310/hbase/data/default/usertable/122e92eabfab38cb15a9965b21db25c5/family/b5325e7ef6d54c65869ab9b110690b41
2014-07-01 15:38:08,233 INFO  [MemStoreFlusher.0] regionserver.HStore: Added hdfs://master:54310/hbase/data/default/usertable/122e92eabfab38cb15a9965b21db25c5/family/b5325e7ef6d54c65869ab9b110690b41, entries=1171500, sequenceid=2624, filesize=83.5m
2014-07-01 15:38:08,233 INFO  [MemStoreFlusher.0] regionserver.HRegion: Finished memstore flush of ~754.0m/790628960, currentsize=544.0m/570466160 for region usertable,user5,1404228265526.122e92eabfab38cb15a9965b21db25c5. in 34159ms, sequenceid=2624, compaction requested=false
2014-07-01 15:38:08,234 DEBUG [MemStoreFlusher.0] regionserver.HRegion: Started memstore flush for usertable,,1404228265525.9cef9e7d34f96f1d9b111be50040a208., current region memstore size 341.8m
2014-07-01 15:38:08,365 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-01 15:38:08,421 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 13602 synced till here 13583
2014-07-01 15:38:08,593 DEBUG [RpcServer.handler=36,port=60020] regionserver.HRegion: Flush requested on usertable,user3,1404228265526.93253e37c284f3174309fede3f32339f.
2014-07-01 15:38:08,593 DEBUG [RpcServer.handler=45,port=60020] regionserver.HRegion: Flush requested on usertable,user5,1404228265526.122e92eabfab38cb15a9965b21db25c5.
2014-07-01 15:38:08,649 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1404253868846/slave1%2C60020%2C1404253868846.1404254284627 with entries=126, filesize=83.0m; new WAL /hbase/WALs/slave1,60020,1404253868846/slave1%2C60020%2C1404253868846.1404254288365
2014-07-01 15:38:08,650 INFO  [regionserver60020.logRoller] wal.FSHLog: Too many hlogs: logs=36, maxlogs=32; forcing flush of 1 regions(s): bb79ae773f71289e0745068f68a0abd9
2014-07-01 15:38:09,920 DEBUG [MemStoreFlusher.1] util.FSUtils: DFS Client does not support most favored nodes create; using default create
2014-07-01 15:38:10,054 DEBUG [MemStoreFlusher.0] util.FSUtils: DFS Client does not support most favored nodes create; using default create
2014-07-01 15:38:11,196 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-01 15:38:11,404 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 13754 synced till here 13726
2014-07-01 15:38:12,517 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1404253868846/slave1%2C60020%2C1404253868846.1404254288365 with entries=152, filesize=98.8m; new WAL /hbase/WALs/slave1,60020,1404253868846/slave1%2C60020%2C1404253868846.1404254291197
2014-07-01 15:38:12,518 INFO  [regionserver60020.logRoller] wal.FSHLog: Too many hlogs: logs=37, maxlogs=32; forcing flush of 1 regions(s): bb79ae773f71289e0745068f68a0abd9
2014-07-01 15:38:13,309 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-01 15:38:13,322 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 13866 synced till here 13853
2014-07-01 15:38:13,375 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1404253868846/slave1%2C60020%2C1404253868846.1404254291197 with entries=112, filesize=72.4m; new WAL /hbase/WALs/slave1,60020,1404253868846/slave1%2C60020%2C1404253868846.1404254293309
2014-07-01 15:38:13,375 INFO  [regionserver60020.logRoller] wal.FSHLog: Too many hlogs: logs=38, maxlogs=32; forcing flush of 1 regions(s): bb79ae773f71289e0745068f68a0abd9
2014-07-01 15:38:14,868 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-01 15:38:14,893 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 13950 synced till here 13949
2014-07-01 15:38:14,919 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1404253868846/slave1%2C60020%2C1404253868846.1404254293309 with entries=84, filesize=63.0m; new WAL /hbase/WALs/slave1,60020,1404253868846/slave1%2C60020%2C1404253868846.1404254294869
2014-07-01 15:38:14,920 INFO  [regionserver60020.logRoller] wal.FSHLog: Too many hlogs: logs=39, maxlogs=32; forcing flush of 1 regions(s): bb79ae773f71289e0745068f68a0abd9
2014-07-01 15:38:16,463 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-01 15:38:16,494 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 14054 synced till here 14029
2014-07-01 15:38:17,506 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1404253868846/slave1%2C60020%2C1404253868846.1404254294869 with entries=104, filesize=79.5m; new WAL /hbase/WALs/slave1,60020,1404253868846/slave1%2C60020%2C1404253868846.1404254296463
2014-07-01 15:38:17,507 INFO  [regionserver60020.logRoller] wal.FSHLog: Too many hlogs: logs=40, maxlogs=32; forcing flush of 1 regions(s): bb79ae773f71289e0745068f68a0abd9
2014-07-01 15:38:18,564 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-01 15:38:18,595 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 14154 synced till here 14150
2014-07-01 15:38:18,653 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1404253868846/slave1%2C60020%2C1404253868846.1404254296463 with entries=100, filesize=64.6m; new WAL /hbase/WALs/slave1,60020,1404253868846/slave1%2C60020%2C1404253868846.1404254298565
2014-07-01 15:38:18,654 INFO  [regionserver60020.logRoller] wal.FSHLog: Too many hlogs: logs=41, maxlogs=32; forcing flush of 1 regions(s): bb79ae773f71289e0745068f68a0abd9
2014-07-01 15:38:20,117 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-01 15:38:20,143 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 14255 synced till here 14253
2014-07-01 15:38:20,188 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1404253868846/slave1%2C60020%2C1404253868846.1404254298565 with entries=101, filesize=64.8m; new WAL /hbase/WALs/slave1,60020,1404253868846/slave1%2C60020%2C1404253868846.1404254300117
2014-07-01 15:38:20,189 INFO  [regionserver60020.logRoller] wal.FSHLog: Too many hlogs: logs=42, maxlogs=32; forcing flush of 1 regions(s): bb79ae773f71289e0745068f68a0abd9
2014-07-01 15:38:21,762 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-01 15:38:21,872 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 14366 synced till here 14362
2014-07-01 15:38:21,985 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1404253868846/slave1%2C60020%2C1404253868846.1404254300117 with entries=111, filesize=67.2m; new WAL /hbase/WALs/slave1,60020,1404253868846/slave1%2C60020%2C1404253868846.1404254301762
2014-07-01 15:38:21,986 INFO  [regionserver60020.logRoller] wal.FSHLog: Too many hlogs: logs=43, maxlogs=32; forcing flush of 1 regions(s): bb79ae773f71289e0745068f68a0abd9
2014-07-01 15:38:23,563 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-01 15:38:24,987 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 14564 synced till here 14554
2014-07-01 15:38:25,242 INFO  [MemStoreFlusher.0] regionserver.DefaultStoreFlusher: Flushed, sequenceid=2406, memsize=260.5m, hasBloomFilter=true, into tmp file hdfs://master:54310/hbase/data/default/usertable/9cef9e7d34f96f1d9b111be50040a208/.tmp/51ea620459a3482c8f384892f831e303
2014-07-01 15:38:25,267 DEBUG [MemStoreFlusher.0] regionserver.HRegionFileSystem: Committing store file hdfs://master:54310/hbase/data/default/usertable/9cef9e7d34f96f1d9b111be50040a208/.tmp/51ea620459a3482c8f384892f831e303 as hdfs://master:54310/hbase/data/default/usertable/9cef9e7d34f96f1d9b111be50040a208/family/51ea620459a3482c8f384892f831e303
2014-07-01 15:38:25,374 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1404253868846/slave1%2C60020%2C1404253868846.1404254301762 with entries=198, filesize=125.8m; new WAL /hbase/WALs/slave1,60020,1404253868846/slave1%2C60020%2C1404253868846.1404254303564
2014-07-01 15:38:25,375 INFO  [regionserver60020.logRoller] wal.FSHLog: Too many hlogs: logs=44, maxlogs=32; forcing flush of 1 regions(s): bb79ae773f71289e0745068f68a0abd9
2014-07-01 15:38:25,494 INFO  [MemStoreFlusher.0] regionserver.HStore: Added hdfs://master:54310/hbase/data/default/usertable/9cef9e7d34f96f1d9b111be50040a208/family/51ea620459a3482c8f384892f831e303, entries=948350, sequenceid=2406, filesize=67.5m
2014-07-01 15:38:25,494 INFO  [MemStoreFlusher.0] regionserver.HRegion: Finished memstore flush of ~343.3m/360022640, currentsize=153.0m/160403120 for region usertable,,1404228265525.9cef9e7d34f96f1d9b111be50040a208. in 17260ms, sequenceid=2406, compaction requested=true
2014-07-01 15:38:25,494 DEBUG [MemStoreFlusher.0] regionserver.CompactSplitThread: Small Compaction requested: system; Because: MemStoreFlusher.0; compaction_queue=(0:18), split_queue=0, merge_queue=0
2014-07-01 15:38:25,495 DEBUG [MemStoreFlusher.0] regionserver.HRegion: Started memstore flush for usertable,user9,1404228265526.bb79ae773f71289e0745068f68a0abd9., current region memstore size 489.6m
2014-07-01 15:38:26,109 DEBUG [MemStoreFlusher.0] util.FSUtils: DFS Client does not support most favored nodes create; using default create
2014-07-01 15:38:26,843 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-01 15:38:26,913 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 14689 synced till here 14675
2014-07-01 15:38:27,170 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1404253868846/slave1%2C60020%2C1404253868846.1404254303564 with entries=125, filesize=75.4m; new WAL /hbase/WALs/slave1,60020,1404253868846/slave1%2C60020%2C1404253868846.1404254306844
2014-07-01 15:38:28,829 INFO  [MemStoreFlusher.1] regionserver.DefaultStoreFlusher: Flushed, sequenceid=2915, memsize=283.0m, hasBloomFilter=true, into tmp file hdfs://master:54310/hbase/data/default/usertable/667c1f2c34fe37d48ba25a789d8aba98/.tmp/925f4c5bb4d949f38b040fc6a1bcf5a2
2014-07-01 15:38:28,849 DEBUG [MemStoreFlusher.1] regionserver.HRegionFileSystem: Committing store file hdfs://master:54310/hbase/data/default/usertable/667c1f2c34fe37d48ba25a789d8aba98/.tmp/925f4c5bb4d949f38b040fc6a1bcf5a2 as hdfs://master:54310/hbase/data/default/usertable/667c1f2c34fe37d48ba25a789d8aba98/family/925f4c5bb4d949f38b040fc6a1bcf5a2
2014-07-01 15:38:28,881 INFO  [MemStoreFlusher.1] regionserver.HStore: Added hdfs://master:54310/hbase/data/default/usertable/667c1f2c34fe37d48ba25a789d8aba98/family/925f4c5bb4d949f38b040fc6a1bcf5a2, entries=1030230, sequenceid=2915, filesize=73.4m
2014-07-01 15:38:28,882 INFO  [MemStoreFlusher.1] regionserver.HRegion: Finished memstore flush of ~937.9m/983455440, currentsize=392.7m/411767920 for region usertable,user7,1404228265526.667c1f2c34fe37d48ba25a789d8aba98. in 21247ms, sequenceid=2915, compaction requested=true
2014-07-01 15:38:28,882 DEBUG [MemStoreFlusher.1] regionserver.CompactSplitThread: Small Compaction requested: system; Because: MemStoreFlusher.1; compaction_queue=(0:19), split_queue=0, merge_queue=0
2014-07-01 15:38:28,883 DEBUG [MemStoreFlusher.1] regionserver.HRegion: Started memstore flush for usertable,user5,1404228265526.122e92eabfab38cb15a9965b21db25c5., current region memstore size 923.0m
2014-07-01 15:38:28,949 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-01 15:38:28,950 DEBUG [RpcServer.handler=44,port=60020] regionserver.HRegion: Flush requested on usertable,user7,1404228265526.667c1f2c34fe37d48ba25a789d8aba98.
2014-07-01 15:38:28,974 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 14800 synced till here 14784
2014-07-01 15:38:29,100 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1404253868846/slave1%2C60020%2C1404253868846.1404254306844 with entries=111, filesize=69.3m; new WAL /hbase/WALs/slave1,60020,1404253868846/slave1%2C60020%2C1404253868846.1404254308949
2014-07-01 15:38:30,165 DEBUG [MemStoreFlusher.1] util.FSUtils: DFS Client does not support most favored nodes create; using default create
2014-07-01 15:38:30,503 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-01 15:38:30,960 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1404253868846/slave1%2C60020%2C1404253868846.1404254308949 with entries=132, filesize=61.5m; new WAL /hbase/WALs/slave1,60020,1404253868846/slave1%2C60020%2C1404253868846.1404254310504
2014-07-01 15:38:32,364 DEBUG [RpcServer.handler=31,port=60020] regionserver.HRegion: Flush requested on usertable,,1404228265525.9cef9e7d34f96f1d9b111be50040a208.
2014-07-01 15:38:32,622 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-01 15:38:32,651 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 15055 synced till here 15037
2014-07-01 15:38:32,749 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1404253868846/slave1%2C60020%2C1404253868846.1404254310504 with entries=123, filesize=68.1m; new WAL /hbase/WALs/slave1,60020,1404253868846/slave1%2C60020%2C1404253868846.1404254312623
2014-07-01 15:38:34,503 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-01 15:38:34,538 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 15144 synced till here 15143
2014-07-01 15:38:34,546 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1404253868846/slave1%2C60020%2C1404253868846.1404254312623 with entries=89, filesize=62.8m; new WAL /hbase/WALs/slave1,60020,1404253868846/slave1%2C60020%2C1404253868846.1404254314503
2014-07-01 15:38:36,891 INFO  [MemStoreFlusher.0] regionserver.DefaultStoreFlusher: Flushed, sequenceid=3121, memsize=168.5m, hasBloomFilter=true, into tmp file hdfs://master:54310/hbase/data/default/usertable/bb79ae773f71289e0745068f68a0abd9/.tmp/dfe213bff7bd4b9e8bd25f87a78f301e
2014-07-01 15:38:36,926 DEBUG [MemStoreFlusher.0] regionserver.HRegionFileSystem: Committing store file hdfs://master:54310/hbase/data/default/usertable/bb79ae773f71289e0745068f68a0abd9/.tmp/dfe213bff7bd4b9e8bd25f87a78f301e as hdfs://master:54310/hbase/data/default/usertable/bb79ae773f71289e0745068f68a0abd9/family/dfe213bff7bd4b9e8bd25f87a78f301e
2014-07-01 15:38:37,280 INFO  [MemStoreFlusher.0] regionserver.HStore: Added hdfs://master:54310/hbase/data/default/usertable/bb79ae773f71289e0745068f68a0abd9/family/dfe213bff7bd4b9e8bd25f87a78f301e, entries=613600, sequenceid=3121, filesize=43.7m
2014-07-01 15:38:37,281 INFO  [MemStoreFlusher.0] regionserver.HRegion: Finished memstore flush of ~491.8m/515654720, currentsize=49.2m/51639440 for region usertable,user9,1404228265526.bb79ae773f71289e0745068f68a0abd9. in 11785ms, sequenceid=3121, compaction requested=true
2014-07-01 15:38:37,281 DEBUG [MemStoreFlusher.0] regionserver.CompactSplitThread: Small Compaction requested: system; Because: MemStoreFlusher.0; compaction_queue=(0:20), split_queue=0, merge_queue=0
2014-07-01 15:38:37,281 DEBUG [MemStoreFlusher.0] regionserver.HRegion: Started memstore flush for usertable,user3,1404228265526.93253e37c284f3174309fede3f32339f., current region memstore size 917.5m
2014-07-01 15:38:37,304 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-01 15:38:38,401 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 15317 synced till here 15304
2014-07-01 15:38:38,777 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1404253868846/slave1%2C60020%2C1404253868846.1404254314503 with entries=173, filesize=109.8m; new WAL /hbase/WALs/slave1,60020,1404253868846/slave1%2C60020%2C1404253868846.1404254317304
2014-07-01 15:38:38,777 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1404253868846/slave1%2C60020%2C1404253868846.1404254197697
2014-07-01 15:38:38,777 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1404253868846/slave1%2C60020%2C1404253868846.1404254199614
2014-07-01 15:38:38,777 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1404253868846/slave1%2C60020%2C1404253868846.1404254200637
2014-07-01 15:38:38,778 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1404253868846/slave1%2C60020%2C1404253868846.1404254203199
2014-07-01 15:38:38,778 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1404253868846/slave1%2C60020%2C1404253868846.1404254205143
2014-07-01 15:38:38,778 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1404253868846/slave1%2C60020%2C1404253868846.1404254206964
2014-07-01 15:38:38,778 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1404253868846/slave1%2C60020%2C1404253868846.1404254209218
2014-07-01 15:38:38,778 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1404253868846/slave1%2C60020%2C1404253868846.1404254211969
2014-07-01 15:38:38,778 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1404253868846/slave1%2C60020%2C1404253868846.1404254213463
2014-07-01 15:38:38,778 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1404253868846/slave1%2C60020%2C1404253868846.1404254217212
2014-07-01 15:38:38,778 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1404253868846/slave1%2C60020%2C1404253868846.1404254220836
2014-07-01 15:38:38,778 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1404253868846/slave1%2C60020%2C1404253868846.1404254224358
2014-07-01 15:38:38,778 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1404253868846/slave1%2C60020%2C1404253868846.1404254229538
2014-07-01 15:38:38,778 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1404253868846/slave1%2C60020%2C1404253868846.1404254232629
2014-07-01 15:38:38,778 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1404253868846/slave1%2C60020%2C1404253868846.1404254235454
2014-07-01 15:38:38,778 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1404253868846/slave1%2C60020%2C1404253868846.1404254238268
2014-07-01 15:38:38,778 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1404253868846/slave1%2C60020%2C1404253868846.1404254240151
2014-07-01 15:38:38,778 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1404253868846/slave1%2C60020%2C1404253868846.1404254243594
2014-07-01 15:38:38,779 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1404253868846/slave1%2C60020%2C1404253868846.1404254245239
2014-07-01 15:38:38,779 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1404253868846/slave1%2C60020%2C1404253868846.1404254247029
2014-07-01 15:38:38,779 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1404253868846/slave1%2C60020%2C1404253868846.1404254248981
2014-07-01 15:38:38,779 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1404253868846/slave1%2C60020%2C1404253868846.1404254251182
2014-07-01 15:38:38,828 DEBUG [MemStoreFlusher.0] util.FSUtils: DFS Client does not support most favored nodes create; using default create
2014-07-01 15:38:40,465 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-01 15:38:40,548 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 15423 synced till here 15399
2014-07-01 15:38:40,851 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1404253868846/slave1%2C60020%2C1404253868846.1404254317304 with entries=106, filesize=76.7m; new WAL /hbase/WALs/slave1,60020,1404253868846/slave1%2C60020%2C1404253868846.1404254320466
2014-07-01 15:38:43,092 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-01 15:38:43,170 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 15533 synced till here 15508
2014-07-01 15:38:43,520 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1404253868846/slave1%2C60020%2C1404253868846.1404254320466 with entries=110, filesize=84.2m; new WAL /hbase/WALs/slave1,60020,1404253868846/slave1%2C60020%2C1404253868846.1404254323093
2014-07-01 15:38:45,570 INFO  [JvmPauseMonitor] util.JvmPauseMonitor: Detected pause in JVM or host machine (eg GC): pause of approximately 1267ms
GC pool 'ParNew' had collection(s): count=1 time=1681ms
2014-07-01 15:38:46,661 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-01 15:38:46,789 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 15656 synced till here 15634
2014-07-01 15:38:48,220 INFO  [JvmPauseMonitor] util.JvmPauseMonitor: Detected pause in JVM or host machine (eg GC): pause of approximately 1148ms
GC pool 'ParNew' had collection(s): count=1 time=1316ms
2014-07-01 15:38:48,278 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1404253868846/slave1%2C60020%2C1404253868846.1404254323093 with entries=123, filesize=98.0m; new WAL /hbase/WALs/slave1,60020,1404253868846/slave1%2C60020%2C1404253868846.1404254326662
2014-07-01 15:38:50,252 INFO  [JvmPauseMonitor] util.JvmPauseMonitor: Detected pause in JVM or host machine (eg GC): pause of approximately 1032ms
GC pool 'ParNew' had collection(s): count=1 time=1260ms
2014-07-01 15:38:50,705 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-01 15:38:51,129 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 15826 synced till here 15791
2014-07-01 15:38:52,456 INFO  [JvmPauseMonitor] util.JvmPauseMonitor: Detected pause in JVM or host machine (eg GC): pause of approximately 1202ms
GC pool 'ParNew' had collection(s): count=1 time=1273ms
2014-07-01 15:38:52,601 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1404253868846/slave1%2C60020%2C1404253868846.1404254326662 with entries=170, filesize=126.1m; new WAL /hbase/WALs/slave1,60020,1404253868846/slave1%2C60020%2C1404253868846.1404254330705
2014-07-01 15:38:54,753 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-01 15:38:54,846 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 15956 synced till here 15929
2014-07-01 15:38:55,060 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1404253868846/slave1%2C60020%2C1404253868846.1404254330705 with entries=130, filesize=97.6m; new WAL /hbase/WALs/slave1,60020,1404253868846/slave1%2C60020%2C1404253868846.1404254334753
2014-07-01 15:38:56,598 INFO  [JvmPauseMonitor] util.JvmPauseMonitor: Detected pause in JVM or host machine (eg GC): pause of approximately 1118ms
GC pool 'ParNew' had collection(s): count=1 time=1223ms
2014-07-01 15:38:57,508 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-01 15:38:57,717 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 16067 synced till here 16063
2014-07-01 15:38:57,824 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1404253868846/slave1%2C60020%2C1404253868846.1404254334753 with entries=111, filesize=84.9m; new WAL /hbase/WALs/slave1,60020,1404253868846/slave1%2C60020%2C1404253868846.1404254337508
2014-07-01 15:38:59,246 INFO  [JvmPauseMonitor] util.JvmPauseMonitor: Detected pause in JVM or host machine (eg GC): pause of approximately 1145ms
GC pool 'ParNew' had collection(s): count=1 time=1297ms
2014-07-01 15:38:59,903 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-01 15:38:59,937 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 16169 synced till here 16134
2014-07-01 15:39:00,119 INFO  [MemStoreFlusher.1] regionserver.DefaultStoreFlusher: Flushed, sequenceid=3204, memsize=325.9m, hasBloomFilter=true, into tmp file hdfs://master:54310/hbase/data/default/usertable/122e92eabfab38cb15a9965b21db25c5/.tmp/cf9c7b67a9d34e4a9e524c182876c966
2014-07-01 15:39:00,128 INFO  [RpcServer.handler=44,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1404253868846: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-01 15:39:00,129 INFO  [RpcServer.handler=16,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1404253868846: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-01 15:39:00,130 INFO  [RpcServer.handler=30,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1404253868846: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-01 15:39:00,130 INFO  [RpcServer.handler=19,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1404253868846: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-01 15:39:00,130 INFO  [RpcServer.handler=42,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1404253868846: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-01 15:39:00,132 INFO  [RpcServer.handler=38,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1404253868846: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-01 15:39:00,132 INFO  [RpcServer.handler=40,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1404253868846: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-01 15:39:00,132 INFO  [RpcServer.handler=23,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1404253868846: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-01 15:39:00,133 INFO  [RpcServer.handler=29,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1404253868846: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-01 15:39:00,134 INFO  [RpcServer.handler=24,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1404253868846: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-01 15:39:00,135 INFO  [RpcServer.handler=3,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1404253868846: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-01 15:39:00,135 INFO  [RpcServer.handler=12,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1404253868846: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-01 15:39:00,136 INFO  [RpcServer.handler=18,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1404253868846: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-01 15:39:00,136 INFO  [RpcServer.handler=46,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1404253868846: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-01 15:39:00,138 INFO  [RpcServer.handler=32,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1404253868846: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-01 15:39:00,139 INFO  [RpcServer.handler=15,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1404253868846: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-01 15:39:00,139 INFO  [RpcServer.handler=14,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1404253868846: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-01 15:39:00,139 INFO  [RpcServer.handler=25,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1404253868846: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-01 15:39:00,139 INFO  [RpcServer.handler=28,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1404253868846: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-01 15:39:00,139 INFO  [RpcServer.handler=4,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1404253868846: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-01 15:39:00,144 INFO  [RpcServer.handler=37,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1404253868846: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-01 15:39:00,148 INFO  [RpcServer.handler=7,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1404253868846: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-01 15:39:00,153 INFO  [RpcServer.handler=9,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1404253868846: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-01 15:39:00,154 INFO  [RpcServer.handler=13,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1404253868846: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-01 15:39:00,154 INFO  [RpcServer.handler=5,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1404253868846: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-01 15:39:00,155 INFO  [RpcServer.handler=11,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1404253868846: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-01 15:39:00,155 INFO  [RpcServer.handler=8,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1404253868846: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-01 15:39:00,156 INFO  [RpcServer.handler=20,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1404253868846: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-01 15:39:00,156 INFO  [RpcServer.handler=22,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1404253868846: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-01 15:39:00,157 INFO  [RpcServer.handler=34,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1404253868846: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-01 15:39:00,157 DEBUG [MemStoreFlusher.1] regionserver.HRegionFileSystem: Committing store file hdfs://master:54310/hbase/data/default/usertable/122e92eabfab38cb15a9965b21db25c5/.tmp/cf9c7b67a9d34e4a9e524c182876c966 as hdfs://master:54310/hbase/data/default/usertable/122e92eabfab38cb15a9965b21db25c5/family/cf9c7b67a9d34e4a9e524c182876c966
2014-07-01 15:39:00,158 INFO  [RpcServer.handler=47,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1404253868846: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-01 15:39:01,413 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1404253868846/slave1%2C60020%2C1404253868846.1404254337508 with entries=102, filesize=84.4m; new WAL /hbase/WALs/slave1,60020,1404253868846/slave1%2C60020%2C1404253868846.1404254339904
2014-07-01 15:39:01,413 INFO  [JvmPauseMonitor] util.JvmPauseMonitor: Detected pause in JVM or host machine (eg GC): pause of approximately 1166ms
GC pool 'ParNew' had collection(s): count=1 time=1251ms
2014-07-01 15:39:01,424 INFO  [MemStoreFlusher.1] regionserver.HStore: Added hdfs://master:54310/hbase/data/default/usertable/122e92eabfab38cb15a9965b21db25c5/family/cf9c7b67a9d34e4a9e524c182876c966, entries=1186560, sequenceid=3204, filesize=84.5m
2014-07-01 15:39:01,425 INFO  [MemStoreFlusher.1] regionserver.HRegion: Finished memstore flush of ~931.7m/976937920, currentsize=470.6m/493475120 for region usertable,user5,1404228265526.122e92eabfab38cb15a9965b21db25c5. in 32542ms, sequenceid=3204, compaction requested=false
2014-07-01 15:39:01,425 DEBUG [MemStoreFlusher.1] regionserver.HRegion: Started memstore flush for usertable,user7,1404228265526.667c1f2c34fe37d48ba25a789d8aba98., current region memstore size 865.0m
2014-07-01 15:39:01,426 WARN  [RpcServer.handler=47,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 1267ms
2014-07-01 15:39:01,426 INFO  [RpcServer.handler=47,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1404253868846
2014-07-01 15:39:01,426 WARN  [RpcServer.handler=34,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 1269ms
2014-07-01 15:39:01,426 INFO  [RpcServer.handler=34,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1404253868846
2014-07-01 15:39:01,426 WARN  [RpcServer.handler=22,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 1270ms
2014-07-01 15:39:01,427 INFO  [RpcServer.handler=22,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1404253868846
2014-07-01 15:39:01,428 WARN  [RpcServer.handler=20,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 1272ms
2014-07-01 15:39:01,429 INFO  [RpcServer.handler=20,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1404253868846
2014-07-01 15:39:01,429 WARN  [RpcServer.handler=8,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 1274ms
2014-07-01 15:39:01,429 INFO  [RpcServer.handler=8,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1404253868846
2014-07-01 15:39:01,430 WARN  [RpcServer.handler=11,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 1275ms
2014-07-01 15:39:01,430 INFO  [RpcServer.handler=11,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1404253868846
2014-07-01 15:39:01,436 WARN  [RpcServer.handler=5,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 1281ms
2014-07-01 15:39:01,436 INFO  [RpcServer.handler=5,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1404253868846
2014-07-01 15:39:01,439 WARN  [RpcServer.handler=13,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 1285ms
2014-07-01 15:39:01,439 INFO  [RpcServer.handler=13,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1404253868846
2014-07-01 15:39:01,440 WARN  [RpcServer.handler=9,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 1287ms
2014-07-01 15:39:01,440 INFO  [RpcServer.handler=9,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1404253868846
2014-07-01 15:39:01,441 WARN  [RpcServer.handler=7,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 1292ms
2014-07-01 15:39:01,441 INFO  [RpcServer.handler=7,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1404253868846
2014-07-01 15:39:01,443 WARN  [RpcServer.handler=37,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 1299ms
2014-07-01 15:39:01,443 INFO  [RpcServer.handler=37,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1404253868846
2014-07-01 15:39:01,443 WARN  [RpcServer.handler=4,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 1307ms
2014-07-01 15:39:01,443 INFO  [RpcServer.handler=4,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1404253868846
2014-07-01 15:39:01,445 WARN  [RpcServer.handler=28,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 1305ms
2014-07-01 15:39:01,445 INFO  [RpcServer.handler=28,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1404253868846
2014-07-01 15:39:01,459 WARN  [RpcServer.handler=25,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 1321ms
2014-07-01 15:39:01,459 INFO  [RpcServer.handler=25,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1404253868846
2014-07-01 15:39:01,459 WARN  [RpcServer.handler=14,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 1324ms
2014-07-01 15:39:01,459 INFO  [RpcServer.handler=14,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1404253868846
2014-07-01 15:39:01,459 WARN  [RpcServer.handler=15,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 1321ms
2014-07-01 15:39:01,459 INFO  [RpcServer.handler=15,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1404253868846
2014-07-01 15:39:01,460 WARN  [RpcServer.handler=32,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 1321ms
2014-07-01 15:39:01,460 INFO  [RpcServer.handler=32,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1404253868846
2014-07-01 15:39:01,460 WARN  [RpcServer.handler=46,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 1324ms
2014-07-01 15:39:01,460 INFO  [RpcServer.handler=46,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1404253868846
2014-07-01 15:39:01,460 WARN  [RpcServer.handler=18,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 1326ms
2014-07-01 15:39:01,460 INFO  [RpcServer.handler=18,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1404253868846
2014-07-01 15:39:01,468 WARN  [RpcServer.handler=12,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 1332ms
2014-07-01 15:39:01,468 INFO  [RpcServer.handler=12,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1404253868846
2014-07-01 15:39:01,468 WARN  [RpcServer.handler=3,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 1333ms
2014-07-01 15:39:01,468 INFO  [RpcServer.handler=3,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1404253868846
2014-07-01 15:39:01,468 WARN  [RpcServer.handler=24,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 1334ms
2014-07-01 15:39:01,468 INFO  [RpcServer.handler=24,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1404253868846
2014-07-01 15:39:01,468 WARN  [RpcServer.handler=29,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 1336ms
2014-07-01 15:39:01,468 INFO  [RpcServer.handler=29,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1404253868846
2014-07-01 15:39:01,469 WARN  [RpcServer.handler=23,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 1337ms
2014-07-01 15:39:01,469 INFO  [RpcServer.handler=23,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1404253868846
2014-07-01 15:39:01,469 WARN  [RpcServer.handler=40,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 1337ms
2014-07-01 15:39:01,469 INFO  [RpcServer.handler=40,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1404253868846
2014-07-01 15:39:01,469 WARN  [RpcServer.handler=38,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 1337ms
2014-07-01 15:39:01,469 INFO  [RpcServer.handler=38,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1404253868846
2014-07-01 15:39:01,475 WARN  [RpcServer.handler=42,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 1345ms
2014-07-01 15:39:01,476 INFO  [RpcServer.handler=42,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1404253868846
2014-07-01 15:39:01,476 WARN  [RpcServer.handler=19,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 1346ms
2014-07-01 15:39:01,476 INFO  [RpcServer.handler=19,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1404253868846
2014-07-01 15:39:01,476 WARN  [RpcServer.handler=30,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 1346ms
2014-07-01 15:39:01,476 INFO  [RpcServer.handler=30,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1404253868846
2014-07-01 15:39:01,476 WARN  [RpcServer.handler=16,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 1347ms
2014-07-01 15:39:01,476 INFO  [RpcServer.handler=16,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1404253868846
2014-07-01 15:39:01,476 WARN  [RpcServer.handler=44,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 1348ms
2014-07-01 15:39:01,476 INFO  [RpcServer.handler=44,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1404253868846
2014-07-01 15:39:01,979 DEBUG [RpcServer.handler=40,port=60020] regionserver.HRegion: Flush requested on usertable,user5,1404228265526.122e92eabfab38cb15a9965b21db25c5.
2014-07-01 15:39:02,668 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-01 15:39:04,531 INFO  [JvmPauseMonitor] util.JvmPauseMonitor: Detected pause in JVM or host machine (eg GC): pause of approximately 1616ms
GC pool 'ParNew' had collection(s): count=1 time=1822ms
2014-07-01 15:39:04,614 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 16307 synced till here 16274
2014-07-01 15:39:04,831 DEBUG [MemStoreFlusher.1] util.FSUtils: DFS Client does not support most favored nodes create; using default create
2014-07-01 15:39:05,028 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1404253868846/slave1%2C60020%2C1404253868846.1404254339904 with entries=138, filesize=102.8m; new WAL /hbase/WALs/slave1,60020,1404253868846/slave1%2C60020%2C1404253868846.1404254342669
2014-07-01 15:39:05,029 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1404253868846/slave1%2C60020%2C1404253868846.1404254253010
2014-07-01 15:39:05,030 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1404253868846/slave1%2C60020%2C1404253868846.1404254256234
2014-07-01 15:39:05,030 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1404253868846/slave1%2C60020%2C1404253868846.1404254257956
2014-07-01 15:39:06,834 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-01 15:39:06,912 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 16424 synced till here 16395
2014-07-01 15:39:07,182 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1404253868846/slave1%2C60020%2C1404253868846.1404254342669 with entries=117, filesize=87.9m; new WAL /hbase/WALs/slave1,60020,1404253868846/slave1%2C60020%2C1404253868846.1404254346835
2014-07-01 15:39:08,814 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-01 15:39:08,856 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 16546 synced till here 16523
2014-07-01 15:39:09,090 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1404253868846/slave1%2C60020%2C1404253868846.1404254346835 with entries=122, filesize=96.8m; new WAL /hbase/WALs/slave1,60020,1404253868846/slave1%2C60020%2C1404253868846.1404254348815
2014-07-01 15:39:11,096 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-01 15:39:11,158 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 16668 synced till here 16643
2014-07-01 15:39:11,350 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1404253868846/slave1%2C60020%2C1404253868846.1404254348815 with entries=122, filesize=82.8m; new WAL /hbase/WALs/slave1,60020,1404253868846/slave1%2C60020%2C1404253868846.1404254351096
2014-07-01 15:39:11,866 DEBUG [regionserver60020-smallCompactions-1404253964856] regionserver.HRegionFileSystem: Committing store file hdfs://master:54310/hbase/data/default/usertable/122e92eabfab38cb15a9965b21db25c5/.tmp/a323c796f89d4c1fb403732654d16b5f as hdfs://master:54310/hbase/data/default/usertable/122e92eabfab38cb15a9965b21db25c5/family/a323c796f89d4c1fb403732654d16b5f
2014-07-01 15:39:12,076 DEBUG [regionserver60020-smallCompactions-1404253964856] regionserver.HStore: Removing store files after compaction...
2014-07-01 15:39:12,091 DEBUG [regionserver60020-smallCompactions-1404253964856] backup.HFileArchiver: Finished archiving from class org.apache.hadoop.hbase.backup.HFileArchiver$FileableStoreFile, file:hdfs://master:54310/hbase/data/default/usertable/122e92eabfab38cb15a9965b21db25c5/family/37d47aff33684f1191cc991aab1081e2, to hdfs://master:54310/hbase/archive/data/default/usertable/122e92eabfab38cb15a9965b21db25c5/family/37d47aff33684f1191cc991aab1081e2
2014-07-01 15:39:12,097 DEBUG [regionserver60020-smallCompactions-1404253964856] backup.HFileArchiver: Finished archiving from class org.apache.hadoop.hbase.backup.HFileArchiver$FileableStoreFile, file:hdfs://master:54310/hbase/data/default/usertable/122e92eabfab38cb15a9965b21db25c5/family/364186ae5d724fa9a969ef43c4fd8fde, to hdfs://master:54310/hbase/archive/data/default/usertable/122e92eabfab38cb15a9965b21db25c5/family/364186ae5d724fa9a969ef43c4fd8fde
2014-07-01 15:39:12,103 DEBUG [regionserver60020-smallCompactions-1404253964856] backup.HFileArchiver: Finished archiving from class org.apache.hadoop.hbase.backup.HFileArchiver$FileableStoreFile, file:hdfs://master:54310/hbase/data/default/usertable/122e92eabfab38cb15a9965b21db25c5/family/7905079665664e82b3df5ceb29d6d35c, to hdfs://master:54310/hbase/archive/data/default/usertable/122e92eabfab38cb15a9965b21db25c5/family/7905079665664e82b3df5ceb29d6d35c
2014-07-01 15:39:12,112 DEBUG [regionserver60020-smallCompactions-1404253964856] backup.HFileArchiver: Finished archiving from class org.apache.hadoop.hbase.backup.HFileArchiver$FileableStoreFile, file:hdfs://master:54310/hbase/data/default/usertable/122e92eabfab38cb15a9965b21db25c5/family/5102e47fde4f44ad99e428f06749b5b0, to hdfs://master:54310/hbase/archive/data/default/usertable/122e92eabfab38cb15a9965b21db25c5/family/5102e47fde4f44ad99e428f06749b5b0
2014-07-01 15:39:12,116 DEBUG [regionserver60020-smallCompactions-1404253964856] backup.HFileArchiver: Finished archiving from class org.apache.hadoop.hbase.backup.HFileArchiver$FileableStoreFile, file:hdfs://master:54310/hbase/data/default/usertable/122e92eabfab38cb15a9965b21db25c5/family/ad6abf67212d44faa242d52d31aecc9c, to hdfs://master:54310/hbase/archive/data/default/usertable/122e92eabfab38cb15a9965b21db25c5/family/ad6abf67212d44faa242d52d31aecc9c
2014-07-01 15:39:12,119 DEBUG [regionserver60020-smallCompactions-1404253964856] backup.HFileArchiver: Finished archiving from class org.apache.hadoop.hbase.backup.HFileArchiver$FileableStoreFile, file:hdfs://master:54310/hbase/data/default/usertable/122e92eabfab38cb15a9965b21db25c5/family/20ca3320de1d4d98b72b768b2445eb90, to hdfs://master:54310/hbase/archive/data/default/usertable/122e92eabfab38cb15a9965b21db25c5/family/20ca3320de1d4d98b72b768b2445eb90
2014-07-01 15:39:12,120 INFO  [regionserver60020-smallCompactions-1404253964856] regionserver.HStore: Completed major compaction of 6 file(s) in family of usertable,user5,1404228265526.122e92eabfab38cb15a9965b21db25c5. into a323c796f89d4c1fb403732654d16b5f(size=281.6m), total size for store is 449.6m. This selection was in queue for 0sec, and took 1mins, 35sec to execute.
2014-07-01 15:39:12,120 INFO  [regionserver60020-smallCompactions-1404253964856] regionserver.CompactSplitThread: Completed compaction: Request = regionName=usertable,user5,1404228265526.122e92eabfab38cb15a9965b21db25c5., storeName=family, fileCount=6, fileSize=345.9m, priority=14, time=74367611695740; duration=1mins, 35sec
2014-07-01 15:39:12,120 DEBUG [regionserver60020-smallCompactions-1404253964856] regionserver.CompactSplitThread: CompactSplitThread Status: compaction_queue=(0:20), split_queue=0, merge_queue=0
2014-07-01 15:39:12,120 DEBUG [regionserver60020-smallCompactions-1404253964856] compactions.RatioBasedCompactionPolicy: Selecting compaction from 7 store files, 0 compacting, 7 eligible, 20 blocking
2014-07-01 15:39:12,121 DEBUG [regionserver60020-smallCompactions-1404253964856] compactions.ExploringCompactionPolicy: Exploring compaction algorithm has selected 7 files of size 486115174 starting at candidate #0 after considering 15 permutations with 15 in ratio
2014-07-01 15:39:12,121 DEBUG [regionserver60020-smallCompactions-1404253964856] regionserver.HStore: 667c1f2c34fe37d48ba25a789d8aba98 - family: Initiating major compaction
2014-07-01 15:39:12,121 INFO  [regionserver60020-smallCompactions-1404253964856] regionserver.HRegion: Starting compaction on family in region usertable,user7,1404228265526.667c1f2c34fe37d48ba25a789d8aba98.
2014-07-01 15:39:12,121 INFO  [regionserver60020-smallCompactions-1404253964856] regionserver.HStore: Starting compaction of 7 file(s) in family of usertable,user7,1404228265526.667c1f2c34fe37d48ba25a789d8aba98. into tmpdir=hdfs://master:54310/hbase/data/default/usertable/667c1f2c34fe37d48ba25a789d8aba98/.tmp, totalSize=463.6m
2014-07-01 15:39:12,121 DEBUG [regionserver60020-smallCompactions-1404253964856] compactions.Compactor: Compacting hdfs://master:54310/hbase/data/default/usertable/667c1f2c34fe37d48ba25a789d8aba98/family/dde913fa18fc4787ba86eb9aa2d45ab4, keycount=162208, bloomtype=ROW, size=115.6m, encoding=NONE, seqNum=863, earliestPutTs=1404228281826
2014-07-01 15:39:12,121 DEBUG [regionserver60020-smallCompactions-1404253964856] compactions.Compactor: Compacting hdfs://master:54310/hbase/data/default/usertable/667c1f2c34fe37d48ba25a789d8aba98/family/448a712f5be34ade9a16c57c0c793359, keycount=51593, bloomtype=ROW, size=36.8m, encoding=NONE, seqNum=1065, earliestPutTs=1404253995081
2014-07-01 15:39:12,121 DEBUG [regionserver60020-smallCompactions-1404253964856] compactions.Compactor: Compacting hdfs://master:54310/hbase/data/default/usertable/667c1f2c34fe37d48ba25a789d8aba98/family/5604109a72ba430fa33c345ec291f6cc, keycount=71660, bloomtype=ROW, size=51.1m, encoding=NONE, seqNum=1371, earliestPutTs=1404254017107
2014-07-01 15:39:12,122 DEBUG [regionserver60020-smallCompactions-1404253964856] compactions.Compactor: Compacting hdfs://master:54310/hbase/data/default/usertable/667c1f2c34fe37d48ba25a789d8aba98/family/72abfd2537aa4422a8b06003dfff6e47, keycount=69602, bloomtype=ROW, size=49.6m, encoding=NONE, seqNum=1594, earliestPutTs=1404254066721
2014-07-01 15:39:12,122 DEBUG [regionserver60020-smallCompactions-1404253964856] compactions.Compactor: Compacting hdfs://master:54310/hbase/data/default/usertable/667c1f2c34fe37d48ba25a789d8aba98/family/92895d11141e499989cdac0102a760ea, keycount=87604, bloomtype=ROW, size=62.4m, encoding=NONE, seqNum=1991, earliestPutTs=1404254095361
2014-07-01 15:39:12,122 DEBUG [regionserver60020-smallCompactions-1404253964856] compactions.Compactor: Compacting hdfs://master:54310/hbase/data/default/usertable/667c1f2c34fe37d48ba25a789d8aba98/family/b50947a863cd4f888cc97a27e8dab5f4, keycount=104778, bloomtype=ROW, size=74.7m, encoding=NONE, seqNum=2341, earliestPutTs=1404254148474
2014-07-01 15:39:12,122 DEBUG [regionserver60020-smallCompactions-1404253964856] compactions.Compactor: Compacting hdfs://master:54310/hbase/data/default/usertable/667c1f2c34fe37d48ba25a789d8aba98/family/925f4c5bb4d949f38b040fc6a1bcf5a2, keycount=103023, bloomtype=ROW, size=73.4m, encoding=NONE, seqNum=2915, earliestPutTs=1404254244018
2014-07-01 15:39:12,990 DEBUG [regionserver60020-smallCompactions-1404253964856] util.FSUtils: DFS Client does not support most favored nodes create; using default create
2014-07-01 15:39:13,195 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-01 15:39:13,274 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 16791 synced till here 16762
2014-07-01 15:39:13,508 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1404253868846/slave1%2C60020%2C1404253868846.1404254351096 with entries=123, filesize=96.2m; new WAL /hbase/WALs/slave1,60020,1404253868846/slave1%2C60020%2C1404253868846.1404254353196
2014-07-01 15:39:13,899 INFO  [RpcServer.handler=46,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1404253868846: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-01 15:39:13,907 INFO  [RpcServer.handler=21,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1404253868846: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-01 15:39:13,907 INFO  [RpcServer.handler=12,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1404253868846: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-01 15:39:13,908 INFO  [RpcServer.handler=9,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1404253868846: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-01 15:39:13,908 INFO  [RpcServer.handler=32,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1404253868846: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-01 15:39:13,908 INFO  [RpcServer.handler=29,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1404253868846: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-01 15:39:13,909 INFO  [RpcServer.handler=24,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1404253868846: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-01 15:39:13,910 INFO  [RpcServer.handler=43,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1404253868846: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-01 15:39:13,910 INFO  [RpcServer.handler=19,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1404253868846: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-01 15:39:13,910 INFO  [RpcServer.handler=5,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1404253868846: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-01 15:39:13,911 INFO  [RpcServer.handler=15,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1404253868846: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-01 15:39:13,914 INFO  [RpcServer.handler=6,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1404253868846: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-01 15:39:13,914 INFO  [RpcServer.handler=1,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1404253868846: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-01 15:39:13,915 INFO  [RpcServer.handler=39,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1404253868846: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-01 15:39:13,915 INFO  [RpcServer.handler=14,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1404253868846: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-01 15:39:13,916 INFO  [RpcServer.handler=30,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1404253868846: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-01 15:39:13,959 INFO  [RpcServer.handler=47,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1404253868846: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-01 15:39:13,961 INFO  [RpcServer.handler=42,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1404253868846: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-01 15:39:13,962 INFO  [RpcServer.handler=33,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1404253868846: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-01 15:39:13,980 INFO  [RpcServer.handler=16,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1404253868846: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-01 15:39:13,981 INFO  [RpcServer.handler=34,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1404253868846: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-01 15:39:13,989 INFO  [RpcServer.handler=3,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1404253868846: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-01 15:39:14,043 INFO  [RpcServer.handler=17,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1404253868846: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-01 15:39:14,044 INFO  [RpcServer.handler=44,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1404253868846: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-01 15:39:14,044 INFO  [RpcServer.handler=10,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1404253868846: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-01 15:39:14,044 INFO  [RpcServer.handler=40,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1404253868846: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-01 15:39:14,953 INFO  [RpcServer.handler=48,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1404253868846: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-01 15:39:14,970 INFO  [RpcServer.handler=35,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1404253868846: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-01 15:39:15,000 INFO  [RpcServer.handler=27,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1404253868846: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-01 15:39:15,022 INFO  [RpcServer.handler=38,port=60020] regionserver.MemStoreFlusher: Blocking updates on slave1,60020,1404253868846: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-01 15:39:16,510 INFO  [MemStoreFlusher.0] regionserver.DefaultStoreFlusher: Flushed, sequenceid=3282, memsize=411.0m, hasBloomFilter=true, into tmp file hdfs://master:54310/hbase/data/default/usertable/93253e37c284f3174309fede3f32339f/.tmp/757063605014422bb142804470c2dfd7
2014-07-01 15:39:16,529 DEBUG [MemStoreFlusher.0] regionserver.HRegionFileSystem: Committing store file hdfs://master:54310/hbase/data/default/usertable/93253e37c284f3174309fede3f32339f/.tmp/757063605014422bb142804470c2dfd7 as hdfs://master:54310/hbase/data/default/usertable/93253e37c284f3174309fede3f32339f/family/757063605014422bb142804470c2dfd7
2014-07-01 15:39:16,549 INFO  [MemStoreFlusher.0] regionserver.HStore: Added hdfs://master:54310/hbase/data/default/usertable/93253e37c284f3174309fede3f32339f/family/757063605014422bb142804470c2dfd7, entries=1496610, sequenceid=3282, filesize=106.5m
2014-07-01 15:39:16,550 INFO  [MemStoreFlusher.0] regionserver.HRegion: Finished memstore flush of ~926.3m/971318560, currentsize=617.5m/647543280 for region usertable,user3,1404228265526.93253e37c284f3174309fede3f32339f. in 39269ms, sequenceid=3282, compaction requested=true
2014-07-01 15:39:16,550 DEBUG [MemStoreFlusher.0] regionserver.CompactSplitThread: Small Compaction requested: system; Because: MemStoreFlusher.0; compaction_queue=(0:20), split_queue=0, merge_queue=0
2014-07-01 15:39:16,551 WARN  [RpcServer.handler=38,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 1529ms
2014-07-01 15:39:16,551 INFO  [RpcServer.handler=38,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1404253868846
2014-07-01 15:39:16,551 WARN  [RpcServer.handler=27,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 1551ms
2014-07-01 15:39:16,551 DEBUG [MemStoreFlusher.0] regionserver.HRegion: Started memstore flush for usertable,,1404228265525.9cef9e7d34f96f1d9b111be50040a208., current region memstore size 462.2m
2014-07-01 15:39:16,551 INFO  [RpcServer.handler=27,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1404253868846
2014-07-01 15:39:16,551 WARN  [RpcServer.handler=35,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 1581ms
2014-07-01 15:39:16,552 INFO  [RpcServer.handler=35,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1404253868846
2014-07-01 15:39:16,555 WARN  [RpcServer.handler=48,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 1602ms
2014-07-01 15:39:16,555 INFO  [RpcServer.handler=48,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1404253868846
2014-07-01 15:39:16,555 WARN  [RpcServer.handler=40,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 2512ms
2014-07-01 15:39:16,555 INFO  [RpcServer.handler=40,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1404253868846
2014-07-01 15:39:16,555 WARN  [RpcServer.handler=10,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 2512ms
2014-07-01 15:39:16,555 INFO  [RpcServer.handler=10,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1404253868846
2014-07-01 15:39:16,556 WARN  [RpcServer.handler=44,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 2512ms
2014-07-01 15:39:16,556 INFO  [RpcServer.handler=44,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1404253868846
2014-07-01 15:39:16,556 WARN  [RpcServer.handler=17,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 2513ms
2014-07-01 15:39:16,556 INFO  [RpcServer.handler=17,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1404253868846
2014-07-01 15:39:16,559 WARN  [RpcServer.handler=3,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 2570ms
2014-07-01 15:39:16,559 INFO  [RpcServer.handler=3,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1404253868846
2014-07-01 15:39:16,559 WARN  [RpcServer.handler=34,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 2578ms
2014-07-01 15:39:16,559 INFO  [RpcServer.handler=34,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1404253868846
2014-07-01 15:39:16,559 WARN  [RpcServer.handler=16,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 2579ms
2014-07-01 15:39:16,559 INFO  [RpcServer.handler=16,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1404253868846
2014-07-01 15:39:16,559 WARN  [RpcServer.handler=33,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 2598ms
2014-07-01 15:39:16,559 INFO  [RpcServer.handler=33,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1404253868846
2014-07-01 15:39:16,560 WARN  [RpcServer.handler=42,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 2599ms
2014-07-01 15:39:16,560 INFO  [RpcServer.handler=42,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1404253868846
2014-07-01 15:39:16,575 WARN  [RpcServer.handler=47,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 2616ms
2014-07-01 15:39:16,575 INFO  [RpcServer.handler=47,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1404253868846
2014-07-01 15:39:16,575 WARN  [RpcServer.handler=30,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 2659ms
2014-07-01 15:39:16,575 INFO  [RpcServer.handler=30,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1404253868846
2014-07-01 15:39:16,583 WARN  [RpcServer.handler=14,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 2668ms
2014-07-01 15:39:16,583 INFO  [RpcServer.handler=14,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1404253868846
2014-07-01 15:39:16,583 WARN  [RpcServer.handler=39,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 2668ms
2014-07-01 15:39:16,583 INFO  [RpcServer.handler=39,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1404253868846
2014-07-01 15:39:16,583 WARN  [RpcServer.handler=1,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 2669ms
2014-07-01 15:39:16,583 INFO  [RpcServer.handler=1,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1404253868846
2014-07-01 15:39:16,583 WARN  [RpcServer.handler=6,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 2669ms
2014-07-01 15:39:16,583 INFO  [RpcServer.handler=6,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1404253868846
2014-07-01 15:39:16,584 WARN  [RpcServer.handler=15,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 2672ms
2014-07-01 15:39:16,584 INFO  [RpcServer.handler=15,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1404253868846
2014-07-01 15:39:16,584 WARN  [RpcServer.handler=5,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 2674ms
2014-07-01 15:39:16,584 INFO  [RpcServer.handler=5,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1404253868846
2014-07-01 15:39:16,584 WARN  [RpcServer.handler=19,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 2674ms
2014-07-01 15:39:16,584 INFO  [RpcServer.handler=19,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1404253868846
2014-07-01 15:39:16,584 WARN  [RpcServer.handler=43,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 2674ms
2014-07-01 15:39:16,584 INFO  [RpcServer.handler=43,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1404253868846
2014-07-01 15:39:16,584 WARN  [RpcServer.handler=24,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 2675ms
2014-07-01 15:39:16,584 INFO  [RpcServer.handler=24,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1404253868846
2014-07-01 15:39:16,584 WARN  [RpcServer.handler=29,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 2676ms
2014-07-01 15:39:16,585 INFO  [RpcServer.handler=29,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1404253868846
2014-07-01 15:39:16,585 WARN  [RpcServer.handler=32,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 2677ms
2014-07-01 15:39:16,585 INFO  [RpcServer.handler=32,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1404253868846
2014-07-01 15:39:16,585 WARN  [RpcServer.handler=9,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 2686ms
2014-07-01 15:39:16,585 INFO  [RpcServer.handler=9,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1404253868846
2014-07-01 15:39:16,585 WARN  [RpcServer.handler=12,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 2686ms
2014-07-01 15:39:16,585 INFO  [RpcServer.handler=12,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1404253868846
2014-07-01 15:39:16,585 WARN  [RpcServer.handler=21,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 2686ms
2014-07-01 15:39:16,586 INFO  [RpcServer.handler=21,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1404253868846
2014-07-01 15:39:16,591 WARN  [RpcServer.handler=46,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 2692ms
2014-07-01 15:39:16,591 INFO  [RpcServer.handler=46,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server slave1,60020,1404253868846
2014-07-01 15:39:16,696 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-01 15:39:16,698 DEBUG [RpcServer.handler=38,port=60020] regionserver.HRegion: Flush requested on usertable,user3,1404228265526.93253e37c284f3174309fede3f32339f.
2014-07-01 15:39:16,722 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 16890 synced till here 16876
2014-07-01 15:39:16,807 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1404253868846/slave1%2C60020%2C1404253868846.1404254353196 with entries=99, filesize=69.9m; new WAL /hbase/WALs/slave1,60020,1404253868846/slave1%2C60020%2C1404253868846.1404254356697
2014-07-01 15:39:16,808 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1404253868846/slave1%2C60020%2C1404253868846.1404254260059
2014-07-01 15:39:16,808 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1404253868846/slave1%2C60020%2C1404253868846.1404254262548
2014-07-01 15:39:16,808 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1404253868846/slave1%2C60020%2C1404253868846.1404254264537
2014-07-01 15:39:16,808 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1404253868846/slave1%2C60020%2C1404253868846.1404254266687
2014-07-01 15:39:16,808 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1404253868846/slave1%2C60020%2C1404253868846.1404254268990
2014-07-01 15:39:16,808 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1404253868846/slave1%2C60020%2C1404253868846.1404254271361
2014-07-01 15:39:16,808 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1404253868846/slave1%2C60020%2C1404253868846.1404254273929
2014-07-01 15:39:16,808 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1404253868846/slave1%2C60020%2C1404253868846.1404254276604
2014-07-01 15:39:16,808 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1404253868846/slave1%2C60020%2C1404253868846.1404254278717
2014-07-01 15:39:16,808 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1404253868846/slave1%2C60020%2C1404253868846.1404254282096
2014-07-01 15:39:16,968 DEBUG [MemStoreFlusher.0] util.FSUtils: DFS Client does not support most favored nodes create; using default create
2014-07-01 15:39:18,512 INFO  [JvmPauseMonitor] util.JvmPauseMonitor: Detected pause in JVM or host machine (eg GC): pause of approximately 1072ms
GC pool 'ParNew' had collection(s): count=1 time=619ms
2014-07-01 15:39:19,067 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-01 15:39:19,119 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 16979 synced till here 16970
2014-07-01 15:39:19,202 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1404253868846/slave1%2C60020%2C1404253868846.1404254356697 with entries=89, filesize=68.4m; new WAL /hbase/WALs/slave1,60020,1404253868846/slave1%2C60020%2C1404253868846.1404254359067
2014-07-01 15:39:19,867 DEBUG [RpcServer.handler=30,port=60020] regionserver.HRegion: Flush requested on usertable,user9,1404228265526.bb79ae773f71289e0745068f68a0abd9.
2014-07-01 15:39:21,297 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-01 15:39:21,314 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 17080 synced till here 17075
2014-07-01 15:39:21,368 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1404253868846/slave1%2C60020%2C1404253868846.1404254359067 with entries=101, filesize=67.8m; new WAL /hbase/WALs/slave1,60020,1404253868846/slave1%2C60020%2C1404253868846.1404254361299
2014-07-01 15:39:23,151 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-01 15:39:23,711 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 17198 synced till here 17197
2014-07-01 15:39:23,726 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1404253868846/slave1%2C60020%2C1404253868846.1404254361299 with entries=118, filesize=71.5m; new WAL /hbase/WALs/slave1,60020,1404253868846/slave1%2C60020%2C1404253868846.1404254363151
2014-07-01 15:39:27,184 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-01 15:39:27,206 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 17311 synced till here 17308
2014-07-01 15:39:27,221 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1404253868846/slave1%2C60020%2C1404253868846.1404254363151 with entries=113, filesize=63.0m; new WAL /hbase/WALs/slave1,60020,1404253868846/slave1%2C60020%2C1404253868846.1404254367185
2014-07-01 15:39:29,192 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-01 15:39:29,195 INFO  [MemStoreFlusher.1] regionserver.DefaultStoreFlusher: Flushed, sequenceid=3442, memsize=405.3m, hasBloomFilter=true, into tmp file hdfs://master:54310/hbase/data/default/usertable/667c1f2c34fe37d48ba25a789d8aba98/.tmp/50f4de22ccd04828b18b93cc72b88964
2014-07-01 15:39:29,219 DEBUG [MemStoreFlusher.1] regionserver.HRegionFileSystem: Committing store file hdfs://master:54310/hbase/data/default/usertable/667c1f2c34fe37d48ba25a789d8aba98/.tmp/50f4de22ccd04828b18b93cc72b88964 as hdfs://master:54310/hbase/data/default/usertable/667c1f2c34fe37d48ba25a789d8aba98/family/50f4de22ccd04828b18b93cc72b88964
2014-07-01 15:39:29,246 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 17419 synced till here 17416
2014-07-01 15:39:29,248 INFO  [MemStoreFlusher.1] regionserver.HStore: Added hdfs://master:54310/hbase/data/default/usertable/667c1f2c34fe37d48ba25a789d8aba98/family/50f4de22ccd04828b18b93cc72b88964, entries=1475650, sequenceid=3442, filesize=105.0m
2014-07-01 15:39:29,249 INFO  [MemStoreFlusher.1] regionserver.HRegion: Finished memstore flush of ~873.8m/916272560, currentsize=420.4m/440788080 for region usertable,user7,1404228265526.667c1f2c34fe37d48ba25a789d8aba98. in 27824ms, sequenceid=3442, compaction requested=false
2014-07-01 15:39:29,249 DEBUG [MemStoreFlusher.1] regionserver.HRegion: Started memstore flush for usertable,user5,1404228265526.122e92eabfab38cb15a9965b21db25c5., current region memstore size 892.5m
2014-07-01 15:39:29,273 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1404253868846/slave1%2C60020%2C1404253868846.1404254367185 with entries=108, filesize=63.8m; new WAL /hbase/WALs/slave1,60020,1404253868846/slave1%2C60020%2C1404253868846.1404254369193
2014-07-01 15:39:29,273 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1404253868846/slave1%2C60020%2C1404253868846.1404254284627
2014-07-01 15:39:29,311 DEBUG [RpcServer.handler=3,port=60020] regionserver.HRegion: Flush requested on usertable,user7,1404228265526.667c1f2c34fe37d48ba25a789d8aba98.
2014-07-01 15:39:30,736 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-01 15:39:30,890 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 17525 synced till here 17509
2014-07-01 15:39:31,001 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1404253868846/slave1%2C60020%2C1404253868846.1404254369193 with entries=106, filesize=81.5m; new WAL /hbase/WALs/slave1,60020,1404253868846/slave1%2C60020%2C1404253868846.1404254370737
2014-07-01 15:39:31,134 DEBUG [MemStoreFlusher.1] util.FSUtils: DFS Client does not support most favored nodes create; using default create
2014-07-01 15:39:32,516 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-01 15:39:32,545 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 17627 synced till here 17620
2014-07-01 15:39:32,617 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1404253868846/slave1%2C60020%2C1404253868846.1404254370737 with entries=102, filesize=66.7m; new WAL /hbase/WALs/slave1,60020,1404253868846/slave1%2C60020%2C1404253868846.1404254372516
2014-07-01 15:39:33,946 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-01 15:39:34,494 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1404253868846/slave1%2C60020%2C1404253868846.1404254372516 with entries=112, filesize=61.3m; new WAL /hbase/WALs/slave1,60020,1404253868846/slave1%2C60020%2C1404253868846.1404254373946
2014-07-01 15:39:35,438 INFO  [MemStoreFlusher.0] regionserver.DefaultStoreFlusher: Flushed, sequenceid=3045, memsize=356.1m, hasBloomFilter=true, into tmp file hdfs://master:54310/hbase/data/default/usertable/9cef9e7d34f96f1d9b111be50040a208/.tmp/44a0953a47b54036b304d1558b547649
2014-07-01 15:39:35,489 DEBUG [MemStoreFlusher.0] regionserver.HRegionFileSystem: Committing store file hdfs://master:54310/hbase/data/default/usertable/9cef9e7d34f96f1d9b111be50040a208/.tmp/44a0953a47b54036b304d1558b547649 as hdfs://master:54310/hbase/data/default/usertable/9cef9e7d34f96f1d9b111be50040a208/family/44a0953a47b54036b304d1558b547649
2014-07-01 15:39:35,672 INFO  [MemStoreFlusher.0] regionserver.HStore: Added hdfs://master:54310/hbase/data/default/usertable/9cef9e7d34f96f1d9b111be50040a208/family/44a0953a47b54036b304d1558b547649, entries=1296560, sequenceid=3045, filesize=92.3m
2014-07-01 15:39:35,673 INFO  [MemStoreFlusher.0] regionserver.HRegion: Finished memstore flush of ~462.2m/484613920, currentsize=181.7m/190495040 for region usertable,,1404228265525.9cef9e7d34f96f1d9b111be50040a208. in 19122ms, sequenceid=3045, compaction requested=true
2014-07-01 15:39:35,673 DEBUG [MemStoreFlusher.0] regionserver.CompactSplitThread: Small Compaction requested: system; Because: MemStoreFlusher.0; compaction_queue=(0:21), split_queue=0, merge_queue=0
2014-07-01 15:39:35,674 DEBUG [MemStoreFlusher.0] regionserver.HRegion: Started memstore flush for usertable,user3,1404228265526.93253e37c284f3174309fede3f32339f., current region memstore size 894.1m
2014-07-01 15:39:35,755 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-01 15:39:35,786 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 17853 synced till here 17833
2014-07-01 15:39:35,880 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1404253868846/slave1%2C60020%2C1404253868846.1404254373946 with entries=114, filesize=72.6m; new WAL /hbase/WALs/slave1,60020,1404253868846/slave1%2C60020%2C1404253868846.1404254375755
2014-07-01 15:39:35,880 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1404253868846/slave1%2C60020%2C1404253868846.1404254288365
2014-07-01 15:39:35,880 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1404253868846/slave1%2C60020%2C1404253868846.1404254291197
2014-07-01 15:39:35,880 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1404253868846/slave1%2C60020%2C1404253868846.1404254293309
2014-07-01 15:39:35,880 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1404253868846/slave1%2C60020%2C1404253868846.1404254294869
2014-07-01 15:39:35,880 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1404253868846/slave1%2C60020%2C1404253868846.1404254296463
2014-07-01 15:39:35,880 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1404253868846/slave1%2C60020%2C1404253868846.1404254298565
2014-07-01 15:39:35,880 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1404253868846/slave1%2C60020%2C1404253868846.1404254300117
2014-07-01 15:39:35,880 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1404253868846/slave1%2C60020%2C1404253868846.1404254301762
2014-07-01 15:39:36,784 DEBUG [MemStoreFlusher.0] util.FSUtils: DFS Client does not support most favored nodes create; using default create
2014-07-01 15:39:38,414 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-01 15:39:38,447 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 17967 synced till here 17966
2014-07-01 15:39:38,862 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1404253868846/slave1%2C60020%2C1404253868846.1404254375755 with entries=114, filesize=63.3m; new WAL /hbase/WALs/slave1,60020,1404253868846/slave1%2C60020%2C1404253868846.1404254378414
2014-07-01 15:39:40,241 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-01 15:39:40,279 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 18054 synced till here 18051
2014-07-01 15:39:40,341 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1404253868846/slave1%2C60020%2C1404253868846.1404254378414 with entries=87, filesize=64.4m; new WAL /hbase/WALs/slave1,60020,1404253868846/slave1%2C60020%2C1404253868846.1404254380242
2014-07-01 15:39:41,174 DEBUG [RpcServer.handler=16,port=60020] regionserver.HRegion: Flush requested on usertable,,1404228265525.9cef9e7d34f96f1d9b111be50040a208.
2014-07-01 15:39:41,680 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-01 15:39:41,870 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 18195 synced till here 18193
2014-07-01 15:39:41,909 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1404253868846/slave1%2C60020%2C1404253868846.1404254380242 with entries=141, filesize=73.4m; new WAL /hbase/WALs/slave1,60020,1404253868846/slave1%2C60020%2C1404253868846.1404254381681
2014-07-01 15:39:43,625 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-01 15:39:43,654 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 18296 synced till here 18294
2014-07-01 15:39:44,227 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1404253868846/slave1%2C60020%2C1404253868846.1404254381681 with entries=101, filesize=62.8m; new WAL /hbase/WALs/slave1,60020,1404253868846/slave1%2C60020%2C1404253868846.1404254383626
2014-07-01 15:39:44,485 INFO  [MemStoreFlusher.1] regionserver.DefaultStoreFlusher: Flushed, sequenceid=3732, memsize=233.0m, hasBloomFilter=true, into tmp file hdfs://master:54310/hbase/data/default/usertable/122e92eabfab38cb15a9965b21db25c5/.tmp/9a97e36b7a674795b574e980910e9062
2014-07-01 15:39:44,985 DEBUG [MemStoreFlusher.1] regionserver.HRegionFileSystem: Committing store file hdfs://master:54310/hbase/data/default/usertable/122e92eabfab38cb15a9965b21db25c5/.tmp/9a97e36b7a674795b574e980910e9062 as hdfs://master:54310/hbase/data/default/usertable/122e92eabfab38cb15a9965b21db25c5/family/9a97e36b7a674795b574e980910e9062
2014-07-01 15:39:45,008 INFO  [MemStoreFlusher.1] regionserver.HStore: Added hdfs://master:54310/hbase/data/default/usertable/122e92eabfab38cb15a9965b21db25c5/family/9a97e36b7a674795b574e980910e9062, entries=848380, sequenceid=3732, filesize=60.4m
2014-07-01 15:39:45,008 INFO  [MemStoreFlusher.1] regionserver.HRegion: Finished memstore flush of ~902.9m/946771600, currentsize=231.2m/242411280 for region usertable,user5,1404228265526.122e92eabfab38cb15a9965b21db25c5. in 15759ms, sequenceid=3732, compaction requested=true
2014-07-01 15:39:45,009 DEBUG [MemStoreFlusher.1] regionserver.CompactSplitThread: Small Compaction requested: system; Because: MemStoreFlusher.1; compaction_queue=(0:22), split_queue=0, merge_queue=0
2014-07-01 15:39:45,009 DEBUG [MemStoreFlusher.1] regionserver.HRegion: Started memstore flush for usertable,user9,1404228265526.bb79ae773f71289e0745068f68a0abd9., current region memstore size 360.8m
2014-07-01 15:39:45,401 DEBUG [MemStoreFlusher.1] util.FSUtils: DFS Client does not support most favored nodes create; using default create
2014-07-01 15:39:46,091 DEBUG [RpcServer.handler=15,port=60020] regionserver.HRegion: Flush requested on usertable,user5,1404228265526.122e92eabfab38cb15a9965b21db25c5.
2014-07-01 15:39:46,112 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-01 15:39:46,156 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 18400 synced till here 18399
2014-07-01 15:39:46,174 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1404253868846/slave1%2C60020%2C1404253868846.1404254383626 with entries=104, filesize=63.3m; new WAL /hbase/WALs/slave1,60020,1404253868846/slave1%2C60020%2C1404253868846.1404254386113
2014-07-01 15:39:47,943 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-01 15:39:48,043 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 18531 synced till here 18525
2014-07-01 15:39:48,098 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1404253868846/slave1%2C60020%2C1404253868846.1404254386113 with entries=131, filesize=82.5m; new WAL /hbase/WALs/slave1,60020,1404253868846/slave1%2C60020%2C1404253868846.1404254387944
2014-07-01 15:39:49,842 INFO  [MemStoreFlusher.0] regionserver.DefaultStoreFlusher: Flushed, sequenceid=3807, memsize=239.1m, hasBloomFilter=true, into tmp file hdfs://master:54310/hbase/data/default/usertable/93253e37c284f3174309fede3f32339f/.tmp/ed6896cb011f48c8a51af3385f8f39dc
2014-07-01 15:39:49,853 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-01 15:39:49,868 DEBUG [MemStoreFlusher.0] regionserver.HRegionFileSystem: Committing store file hdfs://master:54310/hbase/data/default/usertable/93253e37c284f3174309fede3f32339f/.tmp/ed6896cb011f48c8a51af3385f8f39dc as hdfs://master:54310/hbase/data/default/usertable/93253e37c284f3174309fede3f32339f/family/ed6896cb011f48c8a51af3385f8f39dc
2014-07-01 15:39:49,886 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 18651 synced till here 18637
2014-07-01 15:39:49,898 INFO  [MemStoreFlusher.0] regionserver.HStore: Added hdfs://master:54310/hbase/data/default/usertable/93253e37c284f3174309fede3f32339f/family/ed6896cb011f48c8a51af3385f8f39dc, entries=870520, sequenceid=3807, filesize=61.9m
2014-07-01 15:39:49,898 INFO  [MemStoreFlusher.0] regionserver.HRegion: Finished memstore flush of ~897.4m/941009520, currentsize=203.6m/213541200 for region usertable,user3,1404228265526.93253e37c284f3174309fede3f32339f. in 14224ms, sequenceid=3807, compaction requested=true
2014-07-01 15:39:49,899 DEBUG [MemStoreFlusher.0] regionserver.CompactSplitThread: Small Compaction requested: system; Because: MemStoreFlusher.0; compaction_queue=(0:23), split_queue=0, merge_queue=0
2014-07-01 15:39:49,899 DEBUG [MemStoreFlusher.0] regionserver.HRegion: Started memstore flush for usertable,user7,1404228265526.667c1f2c34fe37d48ba25a789d8aba98., current region memstore size 746.7m
2014-07-01 15:39:49,999 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1404253868846/slave1%2C60020%2C1404253868846.1404254387944 with entries=120, filesize=70.3m; new WAL /hbase/WALs/slave1,60020,1404253868846/slave1%2C60020%2C1404253868846.1404254389854
2014-07-01 15:39:51,090 DEBUG [MemStoreFlusher.0] util.FSUtils: DFS Client does not support most favored nodes create; using default create
2014-07-01 15:39:52,773 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-01 15:39:52,798 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 18768 synced till here 18766
2014-07-01 15:39:52,805 INFO  [MemStoreFlusher.1] regionserver.DefaultStoreFlusher: Flushed, sequenceid=3820, memsize=149.2m, hasBloomFilter=true, into tmp file hdfs://master:54310/hbase/data/default/usertable/bb79ae773f71289e0745068f68a0abd9/.tmp/3146255461e7401681b8ae0e5f77640a
2014-07-01 15:39:52,817 DEBUG [MemStoreFlusher.1] regionserver.HRegionFileSystem: Committing store file hdfs://master:54310/hbase/data/default/usertable/bb79ae773f71289e0745068f68a0abd9/.tmp/3146255461e7401681b8ae0e5f77640a as hdfs://master:54310/hbase/data/default/usertable/bb79ae773f71289e0745068f68a0abd9/family/3146255461e7401681b8ae0e5f77640a
2014-07-01 15:39:52,830 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1404253868846/slave1%2C60020%2C1404253868846.1404254389854 with entries=117, filesize=63.6m; new WAL /hbase/WALs/slave1,60020,1404253868846/slave1%2C60020%2C1404253868846.1404254392773
2014-07-01 15:39:53,192 INFO  [MemStoreFlusher.1] regionserver.HStore: Added hdfs://master:54310/hbase/data/default/usertable/bb79ae773f71289e0745068f68a0abd9/family/3146255461e7401681b8ae0e5f77640a, entries=543090, sequenceid=3820, filesize=38.7m
2014-07-01 15:39:53,193 INFO  [MemStoreFlusher.1] regionserver.HRegion: Finished memstore flush of ~360.8m/378371120, currentsize=34.7m/36394160 for region usertable,user9,1404228265526.bb79ae773f71289e0745068f68a0abd9. in 8183ms, sequenceid=3820, compaction requested=true
2014-07-01 15:39:53,193 DEBUG [MemStoreFlusher.1] regionserver.CompactSplitThread: Small Compaction requested: system; Because: MemStoreFlusher.1; compaction_queue=(0:24), split_queue=0, merge_queue=0
2014-07-01 15:39:53,194 DEBUG [MemStoreFlusher.1] regionserver.HRegion: Started memstore flush for usertable,,1404228265525.9cef9e7d34f96f1d9b111be50040a208., current region memstore size 411.0m
2014-07-01 15:39:53,552 DEBUG [MemStoreFlusher.1] util.FSUtils: DFS Client does not support most favored nodes create; using default create
2014-07-01 15:39:54,039 DEBUG [RpcServer.handler=5,port=60020] regionserver.HRegion: Flush requested on usertable,user3,1404228265526.93253e37c284f3174309fede3f32339f.
2014-07-01 15:39:54,127 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-01 15:39:54,154 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 18871 synced till here 18854
2014-07-01 15:39:54,803 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1404253868846/slave1%2C60020%2C1404253868846.1404254392773 with entries=103, filesize=68.2m; new WAL /hbase/WALs/slave1,60020,1404253868846/slave1%2C60020%2C1404253868846.1404254394128
2014-07-01 15:39:54,803 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1404253868846/slave1%2C60020%2C1404253868846.1404254303564
2014-07-01 15:39:54,803 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1404253868846/slave1%2C60020%2C1404253868846.1404254306844
2014-07-01 15:39:54,803 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1404253868846/slave1%2C60020%2C1404253868846.1404254308949
2014-07-01 15:39:54,803 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1404253868846/slave1%2C60020%2C1404253868846.1404254310504
2014-07-01 15:39:54,803 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1404253868846/slave1%2C60020%2C1404253868846.1404254312623
2014-07-01 15:39:54,804 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1404253868846/slave1%2C60020%2C1404253868846.1404254314503
2014-07-01 15:39:54,804 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1404253868846/slave1%2C60020%2C1404253868846.1404254317304
2014-07-01 15:39:54,804 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1404253868846/slave1%2C60020%2C1404253868846.1404254320466
2014-07-01 15:39:54,804 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1404253868846/slave1%2C60020%2C1404253868846.1404254323093
2014-07-01 15:39:54,804 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1404253868846/slave1%2C60020%2C1404253868846.1404254326662
2014-07-01 15:39:54,804 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1404253868846/slave1%2C60020%2C1404253868846.1404254330705
2014-07-01 15:39:54,804 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1404253868846/slave1%2C60020%2C1404253868846.1404254334753
2014-07-01 15:39:54,804 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1404253868846/slave1%2C60020%2C1404253868846.1404254337508
2014-07-01 15:39:56,669 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-01 15:39:57,081 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 19007 synced till here 19000
2014-07-01 15:39:57,317 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1404253868846/slave1%2C60020%2C1404253868846.1404254394128 with entries=136, filesize=66.7m; new WAL /hbase/WALs/slave1,60020,1404253868846/slave1%2C60020%2C1404253868846.1404254396670
2014-07-01 15:39:58,605 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-01 15:39:58,927 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 19127 synced till here 19122
2014-07-01 15:39:58,957 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1404253868846/slave1%2C60020%2C1404253868846.1404254396670 with entries=120, filesize=64.3m; new WAL /hbase/WALs/slave1,60020,1404253868846/slave1%2C60020%2C1404253868846.1404254398605
2014-07-01 15:40:01,966 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-01 15:40:01,994 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 19235 synced till here 19234
2014-07-01 15:40:02,006 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1404253868846/slave1%2C60020%2C1404253868846.1404254398605 with entries=108, filesize=62.2m; new WAL /hbase/WALs/slave1,60020,1404253868846/slave1%2C60020%2C1404253868846.1404254401966
2014-07-01 15:40:04,135 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-01 15:40:04,173 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 19322 synced till here 19314
2014-07-01 15:40:04,265 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1404253868846/slave1%2C60020%2C1404253868846.1404254401966 with entries=87, filesize=66.5m; new WAL /hbase/WALs/slave1,60020,1404253868846/slave1%2C60020%2C1404253868846.1404254404136
2014-07-01 15:40:05,727 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-01 15:40:06,119 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 19451 synced till here 19448
2014-07-01 15:40:06,142 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1404253868846/slave1%2C60020%2C1404253868846.1404254404136 with entries=129, filesize=76.2m; new WAL /hbase/WALs/slave1,60020,1404253868846/slave1%2C60020%2C1404253868846.1404254405727
2014-07-01 15:40:08,295 INFO  [JvmPauseMonitor] util.JvmPauseMonitor: Detected pause in JVM or host machine (eg GC): pause of approximately 1104ms
GC pool 'ParNew' had collection(s): count=1 time=1319ms
2014-07-01 15:40:08,503 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-01 15:40:08,527 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 19570 synced till here 19563
2014-07-01 15:40:08,558 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1404253868846/slave1%2C60020%2C1404253868846.1404254405727 with entries=119, filesize=63.8m; new WAL /hbase/WALs/slave1,60020,1404253868846/slave1%2C60020%2C1404253868846.1404254408504
2014-07-01 15:40:10,335 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-01 15:40:10,394 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 19687 synced till here 19680
2014-07-01 15:40:10,419 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1404253868846/slave1%2C60020%2C1404253868846.1404254408504 with entries=117, filesize=64.5m; new WAL /hbase/WALs/slave1,60020,1404253868846/slave1%2C60020%2C1404253868846.1404254410335
2014-07-01 15:40:11,461 INFO  [MemStoreFlusher.1] regionserver.DefaultStoreFlusher: Flushed, sequenceid=3742, memsize=412.8m, hasBloomFilter=true, into tmp file hdfs://master:54310/hbase/data/default/usertable/9cef9e7d34f96f1d9b111be50040a208/.tmp/b284cd02aa7e4961ba8fce12cba46c67
2014-07-01 15:40:11,493 DEBUG [MemStoreFlusher.1] regionserver.HRegionFileSystem: Committing store file hdfs://master:54310/hbase/data/default/usertable/9cef9e7d34f96f1d9b111be50040a208/.tmp/b284cd02aa7e4961ba8fce12cba46c67 as hdfs://master:54310/hbase/data/default/usertable/9cef9e7d34f96f1d9b111be50040a208/family/b284cd02aa7e4961ba8fce12cba46c67
2014-07-01 15:40:11,512 INFO  [MemStoreFlusher.1] regionserver.HStore: Added hdfs://master:54310/hbase/data/default/usertable/9cef9e7d34f96f1d9b111be50040a208/family/b284cd02aa7e4961ba8fce12cba46c67, entries=1503040, sequenceid=3742, filesize=107.0m
2014-07-01 15:40:11,512 INFO  [MemStoreFlusher.1] regionserver.HRegion: Finished memstore flush of ~417.6m/437879280, currentsize=233.8m/245127360 for region usertable,,1404228265525.9cef9e7d34f96f1d9b111be50040a208. in 18319ms, sequenceid=3742, compaction requested=true
2014-07-01 15:40:11,513 DEBUG [MemStoreFlusher.1] regionserver.CompactSplitThread: Small Compaction requested: system; Because: MemStoreFlusher.1; compaction_queue=(0:25), split_queue=0, merge_queue=0
2014-07-01 15:40:11,513 DEBUG [MemStoreFlusher.1] regionserver.HRegion: Started memstore flush for usertable,user5,1404228265526.122e92eabfab38cb15a9965b21db25c5., current region memstore size 597.2m
2014-07-01 15:40:12,263 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-01 15:40:12,318 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 19795 synced till here 19794
2014-07-01 15:40:12,423 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1404253868846/slave1%2C60020%2C1404253868846.1404254410335 with entries=108, filesize=64.9m; new WAL /hbase/WALs/slave1,60020,1404253868846/slave1%2C60020%2C1404253868846.1404254412264
2014-07-01 15:40:12,462 INFO  [MemStoreFlusher.0] regionserver.DefaultStoreFlusher: Flushed, sequenceid=3881, memsize=428.3m, hasBloomFilter=true, into tmp file hdfs://master:54310/hbase/data/default/usertable/667c1f2c34fe37d48ba25a789d8aba98/.tmp/244e3443161840b5a95504a408c4edc2
2014-07-01 15:40:12,486 DEBUG [MemStoreFlusher.0] regionserver.HRegionFileSystem: Committing store file hdfs://master:54310/hbase/data/default/usertable/667c1f2c34fe37d48ba25a789d8aba98/.tmp/244e3443161840b5a95504a408c4edc2 as hdfs://master:54310/hbase/data/default/usertable/667c1f2c34fe37d48ba25a789d8aba98/family/244e3443161840b5a95504a408c4edc2
2014-07-01 15:40:12,501 INFO  [MemStoreFlusher.0] regionserver.HStore: Added hdfs://master:54310/hbase/data/default/usertable/667c1f2c34fe37d48ba25a789d8aba98/family/244e3443161840b5a95504a408c4edc2, entries=1559270, sequenceid=3881, filesize=111.0m
2014-07-01 15:40:12,501 INFO  [MemStoreFlusher.0] regionserver.HRegion: Finished memstore flush of ~755.2m/791895360, currentsize=280.1m/293675280 for region usertable,user7,1404228265526.667c1f2c34fe37d48ba25a789d8aba98. in 22602ms, sequenceid=3881, compaction requested=false
2014-07-01 15:40:12,501 DEBUG [MemStoreFlusher.0] regionserver.HRegion: Started memstore flush for usertable,user3,1404228265526.93253e37c284f3174309fede3f32339f., current region memstore size 482.8m
2014-07-01 15:40:12,507 DEBUG [RpcServer.handler=22,port=60020] regionserver.HRegion: Flush requested on usertable,user7,1404228265526.667c1f2c34fe37d48ba25a789d8aba98.
2014-07-01 15:40:12,555 DEBUG [MemStoreFlusher.1] util.FSUtils: DFS Client does not support most favored nodes create; using default create
2014-07-01 15:40:12,829 DEBUG [MemStoreFlusher.0] util.FSUtils: DFS Client does not support most favored nodes create; using default create
2014-07-01 15:40:13,633 DEBUG [RpcServer.handler=47,port=60020] regionserver.HRegion: Flush requested on usertable,,1404228265525.9cef9e7d34f96f1d9b111be50040a208.
2014-07-01 15:40:14,924 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-01 15:40:15,408 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 19988 synced till here 19987
2014-07-01 15:40:15,431 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1404253868846/slave1%2C60020%2C1404253868846.1404254412264 with entries=193, filesize=105.8m; new WAL /hbase/WALs/slave1,60020,1404253868846/slave1%2C60020%2C1404253868846.1404254414925
2014-07-01 15:40:15,431 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1404253868846/slave1%2C60020%2C1404253868846.1404254339904
2014-07-01 15:40:15,431 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1404253868846/slave1%2C60020%2C1404253868846.1404254342669
2014-07-01 15:40:15,431 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1404253868846/slave1%2C60020%2C1404253868846.1404254346835
2014-07-01 15:40:15,431 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1404253868846/slave1%2C60020%2C1404253868846.1404254348815
2014-07-01 15:40:15,431 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1404253868846/slave1%2C60020%2C1404253868846.1404254351096
2014-07-01 15:40:15,431 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1404253868846/slave1%2C60020%2C1404253868846.1404254353196
2014-07-01 15:40:15,431 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1404253868846/slave1%2C60020%2C1404253868846.1404254356697
2014-07-01 15:40:15,431 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1404253868846/slave1%2C60020%2C1404253868846.1404254359067
2014-07-01 15:40:15,432 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1404253868846/slave1%2C60020%2C1404253868846.1404254361299
2014-07-01 15:40:15,432 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1404253868846/slave1%2C60020%2C1404253868846.1404254363151
2014-07-01 15:40:15,432 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1404253868846/slave1%2C60020%2C1404253868846.1404254367185
2014-07-01 15:40:17,024 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-01 15:40:17,074 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 20086 synced till here 20074
2014-07-01 15:40:17,611 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1404253868846/slave1%2C60020%2C1404253868846.1404254414925 with entries=98, filesize=70.6m; new WAL /hbase/WALs/slave1,60020,1404253868846/slave1%2C60020%2C1404253868846.1404254417024
2014-07-01 15:40:19,602 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-01 15:40:21,443 INFO  [JvmPauseMonitor] util.JvmPauseMonitor: Detected pause in JVM or host machine (eg GC): pause of approximately 1338ms
GC pool 'ParNew' had collection(s): count=1 time=1726ms
2014-07-01 15:40:21,503 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 20205 synced till here 20177
2014-07-01 15:40:21,689 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1404253868846/slave1%2C60020%2C1404253868846.1404254417024 with entries=119, filesize=80.4m; new WAL /hbase/WALs/slave1,60020,1404253868846/slave1%2C60020%2C1404253868846.1404254419602
2014-07-01 15:40:23,541 INFO  [JvmPauseMonitor] util.JvmPauseMonitor: Detected pause in JVM or host machine (eg GC): pause of approximately 1007ms
GC pool 'ParNew' had collection(s): count=1 time=1061ms
2014-07-01 15:40:23,855 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-01 15:40:23,940 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 20322 synced till here 20284
2014-07-01 15:40:24,227 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1404253868846/slave1%2C60020%2C1404253868846.1404254419602 with entries=117, filesize=91.4m; new WAL /hbase/WALs/slave1,60020,1404253868846/slave1%2C60020%2C1404253868846.1404254423856
2014-07-01 15:40:26,247 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-01 15:40:26,289 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 20444 synced till here 20423
2014-07-01 15:40:27,256 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1404253868846/slave1%2C60020%2C1404253868846.1404254423856 with entries=122, filesize=98.2m; new WAL /hbase/WALs/slave1,60020,1404253868846/slave1%2C60020%2C1404253868846.1404254426247
2014-07-01 15:40:28,995 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-01 15:40:29,085 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 20595 synced till here 20564
2014-07-01 15:40:29,239 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1404253868846/slave1%2C60020%2C1404253868846.1404254426247 with entries=151, filesize=93.2m; new WAL /hbase/WALs/slave1,60020,1404253868846/slave1%2C60020%2C1404253868846.1404254428996
2014-07-01 15:40:30,002 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-01 15:40:30,860 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 20718 synced till here 20693
2014-07-01 15:40:31,026 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1404253868846/slave1%2C60020%2C1404253868846.1404254428996 with entries=123, filesize=78.7m; new WAL /hbase/WALs/slave1,60020,1404253868846/slave1%2C60020%2C1404253868846.1404254430002
2014-07-01 15:40:31,704 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-01 15:40:31,845 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1404253868846/slave1%2C60020%2C1404253868846.1404254430002 with entries=89, filesize=63.7m; new WAL /hbase/WALs/slave1,60020,1404253868846/slave1%2C60020%2C1404253868846.1404254431704
2014-07-01 15:40:34,085 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-01 15:40:34,104 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 20903 synced till here 20889
2014-07-01 15:40:34,190 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1404253868846/slave1%2C60020%2C1404253868846.1404254431704 with entries=96, filesize=79.9m; new WAL /hbase/WALs/slave1,60020,1404253868846/slave1%2C60020%2C1404253868846.1404254434087
2014-07-01 15:40:35,427 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-01 15:40:35,465 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 21001 synced till here 21000
2014-07-01 15:40:35,495 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1404253868846/slave1%2C60020%2C1404253868846.1404254434087 with entries=98, filesize=62.9m; new WAL /hbase/WALs/slave1,60020,1404253868846/slave1%2C60020%2C1404253868846.1404254435428
2014-07-01 15:40:36,738 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-01 15:40:36,767 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 21116 synced till here 21112
2014-07-01 15:40:36,801 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1404253868846/slave1%2C60020%2C1404253868846.1404254435428 with entries=115, filesize=64.3m; new WAL /hbase/WALs/slave1,60020,1404253868846/slave1%2C60020%2C1404253868846.1404254436739
2014-07-01 15:40:38,487 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-01 15:40:39,090 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1404253868846/slave1%2C60020%2C1404253868846.1404254436739 with entries=141, filesize=81.1m; new WAL /hbase/WALs/slave1,60020,1404253868846/slave1%2C60020%2C1404253868846.1404254438487
2014-07-01 15:40:39,245 INFO  [MemStoreFlusher.0] regionserver.DefaultStoreFlusher: Flushed, sequenceid=4085, memsize=469.0m, hasBloomFilter=true, into tmp file hdfs://master:54310/hbase/data/default/usertable/93253e37c284f3174309fede3f32339f/.tmp/1d8435219ed7430e86562f3a0fe4b535
2014-07-01 15:40:39,259 DEBUG [MemStoreFlusher.0] regionserver.HRegionFileSystem: Committing store file hdfs://master:54310/hbase/data/default/usertable/93253e37c284f3174309fede3f32339f/.tmp/1d8435219ed7430e86562f3a0fe4b535 as hdfs://master:54310/hbase/data/default/usertable/93253e37c284f3174309fede3f32339f/family/1d8435219ed7430e86562f3a0fe4b535
2014-07-01 15:40:39,271 INFO  [MemStoreFlusher.0] regionserver.HStore: Added hdfs://master:54310/hbase/data/default/usertable/93253e37c284f3174309fede3f32339f/family/1d8435219ed7430e86562f3a0fe4b535, entries=1707450, sequenceid=4085, filesize=121.6m
2014-07-01 15:40:39,272 INFO  [MemStoreFlusher.0] regionserver.HRegion: Finished memstore flush of ~482.8m/506232160, currentsize=464.0m/486552400 for region usertable,user3,1404228265526.93253e37c284f3174309fede3f32339f. in 26771ms, sequenceid=4085, compaction requested=true
2014-07-01 15:40:39,273 DEBUG [MemStoreFlusher.0] regionserver.CompactSplitThread: Small Compaction requested: system; Because: MemStoreFlusher.0; compaction_queue=(0:26), split_queue=0, merge_queue=0
2014-07-01 15:40:39,273 DEBUG [MemStoreFlusher.0] regionserver.HRegion: Started memstore flush for usertable,user7,1404228265526.667c1f2c34fe37d48ba25a789d8aba98., current region memstore size 744.5m
2014-07-01 15:40:39,303 DEBUG [RpcServer.handler=7,port=60020] regionserver.HRegion: Flush requested on usertable,user3,1404228265526.93253e37c284f3174309fede3f32339f.
2014-07-01 15:40:39,919 DEBUG [MemStoreFlusher.0] util.FSUtils: DFS Client does not support most favored nodes create; using default create
2014-07-01 15:40:41,423 DEBUG [RpcServer.handler=42,port=60020] regionserver.HRegion: Flush requested on usertable,user9,1404228265526.bb79ae773f71289e0745068f68a0abd9.
2014-07-01 15:40:41,501 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-01 15:40:41,525 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 21376 synced till here 21375
2014-07-01 15:40:41,534 INFO  [MemStoreFlusher.1] regionserver.DefaultStoreFlusher: Flushed, sequenceid=4077, memsize=580.0m, hasBloomFilter=true, into tmp file hdfs://master:54310/hbase/data/default/usertable/122e92eabfab38cb15a9965b21db25c5/.tmp/4518275a7d1645c3904d642c1676ae50
2014-07-01 15:40:41,539 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1404253868846/slave1%2C60020%2C1404253868846.1404254438487 with entries=119, filesize=63.0m; new WAL /hbase/WALs/slave1,60020,1404253868846/slave1%2C60020%2C1404253868846.1404254441501
2014-07-01 15:40:41,547 DEBUG [MemStoreFlusher.1] regionserver.HRegionFileSystem: Committing store file hdfs://master:54310/hbase/data/default/usertable/122e92eabfab38cb15a9965b21db25c5/.tmp/4518275a7d1645c3904d642c1676ae50 as hdfs://master:54310/hbase/data/default/usertable/122e92eabfab38cb15a9965b21db25c5/family/4518275a7d1645c3904d642c1676ae50
2014-07-01 15:40:41,561 INFO  [MemStoreFlusher.1] regionserver.HStore: Added hdfs://master:54310/hbase/data/default/usertable/122e92eabfab38cb15a9965b21db25c5/family/4518275a7d1645c3904d642c1676ae50, entries=2111730, sequenceid=4077, filesize=150.3m
2014-07-01 15:40:41,562 INFO  [MemStoreFlusher.1] regionserver.HRegion: Finished memstore flush of ~599.0m/628112400, currentsize=491.0m/514812720 for region usertable,user5,1404228265526.122e92eabfab38cb15a9965b21db25c5. in 30049ms, sequenceid=4077, compaction requested=true
2014-07-01 15:40:41,562 DEBUG [MemStoreFlusher.1] regionserver.CompactSplitThread: Small Compaction requested: system; Because: MemStoreFlusher.1; compaction_queue=(0:27), split_queue=0, merge_queue=0
2014-07-01 15:40:41,562 DEBUG [MemStoreFlusher.1] regionserver.HRegion: Started memstore flush for usertable,,1404228265525.9cef9e7d34f96f1d9b111be50040a208., current region memstore size 506.2m
2014-07-01 15:40:41,676 DEBUG [RpcServer.handler=33,port=60020] regionserver.HRegion: Flush requested on usertable,user5,1404228265526.122e92eabfab38cb15a9965b21db25c5.
2014-07-01 15:40:41,919 DEBUG [MemStoreFlusher.1] util.FSUtils: DFS Client does not support most favored nodes create; using default create
2014-07-01 15:40:42,389 DEBUG [regionserver60020-smallCompactions-1404253964856] regionserver.HRegionFileSystem: Committing store file hdfs://master:54310/hbase/data/default/usertable/667c1f2c34fe37d48ba25a789d8aba98/.tmp/56200eedd338491786b04080265aa716 as hdfs://master:54310/hbase/data/default/usertable/667c1f2c34fe37d48ba25a789d8aba98/family/56200eedd338491786b04080265aa716
2014-07-01 15:40:42,590 DEBUG [regionserver60020-smallCompactions-1404253964856] regionserver.HStore: Removing store files after compaction...
2014-07-01 15:40:42,605 DEBUG [regionserver60020-smallCompactions-1404253964856] backup.HFileArchiver: Finished archiving from class org.apache.hadoop.hbase.backup.HFileArchiver$FileableStoreFile, file:hdfs://master:54310/hbase/data/default/usertable/667c1f2c34fe37d48ba25a789d8aba98/family/dde913fa18fc4787ba86eb9aa2d45ab4, to hdfs://master:54310/hbase/archive/data/default/usertable/667c1f2c34fe37d48ba25a789d8aba98/family/dde913fa18fc4787ba86eb9aa2d45ab4
2014-07-01 15:40:42,611 DEBUG [regionserver60020-smallCompactions-1404253964856] backup.HFileArchiver: Finished archiving from class org.apache.hadoop.hbase.backup.HFileArchiver$FileableStoreFile, file:hdfs://master:54310/hbase/data/default/usertable/667c1f2c34fe37d48ba25a789d8aba98/family/448a712f5be34ade9a16c57c0c793359, to hdfs://master:54310/hbase/archive/data/default/usertable/667c1f2c34fe37d48ba25a789d8aba98/family/448a712f5be34ade9a16c57c0c793359
2014-07-01 15:40:42,615 DEBUG [regionserver60020-smallCompactions-1404253964856] backup.HFileArchiver: Finished archiving from class org.apache.hadoop.hbase.backup.HFileArchiver$FileableStoreFile, file:hdfs://master:54310/hbase/data/default/usertable/667c1f2c34fe37d48ba25a789d8aba98/family/5604109a72ba430fa33c345ec291f6cc, to hdfs://master:54310/hbase/archive/data/default/usertable/667c1f2c34fe37d48ba25a789d8aba98/family/5604109a72ba430fa33c345ec291f6cc
2014-07-01 15:40:42,620 DEBUG [regionserver60020-smallCompactions-1404253964856] backup.HFileArchiver: Finished archiving from class org.apache.hadoop.hbase.backup.HFileArchiver$FileableStoreFile, file:hdfs://master:54310/hbase/data/default/usertable/667c1f2c34fe37d48ba25a789d8aba98/family/72abfd2537aa4422a8b06003dfff6e47, to hdfs://master:54310/hbase/archive/data/default/usertable/667c1f2c34fe37d48ba25a789d8aba98/family/72abfd2537aa4422a8b06003dfff6e47
2014-07-01 15:40:42,627 DEBUG [regionserver60020-smallCompactions-1404253964856] backup.HFileArchiver: Finished archiving from class org.apache.hadoop.hbase.backup.HFileArchiver$FileableStoreFile, file:hdfs://master:54310/hbase/data/default/usertable/667c1f2c34fe37d48ba25a789d8aba98/family/92895d11141e499989cdac0102a760ea, to hdfs://master:54310/hbase/archive/data/default/usertable/667c1f2c34fe37d48ba25a789d8aba98/family/92895d11141e499989cdac0102a760ea
2014-07-01 15:40:42,636 DEBUG [regionserver60020-smallCompactions-1404253964856] backup.HFileArchiver: Finished archiving from class org.apache.hadoop.hbase.backup.HFileArchiver$FileableStoreFile, file:hdfs://master:54310/hbase/data/default/usertable/667c1f2c34fe37d48ba25a789d8aba98/family/b50947a863cd4f888cc97a27e8dab5f4, to hdfs://master:54310/hbase/archive/data/default/usertable/667c1f2c34fe37d48ba25a789d8aba98/family/b50947a863cd4f888cc97a27e8dab5f4
2014-07-01 15:40:42,640 DEBUG [regionserver60020-smallCompactions-1404253964856] backup.HFileArchiver: Finished archiving from class org.apache.hadoop.hbase.backup.HFileArchiver$FileableStoreFile, file:hdfs://master:54310/hbase/data/default/usertable/667c1f2c34fe37d48ba25a789d8aba98/family/925f4c5bb4d949f38b040fc6a1bcf5a2, to hdfs://master:54310/hbase/archive/data/default/usertable/667c1f2c34fe37d48ba25a789d8aba98/family/925f4c5bb4d949f38b040fc6a1bcf5a2
2014-07-01 15:40:42,640 INFO  [regionserver60020-smallCompactions-1404253964856] regionserver.HStore: Completed major compaction of 7 file(s) in family of usertable,user7,1404228265526.667c1f2c34fe37d48ba25a789d8aba98. into 56200eedd338491786b04080265aa716(size=388.9m), total size for store is 604.9m. This selection was in queue for 0sec, and took 1mins, 30sec to execute.
2014-07-01 15:40:42,640 INFO  [regionserver60020-smallCompactions-1404253964856] regionserver.CompactSplitThread: Completed compaction: Request = regionName=usertable,user7,1404228265526.667c1f2c34fe37d48ba25a789d8aba98., storeName=family, fileCount=7, fileSize=463.6m, priority=13, time=74463509958831; duration=1mins, 30sec
2014-07-01 15:40:42,640 DEBUG [regionserver60020-smallCompactions-1404253964856] regionserver.CompactSplitThread: CompactSplitThread Status: compaction_queue=(0:27), split_queue=0, merge_queue=0
2014-07-01 15:40:42,641 DEBUG [regionserver60020-smallCompactions-1404253964856] compactions.RatioBasedCompactionPolicy: Selecting compaction from 9 store files, 0 compacting, 9 eligible, 20 blocking
2014-07-01 15:40:42,641 DEBUG [regionserver60020-smallCompactions-1404253964856] compactions.ExploringCompactionPolicy: Exploring compaction algorithm has selected 9 files of size 741534103 starting at candidate #0 after considering 28 permutations with 28 in ratio
2014-07-01 15:40:42,641 DEBUG [regionserver60020-smallCompactions-1404253964856] regionserver.HStore: 93253e37c284f3174309fede3f32339f - family: Initiating major compaction
2014-07-01 15:40:42,642 INFO  [regionserver60020-smallCompactions-1404253964856] regionserver.HRegion: Starting compaction on family in region usertable,user3,1404228265526.93253e37c284f3174309fede3f32339f.
2014-07-01 15:40:42,642 INFO  [regionserver60020-smallCompactions-1404253964856] regionserver.HStore: Starting compaction of 9 file(s) in family of usertable,user3,1404228265526.93253e37c284f3174309fede3f32339f. into tmpdir=hdfs://master:54310/hbase/data/default/usertable/93253e37c284f3174309fede3f32339f/.tmp, totalSize=707.2m
2014-07-01 15:40:42,642 DEBUG [regionserver60020-smallCompactions-1404253964856] compactions.Compactor: Compacting hdfs://master:54310/hbase/data/default/usertable/93253e37c284f3174309fede3f32339f/family/0f786ade400f4409a3ed4e0f9a493553, keycount=213030, bloomtype=ROW, size=151.8m, encoding=NONE, seqNum=1142, earliestPutTs=1404228281603
2014-07-01 15:40:42,642 DEBUG [regionserver60020-smallCompactions-1404253964856] compactions.Compactor: Compacting hdfs://master:54310/hbase/data/default/usertable/93253e37c284f3174309fede3f32339f/family/a2f5c30d99ef4611ac44a48e6c238e5e, keycount=61687, bloomtype=ROW, size=44.0m, encoding=NONE, seqNum=1448, earliestPutTs=1404254021311
2014-07-01 15:40:42,642 DEBUG [regionserver60020-smallCompactions-1404253964856] compactions.Compactor: Compacting hdfs://master:54310/hbase/data/default/usertable/93253e37c284f3174309fede3f32339f/family/f674b967455d4849b06b82ffdb29ea0e, keycount=54181, bloomtype=ROW, size=38.6m, encoding=NONE, seqNum=1669, earliestPutTs=1404254069487
2014-07-01 15:40:42,642 DEBUG [regionserver60020-smallCompactions-1404253964856] compactions.Compactor: Compacting hdfs://master:54310/hbase/data/default/usertable/93253e37c284f3174309fede3f32339f/family/5294515babe64f0f9f64bceccfbb78aa, keycount=82356, bloomtype=ROW, size=58.7m, encoding=NONE, seqNum=2034, earliestPutTs=1404254097826
2014-07-01 15:40:42,642 DEBUG [regionserver60020-smallCompactions-1404253964856] compactions.Compactor: Compacting hdfs://master:54310/hbase/data/default/usertable/93253e37c284f3174309fede3f32339f/family/847eed712227414fac5ea1ff40f653a1, keycount=74527, bloomtype=ROW, size=53.1m, encoding=NONE, seqNum=2278, earliestPutTs=1404254148069
2014-07-01 15:40:42,642 DEBUG [regionserver60020-smallCompactions-1404253964856] compactions.Compactor: Compacting hdfs://master:54310/hbase/data/default/usertable/93253e37c284f3174309fede3f32339f/family/a1918f1d24b6451792a0de5913dea691, keycount=99287, bloomtype=ROW, size=70.8m, encoding=NONE, seqNum=2711, earliestPutTs=1404254210667
2014-07-01 15:40:42,643 DEBUG [regionserver60020-smallCompactions-1404253964856] compactions.Compactor: Compacting hdfs://master:54310/hbase/data/default/usertable/93253e37c284f3174309fede3f32339f/family/757063605014422bb142804470c2dfd7, keycount=149661, bloomtype=ROW, size=106.5m, encoding=NONE, seqNum=3282, earliestPutTs=1404254270288
2014-07-01 15:40:42,643 DEBUG [regionserver60020-smallCompactions-1404253964856] compactions.Compactor: Compacting hdfs://master:54310/hbase/data/default/usertable/93253e37c284f3174309fede3f32339f/family/ed6896cb011f48c8a51af3385f8f39dc, keycount=87052, bloomtype=ROW, size=61.9m, encoding=NONE, seqNum=3807, earliestPutTs=1404254356597
2014-07-01 15:40:42,643 DEBUG [regionserver60020-smallCompactions-1404253964856] compactions.Compactor: Compacting hdfs://master:54310/hbase/data/default/usertable/93253e37c284f3174309fede3f32339f/family/1d8435219ed7430e86562f3a0fe4b535, keycount=170745, bloomtype=ROW, size=121.6m, encoding=NONE, seqNum=4085, earliestPutTs=1404254375675
2014-07-01 15:40:43,429 DEBUG [regionserver60020-smallCompactions-1404253964856] util.FSUtils: DFS Client does not support most favored nodes create; using default create
2014-07-01 15:40:43,722 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-01 15:40:43,853 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1404253868846/slave1%2C60020%2C1404253868846.1404254441501 with entries=104, filesize=61.7m; new WAL /hbase/WALs/slave1,60020,1404253868846/slave1%2C60020%2C1404253868846.1404254443723
2014-07-01 15:40:43,854 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1404253868846/slave1%2C60020%2C1404253868846.1404254369193
2014-07-01 15:40:43,854 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1404253868846/slave1%2C60020%2C1404253868846.1404254370737
2014-07-01 15:40:43,854 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1404253868846/slave1%2C60020%2C1404253868846.1404254372516
2014-07-01 15:40:43,854 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1404253868846/slave1%2C60020%2C1404253868846.1404254373946
2014-07-01 15:40:43,854 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1404253868846/slave1%2C60020%2C1404253868846.1404254375755
2014-07-01 15:40:43,854 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1404253868846/slave1%2C60020%2C1404253868846.1404254378414
2014-07-01 15:40:43,854 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1404253868846/slave1%2C60020%2C1404253868846.1404254380242
2014-07-01 15:40:43,854 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1404253868846/slave1%2C60020%2C1404253868846.1404254381681
2014-07-01 15:40:45,676 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-01 15:40:45,701 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 21589 synced till here 21586
2014-07-01 15:40:45,710 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1404253868846/slave1%2C60020%2C1404253868846.1404254443723 with entries=109, filesize=62.9m; new WAL /hbase/WALs/slave1,60020,1404253868846/slave1%2C60020%2C1404253868846.1404254445676
2014-07-01 15:40:48,892 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-01 15:40:48,939 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1404253868846/slave1%2C60020%2C1404253868846.1404254445676 with entries=109, filesize=62.4m; new WAL /hbase/WALs/slave1,60020,1404253868846/slave1%2C60020%2C1404253868846.1404254448893
2014-07-01 15:40:50,955 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-01 15:40:50,969 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 21809 synced till here 21805
2014-07-01 15:40:51,029 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1404253868846/slave1%2C60020%2C1404253868846.1404254448893 with entries=111, filesize=64.1m; new WAL /hbase/WALs/slave1,60020,1404253868846/slave1%2C60020%2C1404253868846.1404254450955
2014-07-01 15:40:53,934 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-01 15:40:54,023 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 21932 synced till here 21922
2014-07-01 15:40:54,121 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1404253868846/slave1%2C60020%2C1404253868846.1404254450955 with entries=123, filesize=68.0m; new WAL /hbase/WALs/slave1,60020,1404253868846/slave1%2C60020%2C1404253868846.1404254453934
2014-07-01 15:40:56,163 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-01 15:40:56,202 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 22052 synced till here 22050
2014-07-01 15:40:56,213 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1404253868846/slave1%2C60020%2C1404253868846.1404254453934 with entries=120, filesize=63.5m; new WAL /hbase/WALs/slave1,60020,1404253868846/slave1%2C60020%2C1404253868846.1404254456164
2014-07-01 15:40:57,843 INFO  [MemStoreFlusher.1] regionserver.DefaultStoreFlusher: Flushed, sequenceid=4628, memsize=469.3m, hasBloomFilter=true, into tmp file hdfs://master:54310/hbase/data/default/usertable/9cef9e7d34f96f1d9b111be50040a208/.tmp/c69d742b517d48f3903c54a7145fa3fe
2014-07-01 15:40:57,849 INFO  [MemStoreFlusher.0] regionserver.DefaultStoreFlusher: Flushed, sequenceid=4315, memsize=484.6m, hasBloomFilter=true, into tmp file hdfs://master:54310/hbase/data/default/usertable/667c1f2c34fe37d48ba25a789d8aba98/.tmp/a5468df3646d43fda8dcfd231b11facb
2014-07-01 15:40:57,877 DEBUG [MemStoreFlusher.1] regionserver.HRegionFileSystem: Committing store file hdfs://master:54310/hbase/data/default/usertable/9cef9e7d34f96f1d9b111be50040a208/.tmp/c69d742b517d48f3903c54a7145fa3fe as hdfs://master:54310/hbase/data/default/usertable/9cef9e7d34f96f1d9b111be50040a208/family/c69d742b517d48f3903c54a7145fa3fe
2014-07-01 15:40:57,879 DEBUG [MemStoreFlusher.0] regionserver.HRegionFileSystem: Committing store file hdfs://master:54310/hbase/data/default/usertable/667c1f2c34fe37d48ba25a789d8aba98/.tmp/a5468df3646d43fda8dcfd231b11facb as hdfs://master:54310/hbase/data/default/usertable/667c1f2c34fe37d48ba25a789d8aba98/family/a5468df3646d43fda8dcfd231b11facb
2014-07-01 15:40:57,898 INFO  [MemStoreFlusher.0] regionserver.HStore: Added hdfs://master:54310/hbase/data/default/usertable/667c1f2c34fe37d48ba25a789d8aba98/family/a5468df3646d43fda8dcfd231b11facb, entries=1764560, sequenceid=4315, filesize=125.7m
2014-07-01 15:40:57,898 INFO  [MemStoreFlusher.0] regionserver.HRegion: Finished memstore flush of ~744.5m/780641360, currentsize=192.6m/202003920 for region usertable,user7,1404228265526.667c1f2c34fe37d48ba25a789d8aba98. in 18625ms, sequenceid=4315, compaction requested=true
2014-07-01 15:40:57,899 INFO  [MemStoreFlusher.1] regionserver.HStore: Added hdfs://master:54310/hbase/data/default/usertable/9cef9e7d34f96f1d9b111be50040a208/family/c69d742b517d48f3903c54a7145fa3fe, entries=1708670, sequenceid=4628, filesize=121.7m
2014-07-01 15:40:57,899 INFO  [MemStoreFlusher.1] regionserver.HRegion: Finished memstore flush of ~506.2m/530786400, currentsize=164.5m/172439200 for region usertable,,1404228265525.9cef9e7d34f96f1d9b111be50040a208. in 16337ms, sequenceid=4628, compaction requested=true
2014-07-01 15:40:57,899 DEBUG [MemStoreFlusher.0] regionserver.CompactSplitThread: Small Compaction requested: system; Because: MemStoreFlusher.0; compaction_queue=(0:27), split_queue=0, merge_queue=0
2014-07-01 15:40:57,899 DEBUG [MemStoreFlusher.1] regionserver.CompactSplitThread: Small Compaction requested: system; Because: MemStoreFlusher.1; compaction_queue=(0:28), split_queue=0, merge_queue=0
2014-07-01 15:40:57,899 DEBUG [MemStoreFlusher.0] regionserver.HRegion: Started memstore flush for usertable,user3,1404228265526.93253e37c284f3174309fede3f32339f., current region memstore size 658.3m
2014-07-01 15:40:57,899 DEBUG [MemStoreFlusher.1] regionserver.HRegion: Started memstore flush for usertable,user9,1404228265526.bb79ae773f71289e0745068f68a0abd9., current region memstore size 309.3m
2014-07-01 15:40:58,187 DEBUG [MemStoreFlusher.1] util.FSUtils: DFS Client does not support most favored nodes create; using default create
2014-07-01 15:40:58,545 DEBUG [MemStoreFlusher.0] util.FSUtils: DFS Client does not support most favored nodes create; using default create
2014-07-01 15:40:58,606 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-01 15:40:58,653 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 22151 synced till here 22150
2014-07-01 15:40:58,683 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1404253868846/slave1%2C60020%2C1404253868846.1404254456164 with entries=99, filesize=64.7m; new WAL /hbase/WALs/slave1,60020,1404253868846/slave1%2C60020%2C1404253868846.1404254458607
2014-07-01 15:41:01,018 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-01 15:41:01,039 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 22272 synced till here 22270
2014-07-01 15:41:01,054 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1404253868846/slave1%2C60020%2C1404253868846.1404254458607 with entries=121, filesize=61.3m; new WAL /hbase/WALs/slave1,60020,1404253868846/slave1%2C60020%2C1404253868846.1404254461018
2014-07-01 15:41:01,960 DEBUG [RpcServer.handler=5,port=60020] regionserver.HRegion: Flush requested on usertable,user7,1404228265526.667c1f2c34fe37d48ba25a789d8aba98.
2014-07-01 15:41:03,271 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-01 15:41:03,287 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 22390 synced till here 22389
2014-07-01 15:41:03,702 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1404253868846/slave1%2C60020%2C1404253868846.1404254461018 with entries=118, filesize=63.0m; new WAL /hbase/WALs/slave1,60020,1404253868846/slave1%2C60020%2C1404253868846.1404254463271
2014-07-01 15:41:04,928 DEBUG [RpcServer.handler=49,port=60020] regionserver.HRegion: Flush requested on usertable,,1404228265525.9cef9e7d34f96f1d9b111be50040a208.
2014-07-01 15:41:05,527 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-01 15:41:05,553 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 22502 synced till here 22501
2014-07-01 15:41:05,573 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1404253868846/slave1%2C60020%2C1404253868846.1404254463271 with entries=112, filesize=61.8m; new WAL /hbase/WALs/slave1,60020,1404253868846/slave1%2C60020%2C1404253868846.1404254465527
2014-07-01 15:41:06,419 INFO  [MemStoreFlusher.1] regionserver.DefaultStoreFlusher: Flushed, sequenceid=4417, memsize=230.6m, hasBloomFilter=true, into tmp file hdfs://master:54310/hbase/data/default/usertable/bb79ae773f71289e0745068f68a0abd9/.tmp/5a36124e0535444485a0db68d8da8309
2014-07-01 15:41:06,440 DEBUG [MemStoreFlusher.1] regionserver.HRegionFileSystem: Committing store file hdfs://master:54310/hbase/data/default/usertable/bb79ae773f71289e0745068f68a0abd9/.tmp/5a36124e0535444485a0db68d8da8309 as hdfs://master:54310/hbase/data/default/usertable/bb79ae773f71289e0745068f68a0abd9/family/5a36124e0535444485a0db68d8da8309
2014-07-01 15:41:06,450 INFO  [MemStoreFlusher.1] regionserver.HStore: Added hdfs://master:54310/hbase/data/default/usertable/bb79ae773f71289e0745068f68a0abd9/family/5a36124e0535444485a0db68d8da8309, entries=839490, sequenceid=4417, filesize=59.8m
2014-07-01 15:41:06,450 INFO  [MemStoreFlusher.1] regionserver.HRegion: Finished memstore flush of ~309.3m/324303760, currentsize=32.2m/33774000 for region usertable,user9,1404228265526.bb79ae773f71289e0745068f68a0abd9. in 8551ms, sequenceid=4417, compaction requested=true
2014-07-01 15:41:06,451 DEBUG [MemStoreFlusher.1] regionserver.CompactSplitThread: Small Compaction requested: system; Because: MemStoreFlusher.1; compaction_queue=(0:29), split_queue=0, merge_queue=0
2014-07-01 15:41:06,451 DEBUG [MemStoreFlusher.1] regionserver.HRegion: Started memstore flush for usertable,user5,1404228265526.122e92eabfab38cb15a9965b21db25c5., current region memstore size 764.9m
2014-07-01 15:41:07,015 DEBUG [MemStoreFlusher.1] util.FSUtils: DFS Client does not support most favored nodes create; using default create
2014-07-01 15:41:07,933 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-01 15:41:07,968 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 22612 synced till here 22604
2014-07-01 15:41:08,022 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1404253868846/slave1%2C60020%2C1404253868846.1404254465527 with entries=110, filesize=65.9m; new WAL /hbase/WALs/slave1,60020,1404253868846/slave1%2C60020%2C1404253868846.1404254467933
2014-07-01 15:41:08,023 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1404253868846/slave1%2C60020%2C1404253868846.1404254383626
2014-07-01 15:41:08,023 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1404253868846/slave1%2C60020%2C1404253868846.1404254386113
2014-07-01 15:41:08,023 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1404253868846/slave1%2C60020%2C1404253868846.1404254387944
2014-07-01 15:41:08,023 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1404253868846/slave1%2C60020%2C1404253868846.1404254389854
2014-07-01 15:41:08,023 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1404253868846/slave1%2C60020%2C1404253868846.1404254392773
2014-07-01 15:41:08,023 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1404253868846/slave1%2C60020%2C1404253868846.1404254394128
2014-07-01 15:41:08,023 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1404253868846/slave1%2C60020%2C1404253868846.1404254396670
2014-07-01 15:41:08,023 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1404253868846/slave1%2C60020%2C1404253868846.1404254398605
2014-07-01 15:41:08,023 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1404253868846/slave1%2C60020%2C1404253868846.1404254401966
2014-07-01 15:41:08,023 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1404253868846/slave1%2C60020%2C1404253868846.1404254404136
2014-07-01 15:41:08,023 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1404253868846/slave1%2C60020%2C1404253868846.1404254405727
2014-07-01 15:41:08,024 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1404253868846/slave1%2C60020%2C1404253868846.1404254408504
2014-07-01 15:41:08,939 DEBUG [LruStats #0] hfile.LruBlockCache: Total=4.53 MB, free=3.95 GB, max=3.96 GB, blocks=3, accesses=79975, hits=19279, hitRatio=24.10%, , cachingAccesses=19288, cachingHits=19273, cachingHitsRatio=99.92%, evictions=0, evicted=12, evictedPerRun=Infinity
2014-07-01 15:41:10,845 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-01 15:41:11,154 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 22739 synced till here 22738
2014-07-01 15:41:11,238 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1404253868846/slave1%2C60020%2C1404253868846.1404254467933 with entries=127, filesize=71.6m; new WAL /hbase/WALs/slave1,60020,1404253868846/slave1%2C60020%2C1404253868846.1404254470846
2014-07-01 15:41:13,352 INFO  [MemStoreFlusher.0] regionserver.DefaultStoreFlusher: Flushed, sequenceid=4474, memsize=415.6m, hasBloomFilter=true, into tmp file hdfs://master:54310/hbase/data/default/usertable/93253e37c284f3174309fede3f32339f/.tmp/1fa326c61c4444fab07eb858d830cea5
2014-07-01 15:41:13,373 DEBUG [MemStoreFlusher.0] regionserver.HRegionFileSystem: Committing store file hdfs://master:54310/hbase/data/default/usertable/93253e37c284f3174309fede3f32339f/.tmp/1fa326c61c4444fab07eb858d830cea5 as hdfs://master:54310/hbase/data/default/usertable/93253e37c284f3174309fede3f32339f/family/1fa326c61c4444fab07eb858d830cea5
2014-07-01 15:41:13,517 INFO  [MemStoreFlusher.0] regionserver.HStore: Added hdfs://master:54310/hbase/data/default/usertable/93253e37c284f3174309fede3f32339f/family/1fa326c61c4444fab07eb858d830cea5, entries=1513360, sequenceid=4474, filesize=107.7m
2014-07-01 15:41:13,518 INFO  [MemStoreFlusher.0] regionserver.HRegion: Finished memstore flush of ~665.4m/697689040, currentsize=163.2m/171178640 for region usertable,user3,1404228265526.93253e37c284f3174309fede3f32339f. in 15619ms, sequenceid=4474, compaction requested=false
2014-07-01 15:41:13,518 DEBUG [MemStoreFlusher.0] regionserver.HRegion: Started memstore flush for usertable,user7,1404228265526.667c1f2c34fe37d48ba25a789d8aba98., current region memstore size 364.2m
2014-07-01 15:41:13,771 DEBUG [MemStoreFlusher.0] util.FSUtils: DFS Client does not support most favored nodes create; using default create
2014-07-01 15:41:13,815 INFO  [Thread-927] hdfs.DFSClient: Exception in createBlockOutputStream 9.1.143.59:50010 java.io.IOException: Bad connect ack with firstBadLink as 9.1.143.58:50010
2014-07-01 15:41:13,815 INFO  [Thread-927] hdfs.DFSClient: Abandoning blk_-1267205666411157918_29958
2014-07-01 15:41:13,865 INFO  [Thread-927] hdfs.DFSClient: Excluding datanode 9.1.143.58:50010
2014-07-01 15:41:14,129 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-01 15:41:14,157 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 22851 synced till here 22849
2014-07-01 15:41:14,204 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1404253868846/slave1%2C60020%2C1404253868846.1404254470846 with entries=112, filesize=64.5m; new WAL /hbase/WALs/slave1,60020,1404253868846/slave1%2C60020%2C1404253868846.1404254474130
2014-07-01 15:41:16,850 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-01 15:41:17,195 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 22969 synced till here 22966
2014-07-01 15:41:17,323 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1404253868846/slave1%2C60020%2C1404253868846.1404254474130 with entries=118, filesize=66.5m; new WAL /hbase/WALs/slave1,60020,1404253868846/slave1%2C60020%2C1404253868846.1404254476851
2014-07-01 15:41:20,319 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-01 15:41:20,339 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 23086 synced till here 23083
2014-07-01 15:41:20,365 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1404253868846/slave1%2C60020%2C1404253868846.1404254476851 with entries=117, filesize=63.3m; new WAL /hbase/WALs/slave1,60020,1404253868846/slave1%2C60020%2C1404253868846.1404254480320
2014-07-01 15:41:23,926 DEBUG [RpcServer.handler=25,port=60020] regionserver.HRegion: Flush requested on usertable,user3,1404228265526.93253e37c284f3174309fede3f32339f.
2014-07-01 15:41:24,023 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-01 15:41:24,043 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 23202 synced till here 23201
2014-07-01 15:41:24,057 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1404253868846/slave1%2C60020%2C1404253868846.1404254480320 with entries=116, filesize=62.0m; new WAL /hbase/WALs/slave1,60020,1404253868846/slave1%2C60020%2C1404253868846.1404254484024
2014-07-01 15:41:26,378 INFO  [MemStoreFlusher.1] regionserver.DefaultStoreFlusher: Flushed, sequenceid=4523, memsize=515.3m, hasBloomFilter=true, into tmp file hdfs://master:54310/hbase/data/default/usertable/122e92eabfab38cb15a9965b21db25c5/.tmp/9ddbaa59638243d09cf3db65eb19533c
2014-07-01 15:41:26,394 DEBUG [MemStoreFlusher.1] regionserver.HRegionFileSystem: Committing store file hdfs://master:54310/hbase/data/default/usertable/122e92eabfab38cb15a9965b21db25c5/.tmp/9ddbaa59638243d09cf3db65eb19533c as hdfs://master:54310/hbase/data/default/usertable/122e92eabfab38cb15a9965b21db25c5/family/9ddbaa59638243d09cf3db65eb19533c
2014-07-01 15:41:26,410 INFO  [MemStoreFlusher.1] regionserver.HStore: Added hdfs://master:54310/hbase/data/default/usertable/122e92eabfab38cb15a9965b21db25c5/family/9ddbaa59638243d09cf3db65eb19533c, entries=1876300, sequenceid=4523, filesize=133.6m
2014-07-01 15:41:26,410 INFO  [MemStoreFlusher.1] regionserver.HRegion: Finished memstore flush of ~764.9m/802105600, currentsize=177.1m/185699600 for region usertable,user5,1404228265526.122e92eabfab38cb15a9965b21db25c5. in 19959ms, sequenceid=4523, compaction requested=true
2014-07-01 15:41:26,411 DEBUG [MemStoreFlusher.1] regionserver.CompactSplitThread: Small Compaction requested: system; Because: MemStoreFlusher.1; compaction_queue=(0:30), split_queue=0, merge_queue=0
2014-07-01 15:41:26,411 DEBUG [MemStoreFlusher.1] regionserver.HRegion: Started memstore flush for usertable,,1404228265525.9cef9e7d34f96f1d9b111be50040a208., current region memstore size 438.3m
2014-07-01 15:41:26,680 DEBUG [MemStoreFlusher.1] util.FSUtils: DFS Client does not support most favored nodes create; using default create
2014-07-01 15:41:26,923 INFO  [MemStoreFlusher.0] regionserver.DefaultStoreFlusher: Flushed, sequenceid=4527, memsize=364.2m, hasBloomFilter=true, into tmp file hdfs://master:54310/hbase/data/default/usertable/667c1f2c34fe37d48ba25a789d8aba98/.tmp/5c4ecd1e5f5f40a68639ae8651cdf308
2014-07-01 15:41:26,940 DEBUG [MemStoreFlusher.0] regionserver.HRegionFileSystem: Committing store file hdfs://master:54310/hbase/data/default/usertable/667c1f2c34fe37d48ba25a789d8aba98/.tmp/5c4ecd1e5f5f40a68639ae8651cdf308 as hdfs://master:54310/hbase/data/default/usertable/667c1f2c34fe37d48ba25a789d8aba98/family/5c4ecd1e5f5f40a68639ae8651cdf308
2014-07-01 15:41:26,952 INFO  [MemStoreFlusher.0] regionserver.HStore: Added hdfs://master:54310/hbase/data/default/usertable/667c1f2c34fe37d48ba25a789d8aba98/family/5c4ecd1e5f5f40a68639ae8651cdf308, entries=1326060, sequenceid=4527, filesize=94.4m
2014-07-01 15:41:26,952 INFO  [MemStoreFlusher.0] regionserver.HRegion: Finished memstore flush of ~364.2m/381896480, currentsize=115.3m/120874000 for region usertable,user7,1404228265526.667c1f2c34fe37d48ba25a789d8aba98. in 13434ms, sequenceid=4527, compaction requested=true
2014-07-01 15:41:26,952 DEBUG [MemStoreFlusher.0] regionserver.CompactSplitThread: Small Compaction requested: system; Because: MemStoreFlusher.0; compaction_queue=(0:31), split_queue=0, merge_queue=0
2014-07-01 15:41:26,953 DEBUG [MemStoreFlusher.0] regionserver.HRegion: Started memstore flush for usertable,user3,1404228265526.93253e37c284f3174309fede3f32339f., current region memstore size 276.8m
2014-07-01 15:41:27,119 DEBUG [MemStoreFlusher.0] util.FSUtils: DFS Client does not support most favored nodes create; using default create
2014-07-01 15:41:35,397 INFO  [MemStoreFlusher.0] regionserver.DefaultStoreFlusher: Flushed, sequenceid=4635, memsize=276.8m, hasBloomFilter=true, into tmp file hdfs://master:54310/hbase/data/default/usertable/93253e37c284f3174309fede3f32339f/.tmp/930bd854f2d54a41b075b9875d4b7442
2014-07-01 15:41:35,430 DEBUG [MemStoreFlusher.0] regionserver.HRegionFileSystem: Committing store file hdfs://master:54310/hbase/data/default/usertable/93253e37c284f3174309fede3f32339f/.tmp/930bd854f2d54a41b075b9875d4b7442 as hdfs://master:54310/hbase/data/default/usertable/93253e37c284f3174309fede3f32339f/family/930bd854f2d54a41b075b9875d4b7442
2014-07-01 15:41:35,450 INFO  [MemStoreFlusher.0] regionserver.HStore: Added hdfs://master:54310/hbase/data/default/usertable/93253e37c284f3174309fede3f32339f/family/930bd854f2d54a41b075b9875d4b7442, entries=1007930, sequenceid=4635, filesize=71.8m
2014-07-01 15:41:35,451 INFO  [MemStoreFlusher.0] regionserver.HRegion: Finished memstore flush of ~276.8m/290274320, currentsize=0.0/0 for region usertable,user3,1404228265526.93253e37c284f3174309fede3f32339f. in 8498ms, sequenceid=4635, compaction requested=false
2014-07-01 15:41:39,750 INFO  [MemStoreFlusher.1] regionserver.DefaultStoreFlusher: Flushed, sequenceid=5476, memsize=438.3m, hasBloomFilter=true, into tmp file hdfs://master:54310/hbase/data/default/usertable/9cef9e7d34f96f1d9b111be50040a208/.tmp/c9761ba6ee6045fdacae3b6f1c04d7f0
2014-07-01 15:41:39,765 DEBUG [MemStoreFlusher.1] regionserver.HRegionFileSystem: Committing store file hdfs://master:54310/hbase/data/default/usertable/9cef9e7d34f96f1d9b111be50040a208/.tmp/c9761ba6ee6045fdacae3b6f1c04d7f0 as hdfs://master:54310/hbase/data/default/usertable/9cef9e7d34f96f1d9b111be50040a208/family/c9761ba6ee6045fdacae3b6f1c04d7f0
2014-07-01 15:41:39,780 INFO  [MemStoreFlusher.1] regionserver.HStore: Added hdfs://master:54310/hbase/data/default/usertable/9cef9e7d34f96f1d9b111be50040a208/family/c9761ba6ee6045fdacae3b6f1c04d7f0, entries=1595890, sequenceid=5476, filesize=113.6m
2014-07-01 15:41:39,781 INFO  [MemStoreFlusher.1] regionserver.HRegion: Finished memstore flush of ~438.3m/459604800, currentsize=3.5m/3625920 for region usertable,,1404228265525.9cef9e7d34f96f1d9b111be50040a208. in 13370ms, sequenceid=5476, compaction requested=true
2014-07-01 15:41:39,781 DEBUG [MemStoreFlusher.1] regionserver.CompactSplitThread: Small Compaction requested: system; Because: MemStoreFlusher.1; compaction_queue=(0:32), split_queue=0, merge_queue=0
2014-07-01 15:42:09,807 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-01 15:42:09,877 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 23319 synced till here 23306
2014-07-01 15:42:10,047 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1404253868846/slave1%2C60020%2C1404253868846.1404254484024 with entries=117, filesize=69.4m; new WAL /hbase/WALs/slave1,60020,1404253868846/slave1%2C60020%2C1404253868846.1404254529807
2014-07-01 15:42:10,047 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1404253868846/slave1%2C60020%2C1404253868846.1404254410335
2014-07-01 15:42:10,047 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1404253868846/slave1%2C60020%2C1404253868846.1404254412264
2014-07-01 15:42:10,047 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1404253868846/slave1%2C60020%2C1404253868846.1404254414925
2014-07-01 15:42:10,048 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1404253868846/slave1%2C60020%2C1404253868846.1404254417024
2014-07-01 15:42:10,048 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1404253868846/slave1%2C60020%2C1404253868846.1404254419602
2014-07-01 15:42:10,048 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1404253868846/slave1%2C60020%2C1404253868846.1404254423856
2014-07-01 15:42:10,048 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1404253868846/slave1%2C60020%2C1404253868846.1404254426247
2014-07-01 15:42:10,048 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1404253868846/slave1%2C60020%2C1404253868846.1404254428996
2014-07-01 15:42:10,048 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1404253868846/slave1%2C60020%2C1404253868846.1404254430002
2014-07-01 15:42:10,048 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1404253868846/slave1%2C60020%2C1404253868846.1404254431704
2014-07-01 15:42:10,048 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1404253868846/slave1%2C60020%2C1404253868846.1404254434087
2014-07-01 15:42:10,048 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1404253868846/slave1%2C60020%2C1404253868846.1404254435428
2014-07-01 15:42:10,048 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1404253868846/slave1%2C60020%2C1404253868846.1404254436739
2014-07-01 15:42:10,048 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1404253868846/slave1%2C60020%2C1404253868846.1404254438487
2014-07-01 15:42:10,048 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1404253868846/slave1%2C60020%2C1404253868846.1404254441501
2014-07-01 15:42:10,048 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1404253868846/slave1%2C60020%2C1404253868846.1404254443723
2014-07-01 15:42:10,048 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1404253868846/slave1%2C60020%2C1404253868846.1404254445676
2014-07-01 15:42:10,049 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1404253868846/slave1%2C60020%2C1404253868846.1404254448893
2014-07-01 15:42:10,049 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1404253868846/slave1%2C60020%2C1404253868846.1404254450955
2014-07-01 15:42:10,049 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/slave1,60020,1404253868846/slave1%2C60020%2C1404253868846.1404254453934
2014-07-01 15:42:13,426 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-01 15:42:13,463 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 23432 synced till here 23430
2014-07-01 15:42:13,515 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1404253868846/slave1%2C60020%2C1404253868846.1404254529807 with entries=113, filesize=63.7m; new WAL /hbase/WALs/slave1,60020,1404253868846/slave1%2C60020%2C1404253868846.1404254533427
2014-07-01 15:42:18,838 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-01 15:42:18,890 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 23546 synced till here 23544
2014-07-01 15:42:18,958 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1404253868846/slave1%2C60020%2C1404253868846.1404254533427 with entries=114, filesize=64.2m; new WAL /hbase/WALs/slave1,60020,1404253868846/slave1%2C60020%2C1404253868846.1404254538838
2014-07-01 15:42:19,467 DEBUG [RpcServer.handler=12,port=60020] regionserver.HRegion: Flush requested on usertable,user5,1404228265526.122e92eabfab38cb15a9965b21db25c5.
2014-07-01 15:42:19,467 DEBUG [MemStoreFlusher.1] regionserver.HRegion: Started memstore flush for usertable,user5,1404228265526.122e92eabfab38cb15a9965b21db25c5., current region memstore size 257.0m
2014-07-01 15:42:19,635 DEBUG [MemStoreFlusher.1] util.FSUtils: DFS Client does not support most favored nodes create; using default create
2014-07-01 15:42:21,292 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-01 15:42:21,328 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 23660 synced till here 23659
2014-07-01 15:42:21,347 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1404253868846/slave1%2C60020%2C1404253868846.1404254538838 with entries=114, filesize=63.8m; new WAL /hbase/WALs/slave1,60020,1404253868846/slave1%2C60020%2C1404253868846.1404254541293
2014-07-01 15:42:23,413 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-01 15:42:23,521 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 23773 synced till here 23770
2014-07-01 15:42:23,544 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1404253868846/slave1%2C60020%2C1404253868846.1404254541293 with entries=113, filesize=65.0m; new WAL /hbase/WALs/slave1,60020,1404253868846/slave1%2C60020%2C1404253868846.1404254543413
2014-07-01 15:42:24,174 DEBUG [regionserver60020-smallCompactions-1404253964856] regionserver.HRegionFileSystem: Committing store file hdfs://master:54310/hbase/data/default/usertable/93253e37c284f3174309fede3f32339f/.tmp/3d771a0385964ca2bdb4ca073c1a5821 as hdfs://master:54310/hbase/data/default/usertable/93253e37c284f3174309fede3f32339f/family/3d771a0385964ca2bdb4ca073c1a5821
2014-07-01 15:42:24,338 DEBUG [regionserver60020-smallCompactions-1404253964856] regionserver.HStore: Removing store files after compaction...
2014-07-01 15:42:24,368 DEBUG [regionserver60020-smallCompactions-1404253964856] backup.HFileArchiver: Finished archiving from class org.apache.hadoop.hbase.backup.HFileArchiver$FileableStoreFile, file:hdfs://master:54310/hbase/data/default/usertable/93253e37c284f3174309fede3f32339f/family/0f786ade400f4409a3ed4e0f9a493553, to hdfs://master:54310/hbase/archive/data/default/usertable/93253e37c284f3174309fede3f32339f/family/0f786ade400f4409a3ed4e0f9a493553
2014-07-01 15:42:24,372 DEBUG [regionserver60020-smallCompactions-1404253964856] backup.HFileArchiver: Finished archiving from class org.apache.hadoop.hbase.backup.HFileArchiver$FileableStoreFile, file:hdfs://master:54310/hbase/data/default/usertable/93253e37c284f3174309fede3f32339f/family/a2f5c30d99ef4611ac44a48e6c238e5e, to hdfs://master:54310/hbase/archive/data/default/usertable/93253e37c284f3174309fede3f32339f/family/a2f5c30d99ef4611ac44a48e6c238e5e
2014-07-01 15:42:24,428 DEBUG [regionserver60020-smallCompactions-1404253964856] backup.HFileArchiver: Finished archiving from class org.apache.hadoop.hbase.backup.HFileArchiver$FileableStoreFile, file:hdfs://master:54310/hbase/data/default/usertable/93253e37c284f3174309fede3f32339f/family/f674b967455d4849b06b82ffdb29ea0e, to hdfs://master:54310/hbase/archive/data/default/usertable/93253e37c284f3174309fede3f32339f/family/f674b967455d4849b06b82ffdb29ea0e
2014-07-01 15:42:24,432 DEBUG [regionserver60020-smallCompactions-1404253964856] backup.HFileArchiver: Finished archiving from class org.apache.hadoop.hbase.backup.HFileArchiver$FileableStoreFile, file:hdfs://master:54310/hbase/data/default/usertable/93253e37c284f3174309fede3f32339f/family/5294515babe64f0f9f64bceccfbb78aa, to hdfs://master:54310/hbase/archive/data/default/usertable/93253e37c284f3174309fede3f32339f/family/5294515babe64f0f9f64bceccfbb78aa
2014-07-01 15:42:24,464 DEBUG [regionserver60020-smallCompactions-1404253964856] backup.HFileArchiver: Finished archiving from class org.apache.hadoop.hbase.backup.HFileArchiver$FileableStoreFile, file:hdfs://master:54310/hbase/data/default/usertable/93253e37c284f3174309fede3f32339f/family/847eed712227414fac5ea1ff40f653a1, to hdfs://master:54310/hbase/archive/data/default/usertable/93253e37c284f3174309fede3f32339f/family/847eed712227414fac5ea1ff40f653a1
2014-07-01 15:42:24,468 DEBUG [regionserver60020-smallCompactions-1404253964856] backup.HFileArchiver: Finished archiving from class org.apache.hadoop.hbase.backup.HFileArchiver$FileableStoreFile, file:hdfs://master:54310/hbase/data/default/usertable/93253e37c284f3174309fede3f32339f/family/a1918f1d24b6451792a0de5913dea691, to hdfs://master:54310/hbase/archive/data/default/usertable/93253e37c284f3174309fede3f32339f/family/a1918f1d24b6451792a0de5913dea691
2014-07-01 15:42:24,473 DEBUG [regionserver60020-smallCompactions-1404253964856] backup.HFileArchiver: Finished archiving from class org.apache.hadoop.hbase.backup.HFileArchiver$FileableStoreFile, file:hdfs://master:54310/hbase/data/default/usertable/93253e37c284f3174309fede3f32339f/family/757063605014422bb142804470c2dfd7, to hdfs://master:54310/hbase/archive/data/default/usertable/93253e37c284f3174309fede3f32339f/family/757063605014422bb142804470c2dfd7
2014-07-01 15:42:24,476 DEBUG [regionserver60020-smallCompactions-1404253964856] backup.HFileArchiver: Finished archiving from class org.apache.hadoop.hbase.backup.HFileArchiver$FileableStoreFile, file:hdfs://master:54310/hbase/data/default/usertable/93253e37c284f3174309fede3f32339f/family/ed6896cb011f48c8a51af3385f8f39dc, to hdfs://master:54310/hbase/archive/data/default/usertable/93253e37c284f3174309fede3f32339f/family/ed6896cb011f48c8a51af3385f8f39dc
2014-07-01 15:42:24,479 DEBUG [regionserver60020-smallCompactions-1404253964856] backup.HFileArchiver: Finished archiving from class org.apache.hadoop.hbase.backup.HFileArchiver$FileableStoreFile, file:hdfs://master:54310/hbase/data/default/usertable/93253e37c284f3174309fede3f32339f/family/1d8435219ed7430e86562f3a0fe4b535, to hdfs://master:54310/hbase/archive/data/default/usertable/93253e37c284f3174309fede3f32339f/family/1d8435219ed7430e86562f3a0fe4b535
2014-07-01 15:42:24,479 INFO  [regionserver60020-smallCompactions-1404253964856] regionserver.HStore: Completed major compaction of 9 file(s) in family of usertable,user3,1404228265526.93253e37c284f3174309fede3f32339f. into 3d771a0385964ca2bdb4ca073c1a5821(size=637.5m), total size for store is 817.1m. This selection was in queue for 0sec, and took 1mins, 41sec to execute.
2014-07-01 15:42:24,479 INFO  [regionserver60020-smallCompactions-1404253964856] regionserver.CompactSplitThread: Completed compaction: Request = regionName=usertable,user3,1404228265526.93253e37c284f3174309fede3f32339f., storeName=family, fileCount=9, fileSize=707.2m, priority=11, time=74554030680650; duration=1mins, 41sec
2014-07-01 15:42:24,480 DEBUG [regionserver60020-smallCompactions-1404253964856] regionserver.CompactSplitThread: CompactSplitThread Status: compaction_queue=(0:32), split_queue=0, merge_queue=0
2014-07-01 15:42:24,480 DEBUG [regionserver60020-smallCompactions-1404253964856] compactions.RatioBasedCompactionPolicy: Selecting compaction from 7 store files, 0 compacting, 7 eligible, 20 blocking
2014-07-01 15:42:24,480 DEBUG [regionserver60020-smallCompactions-1404253964856] compactions.ExploringCompactionPolicy: Exploring compaction algorithm has selected 7 files of size 872694021 starting at candidate #0 after considering 15 permutations with 14 in ratio
2014-07-01 15:42:24,480 DEBUG [regionserver60020-smallCompactions-1404253964856] regionserver.HStore: 9cef9e7d34f96f1d9b111be50040a208 - family: Initiating major compaction
2014-07-01 15:42:24,481 INFO  [regionserver60020-smallCompactions-1404253964856] regionserver.HRegion: Starting compaction on family in region usertable,,1404228265525.9cef9e7d34f96f1d9b111be50040a208.
2014-07-01 15:42:24,481 INFO  [regionserver60020-smallCompactions-1404253964856] regionserver.HStore: Starting compaction of 7 file(s) in family of usertable,,1404228265525.9cef9e7d34f96f1d9b111be50040a208. into tmpdir=hdfs://master:54310/hbase/data/default/usertable/9cef9e7d34f96f1d9b111be50040a208/.tmp, totalSize=832.3m
2014-07-01 15:42:24,481 DEBUG [regionserver60020-smallCompactions-1404253964856] compactions.Compactor: Compacting hdfs://master:54310/hbase/data/default/usertable/9cef9e7d34f96f1d9b111be50040a208/family/582d11d149af4c829654b41f188bc10d, keycount=337674, bloomtype=ROW, size=240.6m, encoding=NONE, seqNum=1420, earliestPutTs=1404228281510
2014-07-01 15:42:24,481 DEBUG [regionserver60020-smallCompactions-1404253964856] compactions.Compactor: Compacting hdfs://master:54310/hbase/data/default/usertable/9cef9e7d34f96f1d9b111be50040a208/family/12264dc0884947c1960f156423a73464, keycount=125595, bloomtype=ROW, size=89.5m, encoding=NONE, seqNum=1965, earliestPutTs=1404254123820
2014-07-01 15:42:24,481 DEBUG [regionserver60020-smallCompactions-1404253964856] compactions.Compactor: Compacting hdfs://master:54310/hbase/data/default/usertable/9cef9e7d34f96f1d9b111be50040a208/family/51ea620459a3482c8f384892f831e303, keycount=94835, bloomtype=ROW, size=67.5m, encoding=NONE, seqNum=2406, earliestPutTs=1404254239464
2014-07-01 15:42:24,481 DEBUG [regionserver60020-smallCompactions-1404253964856] compactions.Compactor: Compacting hdfs://master:54310/hbase/data/default/usertable/9cef9e7d34f96f1d9b111be50040a208/family/44a0953a47b54036b304d1558b547649, keycount=129656, bloomtype=ROW, size=92.3m, encoding=NONE, seqNum=3045, earliestPutTs=1404254292586
2014-07-01 15:42:24,481 DEBUG [regionserver60020-smallCompactions-1404253964856] compactions.Compactor: Compacting hdfs://master:54310/hbase/data/default/usertable/9cef9e7d34f96f1d9b111be50040a208/family/b284cd02aa7e4961ba8fce12cba46c67, keycount=150304, bloomtype=ROW, size=107.0m, encoding=NONE, seqNum=3742, earliestPutTs=1404254359986
2014-07-01 15:42:24,482 DEBUG [regionserver60020-smallCompactions-1404253964856] compactions.Compactor: Compacting hdfs://master:54310/hbase/data/default/usertable/9cef9e7d34f96f1d9b111be50040a208/family/c69d742b517d48f3903c54a7145fa3fe, keycount=170867, bloomtype=ROW, size=121.7m, encoding=NONE, seqNum=4628, earliestPutTs=1404254393233
2014-07-01 15:42:24,482 DEBUG [regionserver60020-smallCompactions-1404253964856] compactions.Compactor: Compacting hdfs://master:54310/hbase/data/default/usertable/9cef9e7d34f96f1d9b111be50040a208/family/c9761ba6ee6045fdacae3b6f1c04d7f0, keycount=159589, bloomtype=ROW, size=113.6m, encoding=NONE, seqNum=5476, earliestPutTs=1404254441601
2014-07-01 15:42:24,595 DEBUG [regionserver60020-smallCompactions-1404253964856] util.FSUtils: DFS Client does not support most favored nodes create; using default create
2014-07-01 15:42:25,697 DEBUG [RpcServer.handler=4,port=60020] regionserver.HRegion: Flush requested on usertable,user7,1404228265526.667c1f2c34fe37d48ba25a789d8aba98.
2014-07-01 15:42:25,697 DEBUG [MemStoreFlusher.0] regionserver.HRegion: Started memstore flush for usertable,user7,1404228265526.667c1f2c34fe37d48ba25a789d8aba98., current region memstore size 256.0m
2014-07-01 15:42:25,865 DEBUG [MemStoreFlusher.0] util.FSUtils: DFS Client does not support most favored nodes create; using default create
2014-07-01 15:42:27,569 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-01 15:42:27,629 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/slave1,60020,1404253868846/slave1%2C60020%2C1404253868846.1404254543413 with entries=111, filesize=62.8m; new WAL /hbase/WALs/slave1,60020,1404253868846/slave1%2C60020%2C1404253868846.1404254547570
2014-07-01 15:42:28,787 INFO  [MemStoreFlusher.1] regionserver.DefaultStoreFlusher: Flushed, sequenceid=4674, memsize=255.2m, hasBloomFilter=true, into tmp file hdfs://master:54310/hbase/data/default/usertable/122e92eabfab38cb15a9965b21db25c5/.tmp/ff172869b47640eca1ae361f051b39b8
2014-07-01 15:42:28,802 DEBUG [MemStoreFlusher.1] regionserver.HRegionFileSystem: Committing store file hdfs://master:54310/hbase/data/default/usertable/122e92eabfab38cb15a9965b21db25c5/.tmp/ff172869b47640eca1ae361f051b39b8 as hdfs://master:54310/hbase/data/default/usertable/122e92eabfab38cb15a9965b21db25c5/family/ff172869b47640eca1ae361f051b39b8
2014-07-01 15:42:28,825 INFO  [MemStoreFlusher.1] regionserver.HStore: Added hdfs://master:54310/hbase/data/default/usertable/122e92eabfab38cb15a9965b21db25c5/family/ff172869b47640eca1ae361f051b39b8, entries=929080, sequenceid=4674, filesize=66.2m
2014-07-01 15:42:28,825 INFO  [MemStoreFlusher.1] regionserver.HRegion: Finished memstore flush of ~257.0m/269473600, currentsize=77.2m/80958000 for region usertable,user5,1404228265526.122e92eabfab38cb15a9965b21db25c5. in 9358ms, sequenceid=4674, compaction requested=true
2014-07-01 15:42:28,826 DEBUG [MemStoreFlusher.1] regionserver.CompactSplitThread: Small Compaction requested: system; Because: MemStoreFlusher.1; compaction_queue=(0:32), split_queue=0, merge_queue=0
2014-07-01 15:42:34,030 INFO  [MemStoreFlusher.0] regionserver.DefaultStoreFlusher: Flushed, sequenceid=4678, memsize=254.2m, hasBloomFilter=true, into tmp file hdfs://master:54310/hbase/data/default/usertable/667c1f2c34fe37d48ba25a789d8aba98/.tmp/6c4e4b0a6c594b6db087ac5f958bca9c
2014-07-01 15:42:34,046 DEBUG [MemStoreFlusher.0] regionserver.HRegionFileSystem: Committing store file hdfs://master:54310/hbase/data/default/usertable/667c1f2c34fe37d48ba25a789d8aba98/.tmp/6c4e4b0a6c594b6db087ac5f958bca9c as hdfs://master:54310/hbase/data/default/usertable/667c1f2c34fe37d48ba25a789d8aba98/family/6c4e4b0a6c594b6db087ac5f958bca9c
2014-07-01 15:42:34,061 INFO  [MemStoreFlusher.0] regionserver.HStore: Added hdfs://master:54310/hbase/data/default/usertable/667c1f2c34fe37d48ba25a789d8aba98/family/6c4e4b0a6c594b6db087ac5f958bca9c, entries=925650, sequenceid=4678, filesize=66.0m
2014-07-01 15:42:34,061 INFO  [MemStoreFlusher.0] regionserver.HRegion: Finished memstore flush of ~256.0m/268435760, currentsize=19.5m/20441680 for region usertable,user7,1404228265526.667c1f2c34fe37d48ba25a789d8aba98. in 8364ms, sequenceid=4678, compaction requested=true
2014-07-01 15:42:34,061 DEBUG [MemStoreFlusher.0] regionserver.CompactSplitThread: Small Compaction requested: system; Because: MemStoreFlusher.0; compaction_queue=(0:33), split_queue=0, merge_queue=0
2014-07-01 15:42:47,022 INFO  [RpcServer.handler=46,port=60020] compress.CodecPool: Got brand-new decompressor
2014-07-01 15:42:47,216 INFO  [RpcServer.handler=47,port=60020] compress.CodecPool: Got brand-new decompressor
2014-07-01 15:43:38,366 INFO  [RpcServer.handler=14,port=60020] compress.CodecPool: Got brand-new decompressor
2014-07-01 15:43:38,485 INFO  [RpcServer.handler=0,port=60020] compress.CodecPool: Got brand-new decompressor
2014-07-01 15:43:38,487 INFO  [RpcServer.handler=1,port=60020] compress.CodecPool: Got brand-new decompressor
2014-07-01 15:43:38,487 INFO  [RpcServer.handler=2,port=60020] compress.CodecPool: Got brand-new decompressor
2014-07-01 15:43:38,487 INFO  [RpcServer.handler=29,port=60020] compress.CodecPool: Got brand-new decompressor
2014-07-01 15:43:38,487 INFO  [RpcServer.handler=37,port=60020] compress.CodecPool: Got brand-new decompressor
2014-07-01 15:43:38,489 INFO  [RpcServer.handler=14,port=60020] compress.CodecPool: Got brand-new decompressor
2014-07-01 15:44:31,020 DEBUG [regionserver60020-smallCompactions-1404253964856] regionserver.HRegionFileSystem: Committing store file hdfs://master:54310/hbase/data/default/usertable/9cef9e7d34f96f1d9b111be50040a208/.tmp/fd1299745de743a1aedf8d741c8f5e6a as hdfs://master:54310/hbase/data/default/usertable/9cef9e7d34f96f1d9b111be50040a208/family/fd1299745de743a1aedf8d741c8f5e6a
2014-07-01 15:44:31,054 DEBUG [regionserver60020-smallCompactions-1404253964856] regionserver.HStore: Removing store files after compaction...
2014-07-01 15:44:31,089 DEBUG [regionserver60020-smallCompactions-1404253964856] backup.HFileArchiver: Finished archiving from class org.apache.hadoop.hbase.backup.HFileArchiver$FileableStoreFile, file:hdfs://master:54310/hbase/data/default/usertable/9cef9e7d34f96f1d9b111be50040a208/family/582d11d149af4c829654b41f188bc10d, to hdfs://master:54310/hbase/archive/data/default/usertable/9cef9e7d34f96f1d9b111be50040a208/family/582d11d149af4c829654b41f188bc10d
2014-07-01 15:44:31,092 DEBUG [regionserver60020-smallCompactions-1404253964856] backup.HFileArchiver: Finished archiving from class org.apache.hadoop.hbase.backup.HFileArchiver$FileableStoreFile, file:hdfs://master:54310/hbase/data/default/usertable/9cef9e7d34f96f1d9b111be50040a208/family/12264dc0884947c1960f156423a73464, to hdfs://master:54310/hbase/archive/data/default/usertable/9cef9e7d34f96f1d9b111be50040a208/family/12264dc0884947c1960f156423a73464
2014-07-01 15:44:31,095 DEBUG [regionserver60020-smallCompactions-1404253964856] backup.HFileArchiver: Finished archiving from class org.apache.hadoop.hbase.backup.HFileArchiver$FileableStoreFile, file:hdfs://master:54310/hbase/data/default/usertable/9cef9e7d34f96f1d9b111be50040a208/family/51ea620459a3482c8f384892f831e303, to hdfs://master:54310/hbase/archive/data/default/usertable/9cef9e7d34f96f1d9b111be50040a208/family/51ea620459a3482c8f384892f831e303
2014-07-01 15:44:31,099 DEBUG [regionserver60020-smallCompactions-1404253964856] backup.HFileArchiver: Finished archiving from class org.apache.hadoop.hbase.backup.HFileArchiver$FileableStoreFile, file:hdfs://master:54310/hbase/data/default/usertable/9cef9e7d34f96f1d9b111be50040a208/family/44a0953a47b54036b304d1558b547649, to hdfs://master:54310/hbase/archive/data/default/usertable/9cef9e7d34f96f1d9b111be50040a208/family/44a0953a47b54036b304d1558b547649
2014-07-01 15:44:31,107 DEBUG [regionserver60020-smallCompactions-1404253964856] backup.HFileArchiver: Finished archiving from class org.apache.hadoop.hbase.backup.HFileArchiver$FileableStoreFile, file:hdfs://master:54310/hbase/data/default/usertable/9cef9e7d34f96f1d9b111be50040a208/family/b284cd02aa7e4961ba8fce12cba46c67, to hdfs://master:54310/hbase/archive/data/default/usertable/9cef9e7d34f96f1d9b111be50040a208/family/b284cd02aa7e4961ba8fce12cba46c67
2014-07-01 15:44:31,120 DEBUG [regionserver60020-smallCompactions-1404253964856] backup.HFileArchiver: Finished archiving from class org.apache.hadoop.hbase.backup.HFileArchiver$FileableStoreFile, file:hdfs://master:54310/hbase/data/default/usertable/9cef9e7d34f96f1d9b111be50040a208/family/c69d742b517d48f3903c54a7145fa3fe, to hdfs://master:54310/hbase/archive/data/default/usertable/9cef9e7d34f96f1d9b111be50040a208/family/c69d742b517d48f3903c54a7145fa3fe
2014-07-01 15:44:31,133 DEBUG [regionserver60020-smallCompactions-1404253964856] backup.HFileArchiver: Finished archiving from class org.apache.hadoop.hbase.backup.HFileArchiver$FileableStoreFile, file:hdfs://master:54310/hbase/data/default/usertable/9cef9e7d34f96f1d9b111be50040a208/family/c9761ba6ee6045fdacae3b6f1c04d7f0, to hdfs://master:54310/hbase/archive/data/default/usertable/9cef9e7d34f96f1d9b111be50040a208/family/c9761ba6ee6045fdacae3b6f1c04d7f0
2014-07-01 15:44:31,133 INFO  [regionserver60020-smallCompactions-1404253964856] regionserver.HStore: Completed major compaction of 7 file(s) in family of usertable,,1404228265525.9cef9e7d34f96f1d9b111be50040a208. into fd1299745de743a1aedf8d741c8f5e6a(size=819.0m), total size for store is 819.0m. This selection was in queue for 0sec, and took 2mins, 6sec to execute.
2014-07-01 15:44:31,133 INFO  [regionserver60020-smallCompactions-1404253964856] regionserver.CompactSplitThread: Completed compaction: Request = regionName=usertable,,1404228265525.9cef9e7d34f96f1d9b111be50040a208., storeName=family, fileCount=7, fileSize=832.3m, priority=13, time=74655869730985; duration=2mins, 6sec
2014-07-01 15:44:31,133 DEBUG [regionserver60020-smallCompactions-1404253964856] regionserver.CompactSplitThread: CompactSplitThread Status: compaction_queue=(0:33), split_queue=0, merge_queue=0
2014-07-01 15:44:31,133 DEBUG [regionserver60020-smallCompactions-1404253964856] compactions.RatioBasedCompactionPolicy: Selecting compaction from 7 store files, 0 compacting, 7 eligible, 20 blocking
2014-07-01 15:44:31,134 DEBUG [regionserver60020-smallCompactions-1404253964856] compactions.ExploringCompactionPolicy: Exploring compaction algorithm has selected 7 files of size 246836410 starting at candidate #0 after considering 15 permutations with 15 in ratio
2014-07-01 15:44:31,134 DEBUG [regionserver60020-smallCompactions-1404253964856] regionserver.HStore: bb79ae773f71289e0745068f68a0abd9 - family: Initiating major compaction
2014-07-01 15:44:31,134 INFO  [regionserver60020-smallCompactions-1404253964856] regionserver.HRegion: Starting compaction on family in region usertable,user9,1404228265526.bb79ae773f71289e0745068f68a0abd9.
2014-07-01 15:44:31,134 INFO  [regionserver60020-smallCompactions-1404253964856] regionserver.HStore: Starting compaction of 7 file(s) in family of usertable,user9,1404228265526.bb79ae773f71289e0745068f68a0abd9. into tmpdir=hdfs://master:54310/hbase/data/default/usertable/bb79ae773f71289e0745068f68a0abd9/.tmp, totalSize=235.4m
2014-07-01 15:44:31,134 DEBUG [regionserver60020-smallCompactions-1404253964856] compactions.Compactor: Compacting hdfs://master:54310/hbase/data/default/usertable/bb79ae773f71289e0745068f68a0abd9/family/9a37cf2d20d042d4b5429677f7f034fa, keycount=18891, bloomtype=ROW, size=13.5m, encoding=NONE, seqNum=112, earliestPutTs=1404228282111
2014-07-01 15:44:31,134 DEBUG [regionserver60020-smallCompactions-1404253964856] compactions.Compactor: Compacting hdfs://master:54310/hbase/data/default/usertable/bb79ae773f71289e0745068f68a0abd9/family/ced32cffb61d4e6989176c141cc0ee27, keycount=20764, bloomtype=ROW, size=14.8m, encoding=NONE, seqNum=713, earliestPutTs=1404253897984
2014-07-01 15:44:31,134 DEBUG [regionserver60020-smallCompactions-1404253964856] compactions.Compactor: Compacting hdfs://master:54310/hbase/data/default/usertable/bb79ae773f71289e0745068f68a0abd9/family/c4a2b963ccb947d08390f8e9467a4f58, keycount=43104, bloomtype=ROW, size=30.7m, encoding=NONE, seqNum=1370, earliestPutTs=1404253969669
2014-07-01 15:44:31,135 DEBUG [regionserver60020-smallCompactions-1404253964856] compactions.Compactor: Compacting hdfs://master:54310/hbase/data/default/usertable/bb79ae773f71289e0745068f68a0abd9/family/964a508e7e9441bba2d103f7676241ca, keycount=48008, bloomtype=ROW, size=34.2m, encoding=NONE, seqNum=2117, earliestPutTs=1404254067006
2014-07-01 15:44:31,135 DEBUG [regionserver60020-smallCompactions-1404253964856] compactions.Compactor: Compacting hdfs://master:54310/hbase/data/default/usertable/bb79ae773f71289e0745068f68a0abd9/family/dfe213bff7bd4b9e8bd25f87a78f301e, keycount=61360, bloomtype=ROW, size=43.7m, encoding=NONE, seqNum=3121, earliestPutTs=1404254198601
2014-07-01 15:44:31,135 DEBUG [regionserver60020-smallCompactions-1404253964856] compactions.Compactor: Compacting hdfs://master:54310/hbase/data/default/usertable/bb79ae773f71289e0745068f68a0abd9/family/3146255461e7401681b8ae0e5f77640a, keycount=54309, bloomtype=ROW, size=38.7m, encoding=NONE, seqNum=3820, earliestPutTs=1404254305552
2014-07-01 15:44:31,135 DEBUG [regionserver60020-smallCompactions-1404253964856] compactions.Compactor: Compacting hdfs://master:54310/hbase/data/default/usertable/bb79ae773f71289e0745068f68a0abd9/family/5a36124e0535444485a0db68d8da8309, keycount=83949, bloomtype=ROW, size=59.8m, encoding=NONE, seqNum=4417, earliestPutTs=1404254385331
2014-07-01 15:44:31,164 DEBUG [regionserver60020-smallCompactions-1404253964856] util.FSUtils: DFS Client does not support most favored nodes create; using default create
2014-07-01 15:45:03,465 DEBUG [regionserver60020-smallCompactions-1404253964856] regionserver.HRegionFileSystem: Committing store file hdfs://master:54310/hbase/data/default/usertable/bb79ae773f71289e0745068f68a0abd9/.tmp/461d41f2b1274df0a6ca2ed0e39229fd as hdfs://master:54310/hbase/data/default/usertable/bb79ae773f71289e0745068f68a0abd9/family/461d41f2b1274df0a6ca2ed0e39229fd
2014-07-01 15:45:03,491 DEBUG [regionserver60020-smallCompactions-1404253964856] regionserver.HStore: Removing store files after compaction...
2014-07-01 15:45:03,512 DEBUG [regionserver60020-smallCompactions-1404253964856] backup.HFileArchiver: Finished archiving from class org.apache.hadoop.hbase.backup.HFileArchiver$FileableStoreFile, file:hdfs://master:54310/hbase/data/default/usertable/bb79ae773f71289e0745068f68a0abd9/family/9a37cf2d20d042d4b5429677f7f034fa, to hdfs://master:54310/hbase/archive/data/default/usertable/bb79ae773f71289e0745068f68a0abd9/family/9a37cf2d20d042d4b5429677f7f034fa
2014-07-01 15:45:03,516 DEBUG [regionserver60020-smallCompactions-1404253964856] backup.HFileArchiver: Finished archiving from class org.apache.hadoop.hbase.backup.HFileArchiver$FileableStoreFile, file:hdfs://master:54310/hbase/data/default/usertable/bb79ae773f71289e0745068f68a0abd9/family/ced32cffb61d4e6989176c141cc0ee27, to hdfs://master:54310/hbase/archive/data/default/usertable/bb79ae773f71289e0745068f68a0abd9/family/ced32cffb61d4e6989176c141cc0ee27
2014-07-01 15:45:03,528 DEBUG [regionserver60020-smallCompactions-1404253964856] backup.HFileArchiver: Finished archiving from class org.apache.hadoop.hbase.backup.HFileArchiver$FileableStoreFile, file:hdfs://master:54310/hbase/data/default/usertable/bb79ae773f71289e0745068f68a0abd9/family/c4a2b963ccb947d08390f8e9467a4f58, to hdfs://master:54310/hbase/archive/data/default/usertable/bb79ae773f71289e0745068f68a0abd9/family/c4a2b963ccb947d08390f8e9467a4f58
2014-07-01 15:45:03,531 DEBUG [regionserver60020-smallCompactions-1404253964856] backup.HFileArchiver: Finished archiving from class org.apache.hadoop.hbase.backup.HFileArchiver$FileableStoreFile, file:hdfs://master:54310/hbase/data/default/usertable/bb79ae773f71289e0745068f68a0abd9/family/964a508e7e9441bba2d103f7676241ca, to hdfs://master:54310/hbase/archive/data/default/usertable/bb79ae773f71289e0745068f68a0abd9/family/964a508e7e9441bba2d103f7676241ca
2014-07-01 15:45:03,536 DEBUG [regionserver60020-smallCompactions-1404253964856] backup.HFileArchiver: Finished archiving from class org.apache.hadoop.hbase.backup.HFileArchiver$FileableStoreFile, file:hdfs://master:54310/hbase/data/default/usertable/bb79ae773f71289e0745068f68a0abd9/family/dfe213bff7bd4b9e8bd25f87a78f301e, to hdfs://master:54310/hbase/archive/data/default/usertable/bb79ae773f71289e0745068f68a0abd9/family/dfe213bff7bd4b9e8bd25f87a78f301e
2014-07-01 15:45:03,539 DEBUG [regionserver60020-smallCompactions-1404253964856] backup.HFileArchiver: Finished archiving from class org.apache.hadoop.hbase.backup.HFileArchiver$FileableStoreFile, file:hdfs://master:54310/hbase/data/default/usertable/bb79ae773f71289e0745068f68a0abd9/family/3146255461e7401681b8ae0e5f77640a, to hdfs://master:54310/hbase/archive/data/default/usertable/bb79ae773f71289e0745068f68a0abd9/family/3146255461e7401681b8ae0e5f77640a
2014-07-01 15:45:03,542 DEBUG [regionserver60020-smallCompactions-1404253964856] backup.HFileArchiver: Finished archiving from class org.apache.hadoop.hbase.backup.HFileArchiver$FileableStoreFile, file:hdfs://master:54310/hbase/data/default/usertable/bb79ae773f71289e0745068f68a0abd9/family/5a36124e0535444485a0db68d8da8309, to hdfs://master:54310/hbase/archive/data/default/usertable/bb79ae773f71289e0745068f68a0abd9/family/5a36124e0535444485a0db68d8da8309
2014-07-01 15:45:03,542 INFO  [regionserver60020-smallCompactions-1404253964856] regionserver.HStore: Completed major compaction of 7 file(s) in family of usertable,user9,1404228265526.bb79ae773f71289e0745068f68a0abd9. into 461d41f2b1274df0a6ca2ed0e39229fd(size=223.5m), total size for store is 223.5m. This selection was in queue for 0sec, and took 32sec to execute.
2014-07-01 15:45:03,542 INFO  [regionserver60020-smallCompactions-1404253964856] regionserver.CompactSplitThread: Completed compaction: Request = regionName=usertable,user9,1404228265526.bb79ae773f71289e0745068f68a0abd9., storeName=family, fileCount=7, fileSize=235.4m, priority=13, time=74782523048103; duration=32sec
2014-07-01 15:45:03,542 DEBUG [regionserver60020-smallCompactions-1404253964856] regionserver.CompactSplitThread: CompactSplitThread Status: compaction_queue=(0:32), split_queue=0, merge_queue=0
2014-07-01 15:45:03,543 DEBUG [regionserver60020-smallCompactions-1404253964856] compactions.RatioBasedCompactionPolicy: Selecting compaction from 7 store files, 0 compacting, 7 eligible, 20 blocking
2014-07-01 15:45:03,543 DEBUG [regionserver60020-smallCompactions-1404253964856] compactions.ExploringCompactionPolicy: Exploring compaction algorithm has selected 7 files of size 901917010 starting at candidate #0 after considering 15 permutations with 13 in ratio
2014-07-01 15:45:03,543 DEBUG [regionserver60020-smallCompactions-1404253964856] regionserver.HStore: 122e92eabfab38cb15a9965b21db25c5 - family: Initiating major compaction
2014-07-01 15:45:03,543 INFO  [regionserver60020-smallCompactions-1404253964856] regionserver.HRegion: Starting compaction on family in region usertable,user5,1404228265526.122e92eabfab38cb15a9965b21db25c5.
2014-07-01 15:45:03,543 INFO  [regionserver60020-smallCompactions-1404253964856] regionserver.HStore: Starting compaction of 7 file(s) in family of usertable,user5,1404228265526.122e92eabfab38cb15a9965b21db25c5. into tmpdir=hdfs://master:54310/hbase/data/default/usertable/122e92eabfab38cb15a9965b21db25c5/.tmp, totalSize=860.1m
2014-07-01 15:45:03,544 DEBUG [regionserver60020-smallCompactions-1404253964856] compactions.Compactor: Compacting hdfs://master:54310/hbase/data/default/usertable/122e92eabfab38cb15a9965b21db25c5/family/a323c796f89d4c1fb403732654d16b5f, keycount=395104, bloomtype=ROW, size=281.6m, encoding=NONE, seqNum=2170, earliestPutTs=1404228281724
2014-07-01 15:45:03,544 DEBUG [regionserver60020-smallCompactions-1404253964856] compactions.Compactor: Compacting hdfs://master:54310/hbase/data/default/usertable/122e92eabfab38cb15a9965b21db25c5/family/b5325e7ef6d54c65869ab9b110690b41, keycount=117150, bloomtype=ROW, size=83.5m, encoding=NONE, seqNum=2624, earliestPutTs=1404254199618
2014-07-01 15:45:03,544 DEBUG [regionserver60020-smallCompactions-1404253964856] compactions.Compactor: Compacting hdfs://master:54310/hbase/data/default/usertable/122e92eabfab38cb15a9965b21db25c5/family/cf9c7b67a9d34e4a9e524c182876c966, keycount=118656, bloomtype=ROW, size=84.5m, encoding=NONE, seqNum=3204, earliestPutTs=1404254254860
2014-07-01 15:45:03,544 DEBUG [regionserver60020-smallCompactions-1404253964856] compactions.Compactor: Compacting hdfs://master:54310/hbase/data/default/usertable/122e92eabfab38cb15a9965b21db25c5/family/9a97e36b7a674795b574e980910e9062, keycount=84838, bloomtype=ROW, size=60.4m, encoding=NONE, seqNum=3732, earliestPutTs=1404254309215
2014-07-01 15:45:03,544 DEBUG [regionserver60020-smallCompactions-1404253964856] compactions.Compactor: Compacting hdfs://master:54310/hbase/data/default/usertable/122e92eabfab38cb15a9965b21db25c5/family/4518275a7d1645c3904d642c1676ae50, keycount=211173, bloomtype=ROW, size=150.3m, encoding=NONE, seqNum=4077, earliestPutTs=1404254369302
2014-07-01 15:45:03,544 DEBUG [regionserver60020-smallCompactions-1404253964856] compactions.Compactor: Compacting hdfs://master:54310/hbase/data/default/usertable/122e92eabfab38cb15a9965b21db25c5/family/9ddbaa59638243d09cf3db65eb19533c, keycount=187630, bloomtype=ROW, size=133.6m, encoding=NONE, seqNum=4523, earliestPutTs=1404254412267
2014-07-01 15:45:03,544 DEBUG [regionserver60020-smallCompactions-1404253964856] compactions.Compactor: Compacting hdfs://master:54310/hbase/data/default/usertable/122e92eabfab38cb15a9965b21db25c5/family/ff172869b47640eca1ae361f051b39b8, keycount=92908, bloomtype=ROW, size=66.2m, encoding=NONE, seqNum=4674, earliestPutTs=1404254466603
2014-07-01 15:45:03,614 DEBUG [regionserver60020-smallCompactions-1404253964856] util.FSUtils: DFS Client does not support most favored nodes create; using default create
2014-07-01 15:46:08,969 DEBUG [LruStats #0] hfile.LruBlockCache: Total=702.99 MB, free=3.27 GB, max=3.96 GB, blocks=10985, accesses=2685342, hits=2545633, hitRatio=94.79%, , cachingAccesses=2556999, cachingHits=2542251, cachingHitsRatio=99.42%, evictions=0, evicted=3501, evictedPerRun=Infinity
2014-07-01 15:47:10,601 DEBUG [regionserver60020-smallCompactions-1404253964856] regionserver.HRegionFileSystem: Committing store file hdfs://master:54310/hbase/data/default/usertable/122e92eabfab38cb15a9965b21db25c5/.tmp/f4a8142044e841dcb47eae4bbbacffe5 as hdfs://master:54310/hbase/data/default/usertable/122e92eabfab38cb15a9965b21db25c5/family/f4a8142044e841dcb47eae4bbbacffe5
2014-07-01 15:47:10,646 DEBUG [regionserver60020-smallCompactions-1404253964856] regionserver.HStore: Removing store files after compaction...
2014-07-01 15:47:10,682 DEBUG [regionserver60020-smallCompactions-1404253964856] backup.HFileArchiver: Finished archiving from class org.apache.hadoop.hbase.backup.HFileArchiver$FileableStoreFile, file:hdfs://master:54310/hbase/data/default/usertable/122e92eabfab38cb15a9965b21db25c5/family/a323c796f89d4c1fb403732654d16b5f, to hdfs://master:54310/hbase/archive/data/default/usertable/122e92eabfab38cb15a9965b21db25c5/family/a323c796f89d4c1fb403732654d16b5f
2014-07-01 15:47:10,686 DEBUG [regionserver60020-smallCompactions-1404253964856] backup.HFileArchiver: Finished archiving from class org.apache.hadoop.hbase.backup.HFileArchiver$FileableStoreFile, file:hdfs://master:54310/hbase/data/default/usertable/122e92eabfab38cb15a9965b21db25c5/family/b5325e7ef6d54c65869ab9b110690b41, to hdfs://master:54310/hbase/archive/data/default/usertable/122e92eabfab38cb15a9965b21db25c5/family/b5325e7ef6d54c65869ab9b110690b41
2014-07-01 15:47:10,690 DEBUG [regionserver60020-smallCompactions-1404253964856] backup.HFileArchiver: Finished archiving from class org.apache.hadoop.hbase.backup.HFileArchiver$FileableStoreFile, file:hdfs://master:54310/hbase/data/default/usertable/122e92eabfab38cb15a9965b21db25c5/family/cf9c7b67a9d34e4a9e524c182876c966, to hdfs://master:54310/hbase/archive/data/default/usertable/122e92eabfab38cb15a9965b21db25c5/family/cf9c7b67a9d34e4a9e524c182876c966
2014-07-01 15:47:10,692 DEBUG [regionserver60020-smallCompactions-1404253964856] backup.HFileArchiver: Finished archiving from class org.apache.hadoop.hbase.backup.HFileArchiver$FileableStoreFile, file:hdfs://master:54310/hbase/data/default/usertable/122e92eabfab38cb15a9965b21db25c5/family/9a97e36b7a674795b574e980910e9062, to hdfs://master:54310/hbase/archive/data/default/usertable/122e92eabfab38cb15a9965b21db25c5/family/9a97e36b7a674795b574e980910e9062
2014-07-01 15:47:10,697 DEBUG [regionserver60020-smallCompactions-1404253964856] backup.HFileArchiver: Finished archiving from class org.apache.hadoop.hbase.backup.HFileArchiver$FileableStoreFile, file:hdfs://master:54310/hbase/data/default/usertable/122e92eabfab38cb15a9965b21db25c5/family/4518275a7d1645c3904d642c1676ae50, to hdfs://master:54310/hbase/archive/data/default/usertable/122e92eabfab38cb15a9965b21db25c5/family/4518275a7d1645c3904d642c1676ae50
2014-07-01 15:47:10,700 DEBUG [regionserver60020-smallCompactions-1404253964856] backup.HFileArchiver: Finished archiving from class org.apache.hadoop.hbase.backup.HFileArchiver$FileableStoreFile, file:hdfs://master:54310/hbase/data/default/usertable/122e92eabfab38cb15a9965b21db25c5/family/9ddbaa59638243d09cf3db65eb19533c, to hdfs://master:54310/hbase/archive/data/default/usertable/122e92eabfab38cb15a9965b21db25c5/family/9ddbaa59638243d09cf3db65eb19533c
2014-07-01 15:47:10,705 DEBUG [regionserver60020-smallCompactions-1404253964856] backup.HFileArchiver: Finished archiving from class org.apache.hadoop.hbase.backup.HFileArchiver$FileableStoreFile, file:hdfs://master:54310/hbase/data/default/usertable/122e92eabfab38cb15a9965b21db25c5/family/ff172869b47640eca1ae361f051b39b8, to hdfs://master:54310/hbase/archive/data/default/usertable/122e92eabfab38cb15a9965b21db25c5/family/ff172869b47640eca1ae361f051b39b8
2014-07-01 15:47:10,706 INFO  [regionserver60020-smallCompactions-1404253964856] regionserver.HStore: Completed major compaction of 7 file(s) in family of usertable,user5,1404228265526.122e92eabfab38cb15a9965b21db25c5. into f4a8142044e841dcb47eae4bbbacffe5(size=836.3m), total size for store is 836.3m. This selection was in queue for 0sec, and took 2mins, 7sec to execute.
2014-07-01 15:47:10,708 INFO  [regionserver60020-smallCompactions-1404253964856] regionserver.CompactSplitThread: Completed compaction: Request = regionName=usertable,user5,1404228265526.122e92eabfab38cb15a9965b21db25c5., storeName=family, fileCount=7, fileSize=860.1m, priority=13, time=74814932328103; duration=2mins, 7sec
2014-07-01 15:47:10,709 DEBUG [regionserver60020-smallCompactions-1404253964856] regionserver.CompactSplitThread: CompactSplitThread Status: compaction_queue=(0:31), split_queue=0, merge_queue=0
2014-07-01 15:47:10,709 DEBUG [regionserver60020-smallCompactions-1404253964856] compactions.RatioBasedCompactionPolicy: Selecting compaction from 6 store files, 0 compacting, 6 eligible, 20 blocking
2014-07-01 15:47:10,709 DEBUG [regionserver60020-smallCompactions-1404253964856] compactions.ExploringCompactionPolicy: Exploring compaction algorithm has selected 6 files of size 934259017 starting at candidate #0 after considering 10 permutations with 9 in ratio
2014-07-01 15:47:10,709 DEBUG [regionserver60020-smallCompactions-1404253964856] regionserver.HStore: 667c1f2c34fe37d48ba25a789d8aba98 - family: Initiating major compaction
2014-07-01 15:47:10,709 INFO  [regionserver60020-smallCompactions-1404253964856] regionserver.HRegion: Starting compaction on family in region usertable,user7,1404228265526.667c1f2c34fe37d48ba25a789d8aba98.
2014-07-01 15:47:10,710 INFO  [regionserver60020-smallCompactions-1404253964856] regionserver.HStore: Starting compaction of 6 file(s) in family of usertable,user7,1404228265526.667c1f2c34fe37d48ba25a789d8aba98. into tmpdir=hdfs://master:54310/hbase/data/default/usertable/667c1f2c34fe37d48ba25a789d8aba98/.tmp, totalSize=891.0m
2014-07-01 15:47:10,710 DEBUG [regionserver60020-smallCompactions-1404253964856] compactions.Compactor: Compacting hdfs://master:54310/hbase/data/default/usertable/667c1f2c34fe37d48ba25a789d8aba98/family/56200eedd338491786b04080265aa716, keycount=545792, bloomtype=ROW, size=388.9m, encoding=NONE, seqNum=2915, earliestPutTs=1404228281826
2014-07-01 15:47:10,710 DEBUG [regionserver60020-smallCompactions-1404253964856] compactions.Compactor: Compacting hdfs://master:54310/hbase/data/default/usertable/667c1f2c34fe37d48ba25a789d8aba98/family/50f4de22ccd04828b18b93cc72b88964, keycount=147565, bloomtype=ROW, size=105.0m, encoding=NONE, seqNum=3442, earliestPutTs=1404254293026
2014-07-01 15:47:10,710 DEBUG [regionserver60020-smallCompactions-1404253964856] compactions.Compactor: Compacting hdfs://master:54310/hbase/data/default/usertable/667c1f2c34fe37d48ba25a789d8aba98/family/244e3443161840b5a95504a408c4edc2, keycount=155927, bloomtype=ROW, size=111.0m, encoding=NONE, seqNum=3881, earliestPutTs=1404254357203
2014-07-01 15:47:10,710 DEBUG [regionserver60020-smallCompactions-1404253964856] compactions.Compactor: Compacting hdfs://master:54310/hbase/data/default/usertable/667c1f2c34fe37d48ba25a789d8aba98/family/a5468df3646d43fda8dcfd231b11facb, keycount=176456, bloomtype=ROW, size=125.7m, encoding=NONE, seqNum=4315, earliestPutTs=1404254389941
2014-07-01 15:47:10,710 DEBUG [regionserver60020-smallCompactions-1404253964856] compactions.Compactor: Compacting hdfs://master:54310/hbase/data/default/usertable/667c1f2c34fe37d48ba25a789d8aba98/family/5c4ecd1e5f5f40a68639ae8651cdf308, keycount=132606, bloomtype=ROW, size=94.4m, encoding=NONE, seqNum=4527, earliestPutTs=1404254439322
2014-07-01 15:47:10,710 DEBUG [regionserver60020-smallCompactions-1404253964856] compactions.Compactor: Compacting hdfs://master:54310/hbase/data/default/usertable/667c1f2c34fe37d48ba25a789d8aba98/family/6c4e4b0a6c594b6db087ac5f958bca9c, keycount=92565, bloomtype=ROW, size=66.0m, encoding=NONE, seqNum=4678, earliestPutTs=1404254473587
2014-07-01 15:47:10,753 DEBUG [regionserver60020-smallCompactions-1404253964856] util.FSUtils: DFS Client does not support most favored nodes create; using default create
2014-07-01 15:48:13,496 WARN  [RpcServer.reader=9,port=60020] ipc.RpcServer: RpcServer.listener,port=60020: count of bytes read: 0
java.io.IOException: Connection reset by peer
	at sun.nio.ch.FileDispatcher.read0(Native Method)
	at sun.nio.ch.SocketDispatcher.read(SocketDispatcher.java:39)
	at sun.nio.ch.IOUtil.readIntoNativeBuffer(IOUtil.java:251)
	at sun.nio.ch.IOUtil.read(IOUtil.java:224)
	at sun.nio.ch.SocketChannelImpl.read(SocketChannelImpl.java:254)
	at org.apache.hadoop.hbase.ipc.RpcServer.channelRead(RpcServer.java:2229)
	at org.apache.hadoop.hbase.ipc.RpcServer$Connection.readAndProcess(RpcServer.java:1415)
	at org.apache.hadoop.hbase.ipc.RpcServer$Listener.doRead(RpcServer.java:790)
	at org.apache.hadoop.hbase.ipc.RpcServer$Listener$Reader.doRunLoop(RpcServer.java:581)
	at org.apache.hadoop.hbase.ipc.RpcServer$Listener$Reader.run(RpcServer.java:556)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1146)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:701)
2014-07-01 15:48:50,932 WARN  [DataStreamer for file /hbase/data/default/usertable/667c1f2c34fe37d48ba25a789d8aba98/.tmp/fc32759b35c444c89da742a5c2316f15] hdfs.DFSClient: DataStreamer Exception: java.io.IOException: Call to master/9.1.143.58:54310 failed on local exception: java.io.EOFException
	at org.apache.hadoop.ipc.Client.wrapException(Client.java:1150)
	at org.apache.hadoop.ipc.Client.call(Client.java:1118)
	at org.apache.hadoop.ipc.RPC$Invoker.invoke(RPC.java:229)
	at com.sun.proxy.$Proxy8.addBlock(Unknown Source)
	at sun.reflect.GeneratedMethodAccessor4.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:622)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:85)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:62)
	at com.sun.proxy.$Proxy8.addBlock(Unknown Source)
	at sun.reflect.GeneratedMethodAccessor4.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:622)
	at org.apache.hadoop.hbase.fs.HFileSystem$1.invoke(HFileSystem.java:294)
	at com.sun.proxy.$Proxy9.addBlock(Unknown Source)
	at org.apache.hadoop.hdfs.DFSClient$DFSOutputStream.locateFollowingBlock(DFSClient.java:3720)
	at org.apache.hadoop.hdfs.DFSClient$DFSOutputStream.nextBlockOutputStream(DFSClient.java:3580)
	at org.apache.hadoop.hdfs.DFSClient$DFSOutputStream.access$2600(DFSClient.java:2783)
	at org.apache.hadoop.hdfs.DFSClient$DFSOutputStream$DataStreamer.run(DFSClient.java:3023)
Caused by: java.io.EOFException
	at java.io.DataInputStream.readInt(DataInputStream.java:392)
	at org.apache.hadoop.ipc.Client$Connection.receiveResponse(Client.java:845)
	at org.apache.hadoop.ipc.Client$Connection.run(Client.java:790)

2014-07-01 15:48:50,932 WARN  [DataStreamer for file /hbase/data/default/usertable/667c1f2c34fe37d48ba25a789d8aba98/.tmp/fc32759b35c444c89da742a5c2316f15] hdfs.DFSClient: Error Recovery for blk_4203773071543290358_29993 bad datanode[0] nodes == null
2014-07-01 15:48:50,938 WARN  [DataStreamer for file /hbase/data/default/usertable/667c1f2c34fe37d48ba25a789d8aba98/.tmp/fc32759b35c444c89da742a5c2316f15] hdfs.DFSClient: Could not get block locations. Source file "/hbase/data/default/usertable/667c1f2c34fe37d48ba25a789d8aba98/.tmp/fc32759b35c444c89da742a5c2316f15" - Aborting...
2014-07-01 15:48:51,016 ERROR [regionserver60020-smallCompactions-1404253964856] regionserver.CompactSplitThread: Compaction failed Request = regionName=usertable,user7,1404228265526.667c1f2c34fe37d48ba25a789d8aba98., storeName=family, fileCount=6, fileSize=891.0m (388.9m, 105.0m, 111.0m, 125.7m, 94.4m, 66.0m), priority=14, time=74942098376449
java.io.IOException: Call to master/9.1.143.58:54310 failed on local exception: java.io.EOFException
	at org.apache.hadoop.ipc.Client.wrapException(Client.java:1150)
	at org.apache.hadoop.ipc.Client.call(Client.java:1118)
	at org.apache.hadoop.ipc.RPC$Invoker.invoke(RPC.java:229)
	at com.sun.proxy.$Proxy8.addBlock(Unknown Source)
	at sun.reflect.GeneratedMethodAccessor4.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:622)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:85)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:62)
	at com.sun.proxy.$Proxy8.addBlock(Unknown Source)
	at sun.reflect.GeneratedMethodAccessor4.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:622)
	at org.apache.hadoop.hbase.fs.HFileSystem$1.invoke(HFileSystem.java:294)
	at com.sun.proxy.$Proxy9.addBlock(Unknown Source)
	at org.apache.hadoop.hdfs.DFSClient$DFSOutputStream.locateFollowingBlock(DFSClient.java:3720)
	at org.apache.hadoop.hdfs.DFSClient$DFSOutputStream.nextBlockOutputStream(DFSClient.java:3580)
	at org.apache.hadoop.hdfs.DFSClient$DFSOutputStream.access$2600(DFSClient.java:2783)
	at org.apache.hadoop.hdfs.DFSClient$DFSOutputStream$DataStreamer.run(DFSClient.java:3023)
Caused by: java.io.EOFException
	at java.io.DataInputStream.readInt(DataInputStream.java:392)
	at org.apache.hadoop.ipc.Client$Connection.receiveResponse(Client.java:845)
	at org.apache.hadoop.ipc.Client$Connection.run(Client.java:790)
2014-07-01 15:48:51,017 DEBUG [regionserver60020-smallCompactions-1404253964856] regionserver.CompactSplitThread: CompactSplitThread Status: compaction_queue=(0:30), split_queue=0, merge_queue=0
2014-07-01 15:48:51,017 DEBUG [regionserver60020-smallCompactions-1404253964856] compactions.RatioBasedCompactionPolicy: Selecting compaction from 6 store files, 0 compacting, 6 eligible, 20 blocking
2014-07-01 15:48:51,017 DEBUG [regionserver60020-smallCompactions-1404253964856] compactions.ExploringCompactionPolicy: Exploring compaction algorithm has selected 6 files of size 934259017 starting at candidate #0 after considering 10 permutations with 9 in ratio
2014-07-01 15:48:51,017 DEBUG [regionserver60020-smallCompactions-1404253964856] regionserver.HStore: 667c1f2c34fe37d48ba25a789d8aba98 - family: Initiating major compaction
2014-07-01 15:48:51,018 INFO  [regionserver60020-smallCompactions-1404253964856] regionserver.HRegion: Starting compaction on family in region usertable,user7,1404228265526.667c1f2c34fe37d48ba25a789d8aba98.
2014-07-01 15:48:51,018 INFO  [regionserver60020-smallCompactions-1404253964856] regionserver.HStore: Starting compaction of 6 file(s) in family of usertable,user7,1404228265526.667c1f2c34fe37d48ba25a789d8aba98. into tmpdir=hdfs://master:54310/hbase/data/default/usertable/667c1f2c34fe37d48ba25a789d8aba98/.tmp, totalSize=891.0m
2014-07-01 15:48:51,018 DEBUG [regionserver60020-smallCompactions-1404253964856] compactions.Compactor: Compacting hdfs://master:54310/hbase/data/default/usertable/667c1f2c34fe37d48ba25a789d8aba98/family/56200eedd338491786b04080265aa716, keycount=545792, bloomtype=ROW, size=388.9m, encoding=NONE, seqNum=2915, earliestPutTs=1404228281826
2014-07-01 15:48:51,018 DEBUG [regionserver60020-smallCompactions-1404253964856] compactions.Compactor: Compacting hdfs://master:54310/hbase/data/default/usertable/667c1f2c34fe37d48ba25a789d8aba98/family/50f4de22ccd04828b18b93cc72b88964, keycount=147565, bloomtype=ROW, size=105.0m, encoding=NONE, seqNum=3442, earliestPutTs=1404254293026
2014-07-01 15:48:51,018 DEBUG [regionserver60020-smallCompactions-1404253964856] compactions.Compactor: Compacting hdfs://master:54310/hbase/data/default/usertable/667c1f2c34fe37d48ba25a789d8aba98/family/244e3443161840b5a95504a408c4edc2, keycount=155927, bloomtype=ROW, size=111.0m, encoding=NONE, seqNum=3881, earliestPutTs=1404254357203
2014-07-01 15:48:51,018 DEBUG [regionserver60020-smallCompactions-1404253964856] compactions.Compactor: Compacting hdfs://master:54310/hbase/data/default/usertable/667c1f2c34fe37d48ba25a789d8aba98/family/a5468df3646d43fda8dcfd231b11facb, keycount=176456, bloomtype=ROW, size=125.7m, encoding=NONE, seqNum=4315, earliestPutTs=1404254389941
2014-07-01 15:48:51,018 DEBUG [regionserver60020-smallCompactions-1404253964856] compactions.Compactor: Compacting hdfs://master:54310/hbase/data/default/usertable/667c1f2c34fe37d48ba25a789d8aba98/family/5c4ecd1e5f5f40a68639ae8651cdf308, keycount=132606, bloomtype=ROW, size=94.4m, encoding=NONE, seqNum=4527, earliestPutTs=1404254439322
2014-07-01 15:48:51,019 DEBUG [regionserver60020-smallCompactions-1404253964856] compactions.Compactor: Compacting hdfs://master:54310/hbase/data/default/usertable/667c1f2c34fe37d48ba25a789d8aba98/family/6c4e4b0a6c594b6db087ac5f958bca9c, keycount=92565, bloomtype=ROW, size=66.0m, encoding=NONE, seqNum=4678, earliestPutTs=1404254473587
2014-07-01 15:48:52,053 INFO  [regionserver60020-smallCompactions-1404253964856] ipc.Client: Retrying connect to server: master/9.1.143.58:54310. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1 SECONDS)
2014-07-01 15:48:53,053 INFO  [regionserver60020-smallCompactions-1404253964856] ipc.Client: Retrying connect to server: master/9.1.143.58:54310. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1 SECONDS)
2014-07-01 15:48:54,054 INFO  [regionserver60020-smallCompactions-1404253964856] ipc.Client: Retrying connect to server: master/9.1.143.58:54310. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1 SECONDS)
2014-07-01 15:48:55,056 INFO  [regionserver60020-smallCompactions-1404253964856] ipc.Client: Retrying connect to server: master/9.1.143.58:54310. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1 SECONDS)
2014-07-01 15:48:55,616 WARN  [ResponseProcessor for block blk_-3161611795271398729_29985] hdfs.DFSClient: DFSOutputStream ResponseProcessor exception  for block blk_-3161611795271398729_29985java.io.EOFException
	at java.io.DataInputStream.readFully(DataInputStream.java:197)
	at java.io.DataInputStream.readLong(DataInputStream.java:416)
	at org.apache.hadoop.hdfs.protocol.DataTransferProtocol$PipelineAck.readFields(DataTransferProtocol.java:124)
	at org.apache.hadoop.hdfs.DFSClient$DFSOutputStream$ResponseProcessor.run(DFSClient.java:3161)

2014-07-01 15:48:55,616 WARN  [DataStreamer for file /hbase/WALs/slave1,60020,1404253868846/slave1%2C60020%2C1404253868846.1404254547570 block blk_-3161611795271398729_29985] hdfs.DFSClient: Error Recovery for blk_-3161611795271398729_29985 bad datanode[0] 9.1.143.59:50010
2014-07-01 15:48:55,628 WARN  [DataStreamer for file /hbase/WALs/slave1,60020,1404253868846/slave1%2C60020%2C1404253868846.1404254547570 block blk_-3161611795271398729_29985] hdfs.DFSClient: Error Recovery for block blk_-3161611795271398729_29985 in pipeline 9.1.143.59:50010, 9.1.143.58:50010: bad datanode 9.1.143.59:50010
2014-07-01 15:48:56,057 INFO  [regionserver60020-smallCompactions-1404253964856] ipc.Client: Retrying connect to server: master/9.1.143.58:54310. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1 SECONDS)
2014-07-01 15:48:56,682 INFO  [DataStreamer for file /hbase/WALs/slave1,60020,1404253868846/slave1%2C60020%2C1404253868846.1404254547570 block blk_-3161611795271398729_29985] ipc.Client: Retrying connect to server: master/9.1.143.58:50020. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1 SECONDS)
2014-07-01 15:48:57,058 INFO  [regionserver60020-smallCompactions-1404253964856] ipc.Client: Retrying connect to server: master/9.1.143.58:54310. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1 SECONDS)
2014-07-01 15:48:57,682 INFO  [DataStreamer for file /hbase/WALs/slave1,60020,1404253868846/slave1%2C60020%2C1404253868846.1404254547570 block blk_-3161611795271398729_29985] ipc.Client: Retrying connect to server: master/9.1.143.58:50020. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1 SECONDS)
2014-07-01 15:48:58,060 INFO  [regionserver60020-smallCompactions-1404253964856] ipc.Client: Retrying connect to server: master/9.1.143.58:54310. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1 SECONDS)
2014-07-01 15:48:58,683 INFO  [DataStreamer for file /hbase/WALs/slave1,60020,1404253868846/slave1%2C60020%2C1404253868846.1404254547570 block blk_-3161611795271398729_29985] ipc.Client: Retrying connect to server: master/9.1.143.58:50020. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1 SECONDS)
2014-07-01 15:48:59,061 INFO  [regionserver60020-smallCompactions-1404253964856] ipc.Client: Retrying connect to server: master/9.1.143.58:54310. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1 SECONDS)
2014-07-01 15:48:59,684 INFO  [DataStreamer for file /hbase/WALs/slave1,60020,1404253868846/slave1%2C60020%2C1404253868846.1404254547570 block blk_-3161611795271398729_29985] ipc.Client: Retrying connect to server: master/9.1.143.58:50020. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1 SECONDS)
2014-07-01 15:49:00,061 INFO  [regionserver60020-smallCompactions-1404253964856] ipc.Client: Retrying connect to server: master/9.1.143.58:54310. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1 SECONDS)
2014-07-01 15:49:00,684 INFO  [DataStreamer for file /hbase/WALs/slave1,60020,1404253868846/slave1%2C60020%2C1404253868846.1404254547570 block blk_-3161611795271398729_29985] ipc.Client: Retrying connect to server: master/9.1.143.58:50020. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1 SECONDS)
2014-07-01 15:49:01,062 INFO  [regionserver60020-smallCompactions-1404253964856] ipc.Client: Retrying connect to server: master/9.1.143.58:54310. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1 SECONDS)
2014-07-01 15:49:01,064 ERROR [regionserver60020-smallCompactions-1404253964856] regionserver.CompactSplitThread: Compaction failed Request = regionName=usertable,user7,1404228265526.667c1f2c34fe37d48ba25a789d8aba98., storeName=family, fileCount=6, fileSize=891.0m (388.9m, 105.0m, 111.0m, 125.7m, 94.4m, 66.0m), priority=14, time=75042406692246
java.net.ConnectException: Call to master/9.1.143.58:54310 failed on connection exception: java.net.ConnectException: Connection refused
	at org.apache.hadoop.ipc.Client.wrapException(Client.java:1142)
	at org.apache.hadoop.ipc.Client.call(Client.java:1118)
	at org.apache.hadoop.ipc.RPC$Invoker.invoke(RPC.java:229)
	at com.sun.proxy.$Proxy8.getFileInfo(Unknown Source)
	at sun.reflect.GeneratedMethodAccessor2.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:622)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:85)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:62)
	at com.sun.proxy.$Proxy8.getFileInfo(Unknown Source)
	at sun.reflect.GeneratedMethodAccessor2.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:622)
	at org.apache.hadoop.hbase.fs.HFileSystem$1.invoke(HFileSystem.java:294)
	at com.sun.proxy.$Proxy9.getFileInfo(Unknown Source)
	at org.apache.hadoop.hdfs.DFSClient.getFileInfo(DFSClient.java:1059)
	at org.apache.hadoop.hdfs.DistributedFileSystem.getFileStatus(DistributedFileSystem.java:554)
	at org.apache.hadoop.fs.FilterFileSystem.getFileStatus(FilterFileSystem.java:255)
	at org.apache.hadoop.fs.FileSystem.exists(FileSystem.java:788)
	at org.apache.hadoop.hbase.regionserver.StoreFile$WriterBuilder.build(StoreFile.java:603)
	at org.apache.hadoop.hbase.regionserver.HStore.createWriterInTmp(HStore.java:889)
	at org.apache.hadoop.hbase.regionserver.compactions.DefaultCompactor.compact(DefaultCompactor.java:74)
	at org.apache.hadoop.hbase.regionserver.DefaultStoreEngine$DefaultCompactionContext.compact(DefaultStoreEngine.java:109)
	at org.apache.hadoop.hbase.regionserver.HStore.compact(HStore.java:1086)
	at org.apache.hadoop.hbase.regionserver.HRegion.compact(HRegion.java:1481)
	at org.apache.hadoop.hbase.regionserver.CompactSplitThread$CompactionRunner.run(CompactSplitThread.java:475)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1146)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:701)
Caused by: java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:601)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:511)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:481)
	at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:457)
	at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:583)
	at org.apache.hadoop.ipc.Client$Connection.access$2200(Client.java:205)
	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1249)
	at org.apache.hadoop.ipc.Client.call(Client.java:1093)
	... 27 more
2014-07-01 15:49:01,066 DEBUG [regionserver60020-smallCompactions-1404253964856] regionserver.CompactSplitThread: CompactSplitThread Status: compaction_queue=(0:29), split_queue=0, merge_queue=0
2014-07-01 15:49:01,074 DEBUG [regionserver60020-smallCompactions-1404253964856] compactions.RatioBasedCompactionPolicy: Selecting compaction from 6 store files, 0 compacting, 6 eligible, 20 blocking
2014-07-01 15:49:01,074 DEBUG [regionserver60020-smallCompactions-1404253964856] compactions.ExploringCompactionPolicy: Exploring compaction algorithm has selected 6 files of size 934259017 starting at candidate #0 after considering 10 permutations with 9 in ratio
2014-07-01 15:49:01,074 DEBUG [regionserver60020-smallCompactions-1404253964856] regionserver.HStore: 667c1f2c34fe37d48ba25a789d8aba98 - family: Initiating major compaction
2014-07-01 15:49:01,075 INFO  [regionserver60020-smallCompactions-1404253964856] regionserver.HRegion: Starting compaction on family in region usertable,user7,1404228265526.667c1f2c34fe37d48ba25a789d8aba98.
2014-07-01 15:49:01,075 INFO  [regionserver60020-smallCompactions-1404253964856] regionserver.HStore: Starting compaction of 6 file(s) in family of usertable,user7,1404228265526.667c1f2c34fe37d48ba25a789d8aba98. into tmpdir=hdfs://master:54310/hbase/data/default/usertable/667c1f2c34fe37d48ba25a789d8aba98/.tmp, totalSize=891.0m
2014-07-01 15:49:01,075 DEBUG [regionserver60020-smallCompactions-1404253964856] compactions.Compactor: Compacting hdfs://master:54310/hbase/data/default/usertable/667c1f2c34fe37d48ba25a789d8aba98/family/56200eedd338491786b04080265aa716, keycount=545792, bloomtype=ROW, size=388.9m, encoding=NONE, seqNum=2915, earliestPutTs=1404228281826
2014-07-01 15:49:01,075 DEBUG [regionserver60020-smallCompactions-1404253964856] compactions.Compactor: Compacting hdfs://master:54310/hbase/data/default/usertable/667c1f2c34fe37d48ba25a789d8aba98/family/50f4de22ccd04828b18b93cc72b88964, keycount=147565, bloomtype=ROW, size=105.0m, encoding=NONE, seqNum=3442, earliestPutTs=1404254293026
2014-07-01 15:49:01,075 DEBUG [regionserver60020-smallCompactions-1404253964856] compactions.Compactor: Compacting hdfs://master:54310/hbase/data/default/usertable/667c1f2c34fe37d48ba25a789d8aba98/family/244e3443161840b5a95504a408c4edc2, keycount=155927, bloomtype=ROW, size=111.0m, encoding=NONE, seqNum=3881, earliestPutTs=1404254357203
2014-07-01 15:49:01,075 DEBUG [regionserver60020-smallCompactions-1404253964856] compactions.Compactor: Compacting hdfs://master:54310/hbase/data/default/usertable/667c1f2c34fe37d48ba25a789d8aba98/family/a5468df3646d43fda8dcfd231b11facb, keycount=176456, bloomtype=ROW, size=125.7m, encoding=NONE, seqNum=4315, earliestPutTs=1404254389941
2014-07-01 15:49:01,076 DEBUG [regionserver60020-smallCompactions-1404253964856] compactions.Compactor: Compacting hdfs://master:54310/hbase/data/default/usertable/667c1f2c34fe37d48ba25a789d8aba98/family/5c4ecd1e5f5f40a68639ae8651cdf308, keycount=132606, bloomtype=ROW, size=94.4m, encoding=NONE, seqNum=4527, earliestPutTs=1404254439322
2014-07-01 15:49:01,076 DEBUG [regionserver60020-smallCompactions-1404253964856] compactions.Compactor: Compacting hdfs://master:54310/hbase/data/default/usertable/667c1f2c34fe37d48ba25a789d8aba98/family/6c4e4b0a6c594b6db087ac5f958bca9c, keycount=92565, bloomtype=ROW, size=66.0m, encoding=NONE, seqNum=4678, earliestPutTs=1404254473587
2014-07-01 15:49:01,076 WARN  [regionserver60020-smallCompactions-1404253964856] hdfs.DFSClient: Failed to connect to /9.1.143.59:50010, add to deadNodes and continuejava.net.ConnectException: Connection refused
2014-07-01 15:49:01,090 WARN  [regionserver60020-smallCompactions-1404253964856] hdfs.DFSClient: Failed to connect to /9.1.143.58:50010, add to deadNodes and continuejava.net.ConnectException: Connection refused
2014-07-01 15:49:01,090 INFO  [regionserver60020-smallCompactions-1404253964856] hdfs.DFSClient: Could not obtain blk_221234652107626381_29849 from any node: java.io.IOException: No live nodes contain current block. Will get new block locations from namenode and retry...
2014-07-01 15:49:01,685 INFO  [DataStreamer for file /hbase/WALs/slave1,60020,1404253868846/slave1%2C60020%2C1404253868846.1404254547570 block blk_-3161611795271398729_29985] ipc.Client: Retrying connect to server: master/9.1.143.58:50020. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1 SECONDS)
2014-07-01 15:49:02,686 INFO  [DataStreamer for file /hbase/WALs/slave1,60020,1404253868846/slave1%2C60020%2C1404253868846.1404254547570 block blk_-3161611795271398729_29985] ipc.Client: Retrying connect to server: master/9.1.143.58:50020. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1 SECONDS)
2014-07-01 15:49:03,686 INFO  [DataStreamer for file /hbase/WALs/slave1,60020,1404253868846/slave1%2C60020%2C1404253868846.1404254547570 block blk_-3161611795271398729_29985] ipc.Client: Retrying connect to server: master/9.1.143.58:50020. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1 SECONDS)
2014-07-01 15:49:04,687 INFO  [DataStreamer for file /hbase/WALs/slave1,60020,1404253868846/slave1%2C60020%2C1404253868846.1404254547570 block blk_-3161611795271398729_29985] ipc.Client: Retrying connect to server: master/9.1.143.58:50020. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1 SECONDS)
2014-07-01 15:49:05,091 INFO  [regionserver60020-smallCompactions-1404253964856] ipc.Client: Retrying connect to server: master/9.1.143.58:54310. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1 SECONDS)
2014-07-01 15:49:05,688 INFO  [DataStreamer for file /hbase/WALs/slave1,60020,1404253868846/slave1%2C60020%2C1404253868846.1404254547570 block blk_-3161611795271398729_29985] ipc.Client: Retrying connect to server: master/9.1.143.58:50020. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1 SECONDS)
2014-07-01 15:49:05,688 WARN  [DataStreamer for file /hbase/WALs/slave1,60020,1404253868846/slave1%2C60020%2C1404253868846.1404254547570 block blk_-3161611795271398729_29985] hdfs.DFSClient: Failed recovery attempt #0 from primary datanode 9.1.143.58:50010
java.net.ConnectException: Call to master/9.1.143.58:50020 failed on connection exception: java.net.ConnectException: Connection refused
	at org.apache.hadoop.ipc.Client.wrapException(Client.java:1142)
	at org.apache.hadoop.ipc.Client.call(Client.java:1118)
	at org.apache.hadoop.ipc.RPC$Invoker.invoke(RPC.java:229)
	at com.sun.proxy.$Proxy13.getProtocolVersion(Unknown Source)
	at org.apache.hadoop.ipc.RPC.checkVersion(RPC.java:422)
	at org.apache.hadoop.ipc.RPC.getProxy(RPC.java:414)
	at org.apache.hadoop.ipc.RPC.getProxy(RPC.java:392)
	at org.apache.hadoop.hdfs.DFSClient.createClientDatanodeProtocolProxy(DFSClient.java:201)
	at org.apache.hadoop.hdfs.DFSClient$DFSOutputStream.processDatanodeError(DFSClient.java:3317)
	at org.apache.hadoop.hdfs.DFSClient$DFSOutputStream.access$2200(DFSClient.java:2783)
	at org.apache.hadoop.hdfs.DFSClient$DFSOutputStream$DataStreamer.run(DFSClient.java:2987)
Caused by: java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:601)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:511)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:481)
	at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:457)
	at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:583)
	at org.apache.hadoop.ipc.Client$Connection.access$2200(Client.java:205)
	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1249)
	at org.apache.hadoop.ipc.Client.call(Client.java:1093)
	... 9 more
2014-07-01 15:49:05,689 WARN  [DataStreamer for file /hbase/WALs/slave1,60020,1404253868846/slave1%2C60020%2C1404253868846.1404254547570 block blk_-3161611795271398729_29985] hdfs.DFSClient: Error Recovery for block blk_-3161611795271398729_29985 failed  because recovery from primary datanode 9.1.143.58:50010 failed 1 times.  Pipeline was 9.1.143.59:50010, 9.1.143.58:50010. Will retry...
2014-07-01 15:49:06,092 INFO  [regionserver60020-smallCompactions-1404253964856] ipc.Client: Retrying connect to server: master/9.1.143.58:54310. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1 SECONDS)
2014-07-01 15:49:07,093 INFO  [regionserver60020-smallCompactions-1404253964856] ipc.Client: Retrying connect to server: master/9.1.143.58:54310. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1 SECONDS)
2014-07-01 15:49:08,094 INFO  [regionserver60020-smallCompactions-1404253964856] ipc.Client: Retrying connect to server: master/9.1.143.58:54310. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1 SECONDS)
2014-07-01 15:49:09,094 INFO  [regionserver60020-smallCompactions-1404253964856] ipc.Client: Retrying connect to server: master/9.1.143.58:54310. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1 SECONDS)
2014-07-01 15:49:10,096 INFO  [regionserver60020-smallCompactions-1404253964856] ipc.Client: Retrying connect to server: master/9.1.143.58:54310. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1 SECONDS)
2014-07-01 15:49:11,097 INFO  [regionserver60020-smallCompactions-1404253964856] ipc.Client: Retrying connect to server: master/9.1.143.58:54310. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1 SECONDS)
2014-07-01 15:49:12,098 INFO  [regionserver60020-smallCompactions-1404253964856] ipc.Client: Retrying connect to server: master/9.1.143.58:54310. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1 SECONDS)
2014-07-01 15:49:12,943 INFO  [regionserver60020.leaseChecker] regionserver.HRegionServer: Scanner 61957 lease expired on region usertable,user7,1404228265526.667c1f2c34fe37d48ba25a789d8aba98.
2014-07-01 15:49:13,043 INFO  [regionserver60020.leaseChecker] regionserver.HRegionServer: Scanner 61962 lease expired on region usertable,user7,1404228265526.667c1f2c34fe37d48ba25a789d8aba98.
2014-07-01 15:49:13,044 INFO  [regionserver60020.leaseChecker] regionserver.HRegionServer: Scanner 61958 lease expired on region usertable,user9,1404228265526.bb79ae773f71289e0745068f68a0abd9.
2014-07-01 15:49:13,044 INFO  [regionserver60020.leaseChecker] regionserver.HRegionServer: Scanner 61963 lease expired on region usertable,,1404228265525.9cef9e7d34f96f1d9b111be50040a208.
2014-07-01 15:49:13,044 INFO  [regionserver60020.leaseChecker] regionserver.HRegionServer: Scanner 61960 lease expired on region usertable,,1404228265525.9cef9e7d34f96f1d9b111be50040a208.
2014-07-01 15:49:13,044 INFO  [regionserver60020.leaseChecker] regionserver.HRegionServer: Scanner 61959 lease expired on region usertable,,1404228265525.9cef9e7d34f96f1d9b111be50040a208.
2014-07-01 15:49:13,044 INFO  [regionserver60020.leaseChecker] regionserver.HRegionServer: Scanner 61961 lease expired on region usertable,user7,1404228265526.667c1f2c34fe37d48ba25a789d8aba98.
2014-07-01 15:49:13,099 INFO  [regionserver60020-smallCompactions-1404253964856] ipc.Client: Retrying connect to server: master/9.1.143.58:54310. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1 SECONDS)
2014-07-01 15:49:14,101 INFO  [regionserver60020-smallCompactions-1404253964856] ipc.Client: Retrying connect to server: master/9.1.143.58:54310. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1 SECONDS)
2014-07-01 15:49:14,105 WARN  [regionserver60020-smallCompactions-1404253964856] hdfs.DFSClient: Failed to connect to /9.1.143.58:50010, add to deadNodes and continuejava.net.ConnectException: Connection refused
2014-07-01 15:49:14,105 INFO  [regionserver60020-smallCompactions-1404253964856] hdfs.DFSClient: Could not obtain blk_221234652107626381_29849 from any node: java.io.IOException: No live nodes contain current block. Will get new block locations from namenode and retry...
2014-07-01 15:49:15,104 INFO  [org.apache.hadoop.hdfs.LeaseRenewer$1@4fae46d5] ipc.Client: Retrying connect to server: master/9.1.143.58:54310. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1 SECONDS)
2014-07-01 15:49:16,105 INFO  [org.apache.hadoop.hdfs.LeaseRenewer$1@4fae46d5] ipc.Client: Retrying connect to server: master/9.1.143.58:54310. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1 SECONDS)
2014-07-01 15:49:17,106 INFO  [org.apache.hadoop.hdfs.LeaseRenewer$1@4fae46d5] ipc.Client: Retrying connect to server: master/9.1.143.58:54310. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1 SECONDS)
2014-07-01 15:49:18,107 INFO  [org.apache.hadoop.hdfs.LeaseRenewer$1@4fae46d5] ipc.Client: Retrying connect to server: master/9.1.143.58:54310. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1 SECONDS)
2014-07-01 15:49:19,107 INFO  [org.apache.hadoop.hdfs.LeaseRenewer$1@4fae46d5] ipc.Client: Retrying connect to server: master/9.1.143.58:54310. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1 SECONDS)
2014-07-01 15:49:20,108 INFO  [org.apache.hadoop.hdfs.LeaseRenewer$1@4fae46d5] ipc.Client: Retrying connect to server: master/9.1.143.58:54310. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1 SECONDS)
2014-07-01 15:49:21,109 INFO  [org.apache.hadoop.hdfs.LeaseRenewer$1@4fae46d5] ipc.Client: Retrying connect to server: master/9.1.143.58:54310. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1 SECONDS)
2014-07-01 15:49:22,111 INFO  [org.apache.hadoop.hdfs.LeaseRenewer$1@4fae46d5] ipc.Client: Retrying connect to server: master/9.1.143.58:54310. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1 SECONDS)
2014-07-01 15:49:22,644 WARN  [DataStreamer for file /hbase/WALs/slave1,60020,1404253868846/slave1%2C60020%2C1404253868846.1404254547570 block blk_-3161611795271398729_29985] hdfs.DFSClient: Error Recovery for blk_-3161611795271398729_29985 bad datanode[0] 9.1.143.59:50010
2014-07-01 15:49:22,645 WARN  [DataStreamer for file /hbase/WALs/slave1,60020,1404253868846/slave1%2C60020%2C1404253868846.1404254547570 block blk_-3161611795271398729_29985] hdfs.DFSClient: Error Recovery for block blk_-3161611795271398729_29985 in pipeline 9.1.143.59:50010, 9.1.143.58:50010: bad datanode 9.1.143.59:50010
2014-07-01 15:49:23,112 INFO  [org.apache.hadoop.hdfs.LeaseRenewer$1@4fae46d5] ipc.Client: Retrying connect to server: master/9.1.143.58:54310. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1 SECONDS)
2014-07-01 15:49:23,646 INFO  [DataStreamer for file /hbase/WALs/slave1,60020,1404253868846/slave1%2C60020%2C1404253868846.1404254547570 block blk_-3161611795271398729_29985] ipc.Client: Retrying connect to server: master/9.1.143.58:50020. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1 SECONDS)
2014-07-01 15:49:24,113 INFO  [org.apache.hadoop.hdfs.LeaseRenewer$1@4fae46d5] ipc.Client: Retrying connect to server: master/9.1.143.58:54310. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1 SECONDS)
2014-07-01 15:49:24,114 WARN  [org.apache.hadoop.hdfs.LeaseRenewer$1@4fae46d5] hdfs.LeaseRenewer: Failed to renew lease for [DFSClient_hb_rs_slave1,60020,1404253868846_-130361673_31] for 30 seconds.  Will retry shortly ...
java.net.ConnectException: Call to master/9.1.143.58:54310 failed on connection exception: java.net.ConnectException: Connection refused
	at org.apache.hadoop.ipc.Client.wrapException(Client.java:1142)
	at org.apache.hadoop.ipc.Client.call(Client.java:1118)
	at org.apache.hadoop.ipc.RPC$Invoker.invoke(RPC.java:229)
	at com.sun.proxy.$Proxy8.renewLease(Unknown Source)
	at sun.reflect.GeneratedMethodAccessor29.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:622)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:85)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:62)
	at com.sun.proxy.$Proxy8.renewLease(Unknown Source)
	at sun.reflect.GeneratedMethodAccessor29.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:622)
	at org.apache.hadoop.hbase.fs.HFileSystem$1.invoke(HFileSystem.java:294)
	at com.sun.proxy.$Proxy9.renewLease(Unknown Source)
	at org.apache.hadoop.hdfs.DFSClient.renewLease(DFSClient.java:379)
	at org.apache.hadoop.hdfs.LeaseRenewer.renew(LeaseRenewer.java:378)
	at org.apache.hadoop.hdfs.LeaseRenewer.run(LeaseRenewer.java:400)
	at org.apache.hadoop.hdfs.LeaseRenewer.access$600(LeaseRenewer.java:69)
	at org.apache.hadoop.hdfs.LeaseRenewer$1.run(LeaseRenewer.java:273)
	at java.lang.Thread.run(Thread.java:701)
Caused by: java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:601)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:511)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:481)
	at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:457)
	at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:583)
	at org.apache.hadoop.ipc.Client$Connection.access$2200(Client.java:205)
	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1249)
	at org.apache.hadoop.ipc.Client.call(Client.java:1093)
	... 19 more
2014-07-01 15:49:24,647 INFO  [DataStreamer for file /hbase/WALs/slave1,60020,1404253868846/slave1%2C60020%2C1404253868846.1404254547570 block blk_-3161611795271398729_29985] ipc.Client: Retrying connect to server: master/9.1.143.58:50020. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1 SECONDS)
2014-07-01 15:49:25,114 INFO  [regionserver60020-smallCompactions-1404253964856] ipc.Client: Retrying connect to server: master/9.1.143.58:54310. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1 SECONDS)
2014-07-01 15:49:25,648 INFO  [DataStreamer for file /hbase/WALs/slave1,60020,1404253868846/slave1%2C60020%2C1404253868846.1404254547570 block blk_-3161611795271398729_29985] ipc.Client: Retrying connect to server: master/9.1.143.58:50020. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1 SECONDS)
2014-07-01 15:49:26,116 INFO  [regionserver60020-smallCompactions-1404253964856] ipc.Client: Retrying connect to server: master/9.1.143.58:54310. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1 SECONDS)
2014-07-01 15:49:26,649 INFO  [DataStreamer for file /hbase/WALs/slave1,60020,1404253868846/slave1%2C60020%2C1404253868846.1404254547570 block blk_-3161611795271398729_29985] ipc.Client: Retrying connect to server: master/9.1.143.58:50020. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1 SECONDS)
2014-07-01 15:49:27,117 INFO  [regionserver60020-smallCompactions-1404253964856] ipc.Client: Retrying connect to server: master/9.1.143.58:54310. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1 SECONDS)
2014-07-01 15:49:27,650 INFO  [DataStreamer for file /hbase/WALs/slave1,60020,1404253868846/slave1%2C60020%2C1404253868846.1404254547570 block blk_-3161611795271398729_29985] ipc.Client: Retrying connect to server: master/9.1.143.58:50020. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1 SECONDS)
2014-07-01 15:49:28,118 INFO  [regionserver60020-smallCompactions-1404253964856] ipc.Client: Retrying connect to server: master/9.1.143.58:54310. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1 SECONDS)
2014-07-01 15:49:28,651 INFO  [DataStreamer for file /hbase/WALs/slave1,60020,1404253868846/slave1%2C60020%2C1404253868846.1404254547570 block blk_-3161611795271398729_29985] ipc.Client: Retrying connect to server: master/9.1.143.58:50020. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1 SECONDS)
2014-07-01 15:49:29,119 INFO  [regionserver60020-smallCompactions-1404253964856] ipc.Client: Retrying connect to server: master/9.1.143.58:54310. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1 SECONDS)
2014-07-01 15:49:29,652 INFO  [DataStreamer for file /hbase/WALs/slave1,60020,1404253868846/slave1%2C60020%2C1404253868846.1404254547570 block blk_-3161611795271398729_29985] ipc.Client: Retrying connect to server: master/9.1.143.58:50020. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1 SECONDS)
2014-07-01 15:49:30,121 INFO  [regionserver60020-smallCompactions-1404253964856] ipc.Client: Retrying connect to server: master/9.1.143.58:54310. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1 SECONDS)
2014-07-01 15:49:30,653 INFO  [DataStreamer for file /hbase/WALs/slave1,60020,1404253868846/slave1%2C60020%2C1404253868846.1404254547570 block blk_-3161611795271398729_29985] ipc.Client: Retrying connect to server: master/9.1.143.58:50020. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1 SECONDS)
2014-07-01 15:49:31,125 INFO  [regionserver60020-smallCompactions-1404253964856] ipc.Client: Retrying connect to server: master/9.1.143.58:54310. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1 SECONDS)
2014-07-01 15:49:31,457 INFO  [regionserver60020] regionserver.HRegionServer: Closing user regions
2014-07-01 15:49:31,462 DEBUG [RS_CLOSE_REGION-slave1:60020-0] handler.CloseRegionHandler: Processing close of usertable,user5,1404228265526.122e92eabfab38cb15a9965b21db25c5.
2014-07-01 15:49:31,462 DEBUG [RS_CLOSE_REGION-slave1:60020-1] handler.CloseRegionHandler: Processing close of usertable,user9,1404228265526.bb79ae773f71289e0745068f68a0abd9.
2014-07-01 15:49:31,465 DEBUG [RS_CLOSE_REGION-slave1:60020-2] handler.CloseRegionHandler: Processing close of usertable,,1404228265525.9cef9e7d34f96f1d9b111be50040a208.
2014-07-01 15:49:31,465 DEBUG [RS_CLOSE_REGION-slave1:60020-0] regionserver.HRegion: Closing usertable,user5,1404228265526.122e92eabfab38cb15a9965b21db25c5.: disabling compactions & flushes
2014-07-01 15:49:31,465 DEBUG [RS_CLOSE_REGION-slave1:60020-1] regionserver.HRegion: Closing usertable,user9,1404228265526.bb79ae773f71289e0745068f68a0abd9.: disabling compactions & flushes
2014-07-01 15:49:31,465 INFO  [RS_CLOSE_REGION-slave1:60020-0] regionserver.HRegion: Running close preflush of usertable,user5,1404228265526.122e92eabfab38cb15a9965b21db25c5.
2014-07-01 15:49:31,466 INFO  [RS_CLOSE_REGION-slave1:60020-1] regionserver.HRegion: Running close preflush of usertable,user9,1404228265526.bb79ae773f71289e0745068f68a0abd9.
2014-07-01 15:49:31,465 DEBUG [RS_CLOSE_REGION-slave1:60020-2] regionserver.HRegion: Closing usertable,,1404228265525.9cef9e7d34f96f1d9b111be50040a208.: disabling compactions & flushes
2014-07-01 15:49:31,466 DEBUG [RS_CLOSE_REGION-slave1:60020-1] regionserver.HRegion: Started memstore flush for usertable,user9,1404228265526.bb79ae773f71289e0745068f68a0abd9., current region memstore size 134.8m
2014-07-01 15:49:31,466 DEBUG [RS_CLOSE_REGION-slave1:60020-0] regionserver.HRegion: Started memstore flush for usertable,user5,1404228265526.122e92eabfab38cb15a9965b21db25c5., current region memstore size 84.0m
2014-07-01 15:49:31,466 INFO  [RS_CLOSE_REGION-slave1:60020-2] regionserver.HRegion: Running close preflush of usertable,,1404228265525.9cef9e7d34f96f1d9b111be50040a208.
2014-07-01 15:49:31,466 DEBUG [RS_CLOSE_REGION-slave1:60020-2] regionserver.HRegion: Started memstore flush for usertable,,1404228265525.9cef9e7d34f96f1d9b111be50040a208., current region memstore size 150.4m
2014-07-01 15:49:31,654 INFO  [DataStreamer for file /hbase/WALs/slave1,60020,1404253868846/slave1%2C60020%2C1404253868846.1404254547570 block blk_-3161611795271398729_29985] ipc.Client: Retrying connect to server: master/9.1.143.58:50020. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1 SECONDS)
2014-07-01 15:49:32,125 INFO  [regionserver60020-smallCompactions-1404253964856] ipc.Client: Retrying connect to server: master/9.1.143.58:54310. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1 SECONDS)
2014-07-01 15:49:32,655 INFO  [DataStreamer for file /hbase/WALs/slave1,60020,1404253868846/slave1%2C60020%2C1404253868846.1404254547570 block blk_-3161611795271398729_29985] ipc.Client: Retrying connect to server: master/9.1.143.58:50020. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1 SECONDS)
2014-07-01 15:49:32,656 WARN  [DataStreamer for file /hbase/WALs/slave1,60020,1404253868846/slave1%2C60020%2C1404253868846.1404254547570 block blk_-3161611795271398729_29985] hdfs.DFSClient: Failed recovery attempt #1 from primary datanode 9.1.143.58:50010
java.net.ConnectException: Call to master/9.1.143.58:50020 failed on connection exception: java.net.ConnectException: Connection refused
	at org.apache.hadoop.ipc.Client.wrapException(Client.java:1142)
	at org.apache.hadoop.ipc.Client.call(Client.java:1118)
	at org.apache.hadoop.ipc.RPC$Invoker.invoke(RPC.java:229)
	at com.sun.proxy.$Proxy13.getProtocolVersion(Unknown Source)
	at org.apache.hadoop.ipc.RPC.checkVersion(RPC.java:422)
	at org.apache.hadoop.ipc.RPC.getProxy(RPC.java:414)
	at org.apache.hadoop.ipc.RPC.getProxy(RPC.java:392)
	at org.apache.hadoop.hdfs.DFSClient.createClientDatanodeProtocolProxy(DFSClient.java:201)
	at org.apache.hadoop.hdfs.DFSClient$DFSOutputStream.processDatanodeError(DFSClient.java:3317)
	at org.apache.hadoop.hdfs.DFSClient$DFSOutputStream.access$2200(DFSClient.java:2783)
	at org.apache.hadoop.hdfs.DFSClient$DFSOutputStream$DataStreamer.run(DFSClient.java:2987)
Caused by: java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:601)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:511)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:481)
	at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:457)
	at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:583)
	at org.apache.hadoop.ipc.Client$Connection.access$2200(Client.java:205)
	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1249)
	at org.apache.hadoop.ipc.Client.call(Client.java:1093)
	... 9 more
2014-07-01 15:49:32,656 WARN  [DataStreamer for file /hbase/WALs/slave1,60020,1404253868846/slave1%2C60020%2C1404253868846.1404254547570 block blk_-3161611795271398729_29985] hdfs.DFSClient: Error Recovery for block blk_-3161611795271398729_29985 failed  because recovery from primary datanode 9.1.143.58:50010 failed 2 times.  Pipeline was 9.1.143.59:50010, 9.1.143.58:50010. Will retry...
2014-07-01 15:49:33,126 INFO  [regionserver60020-smallCompactions-1404253964856] ipc.Client: Retrying connect to server: master/9.1.143.58:54310. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1 SECONDS)
2014-07-01 15:49:33,657 WARN  [DataStreamer for file /hbase/WALs/slave1,60020,1404253868846/slave1%2C60020%2C1404253868846.1404254547570 block blk_-3161611795271398729_29985] hdfs.DFSClient: Error Recovery for blk_-3161611795271398729_29985 bad datanode[0] 9.1.143.59:50010
2014-07-01 15:49:33,657 WARN  [DataStreamer for file /hbase/WALs/slave1,60020,1404253868846/slave1%2C60020%2C1404253868846.1404254547570 block blk_-3161611795271398729_29985] hdfs.DFSClient: Error Recovery for block blk_-3161611795271398729_29985 in pipeline 9.1.143.59:50010, 9.1.143.58:50010: bad datanode 9.1.143.59:50010
2014-07-01 15:49:34,129 INFO  [regionserver60020-smallCompactions-1404253964856] ipc.Client: Retrying connect to server: master/9.1.143.58:54310. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1 SECONDS)
2014-07-01 15:49:34,133 WARN  [regionserver60020-smallCompactions-1404253964856] hdfs.DFSClient: DFS Read: java.net.ConnectException: Call to master/9.1.143.58:54310 failed on connection exception: java.net.ConnectException: Connection refused
	at org.apache.hadoop.ipc.Client.wrapException(Client.java:1142)
	at org.apache.hadoop.ipc.Client.call(Client.java:1118)
	at org.apache.hadoop.ipc.RPC$Invoker.invoke(RPC.java:229)
	at com.sun.proxy.$Proxy8.getBlockLocations(Unknown Source)
	at sun.reflect.GeneratedMethodAccessor10.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:622)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:85)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:62)
	at com.sun.proxy.$Proxy8.getBlockLocations(Unknown Source)
	at org.apache.hadoop.hdfs.DFSClient.callGetBlockLocations(DFSClient.java:695)
	at org.apache.hadoop.hdfs.DFSClient$DFSInputStream.fetchLocatedBlocks(DFSClient.java:2003)
	at org.apache.hadoop.hdfs.DFSClient$DFSInputStream.openInfo(DFSClient.java:1975)
	at org.apache.hadoop.hdfs.DFSClient$DFSInputStream.chooseDataNode(DFSClient.java:2474)
	at org.apache.hadoop.hdfs.DFSClient$DFSInputStream.blockSeekTo(DFSClient.java:2252)
	at org.apache.hadoop.hdfs.DFSClient$DFSInputStream.read(DFSClient.java:2415)
	at java.io.DataInputStream.read(DataInputStream.java:149)
	at org.apache.hadoop.io.IOUtils.readFully(IOUtils.java:175)
	at org.apache.hadoop.hbase.io.hfile.HFileBlock$AbstractFSReader.readAtOffset(HFileBlock.java:1210)
	at org.apache.hadoop.hbase.io.hfile.HFileBlock$FSReaderV2.readBlockDataInternal(HFileBlock.java:1483)
	at org.apache.hadoop.hbase.io.hfile.HFileBlock$FSReaderV2.readBlockData(HFileBlock.java:1314)
	at org.apache.hadoop.hbase.io.hfile.HFileReaderV2.readBlock(HFileReaderV2.java:392)
	at org.apache.hadoop.hbase.io.hfile.HFileReaderV2$ScannerV2.seekTo(HFileReaderV2.java:803)
	at org.apache.hadoop.hbase.regionserver.StoreFileScanner.seekAtOrAfter(StoreFileScanner.java:244)
	at org.apache.hadoop.hbase.regionserver.StoreFileScanner.seek(StoreFileScanner.java:152)
	at org.apache.hadoop.hbase.regionserver.StoreScanner.seekScanners(StoreScanner.java:317)
	at org.apache.hadoop.hbase.regionserver.StoreScanner.<init>(StoreScanner.java:240)
	at org.apache.hadoop.hbase.regionserver.StoreScanner.<init>(StoreScanner.java:202)
	at org.apache.hadoop.hbase.regionserver.compactions.Compactor.createScanner(Compactor.java:257)
	at org.apache.hadoop.hbase.regionserver.compactions.DefaultCompactor.compact(DefaultCompactor.java:65)
	at org.apache.hadoop.hbase.regionserver.DefaultStoreEngine$DefaultCompactionContext.compact(DefaultStoreEngine.java:109)
	at org.apache.hadoop.hbase.regionserver.HStore.compact(HStore.java:1086)
	at org.apache.hadoop.hbase.regionserver.HRegion.compact(HRegion.java:1481)
	at org.apache.hadoop.hbase.regionserver.CompactSplitThread$CompactionRunner.run(CompactSplitThread.java:475)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1146)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:701)
Caused by: java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:601)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:511)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:481)
	at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:457)
	at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:583)
	at org.apache.hadoop.ipc.Client$Connection.access$2200(Client.java:205)
	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1249)
	at org.apache.hadoop.ipc.Client.call(Client.java:1093)
	... 35 more

2014-07-01 15:49:34,135 ERROR [regionserver60020-smallCompactions-1404253964856] regionserver.CompactSplitThread: Compaction failed Request = regionName=usertable,user7,1404228265526.667c1f2c34fe37d48ba25a789d8aba98., storeName=family, fileCount=6, fileSize=891.0m (388.9m, 105.0m, 111.0m, 125.7m, 94.4m, 66.0m), priority=14, time=75052463713358
java.io.IOException: Could not seek StoreFileScanner[HFileScanner for reader reader=hdfs://master:54310/hbase/data/default/usertable/667c1f2c34fe37d48ba25a789d8aba98/family/56200eedd338491786b04080265aa716, compression=gz, cacheConf=CacheConfig:enabled [cacheDataOnRead=true] [cacheDataOnWrite=false] [cacheIndexesOnWrite=false] [cacheBloomsOnWrite=false] [cacheEvictOnClose=false] [cacheCompressed=false][prefetchOnOpen=false], firstKey=user7000001172980900254/family:field0/1404254132804/Put, lastKey=user7999998402207300106/family:field9/1404254151918/Put, avgKeyLen=46, avgValueLen=100, entries=5457920, length=407788494, cur=null] to key /family:/LATEST_TIMESTAMP/DeleteFamily/vlen=0/mvcc=0
	at org.apache.hadoop.hbase.regionserver.StoreFileScanner.seek(StoreFileScanner.java:164)
	at org.apache.hadoop.hbase.regionserver.StoreScanner.seekScanners(StoreScanner.java:317)
	at org.apache.hadoop.hbase.regionserver.StoreScanner.<init>(StoreScanner.java:240)
	at org.apache.hadoop.hbase.regionserver.StoreScanner.<init>(StoreScanner.java:202)
	at org.apache.hadoop.hbase.regionserver.compactions.Compactor.createScanner(Compactor.java:257)
	at org.apache.hadoop.hbase.regionserver.compactions.DefaultCompactor.compact(DefaultCompactor.java:65)
	at org.apache.hadoop.hbase.regionserver.DefaultStoreEngine$DefaultCompactionContext.compact(DefaultStoreEngine.java:109)
	at org.apache.hadoop.hbase.regionserver.HStore.compact(HStore.java:1086)
	at org.apache.hadoop.hbase.regionserver.HRegion.compact(HRegion.java:1481)
	at org.apache.hadoop.hbase.regionserver.CompactSplitThread$CompactionRunner.run(CompactSplitThread.java:475)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1146)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:701)
Caused by: java.net.ConnectException: Call to master/9.1.143.58:54310 failed on connection exception: java.net.ConnectException: Connection refused
	at org.apache.hadoop.ipc.Client.wrapException(Client.java:1142)
	at org.apache.hadoop.ipc.Client.call(Client.java:1118)
	at org.apache.hadoop.ipc.RPC$Invoker.invoke(RPC.java:229)
	at com.sun.proxy.$Proxy8.getBlockLocations(Unknown Source)
	at sun.reflect.GeneratedMethodAccessor10.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:622)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:85)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:62)
	at com.sun.proxy.$Proxy8.getBlockLocations(Unknown Source)
	at org.apache.hadoop.hdfs.DFSClient.callGetBlockLocations(DFSClient.java:695)
	at org.apache.hadoop.hdfs.DFSClient$DFSInputStream.fetchLocatedBlocks(DFSClient.java:2003)
	at org.apache.hadoop.hdfs.DFSClient$DFSInputStream.openInfo(DFSClient.java:1975)
	at org.apache.hadoop.hdfs.DFSClient$DFSInputStream.chooseDataNode(DFSClient.java:2474)
	at org.apache.hadoop.hdfs.DFSClient$DFSInputStream.blockSeekTo(DFSClient.java:2252)
	at org.apache.hadoop.hdfs.DFSClient$DFSInputStream.read(DFSClient.java:2415)
	at java.io.DataInputStream.read(DataInputStream.java:149)
	at org.apache.hadoop.io.IOUtils.readFully(IOUtils.java:175)
	at org.apache.hadoop.hbase.io.hfile.HFileBlock$AbstractFSReader.readAtOffset(HFileBlock.java:1210)
	at org.apache.hadoop.hbase.io.hfile.HFileBlock$FSReaderV2.readBlockDataInternal(HFileBlock.java:1483)
	at org.apache.hadoop.hbase.io.hfile.HFileBlock$FSReaderV2.readBlockData(HFileBlock.java:1314)
	at org.apache.hadoop.hbase.io.hfile.HFileReaderV2.readBlock(HFileReaderV2.java:392)
	at org.apache.hadoop.hbase.io.hfile.HFileReaderV2$ScannerV2.seekTo(HFileReaderV2.java:803)
	at org.apache.hadoop.hbase.regionserver.StoreFileScanner.seekAtOrAfter(StoreFileScanner.java:244)
	at org.apache.hadoop.hbase.regionserver.StoreFileScanner.seek(StoreFileScanner.java:152)
	... 12 more
Caused by: java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:601)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:511)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:481)
	at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:457)
	at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:583)
	at org.apache.hadoop.ipc.Client$Connection.access$2200(Client.java:205)
	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1249)
	at org.apache.hadoop.ipc.Client.call(Client.java:1093)
	... 35 more
2014-07-01 15:49:34,136 DEBUG [regionserver60020-smallCompactions-1404253964856] regionserver.CompactSplitThread: CompactSplitThread Status: compaction_queue=(0:28), split_queue=0, merge_queue=0
2014-07-01 15:49:34,137 DEBUG [regionserver60020-smallCompactions-1404253964856] compactions.RatioBasedCompactionPolicy: Selecting compaction from 6 store files, 0 compacting, 6 eligible, 20 blocking
2014-07-01 15:49:34,137 DEBUG [regionserver60020-smallCompactions-1404253964856] compactions.ExploringCompactionPolicy: Exploring compaction algorithm has selected 6 files of size 934259017 starting at candidate #0 after considering 10 permutations with 9 in ratio
2014-07-01 15:49:34,137 DEBUG [regionserver60020-smallCompactions-1404253964856] regionserver.HStore: 667c1f2c34fe37d48ba25a789d8aba98 - family: Initiating major compaction
2014-07-01 15:49:34,138 INFO  [regionserver60020-smallCompactions-1404253964856] regionserver.HRegion: Starting compaction on family in region usertable,user7,1404228265526.667c1f2c34fe37d48ba25a789d8aba98.
2014-07-01 15:49:34,138 INFO  [regionserver60020-smallCompactions-1404253964856] regionserver.HStore: Starting compaction of 6 file(s) in family of usertable,user7,1404228265526.667c1f2c34fe37d48ba25a789d8aba98. into tmpdir=hdfs://master:54310/hbase/data/default/usertable/667c1f2c34fe37d48ba25a789d8aba98/.tmp, totalSize=891.0m
2014-07-01 15:49:34,138 DEBUG [regionserver60020-smallCompactions-1404253964856] compactions.Compactor: Compacting hdfs://master:54310/hbase/data/default/usertable/667c1f2c34fe37d48ba25a789d8aba98/family/56200eedd338491786b04080265aa716, keycount=545792, bloomtype=ROW, size=388.9m, encoding=NONE, seqNum=2915, earliestPutTs=1404228281826
2014-07-01 15:49:34,138 DEBUG [regionserver60020-smallCompactions-1404253964856] compactions.Compactor: Compacting hdfs://master:54310/hbase/data/default/usertable/667c1f2c34fe37d48ba25a789d8aba98/family/50f4de22ccd04828b18b93cc72b88964, keycount=147565, bloomtype=ROW, size=105.0m, encoding=NONE, seqNum=3442, earliestPutTs=1404254293026
2014-07-01 15:49:34,138 DEBUG [regionserver60020-smallCompactions-1404253964856] compactions.Compactor: Compacting hdfs://master:54310/hbase/data/default/usertable/667c1f2c34fe37d48ba25a789d8aba98/family/244e3443161840b5a95504a408c4edc2, keycount=155927, bloomtype=ROW, size=111.0m, encoding=NONE, seqNum=3881, earliestPutTs=1404254357203
2014-07-01 15:49:34,139 DEBUG [regionserver60020-smallCompactions-1404253964856] compactions.Compactor: Compacting hdfs://master:54310/hbase/data/default/usertable/667c1f2c34fe37d48ba25a789d8aba98/family/a5468df3646d43fda8dcfd231b11facb, keycount=176456, bloomtype=ROW, size=125.7m, encoding=NONE, seqNum=4315, earliestPutTs=1404254389941
2014-07-01 15:49:34,139 DEBUG [regionserver60020-smallCompactions-1404253964856] compactions.Compactor: Compacting hdfs://master:54310/hbase/data/default/usertable/667c1f2c34fe37d48ba25a789d8aba98/family/5c4ecd1e5f5f40a68639ae8651cdf308, keycount=132606, bloomtype=ROW, size=94.4m, encoding=NONE, seqNum=4527, earliestPutTs=1404254439322
2014-07-01 15:49:34,139 DEBUG [regionserver60020-smallCompactions-1404253964856] compactions.Compactor: Compacting hdfs://master:54310/hbase/data/default/usertable/667c1f2c34fe37d48ba25a789d8aba98/family/6c4e4b0a6c594b6db087ac5f958bca9c, keycount=92565, bloomtype=ROW, size=66.0m, encoding=NONE, seqNum=4678, earliestPutTs=1404254473587
2014-07-01 15:49:34,140 WARN  [regionserver60020-smallCompactions-1404253964856] hdfs.DFSClient: Failed to connect to /9.1.143.58:50010, add to deadNodes and continuejava.net.ConnectException: Connection refused
2014-07-01 15:49:34,140 INFO  [regionserver60020-smallCompactions-1404253964856] hdfs.DFSClient: Could not obtain blk_221234652107626381_29849 from any node: java.io.IOException: No live nodes contain current block. Will get new block locations from namenode and retry...
2014-07-01 15:49:34,466 INFO  [regionserver60020] regionserver.HRegionServer: Received CLOSE for the region: 122e92eabfab38cb15a9965b21db25c5 ,which we are already trying to CLOSE, but not completed yet
2014-07-01 15:49:34,469 WARN  [regionserver60020] regionserver.HRegionServer: Failed to close usertable,user5,1404228265526.122e92eabfab38cb15a9965b21db25c5. - ignoring and continuing
org.apache.hadoop.hbase.regionserver.RegionAlreadyInTransitionException: The region 122e92eabfab38cb15a9965b21db25c5 was already closing. New CLOSE request is ignored.
	at org.apache.hadoop.hbase.regionserver.HRegionServer.closeRegion(HRegionServer.java:2586)
	at org.apache.hadoop.hbase.regionserver.HRegionServer.closeRegionIgnoreErrors(HRegionServer.java:2508)
	at org.apache.hadoop.hbase.regionserver.HRegionServer.closeUserRegions(HRegionServer.java:2098)
	at org.apache.hadoop.hbase.regionserver.HRegionServer.run(HRegionServer.java:894)
	at java.lang.Thread.run(Thread.java:701)
2014-07-01 15:49:34,469 INFO  [regionserver60020] regionserver.HRegionServer: Received CLOSE for the region: bb79ae773f71289e0745068f68a0abd9 ,which we are already trying to CLOSE, but not completed yet
2014-07-01 15:49:34,469 WARN  [regionserver60020] regionserver.HRegionServer: Failed to close usertable,user9,1404228265526.bb79ae773f71289e0745068f68a0abd9. - ignoring and continuing
org.apache.hadoop.hbase.regionserver.RegionAlreadyInTransitionException: The region bb79ae773f71289e0745068f68a0abd9 was already closing. New CLOSE request is ignored.
	at org.apache.hadoop.hbase.regionserver.HRegionServer.closeRegion(HRegionServer.java:2586)
	at org.apache.hadoop.hbase.regionserver.HRegionServer.closeRegionIgnoreErrors(HRegionServer.java:2508)
	at org.apache.hadoop.hbase.regionserver.HRegionServer.closeUserRegions(HRegionServer.java:2098)
	at org.apache.hadoop.hbase.regionserver.HRegionServer.run(HRegionServer.java:894)
	at java.lang.Thread.run(Thread.java:701)
2014-07-01 15:49:34,470 INFO  [regionserver60020] regionserver.HRegionServer: Received CLOSE for the region: 9cef9e7d34f96f1d9b111be50040a208 ,which we are already trying to CLOSE, but not completed yet
2014-07-01 15:49:34,470 WARN  [regionserver60020] regionserver.HRegionServer: Failed to close usertable,,1404228265525.9cef9e7d34f96f1d9b111be50040a208. - ignoring and continuing
org.apache.hadoop.hbase.regionserver.RegionAlreadyInTransitionException: The region 9cef9e7d34f96f1d9b111be50040a208 was already closing. New CLOSE request is ignored.
	at org.apache.hadoop.hbase.regionserver.HRegionServer.closeRegion(HRegionServer.java:2586)
	at org.apache.hadoop.hbase.regionserver.HRegionServer.closeRegionIgnoreErrors(HRegionServer.java:2508)
	at org.apache.hadoop.hbase.regionserver.HRegionServer.closeUserRegions(HRegionServer.java:2098)
	at org.apache.hadoop.hbase.regionserver.HRegionServer.run(HRegionServer.java:894)
	at java.lang.Thread.run(Thread.java:701)
2014-07-01 15:49:34,470 INFO  [regionserver60020] regionserver.HRegionServer: Received CLOSE for the region: 93253e37c284f3174309fede3f32339f ,which we are already trying to CLOSE, but not completed yet
2014-07-01 15:49:34,470 WARN  [regionserver60020] regionserver.HRegionServer: Failed to close usertable,user3,1404228265526.93253e37c284f3174309fede3f32339f. - ignoring and continuing
org.apache.hadoop.hbase.regionserver.RegionAlreadyInTransitionException: The region 93253e37c284f3174309fede3f32339f was already closing. New CLOSE request is ignored.
	at org.apache.hadoop.hbase.regionserver.HRegionServer.closeRegion(HRegionServer.java:2586)
	at org.apache.hadoop.hbase.regionserver.HRegionServer.closeRegionIgnoreErrors(HRegionServer.java:2508)
	at org.apache.hadoop.hbase.regionserver.HRegionServer.closeUserRegions(HRegionServer.java:2098)
	at org.apache.hadoop.hbase.regionserver.HRegionServer.run(HRegionServer.java:894)
	at java.lang.Thread.run(Thread.java:701)
2014-07-01 15:49:34,470 INFO  [regionserver60020] regionserver.HRegionServer: Received CLOSE for the region: 667c1f2c34fe37d48ba25a789d8aba98 ,which we are already trying to CLOSE, but not completed yet
2014-07-01 15:49:34,471 WARN  [regionserver60020] regionserver.HRegionServer: Failed to close usertable,user7,1404228265526.667c1f2c34fe37d48ba25a789d8aba98. - ignoring and continuing
org.apache.hadoop.hbase.regionserver.RegionAlreadyInTransitionException: The region 667c1f2c34fe37d48ba25a789d8aba98 was already closing. New CLOSE request is ignored.
	at org.apache.hadoop.hbase.regionserver.HRegionServer.closeRegion(HRegionServer.java:2586)
	at org.apache.hadoop.hbase.regionserver.HRegionServer.closeRegionIgnoreErrors(HRegionServer.java:2508)
	at org.apache.hadoop.hbase.regionserver.HRegionServer.closeUserRegions(HRegionServer.java:2098)
	at org.apache.hadoop.hbase.regionserver.HRegionServer.run(HRegionServer.java:894)
	at java.lang.Thread.run(Thread.java:701)
2014-07-01 15:49:34,471 DEBUG [regionserver60020] regionserver.HRegionServer: Waiting on 122e92eabfab38cb15a9965b21db25c5, bb79ae773f71289e0745068f68a0abd9, 9cef9e7d34f96f1d9b111be50040a208, 93253e37c284f3174309fede3f32339f, 667c1f2c34fe37d48ba25a789d8aba98
2014-07-01 15:49:34,659 INFO  [DataStreamer for file /hbase/WALs/slave1,60020,1404253868846/slave1%2C60020%2C1404253868846.1404254547570 block blk_-3161611795271398729_29985] ipc.Client: Retrying connect to server: master/9.1.143.58:50020. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1 SECONDS)
2014-07-01 15:49:35,133 INFO  [RS_CLOSE_REGION-slave1:60020-1] ipc.Client: Retrying connect to server: master/9.1.143.58:54310. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1 SECONDS)
2014-07-01 15:49:35,659 INFO  [DataStreamer for file /hbase/WALs/slave1,60020,1404253868846/slave1%2C60020%2C1404253868846.1404254547570 block blk_-3161611795271398729_29985] ipc.Client: Retrying connect to server: master/9.1.143.58:50020. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1 SECONDS)
2014-07-01 15:49:36,134 INFO  [RS_CLOSE_REGION-slave1:60020-1] ipc.Client: Retrying connect to server: master/9.1.143.58:54310. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1 SECONDS)
2014-07-01 15:49:36,660 INFO  [DataStreamer for file /hbase/WALs/slave1,60020,1404253868846/slave1%2C60020%2C1404253868846.1404254547570 block blk_-3161611795271398729_29985] ipc.Client: Retrying connect to server: master/9.1.143.58:50020. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1 SECONDS)
2014-07-01 15:49:37,135 INFO  [RS_CLOSE_REGION-slave1:60020-1] ipc.Client: Retrying connect to server: master/9.1.143.58:54310. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1 SECONDS)
2014-07-01 15:49:37,474 INFO  [regionserver60020] regionserver.HRegionServer: Received CLOSE for the region: 122e92eabfab38cb15a9965b21db25c5 ,which we are already trying to CLOSE, but not completed yet
2014-07-01 15:49:37,474 WARN  [regionserver60020] regionserver.HRegionServer: Failed to close usertable,user5,1404228265526.122e92eabfab38cb15a9965b21db25c5. - ignoring and continuing
org.apache.hadoop.hbase.regionserver.RegionAlreadyInTransitionException: The region 122e92eabfab38cb15a9965b21db25c5 was already closing. New CLOSE request is ignored.
	at org.apache.hadoop.hbase.regionserver.HRegionServer.closeRegion(HRegionServer.java:2586)
	at org.apache.hadoop.hbase.regionserver.HRegionServer.closeRegionIgnoreErrors(HRegionServer.java:2508)
	at org.apache.hadoop.hbase.regionserver.HRegionServer.closeUserRegions(HRegionServer.java:2098)
	at org.apache.hadoop.hbase.regionserver.HRegionServer.run(HRegionServer.java:894)
	at java.lang.Thread.run(Thread.java:701)
2014-07-01 15:49:37,474 INFO  [regionserver60020] regionserver.HRegionServer: Received CLOSE for the region: bb79ae773f71289e0745068f68a0abd9 ,which we are already trying to CLOSE, but not completed yet
2014-07-01 15:49:37,474 WARN  [regionserver60020] regionserver.HRegionServer: Failed to close usertable,user9,1404228265526.bb79ae773f71289e0745068f68a0abd9. - ignoring and continuing
org.apache.hadoop.hbase.regionserver.RegionAlreadyInTransitionException: The region bb79ae773f71289e0745068f68a0abd9 was already closing. New CLOSE request is ignored.
	at org.apache.hadoop.hbase.regionserver.HRegionServer.closeRegion(HRegionServer.java:2586)
	at org.apache.hadoop.hbase.regionserver.HRegionServer.closeRegionIgnoreErrors(HRegionServer.java:2508)
	at org.apache.hadoop.hbase.regionserver.HRegionServer.closeUserRegions(HRegionServer.java:2098)
	at org.apache.hadoop.hbase.regionserver.HRegionServer.run(HRegionServer.java:894)
	at java.lang.Thread.run(Thread.java:701)
2014-07-01 15:49:37,475 INFO  [regionserver60020] regionserver.HRegionServer: Received CLOSE for the region: 9cef9e7d34f96f1d9b111be50040a208 ,which we are already trying to CLOSE, but not completed yet
2014-07-01 15:49:37,475 WARN  [regionserver60020] regionserver.HRegionServer: Failed to close usertable,,1404228265525.9cef9e7d34f96f1d9b111be50040a208. - ignoring and continuing
org.apache.hadoop.hbase.regionserver.RegionAlreadyInTransitionException: The region 9cef9e7d34f96f1d9b111be50040a208 was already closing. New CLOSE request is ignored.
	at org.apache.hadoop.hbase.regionserver.HRegionServer.closeRegion(HRegionServer.java:2586)
	at org.apache.hadoop.hbase.regionserver.HRegionServer.closeRegionIgnoreErrors(HRegionServer.java:2508)
	at org.apache.hadoop.hbase.regionserver.HRegionServer.closeUserRegions(HRegionServer.java:2098)
	at org.apache.hadoop.hbase.regionserver.HRegionServer.run(HRegionServer.java:894)
	at java.lang.Thread.run(Thread.java:701)
2014-07-01 15:49:37,475 INFO  [regionserver60020] regionserver.HRegionServer: Received CLOSE for the region: 93253e37c284f3174309fede3f32339f ,which we are already trying to CLOSE, but not completed yet
2014-07-01 15:49:37,475 WARN  [regionserver60020] regionserver.HRegionServer: Failed to close usertable,user3,1404228265526.93253e37c284f3174309fede3f32339f. - ignoring and continuing
org.apache.hadoop.hbase.regionserver.RegionAlreadyInTransitionException: The region 93253e37c284f3174309fede3f32339f was already closing. New CLOSE request is ignored.
	at org.apache.hadoop.hbase.regionserver.HRegionServer.closeRegion(HRegionServer.java:2586)
	at org.apache.hadoop.hbase.regionserver.HRegionServer.closeRegionIgnoreErrors(HRegionServer.java:2508)
	at org.apache.hadoop.hbase.regionserver.HRegionServer.closeUserRegions(HRegionServer.java:2098)
	at org.apache.hadoop.hbase.regionserver.HRegionServer.run(HRegionServer.java:894)
	at java.lang.Thread.run(Thread.java:701)
2014-07-01 15:49:37,476 INFO  [regionserver60020] regionserver.HRegionServer: Received CLOSE for the region: 667c1f2c34fe37d48ba25a789d8aba98 ,which we are already trying to CLOSE, but not completed yet
2014-07-01 15:49:37,476 WARN  [regionserver60020] regionserver.HRegionServer: Failed to close usertable,user7,1404228265526.667c1f2c34fe37d48ba25a789d8aba98. - ignoring and continuing
org.apache.hadoop.hbase.regionserver.RegionAlreadyInTransitionException: The region 667c1f2c34fe37d48ba25a789d8aba98 was already closing. New CLOSE request is ignored.
	at org.apache.hadoop.hbase.regionserver.HRegionServer.closeRegion(HRegionServer.java:2586)
	at org.apache.hadoop.hbase.regionserver.HRegionServer.closeRegionIgnoreErrors(HRegionServer.java:2508)
	at org.apache.hadoop.hbase.regionserver.HRegionServer.closeUserRegions(HRegionServer.java:2098)
	at org.apache.hadoop.hbase.regionserver.HRegionServer.run(HRegionServer.java:894)
	at java.lang.Thread.run(Thread.java:701)
2014-07-01 15:49:37,476 DEBUG [regionserver60020] regionserver.HRegionServer: Waiting on 122e92eabfab38cb15a9965b21db25c5, bb79ae773f71289e0745068f68a0abd9, 9cef9e7d34f96f1d9b111be50040a208, 93253e37c284f3174309fede3f32339f, 667c1f2c34fe37d48ba25a789d8aba98
2014-07-01 15:49:37,661 INFO  [DataStreamer for file /hbase/WALs/slave1,60020,1404253868846/slave1%2C60020%2C1404253868846.1404254547570 block blk_-3161611795271398729_29985] ipc.Client: Retrying connect to server: master/9.1.143.58:50020. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1 SECONDS)
2014-07-01 15:49:38,136 INFO  [RS_CLOSE_REGION-slave1:60020-1] ipc.Client: Retrying connect to server: master/9.1.143.58:54310. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1 SECONDS)
2014-07-01 15:49:38,662 INFO  [DataStreamer for file /hbase/WALs/slave1,60020,1404253868846/slave1%2C60020%2C1404253868846.1404254547570 block blk_-3161611795271398729_29985] ipc.Client: Retrying connect to server: master/9.1.143.58:50020. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1 SECONDS)
2014-07-01 15:49:39,137 INFO  [RS_CLOSE_REGION-slave1:60020-1] ipc.Client: Retrying connect to server: master/9.1.143.58:54310. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1 SECONDS)
2014-07-01 15:49:39,663 INFO  [DataStreamer for file /hbase/WALs/slave1,60020,1404253868846/slave1%2C60020%2C1404253868846.1404254547570 block blk_-3161611795271398729_29985] ipc.Client: Retrying connect to server: master/9.1.143.58:50020. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1 SECONDS)
2014-07-01 15:49:40,138 INFO  [RS_CLOSE_REGION-slave1:60020-1] ipc.Client: Retrying connect to server: master/9.1.143.58:54310. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1 SECONDS)
2014-07-01 15:49:40,478 INFO  [regionserver60020] regionserver.HRegionServer: Received CLOSE for the region: 122e92eabfab38cb15a9965b21db25c5 ,which we are already trying to CLOSE, but not completed yet
2014-07-01 15:49:40,478 WARN  [regionserver60020] regionserver.HRegionServer: Failed to close usertable,user5,1404228265526.122e92eabfab38cb15a9965b21db25c5. - ignoring and continuing
org.apache.hadoop.hbase.regionserver.RegionAlreadyInTransitionException: The region 122e92eabfab38cb15a9965b21db25c5 was already closing. New CLOSE request is ignored.
	at org.apache.hadoop.hbase.regionserver.HRegionServer.closeRegion(HRegionServer.java:2586)
	at org.apache.hadoop.hbase.regionserver.HRegionServer.closeRegionIgnoreErrors(HRegionServer.java:2508)
	at org.apache.hadoop.hbase.regionserver.HRegionServer.closeUserRegions(HRegionServer.java:2098)
	at org.apache.hadoop.hbase.regionserver.HRegionServer.run(HRegionServer.java:894)
	at java.lang.Thread.run(Thread.java:701)
2014-07-01 15:49:40,479 INFO  [regionserver60020] regionserver.HRegionServer: Received CLOSE for the region: bb79ae773f71289e0745068f68a0abd9 ,which we are already trying to CLOSE, but not completed yet
2014-07-01 15:49:40,479 WARN  [regionserver60020] regionserver.HRegionServer: Failed to close usertable,user9,1404228265526.bb79ae773f71289e0745068f68a0abd9. - ignoring and continuing
org.apache.hadoop.hbase.regionserver.RegionAlreadyInTransitionException: The region bb79ae773f71289e0745068f68a0abd9 was already closing. New CLOSE request is ignored.
	at org.apache.hadoop.hbase.regionserver.HRegionServer.closeRegion(HRegionServer.java:2586)
	at org.apache.hadoop.hbase.regionserver.HRegionServer.closeRegionIgnoreErrors(HRegionServer.java:2508)
	at org.apache.hadoop.hbase.regionserver.HRegionServer.closeUserRegions(HRegionServer.java:2098)
	at org.apache.hadoop.hbase.regionserver.HRegionServer.run(HRegionServer.java:894)
	at java.lang.Thread.run(Thread.java:701)
2014-07-01 15:49:40,479 INFO  [regionserver60020] regionserver.HRegionServer: Received CLOSE for the region: 9cef9e7d34f96f1d9b111be50040a208 ,which we are already trying to CLOSE, but not completed yet
2014-07-01 15:49:40,479 WARN  [regionserver60020] regionserver.HRegionServer: Failed to close usertable,,1404228265525.9cef9e7d34f96f1d9b111be50040a208. - ignoring and continuing
org.apache.hadoop.hbase.regionserver.RegionAlreadyInTransitionException: The region 9cef9e7d34f96f1d9b111be50040a208 was already closing. New CLOSE request is ignored.
	at org.apache.hadoop.hbase.regionserver.HRegionServer.closeRegion(HRegionServer.java:2586)
	at org.apache.hadoop.hbase.regionserver.HRegionServer.closeRegionIgnoreErrors(HRegionServer.java:2508)
	at org.apache.hadoop.hbase.regionserver.HRegionServer.closeUserRegions(HRegionServer.java:2098)
	at org.apache.hadoop.hbase.regionserver.HRegionServer.run(HRegionServer.java:894)
	at java.lang.Thread.run(Thread.java:701)
2014-07-01 15:49:40,479 INFO  [regionserver60020] regionserver.HRegionServer: Received CLOSE for the region: 93253e37c284f3174309fede3f32339f ,which we are already trying to CLOSE, but not completed yet
2014-07-01 15:49:40,479 WARN  [regionserver60020] regionserver.HRegionServer: Failed to close usertable,user3,1404228265526.93253e37c284f3174309fede3f32339f. - ignoring and continuing
org.apache.hadoop.hbase.regionserver.RegionAlreadyInTransitionException: The region 93253e37c284f3174309fede3f32339f was already closing. New CLOSE request is ignored.
	at org.apache.hadoop.hbase.regionserver.HRegionServer.closeRegion(HRegionServer.java:2586)
	at org.apache.hadoop.hbase.regionserver.HRegionServer.closeRegionIgnoreErrors(HRegionServer.java:2508)
	at org.apache.hadoop.hbase.regionserver.HRegionServer.closeUserRegions(HRegionServer.java:2098)
	at org.apache.hadoop.hbase.regionserver.HRegionServer.run(HRegionServer.java:894)
	at java.lang.Thread.run(Thread.java:701)
2014-07-01 15:49:40,479 INFO  [regionserver60020] regionserver.HRegionServer: Received CLOSE for the region: 667c1f2c34fe37d48ba25a789d8aba98 ,which we are already trying to CLOSE, but not completed yet
2014-07-01 15:49:40,480 WARN  [regionserver60020] regionserver.HRegionServer: Failed to close usertable,user7,1404228265526.667c1f2c34fe37d48ba25a789d8aba98. - ignoring and continuing
org.apache.hadoop.hbase.regionserver.RegionAlreadyInTransitionException: The region 667c1f2c34fe37d48ba25a789d8aba98 was already closing. New CLOSE request is ignored.
	at org.apache.hadoop.hbase.regionserver.HRegionServer.closeRegion(HRegionServer.java:2586)
	at org.apache.hadoop.hbase.regionserver.HRegionServer.closeRegionIgnoreErrors(HRegionServer.java:2508)
	at org.apache.hadoop.hbase.regionserver.HRegionServer.closeUserRegions(HRegionServer.java:2098)
	at org.apache.hadoop.hbase.regionserver.HRegionServer.run(HRegionServer.java:894)
	at java.lang.Thread.run(Thread.java:701)
2014-07-01 15:49:40,480 DEBUG [regionserver60020] regionserver.HRegionServer: Waiting on 122e92eabfab38cb15a9965b21db25c5, bb79ae773f71289e0745068f68a0abd9, 9cef9e7d34f96f1d9b111be50040a208, 93253e37c284f3174309fede3f32339f, 667c1f2c34fe37d48ba25a789d8aba98
2014-07-01 15:49:40,665 INFO  [DataStreamer for file /hbase/WALs/slave1,60020,1404253868846/slave1%2C60020%2C1404253868846.1404254547570 block blk_-3161611795271398729_29985] ipc.Client: Retrying connect to server: master/9.1.143.58:50020. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1 SECONDS)
2014-07-01 15:49:41,139 INFO  [RS_CLOSE_REGION-slave1:60020-1] ipc.Client: Retrying connect to server: master/9.1.143.58:54310. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1 SECONDS)
2014-07-01 15:49:41,665 INFO  [DataStreamer for file /hbase/WALs/slave1,60020,1404253868846/slave1%2C60020%2C1404253868846.1404254547570 block blk_-3161611795271398729_29985] ipc.Client: Retrying connect to server: master/9.1.143.58:50020. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1 SECONDS)
2014-07-01 15:49:42,139 INFO  [RS_CLOSE_REGION-slave1:60020-1] ipc.Client: Retrying connect to server: master/9.1.143.58:54310. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1 SECONDS)
2014-07-01 15:49:42,666 INFO  [DataStreamer for file /hbase/WALs/slave1,60020,1404253868846/slave1%2C60020%2C1404253868846.1404254547570 block blk_-3161611795271398729_29985] ipc.Client: Retrying connect to server: master/9.1.143.58:50020. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1 SECONDS)
2014-07-01 15:49:43,140 INFO  [RS_CLOSE_REGION-slave1:60020-1] ipc.Client: Retrying connect to server: master/9.1.143.58:54310. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1 SECONDS)
2014-07-01 15:49:43,482 INFO  [regionserver60020] regionserver.HRegionServer: Received CLOSE for the region: 122e92eabfab38cb15a9965b21db25c5 ,which we are already trying to CLOSE, but not completed yet
2014-07-01 15:49:43,482 WARN  [regionserver60020] regionserver.HRegionServer: Failed to close usertable,user5,1404228265526.122e92eabfab38cb15a9965b21db25c5. - ignoring and continuing
org.apache.hadoop.hbase.regionserver.RegionAlreadyInTransitionException: The region 122e92eabfab38cb15a9965b21db25c5 was already closing. New CLOSE request is ignored.
	at org.apache.hadoop.hbase.regionserver.HRegionServer.closeRegion(HRegionServer.java:2586)
	at org.apache.hadoop.hbase.regionserver.HRegionServer.closeRegionIgnoreErrors(HRegionServer.java:2508)
	at org.apache.hadoop.hbase.regionserver.HRegionServer.closeUserRegions(HRegionServer.java:2098)
	at org.apache.hadoop.hbase.regionserver.HRegionServer.run(HRegionServer.java:894)
	at java.lang.Thread.run(Thread.java:701)
2014-07-01 15:49:43,482 INFO  [regionserver60020] regionserver.HRegionServer: Received CLOSE for the region: bb79ae773f71289e0745068f68a0abd9 ,which we are already trying to CLOSE, but not completed yet
2014-07-01 15:49:43,482 WARN  [regionserver60020] regionserver.HRegionServer: Failed to close usertable,user9,1404228265526.bb79ae773f71289e0745068f68a0abd9. - ignoring and continuing
org.apache.hadoop.hbase.regionserver.RegionAlreadyInTransitionException: The region bb79ae773f71289e0745068f68a0abd9 was already closing. New CLOSE request is ignored.
	at org.apache.hadoop.hbase.regionserver.HRegionServer.closeRegion(HRegionServer.java:2586)
	at org.apache.hadoop.hbase.regionserver.HRegionServer.closeRegionIgnoreErrors(HRegionServer.java:2508)
	at org.apache.hadoop.hbase.regionserver.HRegionServer.closeUserRegions(HRegionServer.java:2098)
	at org.apache.hadoop.hbase.regionserver.HRegionServer.run(HRegionServer.java:894)
	at java.lang.Thread.run(Thread.java:701)
2014-07-01 15:49:43,483 INFO  [regionserver60020] regionserver.HRegionServer: Received CLOSE for the region: 9cef9e7d34f96f1d9b111be50040a208 ,which we are already trying to CLOSE, but not completed yet
2014-07-01 15:49:43,483 WARN  [regionserver60020] regionserver.HRegionServer: Failed to close usertable,,1404228265525.9cef9e7d34f96f1d9b111be50040a208. - ignoring and continuing
org.apache.hadoop.hbase.regionserver.RegionAlreadyInTransitionException: The region 9cef9e7d34f96f1d9b111be50040a208 was already closing. New CLOSE request is ignored.
	at org.apache.hadoop.hbase.regionserver.HRegionServer.closeRegion(HRegionServer.java:2586)
	at org.apache.hadoop.hbase.regionserver.HRegionServer.closeRegionIgnoreErrors(HRegionServer.java:2508)
	at org.apache.hadoop.hbase.regionserver.HRegionServer.closeUserRegions(HRegionServer.java:2098)
	at org.apache.hadoop.hbase.regionserver.HRegionServer.run(HRegionServer.java:894)
	at java.lang.Thread.run(Thread.java:701)
2014-07-01 15:49:43,483 INFO  [regionserver60020] regionserver.HRegionServer: Received CLOSE for the region: 93253e37c284f3174309fede3f32339f ,which we are already trying to CLOSE, but not completed yet
2014-07-01 15:49:43,483 WARN  [regionserver60020] regionserver.HRegionServer: Failed to close usertable,user3,1404228265526.93253e37c284f3174309fede3f32339f. - ignoring and continuing
org.apache.hadoop.hbase.regionserver.RegionAlreadyInTransitionException: The region 93253e37c284f3174309fede3f32339f was already closing. New CLOSE request is ignored.
	at org.apache.hadoop.hbase.regionserver.HRegionServer.closeRegion(HRegionServer.java:2586)
	at org.apache.hadoop.hbase.regionserver.HRegionServer.closeRegionIgnoreErrors(HRegionServer.java:2508)
	at org.apache.hadoop.hbase.regionserver.HRegionServer.closeUserRegions(HRegionServer.java:2098)
	at org.apache.hadoop.hbase.regionserver.HRegionServer.run(HRegionServer.java:894)
	at java.lang.Thread.run(Thread.java:701)
2014-07-01 15:49:43,483 INFO  [regionserver60020] regionserver.HRegionServer: Received CLOSE for the region: 667c1f2c34fe37d48ba25a789d8aba98 ,which we are already trying to CLOSE, but not completed yet
2014-07-01 15:49:43,483 WARN  [regionserver60020] regionserver.HRegionServer: Failed to close usertable,user7,1404228265526.667c1f2c34fe37d48ba25a789d8aba98. - ignoring and continuing
org.apache.hadoop.hbase.regionserver.RegionAlreadyInTransitionException: The region 667c1f2c34fe37d48ba25a789d8aba98 was already closing. New CLOSE request is ignored.
	at org.apache.hadoop.hbase.regionserver.HRegionServer.closeRegion(HRegionServer.java:2586)
	at org.apache.hadoop.hbase.regionserver.HRegionServer.closeRegionIgnoreErrors(HRegionServer.java:2508)
	at org.apache.hadoop.hbase.regionserver.HRegionServer.closeUserRegions(HRegionServer.java:2098)
	at org.apache.hadoop.hbase.regionserver.HRegionServer.run(HRegionServer.java:894)
	at java.lang.Thread.run(Thread.java:701)
2014-07-01 15:49:43,483 DEBUG [regionserver60020] regionserver.HRegionServer: Waiting on 122e92eabfab38cb15a9965b21db25c5, bb79ae773f71289e0745068f68a0abd9, 9cef9e7d34f96f1d9b111be50040a208, 93253e37c284f3174309fede3f32339f, 667c1f2c34fe37d48ba25a789d8aba98
2014-07-01 15:49:43,666 INFO  [DataStreamer for file /hbase/WALs/slave1,60020,1404253868846/slave1%2C60020%2C1404253868846.1404254547570 block blk_-3161611795271398729_29985] ipc.Client: Retrying connect to server: master/9.1.143.58:50020. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1 SECONDS)
2014-07-01 15:49:43,667 WARN  [DataStreamer for file /hbase/WALs/slave1,60020,1404253868846/slave1%2C60020%2C1404253868846.1404254547570 block blk_-3161611795271398729_29985] hdfs.DFSClient: Failed recovery attempt #2 from primary datanode 9.1.143.58:50010
java.net.ConnectException: Call to master/9.1.143.58:50020 failed on connection exception: java.net.ConnectException: Connection refused
	at org.apache.hadoop.ipc.Client.wrapException(Client.java:1142)
	at org.apache.hadoop.ipc.Client.call(Client.java:1118)
	at org.apache.hadoop.ipc.RPC$Invoker.invoke(RPC.java:229)
	at com.sun.proxy.$Proxy13.getProtocolVersion(Unknown Source)
	at org.apache.hadoop.ipc.RPC.checkVersion(RPC.java:422)
	at org.apache.hadoop.ipc.RPC.getProxy(RPC.java:414)
	at org.apache.hadoop.ipc.RPC.getProxy(RPC.java:392)
	at org.apache.hadoop.hdfs.DFSClient.createClientDatanodeProtocolProxy(DFSClient.java:201)
	at org.apache.hadoop.hdfs.DFSClient$DFSOutputStream.processDatanodeError(DFSClient.java:3317)
	at org.apache.hadoop.hdfs.DFSClient$DFSOutputStream.access$2200(DFSClient.java:2783)
	at org.apache.hadoop.hdfs.DFSClient$DFSOutputStream$DataStreamer.run(DFSClient.java:2987)
Caused by: java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:601)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:511)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:481)
	at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:457)
	at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:583)
	at org.apache.hadoop.ipc.Client$Connection.access$2200(Client.java:205)
	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1249)
	at org.apache.hadoop.ipc.Client.call(Client.java:1093)
	... 9 more
2014-07-01 15:49:43,667 WARN  [DataStreamer for file /hbase/WALs/slave1,60020,1404253868846/slave1%2C60020%2C1404253868846.1404254547570 block blk_-3161611795271398729_29985] hdfs.DFSClient: Error Recovery for block blk_-3161611795271398729_29985 failed  because recovery from primary datanode 9.1.143.58:50010 failed 3 times.  Pipeline was 9.1.143.59:50010, 9.1.143.58:50010. Will retry...
2014-07-01 15:49:44,141 INFO  [RS_CLOSE_REGION-slave1:60020-1] ipc.Client: Retrying connect to server: master/9.1.143.58:54310. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1 SECONDS)
2014-07-01 15:49:44,142 WARN  [RS_CLOSE_REGION-slave1:60020-1] regionserver.HStore: Failed flushing store file, retrying num=0
java.net.ConnectException: Call to master/9.1.143.58:54310 failed on connection exception: java.net.ConnectException: Connection refused
	at org.apache.hadoop.ipc.Client.wrapException(Client.java:1142)
	at org.apache.hadoop.ipc.Client.call(Client.java:1118)
	at org.apache.hadoop.ipc.RPC$Invoker.invoke(RPC.java:229)
	at com.sun.proxy.$Proxy8.getFileInfo(Unknown Source)
	at sun.reflect.GeneratedMethodAccessor2.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:622)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:85)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:62)
	at com.sun.proxy.$Proxy8.getFileInfo(Unknown Source)
	at sun.reflect.GeneratedMethodAccessor2.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:622)
	at org.apache.hadoop.hbase.fs.HFileSystem$1.invoke(HFileSystem.java:294)
	at com.sun.proxy.$Proxy9.getFileInfo(Unknown Source)
	at org.apache.hadoop.hdfs.DFSClient.getFileInfo(DFSClient.java:1059)
	at org.apache.hadoop.hdfs.DistributedFileSystem.getFileStatus(DistributedFileSystem.java:554)
	at org.apache.hadoop.fs.FilterFileSystem.getFileStatus(FilterFileSystem.java:255)
	at org.apache.hadoop.fs.FileSystem.exists(FileSystem.java:788)
	at org.apache.hadoop.hbase.regionserver.StoreFile$WriterBuilder.build(StoreFile.java:603)
	at org.apache.hadoop.hbase.regionserver.HStore.createWriterInTmp(HStore.java:889)
	at org.apache.hadoop.hbase.regionserver.DefaultStoreFlusher.flushSnapshot(DefaultStoreFlusher.java:69)
	at org.apache.hadoop.hbase.regionserver.HStore.flushCache(HStore.java:798)
	at org.apache.hadoop.hbase.regionserver.HStore$StoreFlusherImpl.flushCache(HStore.java:1949)
	at org.apache.hadoop.hbase.regionserver.HRegion.internalFlushcache(HRegion.java:1776)
	at org.apache.hadoop.hbase.regionserver.HRegion.internalFlushcache(HRegion.java:1658)
	at org.apache.hadoop.hbase.regionserver.HRegion.doClose(HRegion.java:1119)
	at org.apache.hadoop.hbase.regionserver.HRegion.close(HRegion.java:1085)
	at org.apache.hadoop.hbase.regionserver.handler.CloseRegionHandler.process(CloseRegionHandler.java:147)
	at org.apache.hadoop.hbase.executor.EventHandler.run(EventHandler.java:128)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1146)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:701)
Caused by: java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:601)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:511)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:481)
	at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:457)
	at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:583)
	at org.apache.hadoop.ipc.Client$Connection.access$2200(Client.java:205)
	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1249)
	at org.apache.hadoop.ipc.Client.call(Client.java:1093)
	... 31 more
2014-07-01 15:49:44,668 WARN  [DataStreamer for file /hbase/WALs/slave1,60020,1404253868846/slave1%2C60020%2C1404253868846.1404254547570 block blk_-3161611795271398729_29985] hdfs.DFSClient: Error Recovery for blk_-3161611795271398729_29985 bad datanode[0] 9.1.143.59:50010
2014-07-01 15:49:44,668 WARN  [DataStreamer for file /hbase/WALs/slave1,60020,1404253868846/slave1%2C60020%2C1404253868846.1404254547570 block blk_-3161611795271398729_29985] hdfs.DFSClient: Error Recovery for block blk_-3161611795271398729_29985 in pipeline 9.1.143.59:50010, 9.1.143.58:50010: bad datanode 9.1.143.59:50010
2014-07-01 15:49:45,143 INFO  [regionserver60020-smallCompactions-1404253964856] ipc.Client: Retrying connect to server: master/9.1.143.58:54310. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1 SECONDS)
2014-07-01 15:49:45,669 INFO  [DataStreamer for file /hbase/WALs/slave1,60020,1404253868846/slave1%2C60020%2C1404253868846.1404254547570 block blk_-3161611795271398729_29985] ipc.Client: Retrying connect to server: master/9.1.143.58:50020. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1 SECONDS)
2014-07-01 15:49:46,143 INFO  [regionserver60020-smallCompactions-1404253964856] ipc.Client: Retrying connect to server: master/9.1.143.58:54310. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1 SECONDS)
2014-07-01 15:49:46,485 INFO  [regionserver60020] regionserver.HRegionServer: Received CLOSE for the region: 122e92eabfab38cb15a9965b21db25c5 ,which we are already trying to CLOSE, but not completed yet
2014-07-01 15:49:46,485 WARN  [regionserver60020] regionserver.HRegionServer: Failed to close usertable,user5,1404228265526.122e92eabfab38cb15a9965b21db25c5. - ignoring and continuing
org.apache.hadoop.hbase.regionserver.RegionAlreadyInTransitionException: The region 122e92eabfab38cb15a9965b21db25c5 was already closing. New CLOSE request is ignored.
	at org.apache.hadoop.hbase.regionserver.HRegionServer.closeRegion(HRegionServer.java:2586)
	at org.apache.hadoop.hbase.regionserver.HRegionServer.closeRegionIgnoreErrors(HRegionServer.java:2508)
	at org.apache.hadoop.hbase.regionserver.HRegionServer.closeUserRegions(HRegionServer.java:2098)
	at org.apache.hadoop.hbase.regionserver.HRegionServer.run(HRegionServer.java:894)
	at java.lang.Thread.run(Thread.java:701)
2014-07-01 15:49:46,486 INFO  [regionserver60020] regionserver.HRegionServer: Received CLOSE for the region: bb79ae773f71289e0745068f68a0abd9 ,which we are already trying to CLOSE, but not completed yet
2014-07-01 15:49:46,486 WARN  [regionserver60020] regionserver.HRegionServer: Failed to close usertable,user9,1404228265526.bb79ae773f71289e0745068f68a0abd9. - ignoring and continuing
org.apache.hadoop.hbase.regionserver.RegionAlreadyInTransitionException: The region bb79ae773f71289e0745068f68a0abd9 was already closing. New CLOSE request is ignored.
	at org.apache.hadoop.hbase.regionserver.HRegionServer.closeRegion(HRegionServer.java:2586)
	at org.apache.hadoop.hbase.regionserver.HRegionServer.closeRegionIgnoreErrors(HRegionServer.java:2508)
	at org.apache.hadoop.hbase.regionserver.HRegionServer.closeUserRegions(HRegionServer.java:2098)
	at org.apache.hadoop.hbase.regionserver.HRegionServer.run(HRegionServer.java:894)
	at java.lang.Thread.run(Thread.java:701)
2014-07-01 15:49:46,486 INFO  [regionserver60020] regionserver.HRegionServer: Received CLOSE for the region: 9cef9e7d34f96f1d9b111be50040a208 ,which we are already trying to CLOSE, but not completed yet
2014-07-01 15:49:46,486 WARN  [regionserver60020] regionserver.HRegionServer: Failed to close usertable,,1404228265525.9cef9e7d34f96f1d9b111be50040a208. - ignoring and continuing
org.apache.hadoop.hbase.regionserver.RegionAlreadyInTransitionException: The region 9cef9e7d34f96f1d9b111be50040a208 was already closing. New CLOSE request is ignored.
	at org.apache.hadoop.hbase.regionserver.HRegionServer.closeRegion(HRegionServer.java:2586)
	at org.apache.hadoop.hbase.regionserver.HRegionServer.closeRegionIgnoreErrors(HRegionServer.java:2508)
	at org.apache.hadoop.hbase.regionserver.HRegionServer.closeUserRegions(HRegionServer.java:2098)
	at org.apache.hadoop.hbase.regionserver.HRegionServer.run(HRegionServer.java:894)
	at java.lang.Thread.run(Thread.java:701)
2014-07-01 15:49:46,486 INFO  [regionserver60020] regionserver.HRegionServer: Received CLOSE for the region: 93253e37c284f3174309fede3f32339f ,which we are already trying to CLOSE, but not completed yet
2014-07-01 15:49:46,486 WARN  [regionserver60020] regionserver.HRegionServer: Failed to close usertable,user3,1404228265526.93253e37c284f3174309fede3f32339f. - ignoring and continuing
org.apache.hadoop.hbase.regionserver.RegionAlreadyInTransitionException: The region 93253e37c284f3174309fede3f32339f was already closing. New CLOSE request is ignored.
	at org.apache.hadoop.hbase.regionserver.HRegionServer.closeRegion(HRegionServer.java:2586)
	at org.apache.hadoop.hbase.regionserver.HRegionServer.closeRegionIgnoreErrors(HRegionServer.java:2508)
	at org.apache.hadoop.hbase.regionserver.HRegionServer.closeUserRegions(HRegionServer.java:2098)
	at org.apache.hadoop.hbase.regionserver.HRegionServer.run(HRegionServer.java:894)
	at java.lang.Thread.run(Thread.java:701)
2014-07-01 15:49:46,486 INFO  [regionserver60020] regionserver.HRegionServer: Received CLOSE for the region: 667c1f2c34fe37d48ba25a789d8aba98 ,which we are already trying to CLOSE, but not completed yet
2014-07-01 15:49:46,486 WARN  [regionserver60020] regionserver.HRegionServer: Failed to close usertable,user7,1404228265526.667c1f2c34fe37d48ba25a789d8aba98. - ignoring and continuing
org.apache.hadoop.hbase.regionserver.RegionAlreadyInTransitionException: The region 667c1f2c34fe37d48ba25a789d8aba98 was already closing. New CLOSE request is ignored.
	at org.apache.hadoop.hbase.regionserver.HRegionServer.closeRegion(HRegionServer.java:2586)
	at org.apache.hadoop.hbase.regionserver.HRegionServer.closeRegionIgnoreErrors(HRegionServer.java:2508)
	at org.apache.hadoop.hbase.regionserver.HRegionServer.closeUserRegions(HRegionServer.java:2098)
	at org.apache.hadoop.hbase.regionserver.HRegionServer.run(HRegionServer.java:894)
	at java.lang.Thread.run(Thread.java:701)
2014-07-01 15:49:46,486 DEBUG [regionserver60020] regionserver.HRegionServer: Waiting on 122e92eabfab38cb15a9965b21db25c5, bb79ae773f71289e0745068f68a0abd9, 9cef9e7d34f96f1d9b111be50040a208, 93253e37c284f3174309fede3f32339f, 667c1f2c34fe37d48ba25a789d8aba98
2014-07-01 15:49:46,670 INFO  [DataStreamer for file /hbase/WALs/slave1,60020,1404253868846/slave1%2C60020%2C1404253868846.1404254547570 block blk_-3161611795271398729_29985] ipc.Client: Retrying connect to server: master/9.1.143.58:50020. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1 SECONDS)
2014-07-01 15:49:47,144 INFO  [regionserver60020-smallCompactions-1404253964856] ipc.Client: Retrying connect to server: master/9.1.143.58:54310. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1 SECONDS)
2014-07-01 15:49:47,671 INFO  [DataStreamer for file /hbase/WALs/slave1,60020,1404253868846/slave1%2C60020%2C1404253868846.1404254547570 block blk_-3161611795271398729_29985] ipc.Client: Retrying connect to server: master/9.1.143.58:50020. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1 SECONDS)
2014-07-01 15:49:48,145 INFO  [regionserver60020-smallCompactions-1404253964856] ipc.Client: Retrying connect to server: master/9.1.143.58:54310. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1 SECONDS)
2014-07-01 15:49:48,671 INFO  [DataStreamer for file /hbase/WALs/slave1,60020,1404253868846/slave1%2C60020%2C1404253868846.1404254547570 block blk_-3161611795271398729_29985] ipc.Client: Retrying connect to server: master/9.1.143.58:50020. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1 SECONDS)
2014-07-01 15:49:49,146 INFO  [regionserver60020-smallCompactions-1404253964856] ipc.Client: Retrying connect to server: master/9.1.143.58:54310. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1 SECONDS)
2014-07-01 15:49:49,489 INFO  [regionserver60020] regionserver.HRegionServer: Received CLOSE for the region: 122e92eabfab38cb15a9965b21db25c5 ,which we are already trying to CLOSE, but not completed yet
2014-07-01 15:49:49,489 WARN  [regionserver60020] regionserver.HRegionServer: Failed to close usertable,user5,1404228265526.122e92eabfab38cb15a9965b21db25c5. - ignoring and continuing
org.apache.hadoop.hbase.regionserver.RegionAlreadyInTransitionException: The region 122e92eabfab38cb15a9965b21db25c5 was already closing. New CLOSE request is ignored.
	at org.apache.hadoop.hbase.regionserver.HRegionServer.closeRegion(HRegionServer.java:2586)
	at org.apache.hadoop.hbase.regionserver.HRegionServer.closeRegionIgnoreErrors(HRegionServer.java:2508)
	at org.apache.hadoop.hbase.regionserver.HRegionServer.closeUserRegions(HRegionServer.java:2098)
	at org.apache.hadoop.hbase.regionserver.HRegionServer.run(HRegionServer.java:894)
	at java.lang.Thread.run(Thread.java:701)
2014-07-01 15:49:49,490 INFO  [regionserver60020] regionserver.HRegionServer: Received CLOSE for the region: bb79ae773f71289e0745068f68a0abd9 ,which we are already trying to CLOSE, but not completed yet
2014-07-01 15:49:49,490 WARN  [regionserver60020] regionserver.HRegionServer: Failed to close usertable,user9,1404228265526.bb79ae773f71289e0745068f68a0abd9. - ignoring and continuing
org.apache.hadoop.hbase.regionserver.RegionAlreadyInTransitionException: The region bb79ae773f71289e0745068f68a0abd9 was already closing. New CLOSE request is ignored.
	at org.apache.hadoop.hbase.regionserver.HRegionServer.closeRegion(HRegionServer.java:2586)
	at org.apache.hadoop.hbase.regionserver.HRegionServer.closeRegionIgnoreErrors(HRegionServer.java:2508)
	at org.apache.hadoop.hbase.regionserver.HRegionServer.closeUserRegions(HRegionServer.java:2098)
	at org.apache.hadoop.hbase.regionserver.HRegionServer.run(HRegionServer.java:894)
	at java.lang.Thread.run(Thread.java:701)
2014-07-01 15:49:49,490 INFO  [regionserver60020] regionserver.HRegionServer: Received CLOSE for the region: 9cef9e7d34f96f1d9b111be50040a208 ,which we are already trying to CLOSE, but not completed yet
2014-07-01 15:49:49,490 WARN  [regionserver60020] regionserver.HRegionServer: Failed to close usertable,,1404228265525.9cef9e7d34f96f1d9b111be50040a208. - ignoring and continuing
org.apache.hadoop.hbase.regionserver.RegionAlreadyInTransitionException: The region 9cef9e7d34f96f1d9b111be50040a208 was already closing. New CLOSE request is ignored.
	at org.apache.hadoop.hbase.regionserver.HRegionServer.closeRegion(HRegionServer.java:2586)
	at org.apache.hadoop.hbase.regionserver.HRegionServer.closeRegionIgnoreErrors(HRegionServer.java:2508)
	at org.apache.hadoop.hbase.regionserver.HRegionServer.closeUserRegions(HRegionServer.java:2098)
	at org.apache.hadoop.hbase.regionserver.HRegionServer.run(HRegionServer.java:894)
	at java.lang.Thread.run(Thread.java:701)
2014-07-01 15:49:49,490 INFO  [regionserver60020] regionserver.HRegionServer: Received CLOSE for the region: 93253e37c284f3174309fede3f32339f ,which we are already trying to CLOSE, but not completed yet
2014-07-01 15:49:49,490 WARN  [regionserver60020] regionserver.HRegionServer: Failed to close usertable,user3,1404228265526.93253e37c284f3174309fede3f32339f. - ignoring and continuing
org.apache.hadoop.hbase.regionserver.RegionAlreadyInTransitionException: The region 93253e37c284f3174309fede3f32339f was already closing. New CLOSE request is ignored.
	at org.apache.hadoop.hbase.regionserver.HRegionServer.closeRegion(HRegionServer.java:2586)
	at org.apache.hadoop.hbase.regionserver.HRegionServer.closeRegionIgnoreErrors(HRegionServer.java:2508)
	at org.apache.hadoop.hbase.regionserver.HRegionServer.closeUserRegions(HRegionServer.java:2098)
	at org.apache.hadoop.hbase.regionserver.HRegionServer.run(HRegionServer.java:894)
	at java.lang.Thread.run(Thread.java:701)
2014-07-01 15:49:49,490 INFO  [regionserver60020] regionserver.HRegionServer: Received CLOSE for the region: 667c1f2c34fe37d48ba25a789d8aba98 ,which we are already trying to CLOSE, but not completed yet
2014-07-01 15:49:49,490 WARN  [regionserver60020] regionserver.HRegionServer: Failed to close usertable,user7,1404228265526.667c1f2c34fe37d48ba25a789d8aba98. - ignoring and continuing
org.apache.hadoop.hbase.regionserver.RegionAlreadyInTransitionException: The region 667c1f2c34fe37d48ba25a789d8aba98 was already closing. New CLOSE request is ignored.
	at org.apache.hadoop.hbase.regionserver.HRegionServer.closeRegion(HRegionServer.java:2586)
	at org.apache.hadoop.hbase.regionserver.HRegionServer.closeRegionIgnoreErrors(HRegionServer.java:2508)
	at org.apache.hadoop.hbase.regionserver.HRegionServer.closeUserRegions(HRegionServer.java:2098)
	at org.apache.hadoop.hbase.regionserver.HRegionServer.run(HRegionServer.java:894)
	at java.lang.Thread.run(Thread.java:701)
2014-07-01 15:49:49,490 DEBUG [regionserver60020] regionserver.HRegionServer: Waiting on 122e92eabfab38cb15a9965b21db25c5, bb79ae773f71289e0745068f68a0abd9, 9cef9e7d34f96f1d9b111be50040a208, 93253e37c284f3174309fede3f32339f, 667c1f2c34fe37d48ba25a789d8aba98
2014-07-01 15:49:49,673 INFO  [DataStreamer for file /hbase/WALs/slave1,60020,1404253868846/slave1%2C60020%2C1404253868846.1404254547570 block blk_-3161611795271398729_29985] ipc.Client: Retrying connect to server: master/9.1.143.58:50020. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1 SECONDS)
2014-07-01 15:49:50,146 INFO  [regionserver60020-smallCompactions-1404253964856] ipc.Client: Retrying connect to server: master/9.1.143.58:54310. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1 SECONDS)
2014-07-01 15:49:50,673 INFO  [DataStreamer for file /hbase/WALs/slave1,60020,1404253868846/slave1%2C60020%2C1404253868846.1404254547570 block blk_-3161611795271398729_29985] ipc.Client: Retrying connect to server: master/9.1.143.58:50020. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1 SECONDS)
2014-07-01 15:49:51,148 INFO  [regionserver60020-smallCompactions-1404253964856] ipc.Client: Retrying connect to server: master/9.1.143.58:54310. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1 SECONDS)
2014-07-01 15:49:51,674 INFO  [DataStreamer for file /hbase/WALs/slave1,60020,1404253868846/slave1%2C60020%2C1404253868846.1404254547570 block blk_-3161611795271398729_29985] ipc.Client: Retrying connect to server: master/9.1.143.58:50020. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1 SECONDS)
2014-07-01 15:49:52,149 INFO  [regionserver60020-smallCompactions-1404253964856] ipc.Client: Retrying connect to server: master/9.1.143.58:54310. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1 SECONDS)
2014-07-01 15:49:52,494 INFO  [regionserver60020] regionserver.HRegionServer: Received CLOSE for the region: 122e92eabfab38cb15a9965b21db25c5 ,which we are already trying to CLOSE, but not completed yet
2014-07-01 15:49:52,494 WARN  [regionserver60020] regionserver.HRegionServer: Failed to close usertable,user5,1404228265526.122e92eabfab38cb15a9965b21db25c5. - ignoring and continuing
org.apache.hadoop.hbase.regionserver.RegionAlreadyInTransitionException: The region 122e92eabfab38cb15a9965b21db25c5 was already closing. New CLOSE request is ignored.
	at org.apache.hadoop.hbase.regionserver.HRegionServer.closeRegion(HRegionServer.java:2586)
	at org.apache.hadoop.hbase.regionserver.HRegionServer.closeRegionIgnoreErrors(HRegionServer.java:2508)
	at org.apache.hadoop.hbase.regionserver.HRegionServer.closeUserRegions(HRegionServer.java:2098)
	at org.apache.hadoop.hbase.regionserver.HRegionServer.run(HRegionServer.java:894)
	at java.lang.Thread.run(Thread.java:701)
2014-07-01 15:49:52,494 INFO  [regionserver60020] regionserver.HRegionServer: Received CLOSE for the region: bb79ae773f71289e0745068f68a0abd9 ,which we are already trying to CLOSE, but not completed yet
2014-07-01 15:49:52,494 WARN  [regionserver60020] regionserver.HRegionServer: Failed to close usertable,user9,1404228265526.bb79ae773f71289e0745068f68a0abd9. - ignoring and continuing
org.apache.hadoop.hbase.regionserver.RegionAlreadyInTransitionException: The region bb79ae773f71289e0745068f68a0abd9 was already closing. New CLOSE request is ignored.
	at org.apache.hadoop.hbase.regionserver.HRegionServer.closeRegion(HRegionServer.java:2586)
	at org.apache.hadoop.hbase.regionserver.HRegionServer.closeRegionIgnoreErrors(HRegionServer.java:2508)
	at org.apache.hadoop.hbase.regionserver.HRegionServer.closeUserRegions(HRegionServer.java:2098)
	at org.apache.hadoop.hbase.regionserver.HRegionServer.run(HRegionServer.java:894)
	at java.lang.Thread.run(Thread.java:701)
2014-07-01 15:49:52,494 INFO  [regionserver60020] regionserver.HRegionServer: Received CLOSE for the region: 9cef9e7d34f96f1d9b111be50040a208 ,which we are already trying to CLOSE, but not completed yet
2014-07-01 15:49:52,494 WARN  [regionserver60020] regionserver.HRegionServer: Failed to close usertable,,1404228265525.9cef9e7d34f96f1d9b111be50040a208. - ignoring and continuing
org.apache.hadoop.hbase.regionserver.RegionAlreadyInTransitionException: The region 9cef9e7d34f96f1d9b111be50040a208 was already closing. New CLOSE request is ignored.
	at org.apache.hadoop.hbase.regionserver.HRegionServer.closeRegion(HRegionServer.java:2586)
	at org.apache.hadoop.hbase.regionserver.HRegionServer.closeRegionIgnoreErrors(HRegionServer.java:2508)
	at org.apache.hadoop.hbase.regionserver.HRegionServer.closeUserRegions(HRegionServer.java:2098)
	at org.apache.hadoop.hbase.regionserver.HRegionServer.run(HRegionServer.java:894)
	at java.lang.Thread.run(Thread.java:701)
2014-07-01 15:49:52,495 INFO  [regionserver60020] regionserver.HRegionServer: Received CLOSE for the region: 93253e37c284f3174309fede3f32339f ,which we are already trying to CLOSE, but not completed yet
2014-07-01 15:49:52,495 WARN  [regionserver60020] regionserver.HRegionServer: Failed to close usertable,user3,1404228265526.93253e37c284f3174309fede3f32339f. - ignoring and continuing
org.apache.hadoop.hbase.regionserver.RegionAlreadyInTransitionException: The region 93253e37c284f3174309fede3f32339f was already closing. New CLOSE request is ignored.
	at org.apache.hadoop.hbase.regionserver.HRegionServer.closeRegion(HRegionServer.java:2586)
	at org.apache.hadoop.hbase.regionserver.HRegionServer.closeRegionIgnoreErrors(HRegionServer.java:2508)
	at org.apache.hadoop.hbase.regionserver.HRegionServer.closeUserRegions(HRegionServer.java:2098)
	at org.apache.hadoop.hbase.regionserver.HRegionServer.run(HRegionServer.java:894)
	at java.lang.Thread.run(Thread.java:701)
2014-07-01 15:49:52,495 INFO  [regionserver60020] regionserver.HRegionServer: Received CLOSE for the region: 667c1f2c34fe37d48ba25a789d8aba98 ,which we are already trying to CLOSE, but not completed yet
2014-07-01 15:49:52,495 WARN  [regionserver60020] regionserver.HRegionServer: Failed to close usertable,user7,1404228265526.667c1f2c34fe37d48ba25a789d8aba98. - ignoring and continuing
org.apache.hadoop.hbase.regionserver.RegionAlreadyInTransitionException: The region 667c1f2c34fe37d48ba25a789d8aba98 was already closing. New CLOSE request is ignored.
	at org.apache.hadoop.hbase.regionserver.HRegionServer.closeRegion(HRegionServer.java:2586)
	at org.apache.hadoop.hbase.regionserver.HRegionServer.closeRegionIgnoreErrors(HRegionServer.java:2508)
	at org.apache.hadoop.hbase.regionserver.HRegionServer.closeUserRegions(HRegionServer.java:2098)
	at org.apache.hadoop.hbase.regionserver.HRegionServer.run(HRegionServer.java:894)
	at java.lang.Thread.run(Thread.java:701)
2014-07-01 15:49:52,495 DEBUG [regionserver60020] regionserver.HRegionServer: Waiting on 122e92eabfab38cb15a9965b21db25c5, bb79ae773f71289e0745068f68a0abd9, 9cef9e7d34f96f1d9b111be50040a208, 93253e37c284f3174309fede3f32339f, 667c1f2c34fe37d48ba25a789d8aba98
2014-07-01 15:49:52,675 INFO  [DataStreamer for file /hbase/WALs/slave1,60020,1404253868846/slave1%2C60020%2C1404253868846.1404254547570 block blk_-3161611795271398729_29985] ipc.Client: Retrying connect to server: master/9.1.143.58:50020. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1 SECONDS)
2014-07-01 15:49:53,150 INFO  [regionserver60020-smallCompactions-1404253964856] ipc.Client: Retrying connect to server: master/9.1.143.58:54310. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1 SECONDS)
2014-07-01 15:49:53,676 INFO  [DataStreamer for file /hbase/WALs/slave1,60020,1404253868846/slave1%2C60020%2C1404253868846.1404254547570 block blk_-3161611795271398729_29985] ipc.Client: Retrying connect to server: master/9.1.143.58:50020. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1 SECONDS)
2014-07-01 15:49:54,150 INFO  [regionserver60020-smallCompactions-1404253964856] ipc.Client: Retrying connect to server: master/9.1.143.58:54310. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1 SECONDS)
2014-07-01 15:49:54,152 WARN  [regionserver60020-smallCompactions-1404253964856] hdfs.DFSClient: Failed to connect to /9.1.143.58:50010, add to deadNodes and continuejava.net.ConnectException: Connection refused
2014-07-01 15:49:54,152 INFO  [regionserver60020-smallCompactions-1404253964856] hdfs.DFSClient: Could not obtain blk_221234652107626381_29849 from any node: java.io.IOException: No live nodes contain current block. Will get new block locations from namenode and retry...
2014-07-01 15:49:54,676 INFO  [DataStreamer for file /hbase/WALs/slave1,60020,1404253868846/slave1%2C60020%2C1404253868846.1404254547570 block blk_-3161611795271398729_29985] ipc.Client: Retrying connect to server: master/9.1.143.58:50020. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1 SECONDS)
2014-07-01 15:49:54,677 WARN  [DataStreamer for file /hbase/WALs/slave1,60020,1404253868846/slave1%2C60020%2C1404253868846.1404254547570 block blk_-3161611795271398729_29985] hdfs.DFSClient: Failed recovery attempt #3 from primary datanode 9.1.143.58:50010
java.net.ConnectException: Call to master/9.1.143.58:50020 failed on connection exception: java.net.ConnectException: Connection refused
	at org.apache.hadoop.ipc.Client.wrapException(Client.java:1142)
	at org.apache.hadoop.ipc.Client.call(Client.java:1118)
	at org.apache.hadoop.ipc.RPC$Invoker.invoke(RPC.java:229)
	at com.sun.proxy.$Proxy13.getProtocolVersion(Unknown Source)
	at org.apache.hadoop.ipc.RPC.checkVersion(RPC.java:422)
	at org.apache.hadoop.ipc.RPC.getProxy(RPC.java:414)
	at org.apache.hadoop.ipc.RPC.getProxy(RPC.java:392)
	at org.apache.hadoop.hdfs.DFSClient.createClientDatanodeProtocolProxy(DFSClient.java:201)
	at org.apache.hadoop.hdfs.DFSClient$DFSOutputStream.processDatanodeError(DFSClient.java:3317)
	at org.apache.hadoop.hdfs.DFSClient$DFSOutputStream.access$2200(DFSClient.java:2783)
	at org.apache.hadoop.hdfs.DFSClient$DFSOutputStream$DataStreamer.run(DFSClient.java:2987)
Caused by: java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:601)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:511)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:481)
	at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:457)
	at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:583)
	at org.apache.hadoop.ipc.Client$Connection.access$2200(Client.java:205)
	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1249)
	at org.apache.hadoop.ipc.Client.call(Client.java:1093)
	... 9 more
2014-07-01 15:49:54,677 WARN  [DataStreamer for file /hbase/WALs/slave1,60020,1404253868846/slave1%2C60020%2C1404253868846.1404254547570 block blk_-3161611795271398729_29985] hdfs.DFSClient: Error Recovery for block blk_-3161611795271398729_29985 failed  because recovery from primary datanode 9.1.143.58:50010 failed 4 times.  Pipeline was 9.1.143.59:50010, 9.1.143.58:50010. Will retry...
2014-07-01 15:49:55,152 INFO  [RS_CLOSE_REGION-slave1:60020-1] ipc.Client: Retrying connect to server: master/9.1.143.58:54310. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1 SECONDS)
2014-07-01 15:49:55,499 INFO  [regionserver60020] regionserver.HRegionServer: Received CLOSE for the region: 122e92eabfab38cb15a9965b21db25c5 ,which we are already trying to CLOSE, but not completed yet
2014-07-01 15:49:55,499 WARN  [regionserver60020] regionserver.HRegionServer: Failed to close usertable,user5,1404228265526.122e92eabfab38cb15a9965b21db25c5. - ignoring and continuing
org.apache.hadoop.hbase.regionserver.RegionAlreadyInTransitionException: The region 122e92eabfab38cb15a9965b21db25c5 was already closing. New CLOSE request is ignored.
	at org.apache.hadoop.hbase.regionserver.HRegionServer.closeRegion(HRegionServer.java:2586)
	at org.apache.hadoop.hbase.regionserver.HRegionServer.closeRegionIgnoreErrors(HRegionServer.java:2508)
	at org.apache.hadoop.hbase.regionserver.HRegionServer.closeUserRegions(HRegionServer.java:2098)
	at org.apache.hadoop.hbase.regionserver.HRegionServer.run(HRegionServer.java:894)
	at java.lang.Thread.run(Thread.java:701)
2014-07-01 15:49:55,499 INFO  [regionserver60020] regionserver.HRegionServer: Received CLOSE for the region: bb79ae773f71289e0745068f68a0abd9 ,which we are already trying to CLOSE, but not completed yet
2014-07-01 15:49:55,499 WARN  [regionserver60020] regionserver.HRegionServer: Failed to close usertable,user9,1404228265526.bb79ae773f71289e0745068f68a0abd9. - ignoring and continuing
org.apache.hadoop.hbase.regionserver.RegionAlreadyInTransitionException: The region bb79ae773f71289e0745068f68a0abd9 was already closing. New CLOSE request is ignored.
	at org.apache.hadoop.hbase.regionserver.HRegionServer.closeRegion(HRegionServer.java:2586)
	at org.apache.hadoop.hbase.regionserver.HRegionServer.closeRegionIgnoreErrors(HRegionServer.java:2508)
	at org.apache.hadoop.hbase.regionserver.HRegionServer.closeUserRegions(HRegionServer.java:2098)
	at org.apache.hadoop.hbase.regionserver.HRegionServer.run(HRegionServer.java:894)
	at java.lang.Thread.run(Thread.java:701)
2014-07-01 15:49:55,499 INFO  [regionserver60020] regionserver.HRegionServer: Received CLOSE for the region: 9cef9e7d34f96f1d9b111be50040a208 ,which we are already trying to CLOSE, but not completed yet
2014-07-01 15:49:55,500 WARN  [regionserver60020] regionserver.HRegionServer: Failed to close usertable,,1404228265525.9cef9e7d34f96f1d9b111be50040a208. - ignoring and continuing
org.apache.hadoop.hbase.regionserver.RegionAlreadyInTransitionException: The region 9cef9e7d34f96f1d9b111be50040a208 was already closing. New CLOSE request is ignored.
	at org.apache.hadoop.hbase.regionserver.HRegionServer.closeRegion(HRegionServer.java:2586)
	at org.apache.hadoop.hbase.regionserver.HRegionServer.closeRegionIgnoreErrors(HRegionServer.java:2508)
	at org.apache.hadoop.hbase.regionserver.HRegionServer.closeUserRegions(HRegionServer.java:2098)
	at org.apache.hadoop.hbase.regionserver.HRegionServer.run(HRegionServer.java:894)
	at java.lang.Thread.run(Thread.java:701)
2014-07-01 15:49:55,500 INFO  [regionserver60020] regionserver.HRegionServer: Received CLOSE for the region: 93253e37c284f3174309fede3f32339f ,which we are already trying to CLOSE, but not completed yet
2014-07-01 15:49:55,500 WARN  [regionserver60020] regionserver.HRegionServer: Failed to close usertable,user3,1404228265526.93253e37c284f3174309fede3f32339f. - ignoring and continuing
org.apache.hadoop.hbase.regionserver.RegionAlreadyInTransitionException: The region 93253e37c284f3174309fede3f32339f was already closing. New CLOSE request is ignored.
	at org.apache.hadoop.hbase.regionserver.HRegionServer.closeRegion(HRegionServer.java:2586)
	at org.apache.hadoop.hbase.regionserver.HRegionServer.closeRegionIgnoreErrors(HRegionServer.java:2508)
	at org.apache.hadoop.hbase.regionserver.HRegionServer.closeUserRegions(HRegionServer.java:2098)
	at org.apache.hadoop.hbase.regionserver.HRegionServer.run(HRegionServer.java:894)
	at java.lang.Thread.run(Thread.java:701)
2014-07-01 15:49:55,500 INFO  [regionserver60020] regionserver.HRegionServer: Received CLOSE for the region: 667c1f2c34fe37d48ba25a789d8aba98 ,which we are already trying to CLOSE, but not completed yet
2014-07-01 15:49:55,500 WARN  [regionserver60020] regionserver.HRegionServer: Failed to close usertable,user7,1404228265526.667c1f2c34fe37d48ba25a789d8aba98. - ignoring and continuing
org.apache.hadoop.hbase.regionserver.RegionAlreadyInTransitionException: The region 667c1f2c34fe37d48ba25a789d8aba98 was already closing. New CLOSE request is ignored.
	at org.apache.hadoop.hbase.regionserver.HRegionServer.closeRegion(HRegionServer.java:2586)
	at org.apache.hadoop.hbase.regionserver.HRegionServer.closeRegionIgnoreErrors(HRegionServer.java:2508)
	at org.apache.hadoop.hbase.regionserver.HRegionServer.closeUserRegions(HRegionServer.java:2098)
	at org.apache.hadoop.hbase.regionserver.HRegionServer.run(HRegionServer.java:894)
	at java.lang.Thread.run(Thread.java:701)
2014-07-01 15:49:55,500 DEBUG [regionserver60020] regionserver.HRegionServer: Waiting on 122e92eabfab38cb15a9965b21db25c5, bb79ae773f71289e0745068f68a0abd9, 9cef9e7d34f96f1d9b111be50040a208, 93253e37c284f3174309fede3f32339f, 667c1f2c34fe37d48ba25a789d8aba98
2014-07-01 15:49:55,677 WARN  [DataStreamer for file /hbase/WALs/slave1,60020,1404253868846/slave1%2C60020%2C1404253868846.1404254547570 block blk_-3161611795271398729_29985] hdfs.DFSClient: Error Recovery for blk_-3161611795271398729_29985 bad datanode[0] 9.1.143.59:50010
2014-07-01 15:49:55,679 WARN  [DataStreamer for file /hbase/WALs/slave1,60020,1404253868846/slave1%2C60020%2C1404253868846.1404254547570 block blk_-3161611795271398729_29985] hdfs.DFSClient: Error Recovery for block blk_-3161611795271398729_29985 in pipeline 9.1.143.59:50010, 9.1.143.58:50010: bad datanode 9.1.143.59:50010
2014-07-01 15:49:56,152 INFO  [RS_CLOSE_REGION-slave1:60020-1] ipc.Client: Retrying connect to server: master/9.1.143.58:54310. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1 SECONDS)
2014-07-01 15:49:56,681 INFO  [DataStreamer for file /hbase/WALs/slave1,60020,1404253868846/slave1%2C60020%2C1404253868846.1404254547570 block blk_-3161611795271398729_29985] ipc.Client: Retrying connect to server: master/9.1.143.58:50020. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1 SECONDS)
2014-07-01 15:49:57,153 INFO  [RS_CLOSE_REGION-slave1:60020-1] ipc.Client: Retrying connect to server: master/9.1.143.58:54310. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1 SECONDS)
2014-07-01 15:49:57,682 INFO  [DataStreamer for file /hbase/WALs/slave1,60020,1404253868846/slave1%2C60020%2C1404253868846.1404254547570 block blk_-3161611795271398729_29985] ipc.Client: Retrying connect to server: master/9.1.143.58:50020. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1 SECONDS)
2014-07-01 15:49:58,154 INFO  [RS_CLOSE_REGION-slave1:60020-1] ipc.Client: Retrying connect to server: master/9.1.143.58:54310. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1 SECONDS)
2014-07-01 15:49:58,503 INFO  [regionserver60020] regionserver.HRegionServer: Received CLOSE for the region: 122e92eabfab38cb15a9965b21db25c5 ,which we are already trying to CLOSE, but not completed yet
2014-07-01 15:49:58,503 WARN  [regionserver60020] regionserver.HRegionServer: Failed to close usertable,user5,1404228265526.122e92eabfab38cb15a9965b21db25c5. - ignoring and continuing
org.apache.hadoop.hbase.regionserver.RegionAlreadyInTransitionException: The region 122e92eabfab38cb15a9965b21db25c5 was already closing. New CLOSE request is ignored.
	at org.apache.hadoop.hbase.regionserver.HRegionServer.closeRegion(HRegionServer.java:2586)
	at org.apache.hadoop.hbase.regionserver.HRegionServer.closeRegionIgnoreErrors(HRegionServer.java:2508)
	at org.apache.hadoop.hbase.regionserver.HRegionServer.closeUserRegions(HRegionServer.java:2098)
	at org.apache.hadoop.hbase.regionserver.HRegionServer.run(HRegionServer.java:894)
	at java.lang.Thread.run(Thread.java:701)
2014-07-01 15:49:58,503 INFO  [regionserver60020] regionserver.HRegionServer: Received CLOSE for the region: bb79ae773f71289e0745068f68a0abd9 ,which we are already trying to CLOSE, but not completed yet
2014-07-01 15:49:58,503 WARN  [regionserver60020] regionserver.HRegionServer: Failed to close usertable,user9,1404228265526.bb79ae773f71289e0745068f68a0abd9. - ignoring and continuing
org.apache.hadoop.hbase.regionserver.RegionAlreadyInTransitionException: The region bb79ae773f71289e0745068f68a0abd9 was already closing. New CLOSE request is ignored.
	at org.apache.hadoop.hbase.regionserver.HRegionServer.closeRegion(HRegionServer.java:2586)
	at org.apache.hadoop.hbase.regionserver.HRegionServer.closeRegionIgnoreErrors(HRegionServer.java:2508)
	at org.apache.hadoop.hbase.regionserver.HRegionServer.closeUserRegions(HRegionServer.java:2098)
	at org.apache.hadoop.hbase.regionserver.HRegionServer.run(HRegionServer.java:894)
	at java.lang.Thread.run(Thread.java:701)
2014-07-01 15:49:58,503 INFO  [regionserver60020] regionserver.HRegionServer: Received CLOSE for the region: 9cef9e7d34f96f1d9b111be50040a208 ,which we are already trying to CLOSE, but not completed yet
2014-07-01 15:49:58,503 WARN  [regionserver60020] regionserver.HRegionServer: Failed to close usertable,,1404228265525.9cef9e7d34f96f1d9b111be50040a208. - ignoring and continuing
org.apache.hadoop.hbase.regionserver.RegionAlreadyInTransitionException: The region 9cef9e7d34f96f1d9b111be50040a208 was already closing. New CLOSE request is ignored.
	at org.apache.hadoop.hbase.regionserver.HRegionServer.closeRegion(HRegionServer.java:2586)
	at org.apache.hadoop.hbase.regionserver.HRegionServer.closeRegionIgnoreErrors(HRegionServer.java:2508)
	at org.apache.hadoop.hbase.regionserver.HRegionServer.closeUserRegions(HRegionServer.java:2098)
	at org.apache.hadoop.hbase.regionserver.HRegionServer.run(HRegionServer.java:894)
	at java.lang.Thread.run(Thread.java:701)
2014-07-01 15:49:58,504 INFO  [regionserver60020] regionserver.HRegionServer: Received CLOSE for the region: 93253e37c284f3174309fede3f32339f ,which we are already trying to CLOSE, but not completed yet
2014-07-01 15:49:58,504 WARN  [regionserver60020] regionserver.HRegionServer: Failed to close usertable,user3,1404228265526.93253e37c284f3174309fede3f32339f. - ignoring and continuing
org.apache.hadoop.hbase.regionserver.RegionAlreadyInTransitionException: The region 93253e37c284f3174309fede3f32339f was already closing. New CLOSE request is ignored.
	at org.apache.hadoop.hbase.regionserver.HRegionServer.closeRegion(HRegionServer.java:2586)
	at org.apache.hadoop.hbase.regionserver.HRegionServer.closeRegionIgnoreErrors(HRegionServer.java:2508)
	at org.apache.hadoop.hbase.regionserver.HRegionServer.closeUserRegions(HRegionServer.java:2098)
	at org.apache.hadoop.hbase.regionserver.HRegionServer.run(HRegionServer.java:894)
	at java.lang.Thread.run(Thread.java:701)
2014-07-01 15:49:58,504 INFO  [regionserver60020] regionserver.HRegionServer: Received CLOSE for the region: 667c1f2c34fe37d48ba25a789d8aba98 ,which we are already trying to CLOSE, but not completed yet
2014-07-01 15:49:58,504 WARN  [regionserver60020] regionserver.HRegionServer: Failed to close usertable,user7,1404228265526.667c1f2c34fe37d48ba25a789d8aba98. - ignoring and continuing
org.apache.hadoop.hbase.regionserver.RegionAlreadyInTransitionException: The region 667c1f2c34fe37d48ba25a789d8aba98 was already closing. New CLOSE request is ignored.
	at org.apache.hadoop.hbase.regionserver.HRegionServer.closeRegion(HRegionServer.java:2586)
	at org.apache.hadoop.hbase.regionserver.HRegionServer.closeRegionIgnoreErrors(HRegionServer.java:2508)
	at org.apache.hadoop.hbase.regionserver.HRegionServer.closeUserRegions(HRegionServer.java:2098)
	at org.apache.hadoop.hbase.regionserver.HRegionServer.run(HRegionServer.java:894)
	at java.lang.Thread.run(Thread.java:701)
2014-07-01 15:49:58,504 DEBUG [regionserver60020] regionserver.HRegionServer: Waiting on 122e92eabfab38cb15a9965b21db25c5, bb79ae773f71289e0745068f68a0abd9, 9cef9e7d34f96f1d9b111be50040a208, 93253e37c284f3174309fede3f32339f, 667c1f2c34fe37d48ba25a789d8aba98
2014-07-01 15:49:58,683 INFO  [DataStreamer for file /hbase/WALs/slave1,60020,1404253868846/slave1%2C60020%2C1404253868846.1404254547570 block blk_-3161611795271398729_29985] ipc.Client: Retrying connect to server: master/9.1.143.58:50020. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1 SECONDS)
2014-07-01 15:49:59,154 INFO  [RS_CLOSE_REGION-slave1:60020-1] ipc.Client: Retrying connect to server: master/9.1.143.58:54310. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1 SECONDS)
2014-07-01 15:49:59,683 INFO  [DataStreamer for file /hbase/WALs/slave1,60020,1404253868846/slave1%2C60020%2C1404253868846.1404254547570 block blk_-3161611795271398729_29985] ipc.Client: Retrying connect to server: master/9.1.143.58:50020. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1 SECONDS)
2014-07-01 15:50:00,155 INFO  [RS_CLOSE_REGION-slave1:60020-1] ipc.Client: Retrying connect to server: master/9.1.143.58:54310. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1 SECONDS)
2014-07-01 15:50:00,684 INFO  [DataStreamer for file /hbase/WALs/slave1,60020,1404253868846/slave1%2C60020%2C1404253868846.1404254547570 block blk_-3161611795271398729_29985] ipc.Client: Retrying connect to server: master/9.1.143.58:50020. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1 SECONDS)
2014-07-01 15:50:01,156 INFO  [RS_CLOSE_REGION-slave1:60020-1] ipc.Client: Retrying connect to server: master/9.1.143.58:54310. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1 SECONDS)
2014-07-01 15:50:01,507 INFO  [regionserver60020] regionserver.HRegionServer: Received CLOSE for the region: 122e92eabfab38cb15a9965b21db25c5 ,which we are already trying to CLOSE, but not completed yet
2014-07-01 15:50:01,507 WARN  [regionserver60020] regionserver.HRegionServer: Failed to close usertable,user5,1404228265526.122e92eabfab38cb15a9965b21db25c5. - ignoring and continuing
org.apache.hadoop.hbase.regionserver.RegionAlreadyInTransitionException: The region 122e92eabfab38cb15a9965b21db25c5 was already closing. New CLOSE request is ignored.
	at org.apache.hadoop.hbase.regionserver.HRegionServer.closeRegion(HRegionServer.java:2586)
	at org.apache.hadoop.hbase.regionserver.HRegionServer.closeRegionIgnoreErrors(HRegionServer.java:2508)
	at org.apache.hadoop.hbase.regionserver.HRegionServer.closeUserRegions(HRegionServer.java:2098)
	at org.apache.hadoop.hbase.regionserver.HRegionServer.run(HRegionServer.java:894)
	at java.lang.Thread.run(Thread.java:701)
2014-07-01 15:50:01,507 INFO  [regionserver60020] regionserver.HRegionServer: Received CLOSE for the region: bb79ae773f71289e0745068f68a0abd9 ,which we are already trying to CLOSE, but not completed yet
2014-07-01 15:50:01,507 WARN  [regionserver60020] regionserver.HRegionServer: Failed to close usertable,user9,1404228265526.bb79ae773f71289e0745068f68a0abd9. - ignoring and continuing
org.apache.hadoop.hbase.regionserver.RegionAlreadyInTransitionException: The region bb79ae773f71289e0745068f68a0abd9 was already closing. New CLOSE request is ignored.
	at org.apache.hadoop.hbase.regionserver.HRegionServer.closeRegion(HRegionServer.java:2586)
	at org.apache.hadoop.hbase.regionserver.HRegionServer.closeRegionIgnoreErrors(HRegionServer.java:2508)
	at org.apache.hadoop.hbase.regionserver.HRegionServer.closeUserRegions(HRegionServer.java:2098)
	at org.apache.hadoop.hbase.regionserver.HRegionServer.run(HRegionServer.java:894)
	at java.lang.Thread.run(Thread.java:701)
2014-07-01 15:50:01,507 INFO  [regionserver60020] regionserver.HRegionServer: Received CLOSE for the region: 9cef9e7d34f96f1d9b111be50040a208 ,which we are already trying to CLOSE, but not completed yet
2014-07-01 15:50:01,507 WARN  [regionserver60020] regionserver.HRegionServer: Failed to close usertable,,1404228265525.9cef9e7d34f96f1d9b111be50040a208. - ignoring and continuing
org.apache.hadoop.hbase.regionserver.RegionAlreadyInTransitionException: The region 9cef9e7d34f96f1d9b111be50040a208 was already closing. New CLOSE request is ignored.
	at org.apache.hadoop.hbase.regionserver.HRegionServer.closeRegion(HRegionServer.java:2586)
	at org.apache.hadoop.hbase.regionserver.HRegionServer.closeRegionIgnoreErrors(HRegionServer.java:2508)
	at org.apache.hadoop.hbase.regionserver.HRegionServer.closeUserRegions(HRegionServer.java:2098)
	at org.apache.hadoop.hbase.regionserver.HRegionServer.run(HRegionServer.java:894)
	at java.lang.Thread.run(Thread.java:701)
2014-07-01 15:50:01,507 INFO  [regionserver60020] regionserver.HRegionServer: Received CLOSE for the region: 93253e37c284f3174309fede3f32339f ,which we are already trying to CLOSE, but not completed yet
2014-07-01 15:50:01,507 WARN  [regionserver60020] regionserver.HRegionServer: Failed to close usertable,user3,1404228265526.93253e37c284f3174309fede3f32339f. - ignoring and continuing
org.apache.hadoop.hbase.regionserver.RegionAlreadyInTransitionException: The region 93253e37c284f3174309fede3f32339f was already closing. New CLOSE request is ignored.
	at org.apache.hadoop.hbase.regionserver.HRegionServer.closeRegion(HRegionServer.java:2586)
	at org.apache.hadoop.hbase.regionserver.HRegionServer.closeRegionIgnoreErrors(HRegionServer.java:2508)
	at org.apache.hadoop.hbase.regionserver.HRegionServer.closeUserRegions(HRegionServer.java:2098)
	at org.apache.hadoop.hbase.regionserver.HRegionServer.run(HRegionServer.java:894)
	at java.lang.Thread.run(Thread.java:701)
2014-07-01 15:50:01,508 INFO  [regionserver60020] regionserver.HRegionServer: Received CLOSE for the region: 667c1f2c34fe37d48ba25a789d8aba98 ,which we are already trying to CLOSE, but not completed yet
2014-07-01 15:50:01,508 WARN  [regionserver60020] regionserver.HRegionServer: Failed to close usertable,user7,1404228265526.667c1f2c34fe37d48ba25a789d8aba98. - ignoring and continuing
org.apache.hadoop.hbase.regionserver.RegionAlreadyInTransitionException: The region 667c1f2c34fe37d48ba25a789d8aba98 was already closing. New CLOSE request is ignored.
	at org.apache.hadoop.hbase.regionserver.HRegionServer.closeRegion(HRegionServer.java:2586)
	at org.apache.hadoop.hbase.regionserver.HRegionServer.closeRegionIgnoreErrors(HRegionServer.java:2508)
	at org.apache.hadoop.hbase.regionserver.HRegionServer.closeUserRegions(HRegionServer.java:2098)
	at org.apache.hadoop.hbase.regionserver.HRegionServer.run(HRegionServer.java:894)
	at java.lang.Thread.run(Thread.java:701)
2014-07-01 15:50:01,508 DEBUG [regionserver60020] regionserver.HRegionServer: Waiting on 122e92eabfab38cb15a9965b21db25c5, bb79ae773f71289e0745068f68a0abd9, 9cef9e7d34f96f1d9b111be50040a208, 93253e37c284f3174309fede3f32339f, 667c1f2c34fe37d48ba25a789d8aba98
2014-07-01 15:50:01,685 INFO  [DataStreamer for file /hbase/WALs/slave1,60020,1404253868846/slave1%2C60020%2C1404253868846.1404254547570 block blk_-3161611795271398729_29985] ipc.Client: Retrying connect to server: master/9.1.143.58:50020. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1 SECONDS)
2014-07-01 15:50:02,157 INFO  [RS_CLOSE_REGION-slave1:60020-1] ipc.Client: Retrying connect to server: master/9.1.143.58:54310. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1 SECONDS)
2014-07-01 15:50:02,687 INFO  [DataStreamer for file /hbase/WALs/slave1,60020,1404253868846/slave1%2C60020%2C1404253868846.1404254547570 block blk_-3161611795271398729_29985] ipc.Client: Retrying connect to server: master/9.1.143.58:50020. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1 SECONDS)
2014-07-01 15:50:03,157 INFO  [RS_CLOSE_REGION-slave1:60020-1] ipc.Client: Retrying connect to server: master/9.1.143.58:54310. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1 SECONDS)
2014-07-01 15:50:03,688 INFO  [DataStreamer for file /hbase/WALs/slave1,60020,1404253868846/slave1%2C60020%2C1404253868846.1404254547570 block blk_-3161611795271398729_29985] ipc.Client: Retrying connect to server: master/9.1.143.58:50020. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1 SECONDS)
2014-07-01 15:50:04,158 INFO  [RS_CLOSE_REGION-slave1:60020-1] ipc.Client: Retrying connect to server: master/9.1.143.58:54310. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1 SECONDS)
2014-07-01 15:50:04,159 WARN  [RS_CLOSE_REGION-slave1:60020-1] regionserver.HStore: Failed flushing store file, retrying num=1
java.net.ConnectException: Call to master/9.1.143.58:54310 failed on connection exception: java.net.ConnectException: Connection refused
	at org.apache.hadoop.ipc.Client.wrapException(Client.java:1142)
	at org.apache.hadoop.ipc.Client.call(Client.java:1118)
	at org.apache.hadoop.ipc.RPC$Invoker.invoke(RPC.java:229)
	at com.sun.proxy.$Proxy8.getFileInfo(Unknown Source)
	at sun.reflect.GeneratedMethodAccessor2.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:622)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:85)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:62)
	at com.sun.proxy.$Proxy8.getFileInfo(Unknown Source)
	at sun.reflect.GeneratedMethodAccessor2.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:622)
	at org.apache.hadoop.hbase.fs.HFileSystem$1.invoke(HFileSystem.java:294)
	at com.sun.proxy.$Proxy9.getFileInfo(Unknown Source)
	at org.apache.hadoop.hdfs.DFSClient.getFileInfo(DFSClient.java:1059)
	at org.apache.hadoop.hdfs.DistributedFileSystem.getFileStatus(DistributedFileSystem.java:554)
	at org.apache.hadoop.fs.FilterFileSystem.getFileStatus(FilterFileSystem.java:255)
	at org.apache.hadoop.fs.FileSystem.exists(FileSystem.java:788)
	at org.apache.hadoop.hbase.regionserver.StoreFile$WriterBuilder.build(StoreFile.java:603)
	at org.apache.hadoop.hbase.regionserver.HStore.createWriterInTmp(HStore.java:889)
	at org.apache.hadoop.hbase.regionserver.DefaultStoreFlusher.flushSnapshot(DefaultStoreFlusher.java:69)
	at org.apache.hadoop.hbase.regionserver.HStore.flushCache(HStore.java:798)
	at org.apache.hadoop.hbase.regionserver.HStore$StoreFlusherImpl.flushCache(HStore.java:1949)
	at org.apache.hadoop.hbase.regionserver.HRegion.internalFlushcache(HRegion.java:1776)
	at org.apache.hadoop.hbase.regionserver.HRegion.internalFlushcache(HRegion.java:1658)
	at org.apache.hadoop.hbase.regionserver.HRegion.doClose(HRegion.java:1119)
	at org.apache.hadoop.hbase.regionserver.HRegion.close(HRegion.java:1085)
	at org.apache.hadoop.hbase.regionserver.handler.CloseRegionHandler.process(CloseRegionHandler.java:147)
	at org.apache.hadoop.hbase.executor.EventHandler.run(EventHandler.java:128)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1146)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:701)
Caused by: java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:601)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:511)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:481)
	at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:457)
	at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:583)
	at org.apache.hadoop.ipc.Client$Connection.access$2200(Client.java:205)
	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1249)
	at org.apache.hadoop.ipc.Client.call(Client.java:1093)
	... 31 more
2014-07-01 15:50:04,510 INFO  [regionserver60020] regionserver.HRegionServer: Received CLOSE for the region: 122e92eabfab38cb15a9965b21db25c5 ,which we are already trying to CLOSE, but not completed yet
2014-07-01 15:50:04,510 WARN  [regionserver60020] regionserver.HRegionServer: Failed to close usertable,user5,1404228265526.122e92eabfab38cb15a9965b21db25c5. - ignoring and continuing
org.apache.hadoop.hbase.regionserver.RegionAlreadyInTransitionException: The region 122e92eabfab38cb15a9965b21db25c5 was already closing. New CLOSE request is ignored.
	at org.apache.hadoop.hbase.regionserver.HRegionServer.closeRegion(HRegionServer.java:2586)
	at org.apache.hadoop.hbase.regionserver.HRegionServer.closeRegionIgnoreErrors(HRegionServer.java:2508)
	at org.apache.hadoop.hbase.regionserver.HRegionServer.closeUserRegions(HRegionServer.java:2098)
	at org.apache.hadoop.hbase.regionserver.HRegionServer.run(HRegionServer.java:894)
	at java.lang.Thread.run(Thread.java:701)
2014-07-01 15:50:04,511 INFO  [regionserver60020] regionserver.HRegionServer: Received CLOSE for the region: bb79ae773f71289e0745068f68a0abd9 ,which we are already trying to CLOSE, but not completed yet
2014-07-01 15:50:04,511 WARN  [regionserver60020] regionserver.HRegionServer: Failed to close usertable,user9,1404228265526.bb79ae773f71289e0745068f68a0abd9. - ignoring and continuing
org.apache.hadoop.hbase.regionserver.RegionAlreadyInTransitionException: The region bb79ae773f71289e0745068f68a0abd9 was already closing. New CLOSE request is ignored.
	at org.apache.hadoop.hbase.regionserver.HRegionServer.closeRegion(HRegionServer.java:2586)
	at org.apache.hadoop.hbase.regionserver.HRegionServer.closeRegionIgnoreErrors(HRegionServer.java:2508)
	at org.apache.hadoop.hbase.regionserver.HRegionServer.closeUserRegions(HRegionServer.java:2098)
	at org.apache.hadoop.hbase.regionserver.HRegionServer.run(HRegionServer.java:894)
	at java.lang.Thread.run(Thread.java:701)
2014-07-01 15:50:04,511 INFO  [regionserver60020] regionserver.HRegionServer: Received CLOSE for the region: 9cef9e7d34f96f1d9b111be50040a208 ,which we are already trying to CLOSE, but not completed yet
2014-07-01 15:50:04,511 WARN  [regionserver60020] regionserver.HRegionServer: Failed to close usertable,,1404228265525.9cef9e7d34f96f1d9b111be50040a208. - ignoring and continuing
org.apache.hadoop.hbase.regionserver.RegionAlreadyInTransitionException: The region 9cef9e7d34f96f1d9b111be50040a208 was already closing. New CLOSE request is ignored.
	at org.apache.hadoop.hbase.regionserver.HRegionServer.closeRegion(HRegionServer.java:2586)
	at org.apache.hadoop.hbase.regionserver.HRegionServer.closeRegionIgnoreErrors(HRegionServer.java:2508)
	at org.apache.hadoop.hbase.regionserver.HRegionServer.closeUserRegions(HRegionServer.java:2098)
	at org.apache.hadoop.hbase.regionserver.HRegionServer.run(HRegionServer.java:894)
	at java.lang.Thread.run(Thread.java:701)
2014-07-01 15:50:04,511 INFO  [regionserver60020] regionserver.HRegionServer: Received CLOSE for the region: 93253e37c284f3174309fede3f32339f ,which we are already trying to CLOSE, but not completed yet
2014-07-01 15:50:04,511 WARN  [regionserver60020] regionserver.HRegionServer: Failed to close usertable,user3,1404228265526.93253e37c284f3174309fede3f32339f. - ignoring and continuing
org.apache.hadoop.hbase.regionserver.RegionAlreadyInTransitionException: The region 93253e37c284f3174309fede3f32339f was already closing. New CLOSE request is ignored.
	at org.apache.hadoop.hbase.regionserver.HRegionServer.closeRegion(HRegionServer.java:2586)
	at org.apache.hadoop.hbase.regionserver.HRegionServer.closeRegionIgnoreErrors(HRegionServer.java:2508)
	at org.apache.hadoop.hbase.regionserver.HRegionServer.closeUserRegions(HRegionServer.java:2098)
	at org.apache.hadoop.hbase.regionserver.HRegionServer.run(HRegionServer.java:894)
	at java.lang.Thread.run(Thread.java:701)
2014-07-01 15:50:04,511 INFO  [regionserver60020] regionserver.HRegionServer: Received CLOSE for the region: 667c1f2c34fe37d48ba25a789d8aba98 ,which we are already trying to CLOSE, but not completed yet
2014-07-01 15:50:04,511 WARN  [regionserver60020] regionserver.HRegionServer: Failed to close usertable,user7,1404228265526.667c1f2c34fe37d48ba25a789d8aba98. - ignoring and continuing
org.apache.hadoop.hbase.regionserver.RegionAlreadyInTransitionException: The region 667c1f2c34fe37d48ba25a789d8aba98 was already closing. New CLOSE request is ignored.
	at org.apache.hadoop.hbase.regionserver.HRegionServer.closeRegion(HRegionServer.java:2586)
	at org.apache.hadoop.hbase.regionserver.HRegionServer.closeRegionIgnoreErrors(HRegionServer.java:2508)
	at org.apache.hadoop.hbase.regionserver.HRegionServer.closeUserRegions(HRegionServer.java:2098)
	at org.apache.hadoop.hbase.regionserver.HRegionServer.run(HRegionServer.java:894)
	at java.lang.Thread.run(Thread.java:701)
2014-07-01 15:50:04,511 DEBUG [regionserver60020] regionserver.HRegionServer: Waiting on 122e92eabfab38cb15a9965b21db25c5, bb79ae773f71289e0745068f68a0abd9, 9cef9e7d34f96f1d9b111be50040a208, 93253e37c284f3174309fede3f32339f, 667c1f2c34fe37d48ba25a789d8aba98
2014-07-01 15:50:04,688 INFO  [DataStreamer for file /hbase/WALs/slave1,60020,1404253868846/slave1%2C60020%2C1404253868846.1404254547570 block blk_-3161611795271398729_29985] ipc.Client: Retrying connect to server: master/9.1.143.58:50020. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1 SECONDS)
2014-07-01 15:50:05,173 INFO  [org.apache.hadoop.hdfs.LeaseRenewer$1@4fae46d5] ipc.Client: Retrying connect to server: master/9.1.143.58:54310. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1 SECONDS)
2014-07-01 15:50:05,689 INFO  [DataStreamer for file /hbase/WALs/slave1,60020,1404253868846/slave1%2C60020%2C1404253868846.1404254547570 block blk_-3161611795271398729_29985] ipc.Client: Retrying connect to server: master/9.1.143.58:50020. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1 SECONDS)
2014-07-01 15:50:05,690 WARN  [DataStreamer for file /hbase/WALs/slave1,60020,1404253868846/slave1%2C60020%2C1404253868846.1404254547570 block blk_-3161611795271398729_29985] hdfs.DFSClient: Failed recovery attempt #4 from primary datanode 9.1.143.58:50010
java.net.ConnectException: Call to master/9.1.143.58:50020 failed on connection exception: java.net.ConnectException: Connection refused
	at org.apache.hadoop.ipc.Client.wrapException(Client.java:1142)
	at org.apache.hadoop.ipc.Client.call(Client.java:1118)
	at org.apache.hadoop.ipc.RPC$Invoker.invoke(RPC.java:229)
	at com.sun.proxy.$Proxy13.getProtocolVersion(Unknown Source)
	at org.apache.hadoop.ipc.RPC.checkVersion(RPC.java:422)
	at org.apache.hadoop.ipc.RPC.getProxy(RPC.java:414)
	at org.apache.hadoop.ipc.RPC.getProxy(RPC.java:392)
	at org.apache.hadoop.hdfs.DFSClient.createClientDatanodeProtocolProxy(DFSClient.java:201)
	at org.apache.hadoop.hdfs.DFSClient$DFSOutputStream.processDatanodeError(DFSClient.java:3317)
	at org.apache.hadoop.hdfs.DFSClient$DFSOutputStream.access$2200(DFSClient.java:2783)
	at org.apache.hadoop.hdfs.DFSClient$DFSOutputStream$DataStreamer.run(DFSClient.java:2987)
Caused by: java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:601)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:511)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:481)
	at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:457)
	at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:583)
	at org.apache.hadoop.ipc.Client$Connection.access$2200(Client.java:205)
	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1249)
	at org.apache.hadoop.ipc.Client.call(Client.java:1093)
	... 9 more
2014-07-01 15:50:05,690 WARN  [DataStreamer for file /hbase/WALs/slave1,60020,1404253868846/slave1%2C60020%2C1404253868846.1404254547570 block blk_-3161611795271398729_29985] hdfs.DFSClient: Error Recovery for block blk_-3161611795271398729_29985 failed  because recovery from primary datanode 9.1.143.58:50010 failed 5 times.  Pipeline was 9.1.143.59:50010, 9.1.143.58:50010. Will retry...
2014-07-01 15:50:06,174 INFO  [org.apache.hadoop.hdfs.LeaseRenewer$1@4fae46d5] ipc.Client: Retrying connect to server: master/9.1.143.58:54310. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1 SECONDS)
2014-07-01 15:50:06,690 WARN  [DataStreamer for file /hbase/WALs/slave1,60020,1404253868846/slave1%2C60020%2C1404253868846.1404254547570 block blk_-3161611795271398729_29985] hdfs.DFSClient: Error Recovery for blk_-3161611795271398729_29985 bad datanode[0] 9.1.143.59:50010
2014-07-01 15:50:06,690 WARN  [DataStreamer for file /hbase/WALs/slave1,60020,1404253868846/slave1%2C60020%2C1404253868846.1404254547570 block blk_-3161611795271398729_29985] hdfs.DFSClient: Error Recovery for block blk_-3161611795271398729_29985 in pipeline 9.1.143.59:50010, 9.1.143.58:50010: bad datanode 9.1.143.59:50010
2014-07-01 15:50:07,174 INFO  [org.apache.hadoop.hdfs.LeaseRenewer$1@4fae46d5] ipc.Client: Retrying connect to server: master/9.1.143.58:54310. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1 SECONDS)
2014-07-01 15:50:07,514 INFO  [regionserver60020] regionserver.HRegionServer: Received CLOSE for the region: 122e92eabfab38cb15a9965b21db25c5 ,which we are already trying to CLOSE, but not completed yet
2014-07-01 15:50:07,514 WARN  [regionserver60020] regionserver.HRegionServer: Failed to close usertable,user5,1404228265526.122e92eabfab38cb15a9965b21db25c5. - ignoring and continuing
org.apache.hadoop.hbase.regionserver.RegionAlreadyInTransitionException: The region 122e92eabfab38cb15a9965b21db25c5 was already closing. New CLOSE request is ignored.
	at org.apache.hadoop.hbase.regionserver.HRegionServer.closeRegion(HRegionServer.java:2586)
	at org.apache.hadoop.hbase.regionserver.HRegionServer.closeRegionIgnoreErrors(HRegionServer.java:2508)
	at org.apache.hadoop.hbase.regionserver.HRegionServer.closeUserRegions(HRegionServer.java:2098)
	at org.apache.hadoop.hbase.regionserver.HRegionServer.run(HRegionServer.java:894)
	at java.lang.Thread.run(Thread.java:701)
2014-07-01 15:50:07,514 INFO  [regionserver60020] regionserver.HRegionServer: Received CLOSE for the region: bb79ae773f71289e0745068f68a0abd9 ,which we are already trying to CLOSE, but not completed yet
2014-07-01 15:50:07,514 WARN  [regionserver60020] regionserver.HRegionServer: Failed to close usertable,user9,1404228265526.bb79ae773f71289e0745068f68a0abd9. - ignoring and continuing
org.apache.hadoop.hbase.regionserver.RegionAlreadyInTransitionException: The region bb79ae773f71289e0745068f68a0abd9 was already closing. New CLOSE request is ignored.
	at org.apache.hadoop.hbase.regionserver.HRegionServer.closeRegion(HRegionServer.java:2586)
	at org.apache.hadoop.hbase.regionserver.HRegionServer.closeRegionIgnoreErrors(HRegionServer.java:2508)
	at org.apache.hadoop.hbase.regionserver.HRegionServer.closeUserRegions(HRegionServer.java:2098)
	at org.apache.hadoop.hbase.regionserver.HRegionServer.run(HRegionServer.java:894)
	at java.lang.Thread.run(Thread.java:701)
2014-07-01 15:50:07,514 INFO  [regionserver60020] regionserver.HRegionServer: Received CLOSE for the region: 9cef9e7d34f96f1d9b111be50040a208 ,which we are already trying to CLOSE, but not completed yet
2014-07-01 15:50:07,514 WARN  [regionserver60020] regionserver.HRegionServer: Failed to close usertable,,1404228265525.9cef9e7d34f96f1d9b111be50040a208. - ignoring and continuing
org.apache.hadoop.hbase.regionserver.RegionAlreadyInTransitionException: The region 9cef9e7d34f96f1d9b111be50040a208 was already closing. New CLOSE request is ignored.
	at org.apache.hadoop.hbase.regionserver.HRegionServer.closeRegion(HRegionServer.java:2586)
	at org.apache.hadoop.hbase.regionserver.HRegionServer.closeRegionIgnoreErrors(HRegionServer.java:2508)
	at org.apache.hadoop.hbase.regionserver.HRegionServer.closeUserRegions(HRegionServer.java:2098)
	at org.apache.hadoop.hbase.regionserver.HRegionServer.run(HRegionServer.java:894)
	at java.lang.Thread.run(Thread.java:701)
2014-07-01 15:50:07,514 INFO  [regionserver60020] regionserver.HRegionServer: Received CLOSE for the region: 93253e37c284f3174309fede3f32339f ,which we are already trying to CLOSE, but not completed yet
2014-07-01 15:50:07,514 WARN  [regionserver60020] regionserver.HRegionServer: Failed to close usertable,user3,1404228265526.93253e37c284f3174309fede3f32339f. - ignoring and continuing
org.apache.hadoop.hbase.regionserver.RegionAlreadyInTransitionException: The region 93253e37c284f3174309fede3f32339f was already closing. New CLOSE request is ignored.
	at org.apache.hadoop.hbase.regionserver.HRegionServer.closeRegion(HRegionServer.java:2586)
	at org.apache.hadoop.hbase.regionserver.HRegionServer.closeRegionIgnoreErrors(HRegionServer.java:2508)
	at org.apache.hadoop.hbase.regionserver.HRegionServer.closeUserRegions(HRegionServer.java:2098)
	at org.apache.hadoop.hbase.regionserver.HRegionServer.run(HRegionServer.java:894)
	at java.lang.Thread.run(Thread.java:701)
2014-07-01 15:50:07,515 INFO  [regionserver60020] regionserver.HRegionServer: Received CLOSE for the region: 667c1f2c34fe37d48ba25a789d8aba98 ,which we are already trying to CLOSE, but not completed yet
2014-07-01 15:50:07,515 WARN  [regionserver60020] regionserver.HRegionServer: Failed to close usertable,user7,1404228265526.667c1f2c34fe37d48ba25a789d8aba98. - ignoring and continuing
org.apache.hadoop.hbase.regionserver.RegionAlreadyInTransitionException: The region 667c1f2c34fe37d48ba25a789d8aba98 was already closing. New CLOSE request is ignored.
	at org.apache.hadoop.hbase.regionserver.HRegionServer.closeRegion(HRegionServer.java:2586)
	at org.apache.hadoop.hbase.regionserver.HRegionServer.closeRegionIgnoreErrors(HRegionServer.java:2508)
	at org.apache.hadoop.hbase.regionserver.HRegionServer.closeUserRegions(HRegionServer.java:2098)
	at org.apache.hadoop.hbase.regionserver.HRegionServer.run(HRegionServer.java:894)
	at java.lang.Thread.run(Thread.java:701)
2014-07-01 15:50:07,515 DEBUG [regionserver60020] regionserver.HRegionServer: Waiting on 122e92eabfab38cb15a9965b21db25c5, bb79ae773f71289e0745068f68a0abd9, 9cef9e7d34f96f1d9b111be50040a208, 93253e37c284f3174309fede3f32339f, 667c1f2c34fe37d48ba25a789d8aba98
2014-07-01 15:50:07,692 INFO  [DataStreamer for file /hbase/WALs/slave1,60020,1404253868846/slave1%2C60020%2C1404253868846.1404254547570 block blk_-3161611795271398729_29985] ipc.Client: Retrying connect to server: master/9.1.143.58:50020. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1 SECONDS)
2014-07-01 15:50:08,175 INFO  [org.apache.hadoop.hdfs.LeaseRenewer$1@4fae46d5] ipc.Client: Retrying connect to server: master/9.1.143.58:54310. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1 SECONDS)
2014-07-01 15:50:08,692 INFO  [DataStreamer for file /hbase/WALs/slave1,60020,1404253868846/slave1%2C60020%2C1404253868846.1404254547570 block blk_-3161611795271398729_29985] ipc.Client: Retrying connect to server: master/9.1.143.58:50020. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1 SECONDS)
2014-07-01 15:50:09,176 INFO  [org.apache.hadoop.hdfs.LeaseRenewer$1@4fae46d5] ipc.Client: Retrying connect to server: master/9.1.143.58:54310. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1 SECONDS)
2014-07-01 15:50:09,694 INFO  [DataStreamer for file /hbase/WALs/slave1,60020,1404253868846/slave1%2C60020%2C1404253868846.1404254547570 block blk_-3161611795271398729_29985] ipc.Client: Retrying connect to server: master/9.1.143.58:50020. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1 SECONDS)
2014-07-01 15:50:10,176 INFO  [org.apache.hadoop.hdfs.LeaseRenewer$1@4fae46d5] ipc.Client: Retrying connect to server: master/9.1.143.58:54310. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1 SECONDS)
2014-07-01 15:50:10,518 INFO  [regionserver60020] regionserver.HRegionServer: Received CLOSE for the region: 122e92eabfab38cb15a9965b21db25c5 ,which we are already trying to CLOSE, but not completed yet
2014-07-01 15:50:10,518 WARN  [regionserver60020] regionserver.HRegionServer: Failed to close usertable,user5,1404228265526.122e92eabfab38cb15a9965b21db25c5. - ignoring and continuing
org.apache.hadoop.hbase.regionserver.RegionAlreadyInTransitionException: The region 122e92eabfab38cb15a9965b21db25c5 was already closing. New CLOSE request is ignored.
	at org.apache.hadoop.hbase.regionserver.HRegionServer.closeRegion(HRegionServer.java:2586)
	at org.apache.hadoop.hbase.regionserver.HRegionServer.closeRegionIgnoreErrors(HRegionServer.java:2508)
	at org.apache.hadoop.hbase.regionserver.HRegionServer.closeUserRegions(HRegionServer.java:2098)
	at org.apache.hadoop.hbase.regionserver.HRegionServer.run(HRegionServer.java:894)
	at java.lang.Thread.run(Thread.java:701)
2014-07-01 15:50:10,519 INFO  [regionserver60020] regionserver.HRegionServer: Received CLOSE for the region: bb79ae773f71289e0745068f68a0abd9 ,which we are already trying to CLOSE, but not completed yet
2014-07-01 15:50:10,519 WARN  [regionserver60020] regionserver.HRegionServer: Failed to close usertable,user9,1404228265526.bb79ae773f71289e0745068f68a0abd9. - ignoring and continuing
org.apache.hadoop.hbase.regionserver.RegionAlreadyInTransitionException: The region bb79ae773f71289e0745068f68a0abd9 was already closing. New CLOSE request is ignored.
	at org.apache.hadoop.hbase.regionserver.HRegionServer.closeRegion(HRegionServer.java:2586)
	at org.apache.hadoop.hbase.regionserver.HRegionServer.closeRegionIgnoreErrors(HRegionServer.java:2508)
	at org.apache.hadoop.hbase.regionserver.HRegionServer.closeUserRegions(HRegionServer.java:2098)
	at org.apache.hadoop.hbase.regionserver.HRegionServer.run(HRegionServer.java:894)
	at java.lang.Thread.run(Thread.java:701)
2014-07-01 15:50:10,519 INFO  [regionserver60020] regionserver.HRegionServer: Received CLOSE for the region: 9cef9e7d34f96f1d9b111be50040a208 ,which we are already trying to CLOSE, but not completed yet
2014-07-01 15:50:10,519 WARN  [regionserver60020] regionserver.HRegionServer: Failed to close usertable,,1404228265525.9cef9e7d34f96f1d9b111be50040a208. - ignoring and continuing
org.apache.hadoop.hbase.regionserver.RegionAlreadyInTransitionException: The region 9cef9e7d34f96f1d9b111be50040a208 was already closing. New CLOSE request is ignored.
	at org.apache.hadoop.hbase.regionserver.HRegionServer.closeRegion(HRegionServer.java:2586)
	at org.apache.hadoop.hbase.regionserver.HRegionServer.closeRegionIgnoreErrors(HRegionServer.java:2508)
	at org.apache.hadoop.hbase.regionserver.HRegionServer.closeUserRegions(HRegionServer.java:2098)
	at org.apache.hadoop.hbase.regionserver.HRegionServer.run(HRegionServer.java:894)
	at java.lang.Thread.run(Thread.java:701)
2014-07-01 15:50:10,520 INFO  [regionserver60020] regionserver.HRegionServer: Received CLOSE for the region: 93253e37c284f3174309fede3f32339f ,which we are already trying to CLOSE, but not completed yet
2014-07-01 15:50:10,520 WARN  [regionserver60020] regionserver.HRegionServer: Failed to close usertable,user3,1404228265526.93253e37c284f3174309fede3f32339f. - ignoring and continuing
org.apache.hadoop.hbase.regionserver.RegionAlreadyInTransitionException: The region 93253e37c284f3174309fede3f32339f was already closing. New CLOSE request is ignored.
	at org.apache.hadoop.hbase.regionserver.HRegionServer.closeRegion(HRegionServer.java:2586)
	at org.apache.hadoop.hbase.regionserver.HRegionServer.closeRegionIgnoreErrors(HRegionServer.java:2508)
	at org.apache.hadoop.hbase.regionserver.HRegionServer.closeUserRegions(HRegionServer.java:2098)
	at org.apache.hadoop.hbase.regionserver.HRegionServer.run(HRegionServer.java:894)
	at java.lang.Thread.run(Thread.java:701)
2014-07-01 15:50:10,520 INFO  [regionserver60020] regionserver.HRegionServer: Received CLOSE for the region: 667c1f2c34fe37d48ba25a789d8aba98 ,which we are already trying to CLOSE, but not completed yet
2014-07-01 15:50:10,520 WARN  [regionserver60020] regionserver.HRegionServer: Failed to close usertable,user7,1404228265526.667c1f2c34fe37d48ba25a789d8aba98. - ignoring and continuing
org.apache.hadoop.hbase.regionserver.RegionAlreadyInTransitionException: The region 667c1f2c34fe37d48ba25a789d8aba98 was already closing. New CLOSE request is ignored.
	at org.apache.hadoop.hbase.regionserver.HRegionServer.closeRegion(HRegionServer.java:2586)
	at org.apache.hadoop.hbase.regionserver.HRegionServer.closeRegionIgnoreErrors(HRegionServer.java:2508)
	at org.apache.hadoop.hbase.regionserver.HRegionServer.closeUserRegions(HRegionServer.java:2098)
	at org.apache.hadoop.hbase.regionserver.HRegionServer.run(HRegionServer.java:894)
	at java.lang.Thread.run(Thread.java:701)
2014-07-01 15:50:10,520 DEBUG [regionserver60020] regionserver.HRegionServer: Waiting on 122e92eabfab38cb15a9965b21db25c5, bb79ae773f71289e0745068f68a0abd9, 9cef9e7d34f96f1d9b111be50040a208, 93253e37c284f3174309fede3f32339f, 667c1f2c34fe37d48ba25a789d8aba98
2014-07-01 15:50:10,694 INFO  [DataStreamer for file /hbase/WALs/slave1,60020,1404253868846/slave1%2C60020%2C1404253868846.1404254547570 block blk_-3161611795271398729_29985] ipc.Client: Retrying connect to server: master/9.1.143.58:50020. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1 SECONDS)
2014-07-01 15:50:11,177 INFO  [org.apache.hadoop.hdfs.LeaseRenewer$1@4fae46d5] ipc.Client: Retrying connect to server: master/9.1.143.58:54310. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1 SECONDS)
2014-07-01 15:50:11,695 INFO  [DataStreamer for file /hbase/WALs/slave1,60020,1404253868846/slave1%2C60020%2C1404253868846.1404254547570 block blk_-3161611795271398729_29985] ipc.Client: Retrying connect to server: master/9.1.143.58:50020. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1 SECONDS)
2014-07-01 15:50:12,178 INFO  [org.apache.hadoop.hdfs.LeaseRenewer$1@4fae46d5] ipc.Client: Retrying connect to server: master/9.1.143.58:54310. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1 SECONDS)
2014-07-01 15:50:12,696 INFO  [DataStreamer for file /hbase/WALs/slave1,60020,1404253868846/slave1%2C60020%2C1404253868846.1404254547570 block blk_-3161611795271398729_29985] ipc.Client: Retrying connect to server: master/9.1.143.58:50020. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1 SECONDS)
2014-07-01 15:50:13,179 INFO  [org.apache.hadoop.hdfs.LeaseRenewer$1@4fae46d5] ipc.Client: Retrying connect to server: master/9.1.143.58:54310. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1 SECONDS)
2014-07-01 15:50:13,523 INFO  [regionserver60020] regionserver.HRegionServer: Received CLOSE for the region: 122e92eabfab38cb15a9965b21db25c5 ,which we are already trying to CLOSE, but not completed yet
2014-07-01 15:50:13,524 WARN  [regionserver60020] regionserver.HRegionServer: Failed to close usertable,user5,1404228265526.122e92eabfab38cb15a9965b21db25c5. - ignoring and continuing
org.apache.hadoop.hbase.regionserver.RegionAlreadyInTransitionException: The region 122e92eabfab38cb15a9965b21db25c5 was already closing. New CLOSE request is ignored.
	at org.apache.hadoop.hbase.regionserver.HRegionServer.closeRegion(HRegionServer.java:2586)
	at org.apache.hadoop.hbase.regionserver.HRegionServer.closeRegionIgnoreErrors(HRegionServer.java:2508)
	at org.apache.hadoop.hbase.regionserver.HRegionServer.closeUserRegions(HRegionServer.java:2098)
	at org.apache.hadoop.hbase.regionserver.HRegionServer.run(HRegionServer.java:894)
	at java.lang.Thread.run(Thread.java:701)
2014-07-01 15:50:13,524 INFO  [regionserver60020] regionserver.HRegionServer: Received CLOSE for the region: bb79ae773f71289e0745068f68a0abd9 ,which we are already trying to CLOSE, but not completed yet
2014-07-01 15:50:13,524 WARN  [regionserver60020] regionserver.HRegionServer: Failed to close usertable,user9,1404228265526.bb79ae773f71289e0745068f68a0abd9. - ignoring and continuing
org.apache.hadoop.hbase.regionserver.RegionAlreadyInTransitionException: The region bb79ae773f71289e0745068f68a0abd9 was already closing. New CLOSE request is ignored.
	at org.apache.hadoop.hbase.regionserver.HRegionServer.closeRegion(HRegionServer.java:2586)
	at org.apache.hadoop.hbase.regionserver.HRegionServer.closeRegionIgnoreErrors(HRegionServer.java:2508)
	at org.apache.hadoop.hbase.regionserver.HRegionServer.closeUserRegions(HRegionServer.java:2098)
	at org.apache.hadoop.hbase.regionserver.HRegionServer.run(HRegionServer.java:894)
	at java.lang.Thread.run(Thread.java:701)
2014-07-01 15:50:13,524 INFO  [regionserver60020] regionserver.HRegionServer: Received CLOSE for the region: 9cef9e7d34f96f1d9b111be50040a208 ,which we are already trying to CLOSE, but not completed yet
2014-07-01 15:50:13,524 WARN  [regionserver60020] regionserver.HRegionServer: Failed to close usertable,,1404228265525.9cef9e7d34f96f1d9b111be50040a208. - ignoring and continuing
org.apache.hadoop.hbase.regionserver.RegionAlreadyInTransitionException: The region 9cef9e7d34f96f1d9b111be50040a208 was already closing. New CLOSE request is ignored.
	at org.apache.hadoop.hbase.regionserver.HRegionServer.closeRegion(HRegionServer.java:2586)
	at org.apache.hadoop.hbase.regionserver.HRegionServer.closeRegionIgnoreErrors(HRegionServer.java:2508)
	at org.apache.hadoop.hbase.regionserver.HRegionServer.closeUserRegions(HRegionServer.java:2098)
	at org.apache.hadoop.hbase.regionserver.HRegionServer.run(HRegionServer.java:894)
	at java.lang.Thread.run(Thread.java:701)
2014-07-01 15:50:13,525 INFO  [regionserver60020] regionserver.HRegionServer: Received CLOSE for the region: 93253e37c284f3174309fede3f32339f ,which we are already trying to CLOSE, but not completed yet
2014-07-01 15:50:13,525 WARN  [regionserver60020] regionserver.HRegionServer: Failed to close usertable,user3,1404228265526.93253e37c284f3174309fede3f32339f. - ignoring and continuing
org.apache.hadoop.hbase.regionserver.RegionAlreadyInTransitionException: The region 93253e37c284f3174309fede3f32339f was already closing. New CLOSE request is ignored.
	at org.apache.hadoop.hbase.regionserver.HRegionServer.closeRegion(HRegionServer.java:2586)
	at org.apache.hadoop.hbase.regionserver.HRegionServer.closeRegionIgnoreErrors(HRegionServer.java:2508)
	at org.apache.hadoop.hbase.regionserver.HRegionServer.closeUserRegions(HRegionServer.java:2098)
	at org.apache.hadoop.hbase.regionserver.HRegionServer.run(HRegionServer.java:894)
	at java.lang.Thread.run(Thread.java:701)
2014-07-01 15:50:13,525 INFO  [regionserver60020] regionserver.HRegionServer: Received CLOSE for the region: 667c1f2c34fe37d48ba25a789d8aba98 ,which we are already trying to CLOSE, but not completed yet
2014-07-01 15:50:13,525 WARN  [regionserver60020] regionserver.HRegionServer: Failed to close usertable,user7,1404228265526.667c1f2c34fe37d48ba25a789d8aba98. - ignoring and continuing
org.apache.hadoop.hbase.regionserver.RegionAlreadyInTransitionException: The region 667c1f2c34fe37d48ba25a789d8aba98 was already closing. New CLOSE request is ignored.
	at org.apache.hadoop.hbase.regionserver.HRegionServer.closeRegion(HRegionServer.java:2586)
	at org.apache.hadoop.hbase.regionserver.HRegionServer.closeRegionIgnoreErrors(HRegionServer.java:2508)
	at org.apache.hadoop.hbase.regionserver.HRegionServer.closeUserRegions(HRegionServer.java:2098)
	at org.apache.hadoop.hbase.regionserver.HRegionServer.run(HRegionServer.java:894)
	at java.lang.Thread.run(Thread.java:701)
2014-07-01 15:50:13,525 DEBUG [regionserver60020] regionserver.HRegionServer: Waiting on 122e92eabfab38cb15a9965b21db25c5, bb79ae773f71289e0745068f68a0abd9, 9cef9e7d34f96f1d9b111be50040a208, 93253e37c284f3174309fede3f32339f, 667c1f2c34fe37d48ba25a789d8aba98
2014-07-01 15:50:13,697 INFO  [DataStreamer for file /hbase/WALs/slave1,60020,1404253868846/slave1%2C60020%2C1404253868846.1404254547570 block blk_-3161611795271398729_29985] ipc.Client: Retrying connect to server: master/9.1.143.58:50020. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1 SECONDS)
2014-07-01 15:50:14,180 INFO  [org.apache.hadoop.hdfs.LeaseRenewer$1@4fae46d5] ipc.Client: Retrying connect to server: master/9.1.143.58:54310. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1 SECONDS)
2014-07-01 15:50:14,181 WARN  [org.apache.hadoop.hdfs.LeaseRenewer$1@4fae46d5] hdfs.LeaseRenewer: Failed to renew lease for [DFSClient_hb_rs_slave1,60020,1404253868846_-130361673_31] for 30 seconds.  Will retry shortly ...
java.net.ConnectException: Call to master/9.1.143.58:54310 failed on connection exception: java.net.ConnectException: Connection refused
	at org.apache.hadoop.ipc.Client.wrapException(Client.java:1142)
	at org.apache.hadoop.ipc.Client.call(Client.java:1118)
	at org.apache.hadoop.ipc.RPC$Invoker.invoke(RPC.java:229)
	at com.sun.proxy.$Proxy8.renewLease(Unknown Source)
	at sun.reflect.GeneratedMethodAccessor29.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:622)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:85)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:62)
	at com.sun.proxy.$Proxy8.renewLease(Unknown Source)
	at sun.reflect.GeneratedMethodAccessor29.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:622)
	at org.apache.hadoop.hbase.fs.HFileSystem$1.invoke(HFileSystem.java:294)
	at com.sun.proxy.$Proxy9.renewLease(Unknown Source)
	at org.apache.hadoop.hdfs.DFSClient.renewLease(DFSClient.java:379)
	at org.apache.hadoop.hdfs.LeaseRenewer.renew(LeaseRenewer.java:378)
	at org.apache.hadoop.hdfs.LeaseRenewer.run(LeaseRenewer.java:400)
	at org.apache.hadoop.hdfs.LeaseRenewer.access$600(LeaseRenewer.java:69)
	at org.apache.hadoop.hdfs.LeaseRenewer$1.run(LeaseRenewer.java:273)
	at java.lang.Thread.run(Thread.java:701)
Caused by: java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:601)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:511)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:481)
	at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:457)
	at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:583)
	at org.apache.hadoop.ipc.Client$Connection.access$2200(Client.java:205)
	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1249)
	at org.apache.hadoop.ipc.Client.call(Client.java:1093)
	... 19 more
2014-07-01 15:50:14,186 WARN  [regionserver60020-smallCompactions-1404253964856] hdfs.DFSClient: DFS Read: java.net.ConnectException: Call to master/9.1.143.58:54310 failed on connection exception: java.net.ConnectException: Connection refused
	at org.apache.hadoop.ipc.Client.wrapException(Client.java:1142)
	at org.apache.hadoop.ipc.Client.call(Client.java:1118)
	at org.apache.hadoop.ipc.RPC$Invoker.invoke(RPC.java:229)
	at com.sun.proxy.$Proxy8.getBlockLocations(Unknown Source)
	at sun.reflect.GeneratedMethodAccessor10.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:622)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:85)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:62)
	at com.sun.proxy.$Proxy8.getBlockLocations(Unknown Source)
	at org.apache.hadoop.hdfs.DFSClient.callGetBlockLocations(DFSClient.java:695)
	at org.apache.hadoop.hdfs.DFSClient$DFSInputStream.fetchLocatedBlocks(DFSClient.java:2003)
	at org.apache.hadoop.hdfs.DFSClient$DFSInputStream.openInfo(DFSClient.java:1975)
	at org.apache.hadoop.hdfs.DFSClient$DFSInputStream.chooseDataNode(DFSClient.java:2474)
	at org.apache.hadoop.hdfs.DFSClient$DFSInputStream.blockSeekTo(DFSClient.java:2252)
	at org.apache.hadoop.hdfs.DFSClient$DFSInputStream.read(DFSClient.java:2415)
	at java.io.DataInputStream.read(DataInputStream.java:149)
	at org.apache.hadoop.io.IOUtils.readFully(IOUtils.java:175)
	at org.apache.hadoop.hbase.io.hfile.HFileBlock$AbstractFSReader.readAtOffset(HFileBlock.java:1210)
	at org.apache.hadoop.hbase.io.hfile.HFileBlock$FSReaderV2.readBlockDataInternal(HFileBlock.java:1483)
	at org.apache.hadoop.hbase.io.hfile.HFileBlock$FSReaderV2.readBlockData(HFileBlock.java:1314)
	at org.apache.hadoop.hbase.io.hfile.HFileReaderV2.readBlock(HFileReaderV2.java:392)
	at org.apache.hadoop.hbase.io.hfile.HFileReaderV2$ScannerV2.seekTo(HFileReaderV2.java:803)
	at org.apache.hadoop.hbase.regionserver.StoreFileScanner.seekAtOrAfter(StoreFileScanner.java:244)
	at org.apache.hadoop.hbase.regionserver.StoreFileScanner.seek(StoreFileScanner.java:152)
	at org.apache.hadoop.hbase.regionserver.StoreScanner.seekScanners(StoreScanner.java:317)
	at org.apache.hadoop.hbase.regionserver.StoreScanner.<init>(StoreScanner.java:240)
	at org.apache.hadoop.hbase.regionserver.StoreScanner.<init>(StoreScanner.java:202)
	at org.apache.hadoop.hbase.regionserver.compactions.Compactor.createScanner(Compactor.java:257)
	at org.apache.hadoop.hbase.regionserver.compactions.DefaultCompactor.compact(DefaultCompactor.java:65)
	at org.apache.hadoop.hbase.regionserver.DefaultStoreEngine$DefaultCompactionContext.compact(DefaultStoreEngine.java:109)
	at org.apache.hadoop.hbase.regionserver.HStore.compact(HStore.java:1086)
	at org.apache.hadoop.hbase.regionserver.HRegion.compact(HRegion.java:1481)
	at org.apache.hadoop.hbase.regionserver.CompactSplitThread$CompactionRunner.run(CompactSplitThread.java:475)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1146)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:701)
Caused by: java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:601)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:511)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:481)
	at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:457)
	at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:583)
	at org.apache.hadoop.ipc.Client$Connection.access$2200(Client.java:205)
	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1249)
	at org.apache.hadoop.ipc.Client.call(Client.java:1093)
	at org.apache.hadoop.ipc.RPC$Invoker.invoke(RPC.java:229)
	at com.sun.proxy.$Proxy8.renewLease(Unknown Source)
	at sun.reflect.GeneratedMethodAccessor29.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:622)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:85)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:62)
	at com.sun.proxy.$Proxy8.renewLease(Unknown Source)
	at sun.reflect.GeneratedMethodAccessor29.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:622)
	at org.apache.hadoop.hbase.fs.HFileSystem$1.invoke(HFileSystem.java:294)
	at com.sun.proxy.$Proxy9.renewLease(Unknown Source)
	at org.apache.hadoop.hdfs.DFSClient.renewLease(DFSClient.java:379)
	at org.apache.hadoop.hdfs.LeaseRenewer.renew(LeaseRenewer.java:378)
	at org.apache.hadoop.hdfs.LeaseRenewer.run(LeaseRenewer.java:400)
	at org.apache.hadoop.hdfs.LeaseRenewer.access$600(LeaseRenewer.java:69)
	at org.apache.hadoop.hdfs.LeaseRenewer$1.run(LeaseRenewer.java:273)
	... 1 more

2014-07-01 15:50:14,187 ERROR [regionserver60020-smallCompactions-1404253964856] regionserver.CompactSplitThread: Compaction failed Request = regionName=usertable,user7,1404228265526.667c1f2c34fe37d48ba25a789d8aba98., storeName=family, fileCount=6, fileSize=891.0m (388.9m, 105.0m, 111.0m, 125.7m, 94.4m, 66.0m), priority=14, time=75085526519696
java.io.IOException: Could not seek StoreFileScanner[HFileScanner for reader reader=hdfs://master:54310/hbase/data/default/usertable/667c1f2c34fe37d48ba25a789d8aba98/family/56200eedd338491786b04080265aa716, compression=gz, cacheConf=CacheConfig:enabled [cacheDataOnRead=true] [cacheDataOnWrite=false] [cacheIndexesOnWrite=false] [cacheBloomsOnWrite=false] [cacheEvictOnClose=false] [cacheCompressed=false][prefetchOnOpen=false], firstKey=user7000001172980900254/family:field0/1404254132804/Put, lastKey=user7999998402207300106/family:field9/1404254151918/Put, avgKeyLen=46, avgValueLen=100, entries=5457920, length=407788494, cur=null] to key /family:/LATEST_TIMESTAMP/DeleteFamily/vlen=0/mvcc=0
	at org.apache.hadoop.hbase.regionserver.StoreFileScanner.seek(StoreFileScanner.java:164)
	at org.apache.hadoop.hbase.regionserver.StoreScanner.seekScanners(StoreScanner.java:317)
	at org.apache.hadoop.hbase.regionserver.StoreScanner.<init>(StoreScanner.java:240)
	at org.apache.hadoop.hbase.regionserver.StoreScanner.<init>(StoreScanner.java:202)
	at org.apache.hadoop.hbase.regionserver.compactions.Compactor.createScanner(Compactor.java:257)
	at org.apache.hadoop.hbase.regionserver.compactions.DefaultCompactor.compact(DefaultCompactor.java:65)
	at org.apache.hadoop.hbase.regionserver.DefaultStoreEngine$DefaultCompactionContext.compact(DefaultStoreEngine.java:109)
	at org.apache.hadoop.hbase.regionserver.HStore.compact(HStore.java:1086)
	at org.apache.hadoop.hbase.regionserver.HRegion.compact(HRegion.java:1481)
	at org.apache.hadoop.hbase.regionserver.CompactSplitThread$CompactionRunner.run(CompactSplitThread.java:475)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1146)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:701)
Caused by: java.net.ConnectException: Call to master/9.1.143.58:54310 failed on connection exception: java.net.ConnectException: Connection refused
	at org.apache.hadoop.ipc.Client.wrapException(Client.java:1142)
	at org.apache.hadoop.ipc.Client.call(Client.java:1118)
	at org.apache.hadoop.ipc.RPC$Invoker.invoke(RPC.java:229)
	at com.sun.proxy.$Proxy8.getBlockLocations(Unknown Source)
	at sun.reflect.GeneratedMethodAccessor10.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:622)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:85)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:62)
	at com.sun.proxy.$Proxy8.getBlockLocations(Unknown Source)
	at org.apache.hadoop.hdfs.DFSClient.callGetBlockLocations(DFSClient.java:695)
	at org.apache.hadoop.hdfs.DFSClient$DFSInputStream.fetchLocatedBlocks(DFSClient.java:2003)
	at org.apache.hadoop.hdfs.DFSClient$DFSInputStream.openInfo(DFSClient.java:1975)
	at org.apache.hadoop.hdfs.DFSClient$DFSInputStream.chooseDataNode(DFSClient.java:2474)
	at org.apache.hadoop.hdfs.DFSClient$DFSInputStream.blockSeekTo(DFSClient.java:2252)
	at org.apache.hadoop.hdfs.DFSClient$DFSInputStream.read(DFSClient.java:2415)
	at java.io.DataInputStream.read(DataInputStream.java:149)
	at org.apache.hadoop.io.IOUtils.readFully(IOUtils.java:175)
	at org.apache.hadoop.hbase.io.hfile.HFileBlock$AbstractFSReader.readAtOffset(HFileBlock.java:1210)
	at org.apache.hadoop.hbase.io.hfile.HFileBlock$FSReaderV2.readBlockDataInternal(HFileBlock.java:1483)
	at org.apache.hadoop.hbase.io.hfile.HFileBlock$FSReaderV2.readBlockData(HFileBlock.java:1314)
	at org.apache.hadoop.hbase.io.hfile.HFileReaderV2.readBlock(HFileReaderV2.java:392)
	at org.apache.hadoop.hbase.io.hfile.HFileReaderV2$ScannerV2.seekTo(HFileReaderV2.java:803)
	at org.apache.hadoop.hbase.regionserver.StoreFileScanner.seekAtOrAfter(StoreFileScanner.java:244)
	at org.apache.hadoop.hbase.regionserver.StoreFileScanner.seek(StoreFileScanner.java:152)
	... 12 more
Caused by: java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:601)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:511)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:481)
	at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:457)
	at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:583)
	at org.apache.hadoop.ipc.Client$Connection.access$2200(Client.java:205)
	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1249)
	at org.apache.hadoop.ipc.Client.call(Client.java:1093)
	at org.apache.hadoop.ipc.RPC$Invoker.invoke(RPC.java:229)
	at com.sun.proxy.$Proxy8.renewLease(Unknown Source)
	at sun.reflect.GeneratedMethodAccessor29.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:622)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:85)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:62)
	at com.sun.proxy.$Proxy8.renewLease(Unknown Source)
	at sun.reflect.GeneratedMethodAccessor29.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:622)
	at org.apache.hadoop.hbase.fs.HFileSystem$1.invoke(HFileSystem.java:294)
	at com.sun.proxy.$Proxy9.renewLease(Unknown Source)
	at org.apache.hadoop.hdfs.DFSClient.renewLease(DFSClient.java:379)
	at org.apache.hadoop.hdfs.LeaseRenewer.renew(LeaseRenewer.java:378)
	at org.apache.hadoop.hdfs.LeaseRenewer.run(LeaseRenewer.java:400)
	at org.apache.hadoop.hdfs.LeaseRenewer.access$600(LeaseRenewer.java:69)
	at org.apache.hadoop.hdfs.LeaseRenewer$1.run(LeaseRenewer.java:273)
	... 1 more
2014-07-01 15:50:14,188 DEBUG [regionserver60020-smallCompactions-1404253964856] regionserver.CompactSplitThread: CompactSplitThread Status: compaction_queue=(0:27), split_queue=0, merge_queue=0
2014-07-01 15:50:14,189 DEBUG [regionserver60020-smallCompactions-1404253964856] compactions.RatioBasedCompactionPolicy: Selecting compaction from 6 store files, 0 compacting, 6 eligible, 20 blocking
2014-07-01 15:50:14,189 DEBUG [regionserver60020-smallCompactions-1404253964856] compactions.ExploringCompactionPolicy: Exploring compaction algorithm has selected 6 files of size 934259017 starting at candidate #0 after considering 10 permutations with 9 in ratio
2014-07-01 15:50:14,190 DEBUG [regionserver60020-smallCompactions-1404253964856] regionserver.HStore: 667c1f2c34fe37d48ba25a789d8aba98 - family: Initiating major compaction
2014-07-01 15:50:14,190 INFO  [regionserver60020-smallCompactions-1404253964856] regionserver.HRegion: Starting compaction on family in region usertable,user7,1404228265526.667c1f2c34fe37d48ba25a789d8aba98.
2014-07-01 15:50:14,190 INFO  [regionserver60020-smallCompactions-1404253964856] regionserver.HStore: Starting compaction of 6 file(s) in family of usertable,user7,1404228265526.667c1f2c34fe37d48ba25a789d8aba98. into tmpdir=hdfs://master:54310/hbase/data/default/usertable/667c1f2c34fe37d48ba25a789d8aba98/.tmp, totalSize=891.0m
2014-07-01 15:50:14,191 DEBUG [regionserver60020-smallCompactions-1404253964856] compactions.Compactor: Compacting hdfs://master:54310/hbase/data/default/usertable/667c1f2c34fe37d48ba25a789d8aba98/family/56200eedd338491786b04080265aa716, keycount=545792, bloomtype=ROW, size=388.9m, encoding=NONE, seqNum=2915, earliestPutTs=1404228281826
2014-07-01 15:50:14,191 DEBUG [regionserver60020-smallCompactions-1404253964856] compactions.Compactor: Compacting hdfs://master:54310/hbase/data/default/usertable/667c1f2c34fe37d48ba25a789d8aba98/family/50f4de22ccd04828b18b93cc72b88964, keycount=147565, bloomtype=ROW, size=105.0m, encoding=NONE, seqNum=3442, earliestPutTs=1404254293026
2014-07-01 15:50:14,191 DEBUG [regionserver60020-smallCompactions-1404253964856] compactions.Compactor: Compacting hdfs://master:54310/hbase/data/default/usertable/667c1f2c34fe37d48ba25a789d8aba98/family/244e3443161840b5a95504a408c4edc2, keycount=155927, bloomtype=ROW, size=111.0m, encoding=NONE, seqNum=3881, earliestPutTs=1404254357203
2014-07-01 15:50:14,191 DEBUG [regionserver60020-smallCompactions-1404253964856] compactions.Compactor: Compacting hdfs://master:54310/hbase/data/default/usertable/667c1f2c34fe37d48ba25a789d8aba98/family/a5468df3646d43fda8dcfd231b11facb, keycount=176456, bloomtype=ROW, size=125.7m, encoding=NONE, seqNum=4315, earliestPutTs=1404254389941
2014-07-01 15:50:14,191 DEBUG [regionserver60020-smallCompactions-1404253964856] compactions.Compactor: Compacting hdfs://master:54310/hbase/data/default/usertable/667c1f2c34fe37d48ba25a789d8aba98/family/5c4ecd1e5f5f40a68639ae8651cdf308, keycount=132606, bloomtype=ROW, size=94.4m, encoding=NONE, seqNum=4527, earliestPutTs=1404254439322
2014-07-01 15:50:14,192 DEBUG [regionserver60020-smallCompactions-1404253964856] compactions.Compactor: Compacting hdfs://master:54310/hbase/data/default/usertable/667c1f2c34fe37d48ba25a789d8aba98/family/6c4e4b0a6c594b6db087ac5f958bca9c, keycount=92565, bloomtype=ROW, size=66.0m, encoding=NONE, seqNum=4678, earliestPutTs=1404254473587
2014-07-01 15:50:14,193 WARN  [regionserver60020-smallCompactions-1404253964856] hdfs.DFSClient: Failed to connect to /9.1.143.58:50010, add to deadNodes and continuejava.net.ConnectException: Connection refused
2014-07-01 15:50:14,193 INFO  [regionserver60020-smallCompactions-1404253964856] hdfs.DFSClient: Could not obtain blk_221234652107626381_29849 from any node: java.io.IOException: No live nodes contain current block. Will get new block locations from namenode and retry...
2014-07-01 15:50:14,698 INFO  [DataStreamer for file /hbase/WALs/slave1,60020,1404253868846/slave1%2C60020%2C1404253868846.1404254547570 block blk_-3161611795271398729_29985] ipc.Client: Retrying connect to server: master/9.1.143.58:50020. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1 SECONDS)
2014-07-01 15:50:15,183 INFO  [RS_CLOSE_REGION-slave1:60020-2] ipc.Client: Retrying connect to server: master/9.1.143.58:54310. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1 SECONDS)
2014-07-01 15:50:15,702 INFO  [DataStreamer for file /hbase/WALs/slave1,60020,1404253868846/slave1%2C60020%2C1404253868846.1404254547570 block blk_-3161611795271398729_29985] ipc.Client: Retrying connect to server: master/9.1.143.58:50020. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1 SECONDS)
2014-07-01 15:50:16,184 INFO  [RS_CLOSE_REGION-slave1:60020-2] ipc.Client: Retrying connect to server: master/9.1.143.58:54310. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1 SECONDS)
2014-07-01 15:50:16,529 INFO  [regionserver60020] regionserver.HRegionServer: Received CLOSE for the region: 122e92eabfab38cb15a9965b21db25c5 ,which we are already trying to CLOSE, but not completed yet
2014-07-01 15:50:16,529 WARN  [regionserver60020] regionserver.HRegionServer: Failed to close usertable,user5,1404228265526.122e92eabfab38cb15a9965b21db25c5. - ignoring and continuing
org.apache.hadoop.hbase.regionserver.RegionAlreadyInTransitionException: The region 122e92eabfab38cb15a9965b21db25c5 was already closing. New CLOSE request is ignored.
	at org.apache.hadoop.hbase.regionserver.HRegionServer.closeRegion(HRegionServer.java:2586)
	at org.apache.hadoop.hbase.regionserver.HRegionServer.closeRegionIgnoreErrors(HRegionServer.java:2508)
	at org.apache.hadoop.hbase.regionserver.HRegionServer.closeUserRegions(HRegionServer.java:2098)
	at org.apache.hadoop.hbase.regionserver.HRegionServer.run(HRegionServer.java:894)
	at java.lang.Thread.run(Thread.java:701)
2014-07-01 15:50:16,529 INFO  [regionserver60020] regionserver.HRegionServer: Received CLOSE for the region: bb79ae773f71289e0745068f68a0abd9 ,which we are already trying to CLOSE, but not completed yet
2014-07-01 15:50:16,529 WARN  [regionserver60020] regionserver.HRegionServer: Failed to close usertable,user9,1404228265526.bb79ae773f71289e0745068f68a0abd9. - ignoring and continuing
org.apache.hadoop.hbase.regionserver.RegionAlreadyInTransitionException: The region bb79ae773f71289e0745068f68a0abd9 was already closing. New CLOSE request is ignored.
	at org.apache.hadoop.hbase.regionserver.HRegionServer.closeRegion(HRegionServer.java:2586)
	at org.apache.hadoop.hbase.regionserver.HRegionServer.closeRegionIgnoreErrors(HRegionServer.java:2508)
	at org.apache.hadoop.hbase.regionserver.HRegionServer.closeUserRegions(HRegionServer.java:2098)
	at org.apache.hadoop.hbase.regionserver.HRegionServer.run(HRegionServer.java:894)
	at java.lang.Thread.run(Thread.java:701)
2014-07-01 15:50:16,530 INFO  [regionserver60020] regionserver.HRegionServer: Received CLOSE for the region: 9cef9e7d34f96f1d9b111be50040a208 ,which we are already trying to CLOSE, but not completed yet
2014-07-01 15:50:16,530 WARN  [regionserver60020] regionserver.HRegionServer: Failed to close usertable,,1404228265525.9cef9e7d34f96f1d9b111be50040a208. - ignoring and continuing
org.apache.hadoop.hbase.regionserver.RegionAlreadyInTransitionException: The region 9cef9e7d34f96f1d9b111be50040a208 was already closing. New CLOSE request is ignored.
	at org.apache.hadoop.hbase.regionserver.HRegionServer.closeRegion(HRegionServer.java:2586)
	at org.apache.hadoop.hbase.regionserver.HRegionServer.closeRegionIgnoreErrors(HRegionServer.java:2508)
	at org.apache.hadoop.hbase.regionserver.HRegionServer.closeUserRegions(HRegionServer.java:2098)
	at org.apache.hadoop.hbase.regionserver.HRegionServer.run(HRegionServer.java:894)
	at java.lang.Thread.run(Thread.java:701)
2014-07-01 15:50:16,530 INFO  [regionserver60020] regionserver.HRegionServer: Received CLOSE for the region: 93253e37c284f3174309fede3f32339f ,which we are already trying to CLOSE, but not completed yet
2014-07-01 15:50:16,530 WARN  [regionserver60020] regionserver.HRegionServer: Failed to close usertable,user3,1404228265526.93253e37c284f3174309fede3f32339f. - ignoring and continuing
org.apache.hadoop.hbase.regionserver.RegionAlreadyInTransitionException: The region 93253e37c284f3174309fede3f32339f was already closing. New CLOSE request is ignored.
	at org.apache.hadoop.hbase.regionserver.HRegionServer.closeRegion(HRegionServer.java:2586)
	at org.apache.hadoop.hbase.regionserver.HRegionServer.closeRegionIgnoreErrors(HRegionServer.java:2508)
	at org.apache.hadoop.hbase.regionserver.HRegionServer.closeUserRegions(HRegionServer.java:2098)
	at org.apache.hadoop.hbase.regionserver.HRegionServer.run(HRegionServer.java:894)
	at java.lang.Thread.run(Thread.java:701)
2014-07-01 15:50:16,530 INFO  [regionserver60020] regionserver.HRegionServer: Received CLOSE for the region: 667c1f2c34fe37d48ba25a789d8aba98 ,which we are already trying to CLOSE, but not completed yet
2014-07-01 15:50:16,530 WARN  [regionserver60020] regionserver.HRegionServer: Failed to close usertable,user7,1404228265526.667c1f2c34fe37d48ba25a789d8aba98. - ignoring and continuing
org.apache.hadoop.hbase.regionserver.RegionAlreadyInTransitionException: The region 667c1f2c34fe37d48ba25a789d8aba98 was already closing. New CLOSE request is ignored.
	at org.apache.hadoop.hbase.regionserver.HRegionServer.closeRegion(HRegionServer.java:2586)
	at org.apache.hadoop.hbase.regionserver.HRegionServer.closeRegionIgnoreErrors(HRegionServer.java:2508)
	at org.apache.hadoop.hbase.regionserver.HRegionServer.closeUserRegions(HRegionServer.java:2098)
	at org.apache.hadoop.hbase.regionserver.HRegionServer.run(HRegionServer.java:894)
	at java.lang.Thread.run(Thread.java:701)
2014-07-01 15:50:16,531 DEBUG [regionserver60020] regionserver.HRegionServer: Waiting on 122e92eabfab38cb15a9965b21db25c5, bb79ae773f71289e0745068f68a0abd9, 9cef9e7d34f96f1d9b111be50040a208, 93253e37c284f3174309fede3f32339f, 667c1f2c34fe37d48ba25a789d8aba98
2014-07-01 15:50:16,703 INFO  [DataStreamer for file /hbase/WALs/slave1,60020,1404253868846/slave1%2C60020%2C1404253868846.1404254547570 block blk_-3161611795271398729_29985] ipc.Client: Retrying connect to server: master/9.1.143.58:50020. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1 SECONDS)
2014-07-01 15:50:16,704 WARN  [DataStreamer for file /hbase/WALs/slave1,60020,1404253868846/slave1%2C60020%2C1404253868846.1404254547570 block blk_-3161611795271398729_29985] hdfs.DFSClient: Failed recovery attempt #5 from primary datanode 9.1.143.58:50010
java.net.ConnectException: Call to master/9.1.143.58:50020 failed on connection exception: java.net.ConnectException: Connection refused
	at org.apache.hadoop.ipc.Client.wrapException(Client.java:1142)
	at org.apache.hadoop.ipc.Client.call(Client.java:1118)
	at org.apache.hadoop.ipc.RPC$Invoker.invoke(RPC.java:229)
	at com.sun.proxy.$Proxy13.getProtocolVersion(Unknown Source)
	at org.apache.hadoop.ipc.RPC.checkVersion(RPC.java:422)
	at org.apache.hadoop.ipc.RPC.getProxy(RPC.java:414)
	at org.apache.hadoop.ipc.RPC.getProxy(RPC.java:392)
	at org.apache.hadoop.hdfs.DFSClient.createClientDatanodeProtocolProxy(DFSClient.java:201)
	at org.apache.hadoop.hdfs.DFSClient$DFSOutputStream.processDatanodeError(DFSClient.java:3317)
	at org.apache.hadoop.hdfs.DFSClient$DFSOutputStream.access$2200(DFSClient.java:2783)
	at org.apache.hadoop.hdfs.DFSClient$DFSOutputStream$DataStreamer.run(DFSClient.java:2987)
Caused by: java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:601)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:511)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:481)
	at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:457)
	at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:583)
	at org.apache.hadoop.ipc.Client$Connection.access$2200(Client.java:205)
	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1249)
	at org.apache.hadoop.ipc.Client.call(Client.java:1093)
	... 9 more
2014-07-01 15:50:16,704 WARN  [DataStreamer for file /hbase/WALs/slave1,60020,1404253868846/slave1%2C60020%2C1404253868846.1404254547570 block blk_-3161611795271398729_29985] hdfs.DFSClient: Error Recovery for block blk_-3161611795271398729_29985 failed  because recovery from primary datanode 9.1.143.58:50010 failed 6 times.  Pipeline was 9.1.143.59:50010, 9.1.143.58:50010. Marking primary datanode as bad.
2014-07-01 15:50:17,185 INFO  [RS_CLOSE_REGION-slave1:60020-2] ipc.Client: Retrying connect to server: master/9.1.143.58:54310. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1 SECONDS)
2014-07-01 15:50:18,186 INFO  [RS_CLOSE_REGION-slave1:60020-2] ipc.Client: Retrying connect to server: master/9.1.143.58:54310. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1 SECONDS)
2014-07-01 15:50:18,708 INFO  [DataStreamer for file /hbase/WALs/slave1,60020,1404253868846/slave1%2C60020%2C1404253868846.1404254547570 block blk_-3161611795271398729_29985] ipc.Client: Retrying connect to server: sceplus-vm49.almaden.ibm.com/9.1.143.59:50020. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1 SECONDS)
2014-07-01 15:50:19,188 INFO  [RS_CLOSE_REGION-slave1:60020-2] ipc.Client: Retrying connect to server: master/9.1.143.58:54310. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1 SECONDS)
2014-07-01 15:50:19,534 INFO  [regionserver60020] regionserver.HRegionServer: Received CLOSE for the region: 122e92eabfab38cb15a9965b21db25c5 ,which we are already trying to CLOSE, but not completed yet
2014-07-01 15:50:19,535 WARN  [regionserver60020] regionserver.HRegionServer: Failed to close usertable,user5,1404228265526.122e92eabfab38cb15a9965b21db25c5. - ignoring and continuing
org.apache.hadoop.hbase.regionserver.RegionAlreadyInTransitionException: The region 122e92eabfab38cb15a9965b21db25c5 was already closing. New CLOSE request is ignored.
	at org.apache.hadoop.hbase.regionserver.HRegionServer.closeRegion(HRegionServer.java:2586)
	at org.apache.hadoop.hbase.regionserver.HRegionServer.closeRegionIgnoreErrors(HRegionServer.java:2508)
	at org.apache.hadoop.hbase.regionserver.HRegionServer.closeUserRegions(HRegionServer.java:2098)
	at org.apache.hadoop.hbase.regionserver.HRegionServer.run(HRegionServer.java:894)
	at java.lang.Thread.run(Thread.java:701)
2014-07-01 15:50:19,535 INFO  [regionserver60020] regionserver.HRegionServer: Received CLOSE for the region: bb79ae773f71289e0745068f68a0abd9 ,which we are already trying to CLOSE, but not completed yet
2014-07-01 15:50:19,535 WARN  [regionserver60020] regionserver.HRegionServer: Failed to close usertable,user9,1404228265526.bb79ae773f71289e0745068f68a0abd9. - ignoring and continuing
org.apache.hadoop.hbase.regionserver.RegionAlreadyInTransitionException: The region bb79ae773f71289e0745068f68a0abd9 was already closing. New CLOSE request is ignored.
	at org.apache.hadoop.hbase.regionserver.HRegionServer.closeRegion(HRegionServer.java:2586)
	at org.apache.hadoop.hbase.regionserver.HRegionServer.closeRegionIgnoreErrors(HRegionServer.java:2508)
	at org.apache.hadoop.hbase.regionserver.HRegionServer.closeUserRegions(HRegionServer.java:2098)
	at org.apache.hadoop.hbase.regionserver.HRegionServer.run(HRegionServer.java:894)
	at java.lang.Thread.run(Thread.java:701)
2014-07-01 15:50:19,536 INFO  [regionserver60020] regionserver.HRegionServer: Received CLOSE for the region: 9cef9e7d34f96f1d9b111be50040a208 ,which we are already trying to CLOSE, but not completed yet
2014-07-01 15:50:19,536 WARN  [regionserver60020] regionserver.HRegionServer: Failed to close usertable,,1404228265525.9cef9e7d34f96f1d9b111be50040a208. - ignoring and continuing
org.apache.hadoop.hbase.regionserver.RegionAlreadyInTransitionException: The region 9cef9e7d34f96f1d9b111be50040a208 was already closing. New CLOSE request is ignored.
	at org.apache.hadoop.hbase.regionserver.HRegionServer.closeRegion(HRegionServer.java:2586)
	at org.apache.hadoop.hbase.regionserver.HRegionServer.closeRegionIgnoreErrors(HRegionServer.java:2508)
	at org.apache.hadoop.hbase.regionserver.HRegionServer.closeUserRegions(HRegionServer.java:2098)
	at org.apache.hadoop.hbase.regionserver.HRegionServer.run(HRegionServer.java:894)
	at java.lang.Thread.run(Thread.java:701)
2014-07-01 15:50:19,536 INFO  [regionserver60020] regionserver.HRegionServer: Received CLOSE for the region: 93253e37c284f3174309fede3f32339f ,which we are already trying to CLOSE, but not completed yet
2014-07-01 15:50:19,536 WARN  [regionserver60020] regionserver.HRegionServer: Failed to close usertable,user3,1404228265526.93253e37c284f3174309fede3f32339f. - ignoring and continuing
org.apache.hadoop.hbase.regionserver.RegionAlreadyInTransitionException: The region 93253e37c284f3174309fede3f32339f was already closing. New CLOSE request is ignored.
	at org.apache.hadoop.hbase.regionserver.HRegionServer.closeRegion(HRegionServer.java:2586)
	at org.apache.hadoop.hbase.regionserver.HRegionServer.closeRegionIgnoreErrors(HRegionServer.java:2508)
	at org.apache.hadoop.hbase.regionserver.HRegionServer.closeUserRegions(HRegionServer.java:2098)
	at org.apache.hadoop.hbase.regionserver.HRegionServer.run(HRegionServer.java:894)
	at java.lang.Thread.run(Thread.java:701)
2014-07-01 15:50:19,536 INFO  [regionserver60020] regionserver.HRegionServer: Received CLOSE for the region: 667c1f2c34fe37d48ba25a789d8aba98 ,which we are already trying to CLOSE, but not completed yet
2014-07-01 15:50:19,537 WARN  [regionserver60020] regionserver.HRegionServer: Failed to close usertable,user7,1404228265526.667c1f2c34fe37d48ba25a789d8aba98. - ignoring and continuing
org.apache.hadoop.hbase.regionserver.RegionAlreadyInTransitionException: The region 667c1f2c34fe37d48ba25a789d8aba98 was already closing. New CLOSE request is ignored.
	at org.apache.hadoop.hbase.regionserver.HRegionServer.closeRegion(HRegionServer.java:2586)
	at org.apache.hadoop.hbase.regionserver.HRegionServer.closeRegionIgnoreErrors(HRegionServer.java:2508)
	at org.apache.hadoop.hbase.regionserver.HRegionServer.closeUserRegions(HRegionServer.java:2098)
	at org.apache.hadoop.hbase.regionserver.HRegionServer.run(HRegionServer.java:894)
	at java.lang.Thread.run(Thread.java:701)
2014-07-01 15:50:19,537 DEBUG [regionserver60020] regionserver.HRegionServer: Waiting on 122e92eabfab38cb15a9965b21db25c5, bb79ae773f71289e0745068f68a0abd9, 9cef9e7d34f96f1d9b111be50040a208, 93253e37c284f3174309fede3f32339f, 667c1f2c34fe37d48ba25a789d8aba98
2014-07-01 15:50:19,709 INFO  [DataStreamer for file /hbase/WALs/slave1,60020,1404253868846/slave1%2C60020%2C1404253868846.1404254547570 block blk_-3161611795271398729_29985] ipc.Client: Retrying connect to server: sceplus-vm49.almaden.ibm.com/9.1.143.59:50020. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1 SECONDS)
2014-07-01 15:50:20,189 INFO  [RS_CLOSE_REGION-slave1:60020-2] ipc.Client: Retrying connect to server: master/9.1.143.58:54310. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1 SECONDS)
2014-07-01 15:50:20,710 INFO  [DataStreamer for file /hbase/WALs/slave1,60020,1404253868846/slave1%2C60020%2C1404253868846.1404254547570 block blk_-3161611795271398729_29985] ipc.Client: Retrying connect to server: sceplus-vm49.almaden.ibm.com/9.1.143.59:50020. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1 SECONDS)
2014-07-01 15:50:21,190 INFO  [RS_CLOSE_REGION-slave1:60020-2] ipc.Client: Retrying connect to server: master/9.1.143.58:54310. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1 SECONDS)
2014-07-01 15:50:21,711 INFO  [DataStreamer for file /hbase/WALs/slave1,60020,1404253868846/slave1%2C60020%2C1404253868846.1404254547570 block blk_-3161611795271398729_29985] ipc.Client: Retrying connect to server: sceplus-vm49.almaden.ibm.com/9.1.143.59:50020. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1 SECONDS)
2014-07-01 15:50:22,191 INFO  [RS_CLOSE_REGION-slave1:60020-2] ipc.Client: Retrying connect to server: master/9.1.143.58:54310. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1 SECONDS)
2014-07-01 15:50:22,540 INFO  [regionserver60020] regionserver.HRegionServer: Received CLOSE for the region: 122e92eabfab38cb15a9965b21db25c5 ,which we are already trying to CLOSE, but not completed yet
2014-07-01 15:50:22,540 WARN  [regionserver60020] regionserver.HRegionServer: Failed to close usertable,user5,1404228265526.122e92eabfab38cb15a9965b21db25c5. - ignoring and continuing
org.apache.hadoop.hbase.regionserver.RegionAlreadyInTransitionException: The region 122e92eabfab38cb15a9965b21db25c5 was already closing. New CLOSE request is ignored.
	at org.apache.hadoop.hbase.regionserver.HRegionServer.closeRegion(HRegionServer.java:2586)
	at org.apache.hadoop.hbase.regionserver.HRegionServer.closeRegionIgnoreErrors(HRegionServer.java:2508)
	at org.apache.hadoop.hbase.regionserver.HRegionServer.closeUserRegions(HRegionServer.java:2098)
	at org.apache.hadoop.hbase.regionserver.HRegionServer.run(HRegionServer.java:894)
	at java.lang.Thread.run(Thread.java:701)
2014-07-01 15:50:22,540 INFO  [regionserver60020] regionserver.HRegionServer: Received CLOSE for the region: bb79ae773f71289e0745068f68a0abd9 ,which we are already trying to CLOSE, but not completed yet
2014-07-01 15:50:22,540 WARN  [regionserver60020] regionserver.HRegionServer: Failed to close usertable,user9,1404228265526.bb79ae773f71289e0745068f68a0abd9. - ignoring and continuing
org.apache.hadoop.hbase.regionserver.RegionAlreadyInTransitionException: The region bb79ae773f71289e0745068f68a0abd9 was already closing. New CLOSE request is ignored.
	at org.apache.hadoop.hbase.regionserver.HRegionServer.closeRegion(HRegionServer.java:2586)
	at org.apache.hadoop.hbase.regionserver.HRegionServer.closeRegionIgnoreErrors(HRegionServer.java:2508)
	at org.apache.hadoop.hbase.regionserver.HRegionServer.closeUserRegions(HRegionServer.java:2098)
	at org.apache.hadoop.hbase.regionserver.HRegionServer.run(HRegionServer.java:894)
	at java.lang.Thread.run(Thread.java:701)
2014-07-01 15:50:22,541 INFO  [regionserver60020] regionserver.HRegionServer: Received CLOSE for the region: 9cef9e7d34f96f1d9b111be50040a208 ,which we are already trying to CLOSE, but not completed yet
2014-07-01 15:50:22,541 WARN  [regionserver60020] regionserver.HRegionServer: Failed to close usertable,,1404228265525.9cef9e7d34f96f1d9b111be50040a208. - ignoring and continuing
org.apache.hadoop.hbase.regionserver.RegionAlreadyInTransitionException: The region 9cef9e7d34f96f1d9b111be50040a208 was already closing. New CLOSE request is ignored.
	at org.apache.hadoop.hbase.regionserver.HRegionServer.closeRegion(HRegionServer.java:2586)
	at org.apache.hadoop.hbase.regionserver.HRegionServer.closeRegionIgnoreErrors(HRegionServer.java:2508)
	at org.apache.hadoop.hbase.regionserver.HRegionServer.closeUserRegions(HRegionServer.java:2098)
	at org.apache.hadoop.hbase.regionserver.HRegionServer.run(HRegionServer.java:894)
	at java.lang.Thread.run(Thread.java:701)
2014-07-01 15:50:22,541 INFO  [regionserver60020] regionserver.HRegionServer: Received CLOSE for the region: 93253e37c284f3174309fede3f32339f ,which we are already trying to CLOSE, but not completed yet
2014-07-01 15:50:22,541 WARN  [regionserver60020] regionserver.HRegionServer: Failed to close usertable,user3,1404228265526.93253e37c284f3174309fede3f32339f. - ignoring and continuing
org.apache.hadoop.hbase.regionserver.RegionAlreadyInTransitionException: The region 93253e37c284f3174309fede3f32339f was already closing. New CLOSE request is ignored.
	at org.apache.hadoop.hbase.regionserver.HRegionServer.closeRegion(HRegionServer.java:2586)
	at org.apache.hadoop.hbase.regionserver.HRegionServer.closeRegionIgnoreErrors(HRegionServer.java:2508)
	at org.apache.hadoop.hbase.regionserver.HRegionServer.closeUserRegions(HRegionServer.java:2098)
	at org.apache.hadoop.hbase.regionserver.HRegionServer.run(HRegionServer.java:894)
	at java.lang.Thread.run(Thread.java:701)
2014-07-01 15:50:22,541 INFO  [regionserver60020] regionserver.HRegionServer: Received CLOSE for the region: 667c1f2c34fe37d48ba25a789d8aba98 ,which we are already trying to CLOSE, but not completed yet
2014-07-01 15:50:22,542 WARN  [regionserver60020] regionserver.HRegionServer: Failed to close usertable,user7,1404228265526.667c1f2c34fe37d48ba25a789d8aba98. - ignoring and continuing
org.apache.hadoop.hbase.regionserver.RegionAlreadyInTransitionException: The region 667c1f2c34fe37d48ba25a789d8aba98 was already closing. New CLOSE request is ignored.
	at org.apache.hadoop.hbase.regionserver.HRegionServer.closeRegion(HRegionServer.java:2586)
	at org.apache.hadoop.hbase.regionserver.HRegionServer.closeRegionIgnoreErrors(HRegionServer.java:2508)
	at org.apache.hadoop.hbase.regionserver.HRegionServer.closeUserRegions(HRegionServer.java:2098)
	at org.apache.hadoop.hbase.regionserver.HRegionServer.run(HRegionServer.java:894)
	at java.lang.Thread.run(Thread.java:701)
2014-07-01 15:50:22,542 DEBUG [regionserver60020] regionserver.HRegionServer: Waiting on 122e92eabfab38cb15a9965b21db25c5, bb79ae773f71289e0745068f68a0abd9, 9cef9e7d34f96f1d9b111be50040a208, 93253e37c284f3174309fede3f32339f, 667c1f2c34fe37d48ba25a789d8aba98
2014-07-01 15:50:22,711 INFO  [DataStreamer for file /hbase/WALs/slave1,60020,1404253868846/slave1%2C60020%2C1404253868846.1404254547570 block blk_-3161611795271398729_29985] ipc.Client: Retrying connect to server: sceplus-vm49.almaden.ibm.com/9.1.143.59:50020. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1 SECONDS)
2014-07-01 15:50:23,192 INFO  [RS_CLOSE_REGION-slave1:60020-2] ipc.Client: Retrying connect to server: master/9.1.143.58:54310. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1 SECONDS)
2014-07-01 15:50:23,712 INFO  [DataStreamer for file /hbase/WALs/slave1,60020,1404253868846/slave1%2C60020%2C1404253868846.1404254547570 block blk_-3161611795271398729_29985] ipc.Client: Retrying connect to server: sceplus-vm49.almaden.ibm.com/9.1.143.59:50020. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1 SECONDS)
2014-07-01 15:50:24,193 INFO  [RS_CLOSE_REGION-slave1:60020-2] ipc.Client: Retrying connect to server: master/9.1.143.58:54310. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1 SECONDS)
2014-07-01 15:50:24,194 WARN  [RS_CLOSE_REGION-slave1:60020-2] regionserver.HStore: Failed flushing store file, retrying num=0
java.net.ConnectException: Call to master/9.1.143.58:54310 failed on connection exception: java.net.ConnectException: Connection refused
	at org.apache.hadoop.ipc.Client.wrapException(Client.java:1142)
	at org.apache.hadoop.ipc.Client.call(Client.java:1118)
	at org.apache.hadoop.ipc.RPC$Invoker.invoke(RPC.java:229)
	at com.sun.proxy.$Proxy8.getFileInfo(Unknown Source)
	at sun.reflect.GeneratedMethodAccessor2.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:622)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:85)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:62)
	at com.sun.proxy.$Proxy8.getFileInfo(Unknown Source)
	at sun.reflect.GeneratedMethodAccessor2.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:622)
	at org.apache.hadoop.hbase.fs.HFileSystem$1.invoke(HFileSystem.java:294)
	at com.sun.proxy.$Proxy9.getFileInfo(Unknown Source)
	at org.apache.hadoop.hdfs.DFSClient.getFileInfo(DFSClient.java:1059)
	at org.apache.hadoop.hdfs.DistributedFileSystem.getFileStatus(DistributedFileSystem.java:554)
	at org.apache.hadoop.fs.FilterFileSystem.getFileStatus(FilterFileSystem.java:255)
	at org.apache.hadoop.fs.FileSystem.exists(FileSystem.java:788)
	at org.apache.hadoop.hbase.regionserver.StoreFile$WriterBuilder.build(StoreFile.java:603)
	at org.apache.hadoop.hbase.regionserver.HStore.createWriterInTmp(HStore.java:889)
	at org.apache.hadoop.hbase.regionserver.DefaultStoreFlusher.flushSnapshot(DefaultStoreFlusher.java:69)
	at org.apache.hadoop.hbase.regionserver.HStore.flushCache(HStore.java:798)
	at org.apache.hadoop.hbase.regionserver.HStore$StoreFlusherImpl.flushCache(HStore.java:1949)
	at org.apache.hadoop.hbase.regionserver.HRegion.internalFlushcache(HRegion.java:1776)
	at org.apache.hadoop.hbase.regionserver.HRegion.internalFlushcache(HRegion.java:1658)
	at org.apache.hadoop.hbase.regionserver.HRegion.doClose(HRegion.java:1119)
	at org.apache.hadoop.hbase.regionserver.HRegion.close(HRegion.java:1085)
	at org.apache.hadoop.hbase.regionserver.handler.CloseRegionHandler.process(CloseRegionHandler.java:147)
	at org.apache.hadoop.hbase.executor.EventHandler.run(EventHandler.java:128)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1146)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:701)
Caused by: java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:601)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:511)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:481)
	at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:457)
	at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:583)
	at org.apache.hadoop.ipc.Client$Connection.access$2200(Client.java:205)
	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1249)
	at org.apache.hadoop.ipc.Client.call(Client.java:1093)
	... 31 more
2014-07-01 15:50:24,713 INFO  [DataStreamer for file /hbase/WALs/slave1,60020,1404253868846/slave1%2C60020%2C1404253868846.1404254547570 block blk_-3161611795271398729_29985] ipc.Client: Retrying connect to server: sceplus-vm49.almaden.ibm.com/9.1.143.59:50020. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1 SECONDS)
2014-07-01 15:50:25,198 INFO  [regionserver60020-smallCompactions-1404253964856] ipc.Client: Retrying connect to server: master/9.1.143.58:54310. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1 SECONDS)
2014-07-01 15:50:25,545 INFO  [regionserver60020] regionserver.HRegionServer: Received CLOSE for the region: 122e92eabfab38cb15a9965b21db25c5 ,which we are already trying to CLOSE, but not completed yet
2014-07-01 15:50:25,545 WARN  [regionserver60020] regionserver.HRegionServer: Failed to close usertable,user5,1404228265526.122e92eabfab38cb15a9965b21db25c5. - ignoring and continuing
org.apache.hadoop.hbase.regionserver.RegionAlreadyInTransitionException: The region 122e92eabfab38cb15a9965b21db25c5 was already closing. New CLOSE request is ignored.
	at org.apache.hadoop.hbase.regionserver.HRegionServer.closeRegion(HRegionServer.java:2586)
	at org.apache.hadoop.hbase.regionserver.HRegionServer.closeRegionIgnoreErrors(HRegionServer.java:2508)
	at org.apache.hadoop.hbase.regionserver.HRegionServer.closeUserRegions(HRegionServer.java:2098)
	at org.apache.hadoop.hbase.regionserver.HRegionServer.run(HRegionServer.java:894)
	at java.lang.Thread.run(Thread.java:701)
2014-07-01 15:50:25,546 INFO  [regionserver60020] regionserver.HRegionServer: Received CLOSE for the region: bb79ae773f71289e0745068f68a0abd9 ,which we are already trying to CLOSE, but not completed yet
2014-07-01 15:50:25,546 WARN  [regionserver60020] regionserver.HRegionServer: Failed to close usertable,user9,1404228265526.bb79ae773f71289e0745068f68a0abd9. - ignoring and continuing
org.apache.hadoop.hbase.regionserver.RegionAlreadyInTransitionException: The region bb79ae773f71289e0745068f68a0abd9 was already closing. New CLOSE request is ignored.
	at org.apache.hadoop.hbase.regionserver.HRegionServer.closeRegion(HRegionServer.java:2586)
	at org.apache.hadoop.hbase.regionserver.HRegionServer.closeRegionIgnoreErrors(HRegionServer.java:2508)
	at org.apache.hadoop.hbase.regionserver.HRegionServer.closeUserRegions(HRegionServer.java:2098)
	at org.apache.hadoop.hbase.regionserver.HRegionServer.run(HRegionServer.java:894)
	at java.lang.Thread.run(Thread.java:701)
2014-07-01 15:50:25,546 INFO  [regionserver60020] regionserver.HRegionServer: Received CLOSE for the region: 9cef9e7d34f96f1d9b111be50040a208 ,which we are already trying to CLOSE, but not completed yet
2014-07-01 15:50:25,546 WARN  [regionserver60020] regionserver.HRegionServer: Failed to close usertable,,1404228265525.9cef9e7d34f96f1d9b111be50040a208. - ignoring and continuing
org.apache.hadoop.hbase.regionserver.RegionAlreadyInTransitionException: The region 9cef9e7d34f96f1d9b111be50040a208 was already closing. New CLOSE request is ignored.
	at org.apache.hadoop.hbase.regionserver.HRegionServer.closeRegion(HRegionServer.java:2586)
	at org.apache.hadoop.hbase.regionserver.HRegionServer.closeRegionIgnoreErrors(HRegionServer.java:2508)
	at org.apache.hadoop.hbase.regionserver.HRegionServer.closeUserRegions(HRegionServer.java:2098)
	at org.apache.hadoop.hbase.regionserver.HRegionServer.run(HRegionServer.java:894)
	at java.lang.Thread.run(Thread.java:701)
2014-07-01 15:50:25,546 INFO  [regionserver60020] regionserver.HRegionServer: Received CLOSE for the region: 93253e37c284f3174309fede3f32339f ,which we are already trying to CLOSE, but not completed yet
2014-07-01 15:50:25,547 WARN  [regionserver60020] regionserver.HRegionServer: Failed to close usertable,user3,1404228265526.93253e37c284f3174309fede3f32339f. - ignoring and continuing
org.apache.hadoop.hbase.regionserver.RegionAlreadyInTransitionException: The region 93253e37c284f3174309fede3f32339f was already closing. New CLOSE request is ignored.
	at org.apache.hadoop.hbase.regionserver.HRegionServer.closeRegion(HRegionServer.java:2586)
	at org.apache.hadoop.hbase.regionserver.HRegionServer.closeRegionIgnoreErrors(HRegionServer.java:2508)
	at org.apache.hadoop.hbase.regionserver.HRegionServer.closeUserRegions(HRegionServer.java:2098)
	at org.apache.hadoop.hbase.regionserver.HRegionServer.run(HRegionServer.java:894)
	at java.lang.Thread.run(Thread.java:701)
2014-07-01 15:50:25,547 INFO  [regionserver60020] regionserver.HRegionServer: Received CLOSE for the region: 667c1f2c34fe37d48ba25a789d8aba98 ,which we are already trying to CLOSE, but not completed yet
2014-07-01 15:50:25,547 WARN  [regionserver60020] regionserver.HRegionServer: Failed to close usertable,user7,1404228265526.667c1f2c34fe37d48ba25a789d8aba98. - ignoring and continuing
org.apache.hadoop.hbase.regionserver.RegionAlreadyInTransitionException: The region 667c1f2c34fe37d48ba25a789d8aba98 was already closing. New CLOSE request is ignored.
	at org.apache.hadoop.hbase.regionserver.HRegionServer.closeRegion(HRegionServer.java:2586)
	at org.apache.hadoop.hbase.regionserver.HRegionServer.closeRegionIgnoreErrors(HRegionServer.java:2508)
	at org.apache.hadoop.hbase.regionserver.HRegionServer.closeUserRegions(HRegionServer.java:2098)
	at org.apache.hadoop.hbase.regionserver.HRegionServer.run(HRegionServer.java:894)
	at java.lang.Thread.run(Thread.java:701)
2014-07-01 15:50:25,547 DEBUG [regionserver60020] regionserver.HRegionServer: Waiting on 122e92eabfab38cb15a9965b21db25c5, bb79ae773f71289e0745068f68a0abd9, 9cef9e7d34f96f1d9b111be50040a208, 93253e37c284f3174309fede3f32339f, 667c1f2c34fe37d48ba25a789d8aba98
2014-07-01 15:50:25,713 INFO  [DataStreamer for file /hbase/WALs/slave1,60020,1404253868846/slave1%2C60020%2C1404253868846.1404254547570 block blk_-3161611795271398729_29985] ipc.Client: Retrying connect to server: sceplus-vm49.almaden.ibm.com/9.1.143.59:50020. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1 SECONDS)
2014-07-01 15:50:26,199 INFO  [regionserver60020-smallCompactions-1404253964856] ipc.Client: Retrying connect to server: master/9.1.143.58:54310. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1 SECONDS)
2014-07-01 15:50:26,714 INFO  [DataStreamer for file /hbase/WALs/slave1,60020,1404253868846/slave1%2C60020%2C1404253868846.1404254547570 block blk_-3161611795271398729_29985] ipc.Client: Retrying connect to server: sceplus-vm49.almaden.ibm.com/9.1.143.59:50020. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1 SECONDS)
2014-07-01 15:50:27,200 INFO  [regionserver60020-smallCompactions-1404253964856] ipc.Client: Retrying connect to server: master/9.1.143.58:54310. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1 SECONDS)
2014-07-01 15:50:27,714 INFO  [DataStreamer for file /hbase/WALs/slave1,60020,1404253868846/slave1%2C60020%2C1404253868846.1404254547570 block blk_-3161611795271398729_29985] ipc.Client: Retrying connect to server: sceplus-vm49.almaden.ibm.com/9.1.143.59:50020. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1 SECONDS)
2014-07-01 15:50:27,715 WARN  [DataStreamer for file /hbase/WALs/slave1,60020,1404253868846/slave1%2C60020%2C1404253868846.1404254547570 block blk_-3161611795271398729_29985] hdfs.DFSClient: Failed recovery attempt #0 from primary datanode 9.1.143.59:50010
java.net.ConnectException: Call to sceplus-vm49.almaden.ibm.com/9.1.143.59:50020 failed on connection exception: java.net.ConnectException: Connection refused
	at org.apache.hadoop.ipc.Client.wrapException(Client.java:1142)
	at org.apache.hadoop.ipc.Client.call(Client.java:1118)
	at org.apache.hadoop.ipc.RPC$Invoker.invoke(RPC.java:229)
	at com.sun.proxy.$Proxy13.getProtocolVersion(Unknown Source)
	at org.apache.hadoop.ipc.RPC.checkVersion(RPC.java:422)
	at org.apache.hadoop.ipc.RPC.getProxy(RPC.java:414)
	at org.apache.hadoop.ipc.RPC.getProxy(RPC.java:392)
	at org.apache.hadoop.hdfs.DFSClient.createClientDatanodeProtocolProxy(DFSClient.java:201)
	at org.apache.hadoop.hdfs.DFSClient$DFSOutputStream.processDatanodeError(DFSClient.java:3317)
	at org.apache.hadoop.hdfs.DFSClient$DFSOutputStream.access$2200(DFSClient.java:2783)
	at org.apache.hadoop.hdfs.DFSClient$DFSOutputStream$DataStreamer.run(DFSClient.java:2987)
Caused by: java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:601)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:511)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:481)
	at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:457)
	at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:583)
	at org.apache.hadoop.ipc.Client$Connection.access$2200(Client.java:205)
	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1249)
	at org.apache.hadoop.ipc.Client.call(Client.java:1093)
	... 9 more
2014-07-01 15:50:27,716 WARN  [DataStreamer for file /hbase/WALs/slave1,60020,1404253868846/slave1%2C60020%2C1404253868846.1404254547570 block blk_-3161611795271398729_29985] hdfs.DFSClient: Error Recovery for block blk_-3161611795271398729_29985 failed  because recovery from primary datanode 9.1.143.59:50010 failed 1 times.  Pipeline was 9.1.143.59:50010. Will retry...
2014-07-01 15:50:28,201 INFO  [regionserver60020-smallCompactions-1404253964856] ipc.Client: Retrying connect to server: master/9.1.143.58:54310. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1 SECONDS)
2014-07-01 15:50:28,550 INFO  [regionserver60020] regionserver.HRegionServer: Received CLOSE for the region: 122e92eabfab38cb15a9965b21db25c5 ,which we are already trying to CLOSE, but not completed yet
2014-07-01 15:50:28,550 WARN  [regionserver60020] regionserver.HRegionServer: Failed to close usertable,user5,1404228265526.122e92eabfab38cb15a9965b21db25c5. - ignoring and continuing
org.apache.hadoop.hbase.regionserver.RegionAlreadyInTransitionException: The region 122e92eabfab38cb15a9965b21db25c5 was already closing. New CLOSE request is ignored.
	at org.apache.hadoop.hbase.regionserver.HRegionServer.closeRegion(HRegionServer.java:2586)
	at org.apache.hadoop.hbase.regionserver.HRegionServer.closeRegionIgnoreErrors(HRegionServer.java:2508)
	at org.apache.hadoop.hbase.regionserver.HRegionServer.closeUserRegions(HRegionServer.java:2098)
	at org.apache.hadoop.hbase.regionserver.HRegionServer.run(HRegionServer.java:894)
	at java.lang.Thread.run(Thread.java:701)
2014-07-01 15:50:28,551 INFO  [regionserver60020] regionserver.HRegionServer: Received CLOSE for the region: bb79ae773f71289e0745068f68a0abd9 ,which we are already trying to CLOSE, but not completed yet
2014-07-01 15:50:28,551 WARN  [regionserver60020] regionserver.HRegionServer: Failed to close usertable,user9,1404228265526.bb79ae773f71289e0745068f68a0abd9. - ignoring and continuing
org.apache.hadoop.hbase.regionserver.RegionAlreadyInTransitionException: The region bb79ae773f71289e0745068f68a0abd9 was already closing. New CLOSE request is ignored.
	at org.apache.hadoop.hbase.regionserver.HRegionServer.closeRegion(HRegionServer.java:2586)
	at org.apache.hadoop.hbase.regionserver.HRegionServer.closeRegionIgnoreErrors(HRegionServer.java:2508)
	at org.apache.hadoop.hbase.regionserver.HRegionServer.closeUserRegions(HRegionServer.java:2098)
	at org.apache.hadoop.hbase.regionserver.HRegionServer.run(HRegionServer.java:894)
	at java.lang.Thread.run(Thread.java:701)
2014-07-01 15:50:28,551 INFO  [regionserver60020] regionserver.HRegionServer: Received CLOSE for the region: 9cef9e7d34f96f1d9b111be50040a208 ,which we are already trying to CLOSE, but not completed yet
2014-07-01 15:50:28,551 WARN  [regionserver60020] regionserver.HRegionServer: Failed to close usertable,,1404228265525.9cef9e7d34f96f1d9b111be50040a208. - ignoring and continuing
org.apache.hadoop.hbase.regionserver.RegionAlreadyInTransitionException: The region 9cef9e7d34f96f1d9b111be50040a208 was already closing. New CLOSE request is ignored.
	at org.apache.hadoop.hbase.regionserver.HRegionServer.closeRegion(HRegionServer.java:2586)
	at org.apache.hadoop.hbase.regionserver.HRegionServer.closeRegionIgnoreErrors(HRegionServer.java:2508)
	at org.apache.hadoop.hbase.regionserver.HRegionServer.closeUserRegions(HRegionServer.java:2098)
	at org.apache.hadoop.hbase.regionserver.HRegionServer.run(HRegionServer.java:894)
	at java.lang.Thread.run(Thread.java:701)
2014-07-01 15:50:28,551 INFO  [regionserver60020] regionserver.HRegionServer: Received CLOSE for the region: 93253e37c284f3174309fede3f32339f ,which we are already trying to CLOSE, but not completed yet
2014-07-01 15:50:28,552 WARN  [regionserver60020] regionserver.HRegionServer: Failed to close usertable,user3,1404228265526.93253e37c284f3174309fede3f32339f. - ignoring and continuing
org.apache.hadoop.hbase.regionserver.RegionAlreadyInTransitionException: The region 93253e37c284f3174309fede3f32339f was already closing. New CLOSE request is ignored.
	at org.apache.hadoop.hbase.regionserver.HRegionServer.closeRegion(HRegionServer.java:2586)
	at org.apache.hadoop.hbase.regionserver.HRegionServer.closeRegionIgnoreErrors(HRegionServer.java:2508)
	at org.apache.hadoop.hbase.regionserver.HRegionServer.closeUserRegions(HRegionServer.java:2098)
	at org.apache.hadoop.hbase.regionserver.HRegionServer.run(HRegionServer.java:894)
	at java.lang.Thread.run(Thread.java:701)
2014-07-01 15:50:28,552 INFO  [regionserver60020] regionserver.HRegionServer: Received CLOSE for the region: 667c1f2c34fe37d48ba25a789d8aba98 ,which we are already trying to CLOSE, but not completed yet
2014-07-01 15:50:28,552 WARN  [regionserver60020] regionserver.HRegionServer: Failed to close usertable,user7,1404228265526.667c1f2c34fe37d48ba25a789d8aba98. - ignoring and continuing
org.apache.hadoop.hbase.regionserver.RegionAlreadyInTransitionException: The region 667c1f2c34fe37d48ba25a789d8aba98 was already closing. New CLOSE request is ignored.
	at org.apache.hadoop.hbase.regionserver.HRegionServer.closeRegion(HRegionServer.java:2586)
	at org.apache.hadoop.hbase.regionserver.HRegionServer.closeRegionIgnoreErrors(HRegionServer.java:2508)
	at org.apache.hadoop.hbase.regionserver.HRegionServer.closeUserRegions(HRegionServer.java:2098)
	at org.apache.hadoop.hbase.regionserver.HRegionServer.run(HRegionServer.java:894)
	at java.lang.Thread.run(Thread.java:701)
2014-07-01 15:50:28,552 DEBUG [regionserver60020] regionserver.HRegionServer: Waiting on 122e92eabfab38cb15a9965b21db25c5, bb79ae773f71289e0745068f68a0abd9, 9cef9e7d34f96f1d9b111be50040a208, 93253e37c284f3174309fede3f32339f, 667c1f2c34fe37d48ba25a789d8aba98
2014-07-01 15:50:29,202 INFO  [regionserver60020-smallCompactions-1404253964856] ipc.Client: Retrying connect to server: master/9.1.143.58:54310. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1 SECONDS)
2014-07-01 15:50:29,718 INFO  [DataStreamer for file /hbase/WALs/slave1,60020,1404253868846/slave1%2C60020%2C1404253868846.1404254547570 block blk_-3161611795271398729_29985] ipc.Client: Retrying connect to server: sceplus-vm49.almaden.ibm.com/9.1.143.59:50020. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1 SECONDS)
2014-07-01 15:50:30,203 INFO  [regionserver60020-smallCompactions-1404253964856] ipc.Client: Retrying connect to server: master/9.1.143.58:54310. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1 SECONDS)
2014-07-01 15:50:30,718 INFO  [DataStreamer for file /hbase/WALs/slave1,60020,1404253868846/slave1%2C60020%2C1404253868846.1404254547570 block blk_-3161611795271398729_29985] ipc.Client: Retrying connect to server: sceplus-vm49.almaden.ibm.com/9.1.143.59:50020. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1 SECONDS)
2014-07-01 15:50:31,205 INFO  [regionserver60020-smallCompactions-1404253964856] ipc.Client: Retrying connect to server: master/9.1.143.58:54310. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1 SECONDS)
2014-07-01 15:50:31,555 INFO  [regionserver60020] regionserver.HRegionServer: Received CLOSE for the region: 122e92eabfab38cb15a9965b21db25c5 ,which we are already trying to CLOSE, but not completed yet
2014-07-01 15:50:31,555 WARN  [regionserver60020] regionserver.HRegionServer: Failed to close usertable,user5,1404228265526.122e92eabfab38cb15a9965b21db25c5. - ignoring and continuing
org.apache.hadoop.hbase.regionserver.RegionAlreadyInTransitionException: The region 122e92eabfab38cb15a9965b21db25c5 was already closing. New CLOSE request is ignored.
	at org.apache.hadoop.hbase.regionserver.HRegionServer.closeRegion(HRegionServer.java:2586)
	at org.apache.hadoop.hbase.regionserver.HRegionServer.closeRegionIgnoreErrors(HRegionServer.java:2508)
	at org.apache.hadoop.hbase.regionserver.HRegionServer.closeUserRegions(HRegionServer.java:2098)
	at org.apache.hadoop.hbase.regionserver.HRegionServer.run(HRegionServer.java:894)
	at java.lang.Thread.run(Thread.java:701)
2014-07-01 15:50:31,556 INFO  [regionserver60020] regionserver.HRegionServer: Received CLOSE for the region: bb79ae773f71289e0745068f68a0abd9 ,which we are already trying to CLOSE, but not completed yet
2014-07-01 15:50:31,556 WARN  [regionserver60020] regionserver.HRegionServer: Failed to close usertable,user9,1404228265526.bb79ae773f71289e0745068f68a0abd9. - ignoring and continuing
org.apache.hadoop.hbase.regionserver.RegionAlreadyInTransitionException: The region bb79ae773f71289e0745068f68a0abd9 was already closing. New CLOSE request is ignored.
	at org.apache.hadoop.hbase.regionserver.HRegionServer.closeRegion(HRegionServer.java:2586)
	at org.apache.hadoop.hbase.regionserver.HRegionServer.closeRegionIgnoreErrors(HRegionServer.java:2508)
	at org.apache.hadoop.hbase.regionserver.HRegionServer.closeUserRegions(HRegionServer.java:2098)
	at org.apache.hadoop.hbase.regionserver.HRegionServer.run(HRegionServer.java:894)
	at java.lang.Thread.run(Thread.java:701)
2014-07-01 15:50:31,556 INFO  [regionserver60020] regionserver.HRegionServer: Received CLOSE for the region: 9cef9e7d34f96f1d9b111be50040a208 ,which we are already trying to CLOSE, but not completed yet
2014-07-01 15:50:31,556 WARN  [regionserver60020] regionserver.HRegionServer: Failed to close usertable,,1404228265525.9cef9e7d34f96f1d9b111be50040a208. - ignoring and continuing
org.apache.hadoop.hbase.regionserver.RegionAlreadyInTransitionException: The region 9cef9e7d34f96f1d9b111be50040a208 was already closing. New CLOSE request is ignored.
	at org.apache.hadoop.hbase.regionserver.HRegionServer.closeRegion(HRegionServer.java:2586)
	at org.apache.hadoop.hbase.regionserver.HRegionServer.closeRegionIgnoreErrors(HRegionServer.java:2508)
	at org.apache.hadoop.hbase.regionserver.HRegionServer.closeUserRegions(HRegionServer.java:2098)
	at org.apache.hadoop.hbase.regionserver.HRegionServer.run(HRegionServer.java:894)
	at java.lang.Thread.run(Thread.java:701)
2014-07-01 15:50:31,556 INFO  [regionserver60020] regionserver.HRegionServer: Received CLOSE for the region: 93253e37c284f3174309fede3f32339f ,which we are already trying to CLOSE, but not completed yet
2014-07-01 15:50:31,556 WARN  [regionserver60020] regionserver.HRegionServer: Failed to close usertable,user3,1404228265526.93253e37c284f3174309fede3f32339f. - ignoring and continuing
org.apache.hadoop.hbase.regionserver.RegionAlreadyInTransitionException: The region 93253e37c284f3174309fede3f32339f was already closing. New CLOSE request is ignored.
	at org.apache.hadoop.hbase.regionserver.HRegionServer.closeRegion(HRegionServer.java:2586)
	at org.apache.hadoop.hbase.regionserver.HRegionServer.closeRegionIgnoreErrors(HRegionServer.java:2508)
	at org.apache.hadoop.hbase.regionserver.HRegionServer.closeUserRegions(HRegionServer.java:2098)
	at org.apache.hadoop.hbase.regionserver.HRegionServer.run(HRegionServer.java:894)
	at java.lang.Thread.run(Thread.java:701)
2014-07-01 15:50:31,557 INFO  [regionserver60020] regionserver.HRegionServer: Received CLOSE for the region: 667c1f2c34fe37d48ba25a789d8aba98 ,which we are already trying to CLOSE, but not completed yet
2014-07-01 15:50:31,557 WARN  [regionserver60020] regionserver.HRegionServer: Failed to close usertable,user7,1404228265526.667c1f2c34fe37d48ba25a789d8aba98. - ignoring and continuing
org.apache.hadoop.hbase.regionserver.RegionAlreadyInTransitionException: The region 667c1f2c34fe37d48ba25a789d8aba98 was already closing. New CLOSE request is ignored.
	at org.apache.hadoop.hbase.regionserver.HRegionServer.closeRegion(HRegionServer.java:2586)
	at org.apache.hadoop.hbase.regionserver.HRegionServer.closeRegionIgnoreErrors(HRegionServer.java:2508)
	at org.apache.hadoop.hbase.regionserver.HRegionServer.closeUserRegions(HRegionServer.java:2098)
	at org.apache.hadoop.hbase.regionserver.HRegionServer.run(HRegionServer.java:894)
	at java.lang.Thread.run(Thread.java:701)
2014-07-01 15:50:31,557 DEBUG [regionserver60020] regionserver.HRegionServer: Waiting on 122e92eabfab38cb15a9965b21db25c5, bb79ae773f71289e0745068f68a0abd9, 9cef9e7d34f96f1d9b111be50040a208, 93253e37c284f3174309fede3f32339f, 667c1f2c34fe37d48ba25a789d8aba98
2014-07-01 15:50:31,719 INFO  [DataStreamer for file /hbase/WALs/slave1,60020,1404253868846/slave1%2C60020%2C1404253868846.1404254547570 block blk_-3161611795271398729_29985] ipc.Client: Retrying connect to server: sceplus-vm49.almaden.ibm.com/9.1.143.59:50020. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1 SECONDS)
2014-07-01 15:50:32,206 INFO  [regionserver60020-smallCompactions-1404253964856] ipc.Client: Retrying connect to server: master/9.1.143.58:54310. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1 SECONDS)
2014-07-01 15:50:32,720 INFO  [DataStreamer for file /hbase/WALs/slave1,60020,1404253868846/slave1%2C60020%2C1404253868846.1404254547570 block blk_-3161611795271398729_29985] ipc.Client: Retrying connect to server: sceplus-vm49.almaden.ibm.com/9.1.143.59:50020. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1 SECONDS)
2014-07-01 15:50:33,209 INFO  [regionserver60020-smallCompactions-1404253964856] ipc.Client: Retrying connect to server: master/9.1.143.58:54310. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1 SECONDS)
2014-07-01 15:50:33,720 INFO  [DataStreamer for file /hbase/WALs/slave1,60020,1404253868846/slave1%2C60020%2C1404253868846.1404254547570 block blk_-3161611795271398729_29985] ipc.Client: Retrying connect to server: sceplus-vm49.almaden.ibm.com/9.1.143.59:50020. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1 SECONDS)
2014-07-01 15:50:34,210 INFO  [regionserver60020-smallCompactions-1404253964856] ipc.Client: Retrying connect to server: master/9.1.143.58:54310. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1 SECONDS)
2014-07-01 15:50:34,213 WARN  [regionserver60020-smallCompactions-1404253964856] hdfs.DFSClient: Failed to connect to /9.1.143.58:50010, add to deadNodes and continuejava.net.ConnectException: Connection refused
2014-07-01 15:50:34,213 INFO  [regionserver60020-smallCompactions-1404253964856] hdfs.DFSClient: Could not obtain blk_221234652107626381_29849 from any node: java.io.IOException: No live nodes contain current block. Will get new block locations from namenode and retry...
2014-07-01 15:50:34,560 INFO  [regionserver60020] regionserver.HRegionServer: Received CLOSE for the region: 122e92eabfab38cb15a9965b21db25c5 ,which we are already trying to CLOSE, but not completed yet
2014-07-01 15:50:34,560 WARN  [regionserver60020] regionserver.HRegionServer: Failed to close usertable,user5,1404228265526.122e92eabfab38cb15a9965b21db25c5. - ignoring and continuing
org.apache.hadoop.hbase.regionserver.RegionAlreadyInTransitionException: The region 122e92eabfab38cb15a9965b21db25c5 was already closing. New CLOSE request is ignored.
	at org.apache.hadoop.hbase.regionserver.HRegionServer.closeRegion(HRegionServer.java:2586)
	at org.apache.hadoop.hbase.regionserver.HRegionServer.closeRegionIgnoreErrors(HRegionServer.java:2508)
	at org.apache.hadoop.hbase.regionserver.HRegionServer.closeUserRegions(HRegionServer.java:2098)
	at org.apache.hadoop.hbase.regionserver.HRegionServer.run(HRegionServer.java:894)
	at java.lang.Thread.run(Thread.java:701)
2014-07-01 15:50:34,561 INFO  [regionserver60020] regionserver.HRegionServer: Received CLOSE for the region: bb79ae773f71289e0745068f68a0abd9 ,which we are already trying to CLOSE, but not completed yet
2014-07-01 15:50:34,561 WARN  [regionserver60020] regionserver.HRegionServer: Failed to close usertable,user9,1404228265526.bb79ae773f71289e0745068f68a0abd9. - ignoring and continuing
org.apache.hadoop.hbase.regionserver.RegionAlreadyInTransitionException: The region bb79ae773f71289e0745068f68a0abd9 was already closing. New CLOSE request is ignored.
	at org.apache.hadoop.hbase.regionserver.HRegionServer.closeRegion(HRegionServer.java:2586)
	at org.apache.hadoop.hbase.regionserver.HRegionServer.closeRegionIgnoreErrors(HRegionServer.java:2508)
	at org.apache.hadoop.hbase.regionserver.HRegionServer.closeUserRegions(HRegionServer.java:2098)
	at org.apache.hadoop.hbase.regionserver.HRegionServer.run(HRegionServer.java:894)
	at java.lang.Thread.run(Thread.java:701)
2014-07-01 15:50:34,561 INFO  [regionserver60020] regionserver.HRegionServer: Received CLOSE for the region: 9cef9e7d34f96f1d9b111be50040a208 ,which we are already trying to CLOSE, but not completed yet
2014-07-01 15:50:34,561 WARN  [regionserver60020] regionserver.HRegionServer: Failed to close usertable,,1404228265525.9cef9e7d34f96f1d9b111be50040a208. - ignoring and continuing
org.apache.hadoop.hbase.regionserver.RegionAlreadyInTransitionException: The region 9cef9e7d34f96f1d9b111be50040a208 was already closing. New CLOSE request is ignored.
	at org.apache.hadoop.hbase.regionserver.HRegionServer.closeRegion(HRegionServer.java:2586)
	at org.apache.hadoop.hbase.regionserver.HRegionServer.closeRegionIgnoreErrors(HRegionServer.java:2508)
	at org.apache.hadoop.hbase.regionserver.HRegionServer.closeUserRegions(HRegionServer.java:2098)
	at org.apache.hadoop.hbase.regionserver.HRegionServer.run(HRegionServer.java:894)
	at java.lang.Thread.run(Thread.java:701)
2014-07-01 15:50:34,561 INFO  [regionserver60020] regionserver.HRegionServer: Received CLOSE for the region: 93253e37c284f3174309fede3f32339f ,which we are already trying to CLOSE, but not completed yet
2014-07-01 15:50:34,562 WARN  [regionserver60020] regionserver.HRegionServer: Failed to close usertable,user3,1404228265526.93253e37c284f3174309fede3f32339f. - ignoring and continuing
org.apache.hadoop.hbase.regionserver.RegionAlreadyInTransitionException: The region 93253e37c284f3174309fede3f32339f was already closing. New CLOSE request is ignored.
	at org.apache.hadoop.hbase.regionserver.HRegionServer.closeRegion(HRegionServer.java:2586)
	at org.apache.hadoop.hbase.regionserver.HRegionServer.closeRegionIgnoreErrors(HRegionServer.java:2508)
	at org.apache.hadoop.hbase.regionserver.HRegionServer.closeUserRegions(HRegionServer.java:2098)
	at org.apache.hadoop.hbase.regionserver.HRegionServer.run(HRegionServer.java:894)
	at java.lang.Thread.run(Thread.java:701)
2014-07-01 15:50:34,562 INFO  [regionserver60020] regionserver.HRegionServer: Received CLOSE for the region: 667c1f2c34fe37d48ba25a789d8aba98 ,which we are already trying to CLOSE, but not completed yet
2014-07-01 15:50:34,562 WARN  [regionserver60020] regionserver.HRegionServer: Failed to close usertable,user7,1404228265526.667c1f2c34fe37d48ba25a789d8aba98. - ignoring and continuing
org.apache.hadoop.hbase.regionserver.RegionAlreadyInTransitionException: The region 667c1f2c34fe37d48ba25a789d8aba98 was already closing. New CLOSE request is ignored.
	at org.apache.hadoop.hbase.regionserver.HRegionServer.closeRegion(HRegionServer.java:2586)
	at org.apache.hadoop.hbase.regionserver.HRegionServer.closeRegionIgnoreErrors(HRegionServer.java:2508)
	at org.apache.hadoop.hbase.regionserver.HRegionServer.closeUserRegions(HRegionServer.java:2098)
	at org.apache.hadoop.hbase.regionserver.HRegionServer.run(HRegionServer.java:894)
	at java.lang.Thread.run(Thread.java:701)
2014-07-01 15:50:34,562 DEBUG [regionserver60020] regionserver.HRegionServer: Waiting on 122e92eabfab38cb15a9965b21db25c5, bb79ae773f71289e0745068f68a0abd9, 9cef9e7d34f96f1d9b111be50040a208, 93253e37c284f3174309fede3f32339f, 667c1f2c34fe37d48ba25a789d8aba98
2014-07-01 15:50:34,721 INFO  [DataStreamer for file /hbase/WALs/slave1,60020,1404253868846/slave1%2C60020%2C1404253868846.1404254547570 block blk_-3161611795271398729_29985] ipc.Client: Retrying connect to server: sceplus-vm49.almaden.ibm.com/9.1.143.59:50020. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1 SECONDS)
2014-07-01 15:50:35,213 INFO  [RS_CLOSE_REGION-slave1:60020-2] ipc.Client: Retrying connect to server: master/9.1.143.58:54310. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1 SECONDS)
2014-07-01 15:50:35,722 INFO  [DataStreamer for file /hbase/WALs/slave1,60020,1404253868846/slave1%2C60020%2C1404253868846.1404254547570 block blk_-3161611795271398729_29985] ipc.Client: Retrying connect to server: sceplus-vm49.almaden.ibm.com/9.1.143.59:50020. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1 SECONDS)
2014-07-01 15:50:36,214 INFO  [RS_CLOSE_REGION-slave1:60020-2] ipc.Client: Retrying connect to server: master/9.1.143.58:54310. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1 SECONDS)
2014-07-01 15:50:36,722 INFO  [DataStreamer for file /hbase/WALs/slave1,60020,1404253868846/slave1%2C60020%2C1404253868846.1404254547570 block blk_-3161611795271398729_29985] ipc.Client: Retrying connect to server: sceplus-vm49.almaden.ibm.com/9.1.143.59:50020. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1 SECONDS)
2014-07-01 15:50:37,215 INFO  [RS_CLOSE_REGION-slave1:60020-2] ipc.Client: Retrying connect to server: master/9.1.143.58:54310. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1 SECONDS)
2014-07-01 15:50:37,565 INFO  [regionserver60020] regionserver.HRegionServer: Received CLOSE for the region: 122e92eabfab38cb15a9965b21db25c5 ,which we are already trying to CLOSE, but not completed yet
2014-07-01 15:50:37,565 WARN  [regionserver60020] regionserver.HRegionServer: Failed to close usertable,user5,1404228265526.122e92eabfab38cb15a9965b21db25c5. - ignoring and continuing
org.apache.hadoop.hbase.regionserver.RegionAlreadyInTransitionException: The region 122e92eabfab38cb15a9965b21db25c5 was already closing. New CLOSE request is ignored.
	at org.apache.hadoop.hbase.regionserver.HRegionServer.closeRegion(HRegionServer.java:2586)
	at org.apache.hadoop.hbase.regionserver.HRegionServer.closeRegionIgnoreErrors(HRegionServer.java:2508)
	at org.apache.hadoop.hbase.regionserver.HRegionServer.closeUserRegions(HRegionServer.java:2098)
	at org.apache.hadoop.hbase.regionserver.HRegionServer.run(HRegionServer.java:894)
	at java.lang.Thread.run(Thread.java:701)
2014-07-01 15:50:37,566 INFO  [regionserver60020] regionserver.HRegionServer: Received CLOSE for the region: bb79ae773f71289e0745068f68a0abd9 ,which we are already trying to CLOSE, but not completed yet
2014-07-01 15:50:37,566 WARN  [regionserver60020] regionserver.HRegionServer: Failed to close usertable,user9,1404228265526.bb79ae773f71289e0745068f68a0abd9. - ignoring and continuing
org.apache.hadoop.hbase.regionserver.RegionAlreadyInTransitionException: The region bb79ae773f71289e0745068f68a0abd9 was already closing. New CLOSE request is ignored.
	at org.apache.hadoop.hbase.regionserver.HRegionServer.closeRegion(HRegionServer.java:2586)
	at org.apache.hadoop.hbase.regionserver.HRegionServer.closeRegionIgnoreErrors(HRegionServer.java:2508)
	at org.apache.hadoop.hbase.regionserver.HRegionServer.closeUserRegions(HRegionServer.java:2098)
	at org.apache.hadoop.hbase.regionserver.HRegionServer.run(HRegionServer.java:894)
	at java.lang.Thread.run(Thread.java:701)
2014-07-01 15:50:37,566 INFO  [regionserver60020] regionserver.HRegionServer: Received CLOSE for the region: 9cef9e7d34f96f1d9b111be50040a208 ,which we are already trying to CLOSE, but not completed yet
2014-07-01 15:50:37,566 WARN  [regionserver60020] regionserver.HRegionServer: Failed to close usertable,,1404228265525.9cef9e7d34f96f1d9b111be50040a208. - ignoring and continuing
org.apache.hadoop.hbase.regionserver.RegionAlreadyInTransitionException: The region 9cef9e7d34f96f1d9b111be50040a208 was already closing. New CLOSE request is ignored.
	at org.apache.hadoop.hbase.regionserver.HRegionServer.closeRegion(HRegionServer.java:2586)
	at org.apache.hadoop.hbase.regionserver.HRegionServer.closeRegionIgnoreErrors(HRegionServer.java:2508)
	at org.apache.hadoop.hbase.regionserver.HRegionServer.closeUserRegions(HRegionServer.java:2098)
	at org.apache.hadoop.hbase.regionserver.HRegionServer.run(HRegionServer.java:894)
	at java.lang.Thread.run(Thread.java:701)
2014-07-01 15:50:37,567 INFO  [regionserver60020] regionserver.HRegionServer: Received CLOSE for the region: 93253e37c284f3174309fede3f32339f ,which we are already trying to CLOSE, but not completed yet
2014-07-01 15:50:37,567 WARN  [regionserver60020] regionserver.HRegionServer: Failed to close usertable,user3,1404228265526.93253e37c284f3174309fede3f32339f. - ignoring and continuing
org.apache.hadoop.hbase.regionserver.RegionAlreadyInTransitionException: The region 93253e37c284f3174309fede3f32339f was already closing. New CLOSE request is ignored.
	at org.apache.hadoop.hbase.regionserver.HRegionServer.closeRegion(HRegionServer.java:2586)
	at org.apache.hadoop.hbase.regionserver.HRegionServer.closeRegionIgnoreErrors(HRegionServer.java:2508)
	at org.apache.hadoop.hbase.regionserver.HRegionServer.closeUserRegions(HRegionServer.java:2098)
	at org.apache.hadoop.hbase.regionserver.HRegionServer.run(HRegionServer.java:894)
	at java.lang.Thread.run(Thread.java:701)
2014-07-01 15:50:37,567 INFO  [regionserver60020] regionserver.HRegionServer: Received CLOSE for the region: 667c1f2c34fe37d48ba25a789d8aba98 ,which we are already trying to CLOSE, but not completed yet
2014-07-01 15:50:37,567 WARN  [regionserver60020] regionserver.HRegionServer: Failed to close usertable,user7,1404228265526.667c1f2c34fe37d48ba25a789d8aba98. - ignoring and continuing
org.apache.hadoop.hbase.regionserver.RegionAlreadyInTransitionException: The region 667c1f2c34fe37d48ba25a789d8aba98 was already closing. New CLOSE request is ignored.
	at org.apache.hadoop.hbase.regionserver.HRegionServer.closeRegion(HRegionServer.java:2586)
	at org.apache.hadoop.hbase.regionserver.HRegionServer.closeRegionIgnoreErrors(HRegionServer.java:2508)
	at org.apache.hadoop.hbase.regionserver.HRegionServer.closeUserRegions(HRegionServer.java:2098)
	at org.apache.hadoop.hbase.regionserver.HRegionServer.run(HRegionServer.java:894)
	at java.lang.Thread.run(Thread.java:701)
2014-07-01 15:50:37,567 DEBUG [regionserver60020] regionserver.HRegionServer: Waiting on 122e92eabfab38cb15a9965b21db25c5, bb79ae773f71289e0745068f68a0abd9, 9cef9e7d34f96f1d9b111be50040a208, 93253e37c284f3174309fede3f32339f, 667c1f2c34fe37d48ba25a789d8aba98
2014-07-01 15:50:37,723 INFO  [DataStreamer for file /hbase/WALs/slave1,60020,1404253868846/slave1%2C60020%2C1404253868846.1404254547570 block blk_-3161611795271398729_29985] ipc.Client: Retrying connect to server: sceplus-vm49.almaden.ibm.com/9.1.143.59:50020. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1 SECONDS)
2014-07-01 15:50:38,216 INFO  [RS_CLOSE_REGION-slave1:60020-2] ipc.Client: Retrying connect to server: master/9.1.143.58:54310. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1 SECONDS)
2014-07-01 15:50:38,724 INFO  [DataStreamer for file /hbase/WALs/slave1,60020,1404253868846/slave1%2C60020%2C1404253868846.1404254547570 block blk_-3161611795271398729_29985] ipc.Client: Retrying connect to server: sceplus-vm49.almaden.ibm.com/9.1.143.59:50020. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1 SECONDS)
2014-07-01 15:50:38,724 WARN  [DataStreamer for file /hbase/WALs/slave1,60020,1404253868846/slave1%2C60020%2C1404253868846.1404254547570 block blk_-3161611795271398729_29985] hdfs.DFSClient: Failed recovery attempt #1 from primary datanode 9.1.143.59:50010
java.net.ConnectException: Call to sceplus-vm49.almaden.ibm.com/9.1.143.59:50020 failed on connection exception: java.net.ConnectException: Connection refused
	at org.apache.hadoop.ipc.Client.wrapException(Client.java:1142)
	at org.apache.hadoop.ipc.Client.call(Client.java:1118)
	at org.apache.hadoop.ipc.RPC$Invoker.invoke(RPC.java:229)
	at com.sun.proxy.$Proxy13.getProtocolVersion(Unknown Source)
	at org.apache.hadoop.ipc.RPC.checkVersion(RPC.java:422)
	at org.apache.hadoop.ipc.RPC.getProxy(RPC.java:414)
	at org.apache.hadoop.ipc.RPC.getProxy(RPC.java:392)
	at org.apache.hadoop.hdfs.DFSClient.createClientDatanodeProtocolProxy(DFSClient.java:201)
	at org.apache.hadoop.hdfs.DFSClient$DFSOutputStream.processDatanodeError(DFSClient.java:3317)
	at org.apache.hadoop.hdfs.DFSClient$DFSOutputStream.access$2200(DFSClient.java:2783)
	at org.apache.hadoop.hdfs.DFSClient$DFSOutputStream$DataStreamer.run(DFSClient.java:2987)
Caused by: java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:601)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:511)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:481)
	at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:457)
	at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:583)
	at org.apache.hadoop.ipc.Client$Connection.access$2200(Client.java:205)
	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1249)
	at org.apache.hadoop.ipc.Client.call(Client.java:1093)
	... 9 more
2014-07-01 15:50:38,725 WARN  [DataStreamer for file /hbase/WALs/slave1,60020,1404253868846/slave1%2C60020%2C1404253868846.1404254547570 block blk_-3161611795271398729_29985] hdfs.DFSClient: Error Recovery for block blk_-3161611795271398729_29985 failed  because recovery from primary datanode 9.1.143.59:50010 failed 2 times.  Pipeline was 9.1.143.59:50010. Will retry...
2014-07-01 15:50:39,217 INFO  [RS_CLOSE_REGION-slave1:60020-2] ipc.Client: Retrying connect to server: master/9.1.143.58:54310. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1 SECONDS)
2014-07-01 15:50:40,218 INFO  [RS_CLOSE_REGION-slave1:60020-2] ipc.Client: Retrying connect to server: master/9.1.143.58:54310. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1 SECONDS)
2014-07-01 15:50:40,570 INFO  [regionserver60020] regionserver.HRegionServer: Received CLOSE for the region: 122e92eabfab38cb15a9965b21db25c5 ,which we are already trying to CLOSE, but not completed yet
2014-07-01 15:50:40,570 WARN  [regionserver60020] regionserver.HRegionServer: Failed to close usertable,user5,1404228265526.122e92eabfab38cb15a9965b21db25c5. - ignoring and continuing
org.apache.hadoop.hbase.regionserver.RegionAlreadyInTransitionException: The region 122e92eabfab38cb15a9965b21db25c5 was already closing. New CLOSE request is ignored.
	at org.apache.hadoop.hbase.regionserver.HRegionServer.closeRegion(HRegionServer.java:2586)
	at org.apache.hadoop.hbase.regionserver.HRegionServer.closeRegionIgnoreErrors(HRegionServer.java:2508)
	at org.apache.hadoop.hbase.regionserver.HRegionServer.closeUserRegions(HRegionServer.java:2098)
	at org.apache.hadoop.hbase.regionserver.HRegionServer.run(HRegionServer.java:894)
	at java.lang.Thread.run(Thread.java:701)
2014-07-01 15:50:40,570 INFO  [regionserver60020] regionserver.HRegionServer: Received CLOSE for the region: bb79ae773f71289e0745068f68a0abd9 ,which we are already trying to CLOSE, but not completed yet
2014-07-01 15:50:40,570 WARN  [regionserver60020] regionserver.HRegionServer: Failed to close usertable,user9,1404228265526.bb79ae773f71289e0745068f68a0abd9. - ignoring and continuing
org.apache.hadoop.hbase.regionserver.RegionAlreadyInTransitionException: The region bb79ae773f71289e0745068f68a0abd9 was already closing. New CLOSE request is ignored.
	at org.apache.hadoop.hbase.regionserver.HRegionServer.closeRegion(HRegionServer.java:2586)
	at org.apache.hadoop.hbase.regionserver.HRegionServer.closeRegionIgnoreErrors(HRegionServer.java:2508)
	at org.apache.hadoop.hbase.regionserver.HRegionServer.closeUserRegions(HRegionServer.java:2098)
	at org.apache.hadoop.hbase.regionserver.HRegionServer.run(HRegionServer.java:894)
	at java.lang.Thread.run(Thread.java:701)
2014-07-01 15:50:40,571 INFO  [regionserver60020] regionserver.HRegionServer: Received CLOSE for the region: 9cef9e7d34f96f1d9b111be50040a208 ,which we are already trying to CLOSE, but not completed yet
2014-07-01 15:50:40,571 WARN  [regionserver60020] regionserver.HRegionServer: Failed to close usertable,,1404228265525.9cef9e7d34f96f1d9b111be50040a208. - ignoring and continuing
org.apache.hadoop.hbase.regionserver.RegionAlreadyInTransitionException: The region 9cef9e7d34f96f1d9b111be50040a208 was already closing. New CLOSE request is ignored.
	at org.apache.hadoop.hbase.regionserver.HRegionServer.closeRegion(HRegionServer.java:2586)
	at org.apache.hadoop.hbase.regionserver.HRegionServer.closeRegionIgnoreErrors(HRegionServer.java:2508)
	at org.apache.hadoop.hbase.regionserver.HRegionServer.closeUserRegions(HRegionServer.java:2098)
	at org.apache.hadoop.hbase.regionserver.HRegionServer.run(HRegionServer.java:894)
	at java.lang.Thread.run(Thread.java:701)
2014-07-01 15:50:40,571 INFO  [regionserver60020] regionserver.HRegionServer: Received CLOSE for the region: 93253e37c284f3174309fede3f32339f ,which we are already trying to CLOSE, but not completed yet
2014-07-01 15:50:40,571 WARN  [regionserver60020] regionserver.HRegionServer: Failed to close usertable,user3,1404228265526.93253e37c284f3174309fede3f32339f. - ignoring and continuing
org.apache.hadoop.hbase.regionserver.RegionAlreadyInTransitionException: The region 93253e37c284f3174309fede3f32339f was already closing. New CLOSE request is ignored.
	at org.apache.hadoop.hbase.regionserver.HRegionServer.closeRegion(HRegionServer.java:2586)
	at org.apache.hadoop.hbase.regionserver.HRegionServer.closeRegionIgnoreErrors(HRegionServer.java:2508)
	at org.apache.hadoop.hbase.regionserver.HRegionServer.closeUserRegions(HRegionServer.java:2098)
	at org.apache.hadoop.hbase.regionserver.HRegionServer.run(HRegionServer.java:894)
	at java.lang.Thread.run(Thread.java:701)
2014-07-01 15:50:40,572 INFO  [regionserver60020] regionserver.HRegionServer: Received CLOSE for the region: 667c1f2c34fe37d48ba25a789d8aba98 ,which we are already trying to CLOSE, but not completed yet
2014-07-01 15:50:40,572 WARN  [regionserver60020] regionserver.HRegionServer: Failed to close usertable,user7,1404228265526.667c1f2c34fe37d48ba25a789d8aba98. - ignoring and continuing
org.apache.hadoop.hbase.regionserver.RegionAlreadyInTransitionException: The region 667c1f2c34fe37d48ba25a789d8aba98 was already closing. New CLOSE request is ignored.
	at org.apache.hadoop.hbase.regionserver.HRegionServer.closeRegion(HRegionServer.java:2586)
	at org.apache.hadoop.hbase.regionserver.HRegionServer.closeRegionIgnoreErrors(HRegionServer.java:2508)
	at org.apache.hadoop.hbase.regionserver.HRegionServer.closeUserRegions(HRegionServer.java:2098)
	at org.apache.hadoop.hbase.regionserver.HRegionServer.run(HRegionServer.java:894)
	at java.lang.Thread.run(Thread.java:701)
2014-07-01 15:50:40,572 DEBUG [regionserver60020] regionserver.HRegionServer: Waiting on 122e92eabfab38cb15a9965b21db25c5, bb79ae773f71289e0745068f68a0abd9, 9cef9e7d34f96f1d9b111be50040a208, 93253e37c284f3174309fede3f32339f, 667c1f2c34fe37d48ba25a789d8aba98
2014-07-01 15:50:40,727 INFO  [DataStreamer for file /hbase/WALs/slave1,60020,1404253868846/slave1%2C60020%2C1404253868846.1404254547570 block blk_-3161611795271398729_29985] ipc.Client: Retrying connect to server: sceplus-vm49.almaden.ibm.com/9.1.143.59:50020. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1 SECONDS)
2014-07-01 15:50:41,219 INFO  [RS_CLOSE_REGION-slave1:60020-2] ipc.Client: Retrying connect to server: master/9.1.143.58:54310. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1 SECONDS)
2014-07-01 15:50:41,727 INFO  [DataStreamer for file /hbase/WALs/slave1,60020,1404253868846/slave1%2C60020%2C1404253868846.1404254547570 block blk_-3161611795271398729_29985] ipc.Client: Retrying connect to server: sceplus-vm49.almaden.ibm.com/9.1.143.59:50020. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1 SECONDS)
2014-07-01 15:50:42,220 INFO  [RS_CLOSE_REGION-slave1:60020-2] ipc.Client: Retrying connect to server: master/9.1.143.58:54310. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1 SECONDS)
2014-07-01 15:50:42,728 INFO  [DataStreamer for file /hbase/WALs/slave1,60020,1404253868846/slave1%2C60020%2C1404253868846.1404254547570 block blk_-3161611795271398729_29985] ipc.Client: Retrying connect to server: sceplus-vm49.almaden.ibm.com/9.1.143.59:50020. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1 SECONDS)
2014-07-01 15:50:43,221 INFO  [RS_CLOSE_REGION-slave1:60020-2] ipc.Client: Retrying connect to server: master/9.1.143.58:54310. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1 SECONDS)
2014-07-01 15:50:43,575 INFO  [regionserver60020] regionserver.HRegionServer: Received CLOSE for the region: 122e92eabfab38cb15a9965b21db25c5 ,which we are already trying to CLOSE, but not completed yet
2014-07-01 15:50:43,575 WARN  [regionserver60020] regionserver.HRegionServer: Failed to close usertable,user5,1404228265526.122e92eabfab38cb15a9965b21db25c5. - ignoring and continuing
org.apache.hadoop.hbase.regionserver.RegionAlreadyInTransitionException: The region 122e92eabfab38cb15a9965b21db25c5 was already closing. New CLOSE request is ignored.
	at org.apache.hadoop.hbase.regionserver.HRegionServer.closeRegion(HRegionServer.java:2586)
	at org.apache.hadoop.hbase.regionserver.HRegionServer.closeRegionIgnoreErrors(HRegionServer.java:2508)
	at org.apache.hadoop.hbase.regionserver.HRegionServer.closeUserRegions(HRegionServer.java:2098)
	at org.apache.hadoop.hbase.regionserver.HRegionServer.run(HRegionServer.java:894)
	at java.lang.Thread.run(Thread.java:701)
2014-07-01 15:50:43,576 INFO  [regionserver60020] regionserver.HRegionServer: Received CLOSE for the region: bb79ae773f71289e0745068f68a0abd9 ,which we are already trying to CLOSE, but not completed yet
2014-07-01 15:50:43,576 WARN  [regionserver60020] regionserver.HRegionServer: Failed to close usertable,user9,1404228265526.bb79ae773f71289e0745068f68a0abd9. - ignoring and continuing
org.apache.hadoop.hbase.regionserver.RegionAlreadyInTransitionException: The region bb79ae773f71289e0745068f68a0abd9 was already closing. New CLOSE request is ignored.
	at org.apache.hadoop.hbase.regionserver.HRegionServer.closeRegion(HRegionServer.java:2586)
	at org.apache.hadoop.hbase.regionserver.HRegionServer.closeRegionIgnoreErrors(HRegionServer.java:2508)
	at org.apache.hadoop.hbase.regionserver.HRegionServer.closeUserRegions(HRegionServer.java:2098)
	at org.apache.hadoop.hbase.regionserver.HRegionServer.run(HRegionServer.java:894)
	at java.lang.Thread.run(Thread.java:701)
2014-07-01 15:50:43,576 INFO  [regionserver60020] regionserver.HRegionServer: Received CLOSE for the region: 9cef9e7d34f96f1d9b111be50040a208 ,which we are already trying to CLOSE, but not completed yet
2014-07-01 15:50:43,576 WARN  [regionserver60020] regionserver.HRegionServer: Failed to close usertable,,1404228265525.9cef9e7d34f96f1d9b111be50040a208. - ignoring and continuing
org.apache.hadoop.hbase.regionserver.RegionAlreadyInTransitionException: The region 9cef9e7d34f96f1d9b111be50040a208 was already closing. New CLOSE request is ignored.
	at org.apache.hadoop.hbase.regionserver.HRegionServer.closeRegion(HRegionServer.java:2586)
	at org.apache.hadoop.hbase.regionserver.HRegionServer.closeRegionIgnoreErrors(HRegionServer.java:2508)
	at org.apache.hadoop.hbase.regionserver.HRegionServer.closeUserRegions(HRegionServer.java:2098)
	at org.apache.hadoop.hbase.regionserver.HRegionServer.run(HRegionServer.java:894)
	at java.lang.Thread.run(Thread.java:701)
2014-07-01 15:50:43,576 INFO  [regionserver60020] regionserver.HRegionServer: Received CLOSE for the region: 93253e37c284f3174309fede3f32339f ,which we are already trying to CLOSE, but not completed yet
2014-07-01 15:50:43,576 WARN  [regionserver60020] regionserver.HRegionServer: Failed to close usertable,user3,1404228265526.93253e37c284f3174309fede3f32339f. - ignoring and continuing
org.apache.hadoop.hbase.regionserver.RegionAlreadyInTransitionException: The region 93253e37c284f3174309fede3f32339f was already closing. New CLOSE request is ignored.
	at org.apache.hadoop.hbase.regionserver.HRegionServer.closeRegion(HRegionServer.java:2586)
	at org.apache.hadoop.hbase.regionserver.HRegionServer.closeRegionIgnoreErrors(HRegionServer.java:2508)
	at org.apache.hadoop.hbase.regionserver.HRegionServer.closeUserRegions(HRegionServer.java:2098)
	at org.apache.hadoop.hbase.regionserver.HRegionServer.run(HRegionServer.java:894)
	at java.lang.Thread.run(Thread.java:701)
2014-07-01 15:50:43,577 INFO  [regionserver60020] regionserver.HRegionServer: Received CLOSE for the region: 667c1f2c34fe37d48ba25a789d8aba98 ,which we are already trying to CLOSE, but not completed yet
2014-07-01 15:50:43,577 WARN  [regionserver60020] regionserver.HRegionServer: Failed to close usertable,user7,1404228265526.667c1f2c34fe37d48ba25a789d8aba98. - ignoring and continuing
org.apache.hadoop.hbase.regionserver.RegionAlreadyInTransitionException: The region 667c1f2c34fe37d48ba25a789d8aba98 was already closing. New CLOSE request is ignored.
	at org.apache.hadoop.hbase.regionserver.HRegionServer.closeRegion(HRegionServer.java:2586)
	at org.apache.hadoop.hbase.regionserver.HRegionServer.closeRegionIgnoreErrors(HRegionServer.java:2508)
	at org.apache.hadoop.hbase.regionserver.HRegionServer.closeUserRegions(HRegionServer.java:2098)
	at org.apache.hadoop.hbase.regionserver.HRegionServer.run(HRegionServer.java:894)
	at java.lang.Thread.run(Thread.java:701)
2014-07-01 15:50:43,577 DEBUG [regionserver60020] regionserver.HRegionServer: Waiting on 122e92eabfab38cb15a9965b21db25c5, bb79ae773f71289e0745068f68a0abd9, 9cef9e7d34f96f1d9b111be50040a208, 93253e37c284f3174309fede3f32339f, 667c1f2c34fe37d48ba25a789d8aba98
2014-07-01 15:50:43,729 INFO  [DataStreamer for file /hbase/WALs/slave1,60020,1404253868846/slave1%2C60020%2C1404253868846.1404254547570 block blk_-3161611795271398729_29985] ipc.Client: Retrying connect to server: sceplus-vm49.almaden.ibm.com/9.1.143.59:50020. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1 SECONDS)
2014-07-01 15:50:44,222 INFO  [RS_CLOSE_REGION-slave1:60020-2] ipc.Client: Retrying connect to server: master/9.1.143.58:54310. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1 SECONDS)
2014-07-01 15:50:44,226 WARN  [RS_CLOSE_REGION-slave1:60020-2] regionserver.HStore: Failed flushing store file, retrying num=1
java.net.ConnectException: Call to master/9.1.143.58:54310 failed on connection exception: java.net.ConnectException: Connection refused
	at org.apache.hadoop.ipc.Client.wrapException(Client.java:1142)
	at org.apache.hadoop.ipc.Client.call(Client.java:1118)
	at org.apache.hadoop.ipc.RPC$Invoker.invoke(RPC.java:229)
	at com.sun.proxy.$Proxy8.getFileInfo(Unknown Source)
	at sun.reflect.GeneratedMethodAccessor2.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:622)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:85)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:62)
	at com.sun.proxy.$Proxy8.getFileInfo(Unknown Source)
	at sun.reflect.GeneratedMethodAccessor2.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:622)
	at org.apache.hadoop.hbase.fs.HFileSystem$1.invoke(HFileSystem.java:294)
	at com.sun.proxy.$Proxy9.getFileInfo(Unknown Source)
	at org.apache.hadoop.hdfs.DFSClient.getFileInfo(DFSClient.java:1059)
	at org.apache.hadoop.hdfs.DistributedFileSystem.getFileStatus(DistributedFileSystem.java:554)
	at org.apache.hadoop.fs.FilterFileSystem.getFileStatus(FilterFileSystem.java:255)
	at org.apache.hadoop.fs.FileSystem.exists(FileSystem.java:788)
	at org.apache.hadoop.hbase.regionserver.StoreFile$WriterBuilder.build(StoreFile.java:603)
	at org.apache.hadoop.hbase.regionserver.HStore.createWriterInTmp(HStore.java:889)
	at org.apache.hadoop.hbase.regionserver.DefaultStoreFlusher.flushSnapshot(DefaultStoreFlusher.java:69)
	at org.apache.hadoop.hbase.regionserver.HStore.flushCache(HStore.java:798)
	at org.apache.hadoop.hbase.regionserver.HStore$StoreFlusherImpl.flushCache(HStore.java:1949)
	at org.apache.hadoop.hbase.regionserver.HRegion.internalFlushcache(HRegion.java:1776)
	at org.apache.hadoop.hbase.regionserver.HRegion.internalFlushcache(HRegion.java:1658)
	at org.apache.hadoop.hbase.regionserver.HRegion.doClose(HRegion.java:1119)
	at org.apache.hadoop.hbase.regionserver.HRegion.close(HRegion.java:1085)
	at org.apache.hadoop.hbase.regionserver.handler.CloseRegionHandler.process(CloseRegionHandler.java:147)
	at org.apache.hadoop.hbase.executor.EventHandler.run(EventHandler.java:128)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1146)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:701)
Caused by: java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:601)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:511)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:481)
	at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:457)
	at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:583)
	at org.apache.hadoop.ipc.Client$Connection.access$2200(Client.java:205)
	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1249)
	at org.apache.hadoop.ipc.Client.call(Client.java:1093)
	... 31 more
2014-07-01 15:50:44,729 INFO  [DataStreamer for file /hbase/WALs/slave1,60020,1404253868846/slave1%2C60020%2C1404253868846.1404254547570 block blk_-3161611795271398729_29985] ipc.Client: Retrying connect to server: sceplus-vm49.almaden.ibm.com/9.1.143.59:50020. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1 SECONDS)
2014-07-01 15:50:45,227 INFO  [regionserver60020-smallCompactions-1404253964856] ipc.Client: Retrying connect to server: master/9.1.143.58:54310. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1 SECONDS)
2014-07-01 15:50:45,730 INFO  [DataStreamer for file /hbase/WALs/slave1,60020,1404253868846/slave1%2C60020%2C1404253868846.1404254547570 block blk_-3161611795271398729_29985] ipc.Client: Retrying connect to server: sceplus-vm49.almaden.ibm.com/9.1.143.59:50020. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1 SECONDS)
2014-07-01 15:50:46,228 INFO  [regionserver60020-smallCompactions-1404253964856] ipc.Client: Retrying connect to server: master/9.1.143.58:54310. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1 SECONDS)
2014-07-01 15:50:46,580 INFO  [regionserver60020] regionserver.HRegionServer: Received CLOSE for the region: 122e92eabfab38cb15a9965b21db25c5 ,which we are already trying to CLOSE, but not completed yet
2014-07-01 15:50:46,580 WARN  [regionserver60020] regionserver.HRegionServer: Failed to close usertable,user5,1404228265526.122e92eabfab38cb15a9965b21db25c5. - ignoring and continuing
org.apache.hadoop.hbase.regionserver.RegionAlreadyInTransitionException: The region 122e92eabfab38cb15a9965b21db25c5 was already closing. New CLOSE request is ignored.
	at org.apache.hadoop.hbase.regionserver.HRegionServer.closeRegion(HRegionServer.java:2586)
	at org.apache.hadoop.hbase.regionserver.HRegionServer.closeRegionIgnoreErrors(HRegionServer.java:2508)
	at org.apache.hadoop.hbase.regionserver.HRegionServer.closeUserRegions(HRegionServer.java:2098)
	at org.apache.hadoop.hbase.regionserver.HRegionServer.run(HRegionServer.java:894)
	at java.lang.Thread.run(Thread.java:701)
2014-07-01 15:50:46,581 INFO  [regionserver60020] regionserver.HRegionServer: Received CLOSE for the region: bb79ae773f71289e0745068f68a0abd9 ,which we are already trying to CLOSE, but not completed yet
2014-07-01 15:50:46,581 WARN  [regionserver60020] regionserver.HRegionServer: Failed to close usertable,user9,1404228265526.bb79ae773f71289e0745068f68a0abd9. - ignoring and continuing
org.apache.hadoop.hbase.regionserver.RegionAlreadyInTransitionException: The region bb79ae773f71289e0745068f68a0abd9 was already closing. New CLOSE request is ignored.
	at org.apache.hadoop.hbase.regionserver.HRegionServer.closeRegion(HRegionServer.java:2586)
	at org.apache.hadoop.hbase.regionserver.HRegionServer.closeRegionIgnoreErrors(HRegionServer.java:2508)
	at org.apache.hadoop.hbase.regionserver.HRegionServer.closeUserRegions(HRegionServer.java:2098)
	at org.apache.hadoop.hbase.regionserver.HRegionServer.run(HRegionServer.java:894)
	at java.lang.Thread.run(Thread.java:701)
2014-07-01 15:50:46,581 INFO  [regionserver60020] regionserver.HRegionServer: Received CLOSE for the region: 9cef9e7d34f96f1d9b111be50040a208 ,which we are already trying to CLOSE, but not completed yet
2014-07-01 15:50:46,581 WARN  [regionserver60020] regionserver.HRegionServer: Failed to close usertable,,1404228265525.9cef9e7d34f96f1d9b111be50040a208. - ignoring and continuing
org.apache.hadoop.hbase.regionserver.RegionAlreadyInTransitionException: The region 9cef9e7d34f96f1d9b111be50040a208 was already closing. New CLOSE request is ignored.
	at org.apache.hadoop.hbase.regionserver.HRegionServer.closeRegion(HRegionServer.java:2586)
	at org.apache.hadoop.hbase.regionserver.HRegionServer.closeRegionIgnoreErrors(HRegionServer.java:2508)
	at org.apache.hadoop.hbase.regionserver.HRegionServer.closeUserRegions(HRegionServer.java:2098)
	at org.apache.hadoop.hbase.regionserver.HRegionServer.run(HRegionServer.java:894)
	at java.lang.Thread.run(Thread.java:701)
2014-07-01 15:50:46,581 INFO  [regionserver60020] regionserver.HRegionServer: Received CLOSE for the region: 93253e37c284f3174309fede3f32339f ,which we are already trying to CLOSE, but not completed yet
2014-07-01 15:50:46,582 WARN  [regionserver60020] regionserver.HRegionServer: Failed to close usertable,user3,1404228265526.93253e37c284f3174309fede3f32339f. - ignoring and continuing
org.apache.hadoop.hbase.regionserver.RegionAlreadyInTransitionException: The region 93253e37c284f3174309fede3f32339f was already closing. New CLOSE request is ignored.
	at org.apache.hadoop.hbase.regionserver.HRegionServer.closeRegion(HRegionServer.java:2586)
	at org.apache.hadoop.hbase.regionserver.HRegionServer.closeRegionIgnoreErrors(HRegionServer.java:2508)
	at org.apache.hadoop.hbase.regionserver.HRegionServer.closeUserRegions(HRegionServer.java:2098)
	at org.apache.hadoop.hbase.regionserver.HRegionServer.run(HRegionServer.java:894)
	at java.lang.Thread.run(Thread.java:701)
2014-07-01 15:50:46,582 INFO  [regionserver60020] regionserver.HRegionServer: Received CLOSE for the region: 667c1f2c34fe37d48ba25a789d8aba98 ,which we are already trying to CLOSE, but not completed yet
2014-07-01 15:50:46,582 WARN  [regionserver60020] regionserver.HRegionServer: Failed to close usertable,user7,1404228265526.667c1f2c34fe37d48ba25a789d8aba98. - ignoring and continuing
org.apache.hadoop.hbase.regionserver.RegionAlreadyInTransitionException: The region 667c1f2c34fe37d48ba25a789d8aba98 was already closing. New CLOSE request is ignored.
	at org.apache.hadoop.hbase.regionserver.HRegionServer.closeRegion(HRegionServer.java:2586)
	at org.apache.hadoop.hbase.regionserver.HRegionServer.closeRegionIgnoreErrors(HRegionServer.java:2508)
	at org.apache.hadoop.hbase.regionserver.HRegionServer.closeUserRegions(HRegionServer.java:2098)
	at org.apache.hadoop.hbase.regionserver.HRegionServer.run(HRegionServer.java:894)
	at java.lang.Thread.run(Thread.java:701)
2014-07-01 15:50:46,582 DEBUG [regionserver60020] regionserver.HRegionServer: Waiting on 122e92eabfab38cb15a9965b21db25c5, bb79ae773f71289e0745068f68a0abd9, 9cef9e7d34f96f1d9b111be50040a208, 93253e37c284f3174309fede3f32339f, 667c1f2c34fe37d48ba25a789d8aba98
2014-07-01 15:50:46,731 INFO  [DataStreamer for file /hbase/WALs/slave1,60020,1404253868846/slave1%2C60020%2C1404253868846.1404254547570 block blk_-3161611795271398729_29985] ipc.Client: Retrying connect to server: sceplus-vm49.almaden.ibm.com/9.1.143.59:50020. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1 SECONDS)
2014-07-01 15:50:47,229 INFO  [regionserver60020-smallCompactions-1404253964856] ipc.Client: Retrying connect to server: master/9.1.143.58:54310. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1 SECONDS)
2014-07-01 15:50:47,732 INFO  [DataStreamer for file /hbase/WALs/slave1,60020,1404253868846/slave1%2C60020%2C1404253868846.1404254547570 block blk_-3161611795271398729_29985] ipc.Client: Retrying connect to server: sceplus-vm49.almaden.ibm.com/9.1.143.59:50020. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1 SECONDS)
2014-07-01 15:50:48,230 INFO  [regionserver60020-smallCompactions-1404253964856] ipc.Client: Retrying connect to server: master/9.1.143.58:54310. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1 SECONDS)
2014-07-01 15:50:48,733 INFO  [DataStreamer for file /hbase/WALs/slave1,60020,1404253868846/slave1%2C60020%2C1404253868846.1404254547570 block blk_-3161611795271398729_29985] ipc.Client: Retrying connect to server: sceplus-vm49.almaden.ibm.com/9.1.143.59:50020. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1 SECONDS)
2014-07-01 15:50:49,231 INFO  [regionserver60020-smallCompactions-1404253964856] ipc.Client: Retrying connect to server: master/9.1.143.58:54310. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1 SECONDS)
2014-07-01 15:50:49,585 INFO  [regionserver60020] regionserver.HRegionServer: Received CLOSE for the region: 122e92eabfab38cb15a9965b21db25c5 ,which we are already trying to CLOSE, but not completed yet
2014-07-01 15:50:49,585 WARN  [regionserver60020] regionserver.HRegionServer: Failed to close usertable,user5,1404228265526.122e92eabfab38cb15a9965b21db25c5. - ignoring and continuing
org.apache.hadoop.hbase.regionserver.RegionAlreadyInTransitionException: The region 122e92eabfab38cb15a9965b21db25c5 was already closing. New CLOSE request is ignored.
	at org.apache.hadoop.hbase.regionserver.HRegionServer.closeRegion(HRegionServer.java:2586)
	at org.apache.hadoop.hbase.regionserver.HRegionServer.closeRegionIgnoreErrors(HRegionServer.java:2508)
	at org.apache.hadoop.hbase.regionserver.HRegionServer.closeUserRegions(HRegionServer.java:2098)
	at org.apache.hadoop.hbase.regionserver.HRegionServer.run(HRegionServer.java:894)
	at java.lang.Thread.run(Thread.java:701)
2014-07-01 15:50:49,586 INFO  [regionserver60020] regionserver.HRegionServer: Received CLOSE for the region: bb79ae773f71289e0745068f68a0abd9 ,which we are already trying to CLOSE, but not completed yet
2014-07-01 15:50:49,586 WARN  [regionserver60020] regionserver.HRegionServer: Failed to close usertable,user9,1404228265526.bb79ae773f71289e0745068f68a0abd9. - ignoring and continuing
org.apache.hadoop.hbase.regionserver.RegionAlreadyInTransitionException: The region bb79ae773f71289e0745068f68a0abd9 was already closing. New CLOSE request is ignored.
	at org.apache.hadoop.hbase.regionserver.HRegionServer.closeRegion(HRegionServer.java:2586)
	at org.apache.hadoop.hbase.regionserver.HRegionServer.closeRegionIgnoreErrors(HRegionServer.java:2508)
	at org.apache.hadoop.hbase.regionserver.HRegionServer.closeUserRegions(HRegionServer.java:2098)
	at org.apache.hadoop.hbase.regionserver.HRegionServer.run(HRegionServer.java:894)
	at java.lang.Thread.run(Thread.java:701)
2014-07-01 15:50:49,586 INFO  [regionserver60020] regionserver.HRegionServer: Received CLOSE for the region: 9cef9e7d34f96f1d9b111be50040a208 ,which we are already trying to CLOSE, but not completed yet
2014-07-01 15:50:49,586 WARN  [regionserver60020] regionserver.HRegionServer: Failed to close usertable,,1404228265525.9cef9e7d34f96f1d9b111be50040a208. - ignoring and continuing
org.apache.hadoop.hbase.regionserver.RegionAlreadyInTransitionException: The region 9cef9e7d34f96f1d9b111be50040a208 was already closing. New CLOSE request is ignored.
	at org.apache.hadoop.hbase.regionserver.HRegionServer.closeRegion(HRegionServer.java:2586)
	at org.apache.hadoop.hbase.regionserver.HRegionServer.closeRegionIgnoreErrors(HRegionServer.java:2508)
	at org.apache.hadoop.hbase.regionserver.HRegionServer.closeUserRegions(HRegionServer.java:2098)
	at org.apache.hadoop.hbase.regionserver.HRegionServer.run(HRegionServer.java:894)
	at java.lang.Thread.run(Thread.java:701)
2014-07-01 15:50:49,587 INFO  [regionserver60020] regionserver.HRegionServer: Received CLOSE for the region: 93253e37c284f3174309fede3f32339f ,which we are already trying to CLOSE, but not completed yet
2014-07-01 15:50:49,587 WARN  [regionserver60020] regionserver.HRegionServer: Failed to close usertable,user3,1404228265526.93253e37c284f3174309fede3f32339f. - ignoring and continuing
org.apache.hadoop.hbase.regionserver.RegionAlreadyInTransitionException: The region 93253e37c284f3174309fede3f32339f was already closing. New CLOSE request is ignored.
	at org.apache.hadoop.hbase.regionserver.HRegionServer.closeRegion(HRegionServer.java:2586)
	at org.apache.hadoop.hbase.regionserver.HRegionServer.closeRegionIgnoreErrors(HRegionServer.java:2508)
	at org.apache.hadoop.hbase.regionserver.HRegionServer.closeUserRegions(HRegionServer.java:2098)
	at org.apache.hadoop.hbase.regionserver.HRegionServer.run(HRegionServer.java:894)
	at java.lang.Thread.run(Thread.java:701)
2014-07-01 15:50:49,587 INFO  [regionserver60020] regionserver.HRegionServer: Received CLOSE for the region: 667c1f2c34fe37d48ba25a789d8aba98 ,which we are already trying to CLOSE, but not completed yet
2014-07-01 15:50:49,587 WARN  [regionserver60020] regionserver.HRegionServer: Failed to close usertable,user7,1404228265526.667c1f2c34fe37d48ba25a789d8aba98. - ignoring and continuing
org.apache.hadoop.hbase.regionserver.RegionAlreadyInTransitionException: The region 667c1f2c34fe37d48ba25a789d8aba98 was already closing. New CLOSE request is ignored.
	at org.apache.hadoop.hbase.regionserver.HRegionServer.closeRegion(HRegionServer.java:2586)
	at org.apache.hadoop.hbase.regionserver.HRegionServer.closeRegionIgnoreErrors(HRegionServer.java:2508)
	at org.apache.hadoop.hbase.regionserver.HRegionServer.closeUserRegions(HRegionServer.java:2098)
	at org.apache.hadoop.hbase.regionserver.HRegionServer.run(HRegionServer.java:894)
	at java.lang.Thread.run(Thread.java:701)
2014-07-01 15:50:49,587 DEBUG [regionserver60020] regionserver.HRegionServer: Waiting on 122e92eabfab38cb15a9965b21db25c5, bb79ae773f71289e0745068f68a0abd9, 9cef9e7d34f96f1d9b111be50040a208, 93253e37c284f3174309fede3f32339f, 667c1f2c34fe37d48ba25a789d8aba98
2014-07-01 15:50:49,733 INFO  [DataStreamer for file /hbase/WALs/slave1,60020,1404253868846/slave1%2C60020%2C1404253868846.1404254547570 block blk_-3161611795271398729_29985] ipc.Client: Retrying connect to server: sceplus-vm49.almaden.ibm.com/9.1.143.59:50020. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1 SECONDS)
2014-07-01 15:50:49,734 WARN  [DataStreamer for file /hbase/WALs/slave1,60020,1404253868846/slave1%2C60020%2C1404253868846.1404254547570 block blk_-3161611795271398729_29985] hdfs.DFSClient: Failed recovery attempt #2 from primary datanode 9.1.143.59:50010
java.net.ConnectException: Call to sceplus-vm49.almaden.ibm.com/9.1.143.59:50020 failed on connection exception: java.net.ConnectException: Connection refused
	at org.apache.hadoop.ipc.Client.wrapException(Client.java:1142)
	at org.apache.hadoop.ipc.Client.call(Client.java:1118)
	at org.apache.hadoop.ipc.RPC$Invoker.invoke(RPC.java:229)
	at com.sun.proxy.$Proxy13.getProtocolVersion(Unknown Source)
	at org.apache.hadoop.ipc.RPC.checkVersion(RPC.java:422)
	at org.apache.hadoop.ipc.RPC.getProxy(RPC.java:414)
	at org.apache.hadoop.ipc.RPC.getProxy(RPC.java:392)
	at org.apache.hadoop.hdfs.DFSClient.createClientDatanodeProtocolProxy(DFSClient.java:201)
	at org.apache.hadoop.hdfs.DFSClient$DFSOutputStream.processDatanodeError(DFSClient.java:3317)
	at org.apache.hadoop.hdfs.DFSClient$DFSOutputStream.access$2200(DFSClient.java:2783)
	at org.apache.hadoop.hdfs.DFSClient$DFSOutputStream$DataStreamer.run(DFSClient.java:2987)
Caused by: java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:601)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:511)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:481)
	at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:457)
	at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:583)
	at org.apache.hadoop.ipc.Client$Connection.access$2200(Client.java:205)
	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1249)
	at org.apache.hadoop.ipc.Client.call(Client.java:1093)
	... 9 more
2014-07-01 15:50:49,734 WARN  [DataStreamer for file /hbase/WALs/slave1,60020,1404253868846/slave1%2C60020%2C1404253868846.1404254547570 block blk_-3161611795271398729_29985] hdfs.DFSClient: Error Recovery for block blk_-3161611795271398729_29985 failed  because recovery from primary datanode 9.1.143.59:50010 failed 3 times.  Pipeline was 9.1.143.59:50010. Will retry...
2014-07-01 15:50:50,232 INFO  [regionserver60020-smallCompactions-1404253964856] ipc.Client: Retrying connect to server: master/9.1.143.58:54310. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1 SECONDS)
2014-07-01 15:50:51,233 INFO  [regionserver60020-smallCompactions-1404253964856] ipc.Client: Retrying connect to server: master/9.1.143.58:54310. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1 SECONDS)
2014-07-01 15:50:51,736 INFO  [DataStreamer for file /hbase/WALs/slave1,60020,1404253868846/slave1%2C60020%2C1404253868846.1404254547570 block blk_-3161611795271398729_29985] ipc.Client: Retrying connect to server: sceplus-vm49.almaden.ibm.com/9.1.143.59:50020. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1 SECONDS)
2014-07-01 15:50:52,234 INFO  [regionserver60020-smallCompactions-1404253964856] ipc.Client: Retrying connect to server: master/9.1.143.58:54310. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1 SECONDS)
2014-07-01 15:50:52,590 INFO  [regionserver60020] regionserver.HRegionServer: Received CLOSE for the region: 122e92eabfab38cb15a9965b21db25c5 ,which we are already trying to CLOSE, but not completed yet
2014-07-01 15:50:52,591 WARN  [regionserver60020] regionserver.HRegionServer: Failed to close usertable,user5,1404228265526.122e92eabfab38cb15a9965b21db25c5. - ignoring and continuing
org.apache.hadoop.hbase.regionserver.RegionAlreadyInTransitionException: The region 122e92eabfab38cb15a9965b21db25c5 was already closing. New CLOSE request is ignored.
	at org.apache.hadoop.hbase.regionserver.HRegionServer.closeRegion(HRegionServer.java:2586)
	at org.apache.hadoop.hbase.regionserver.HRegionServer.closeRegionIgnoreErrors(HRegionServer.java:2508)
	at org.apache.hadoop.hbase.regionserver.HRegionServer.closeUserRegions(HRegionServer.java:2098)
	at org.apache.hadoop.hbase.regionserver.HRegionServer.run(HRegionServer.java:894)
	at java.lang.Thread.run(Thread.java:701)
2014-07-01 15:50:52,591 INFO  [regionserver60020] regionserver.HRegionServer: Received CLOSE for the region: bb79ae773f71289e0745068f68a0abd9 ,which we are already trying to CLOSE, but not completed yet
2014-07-01 15:50:52,591 WARN  [regionserver60020] regionserver.HRegionServer: Failed to close usertable,user9,1404228265526.bb79ae773f71289e0745068f68a0abd9. - ignoring and continuing
org.apache.hadoop.hbase.regionserver.RegionAlreadyInTransitionException: The region bb79ae773f71289e0745068f68a0abd9 was already closing. New CLOSE request is ignored.
	at org.apache.hadoop.hbase.regionserver.HRegionServer.closeRegion(HRegionServer.java:2586)
	at org.apache.hadoop.hbase.regionserver.HRegionServer.closeRegionIgnoreErrors(HRegionServer.java:2508)
	at org.apache.hadoop.hbase.regionserver.HRegionServer.closeUserRegions(HRegionServer.java:2098)
	at org.apache.hadoop.hbase.regionserver.HRegionServer.run(HRegionServer.java:894)
	at java.lang.Thread.run(Thread.java:701)
2014-07-01 15:50:52,591 INFO  [regionserver60020] regionserver.HRegionServer: Received CLOSE for the region: 9cef9e7d34f96f1d9b111be50040a208 ,which we are already trying to CLOSE, but not completed yet
2014-07-01 15:50:52,591 WARN  [regionserver60020] regionserver.HRegionServer: Failed to close usertable,,1404228265525.9cef9e7d34f96f1d9b111be50040a208. - ignoring and continuing
org.apache.hadoop.hbase.regionserver.RegionAlreadyInTransitionException: The region 9cef9e7d34f96f1d9b111be50040a208 was already closing. New CLOSE request is ignored.
	at org.apache.hadoop.hbase.regionserver.HRegionServer.closeRegion(HRegionServer.java:2586)
	at org.apache.hadoop.hbase.regionserver.HRegionServer.closeRegionIgnoreErrors(HRegionServer.java:2508)
	at org.apache.hadoop.hbase.regionserver.HRegionServer.closeUserRegions(HRegionServer.java:2098)
	at org.apache.hadoop.hbase.regionserver.HRegionServer.run(HRegionServer.java:894)
	at java.lang.Thread.run(Thread.java:701)
2014-07-01 15:50:52,592 INFO  [regionserver60020] regionserver.HRegionServer: Received CLOSE for the region: 93253e37c284f3174309fede3f32339f ,which we are already trying to CLOSE, but not completed yet
2014-07-01 15:50:52,592 WARN  [regionserver60020] regionserver.HRegionServer: Failed to close usertable,user3,1404228265526.93253e37c284f3174309fede3f32339f. - ignoring and continuing
org.apache.hadoop.hbase.regionserver.RegionAlreadyInTransitionException: The region 93253e37c284f3174309fede3f32339f was already closing. New CLOSE request is ignored.
	at org.apache.hadoop.hbase.regionserver.HRegionServer.closeRegion(HRegionServer.java:2586)
	at org.apache.hadoop.hbase.regionserver.HRegionServer.closeRegionIgnoreErrors(HRegionServer.java:2508)
	at org.apache.hadoop.hbase.regionserver.HRegionServer.closeUserRegions(HRegionServer.java:2098)
	at org.apache.hadoop.hbase.regionserver.HRegionServer.run(HRegionServer.java:894)
	at java.lang.Thread.run(Thread.java:701)
2014-07-01 15:50:52,592 INFO  [regionserver60020] regionserver.HRegionServer: Received CLOSE for the region: 667c1f2c34fe37d48ba25a789d8aba98 ,which we are already trying to CLOSE, but not completed yet
2014-07-01 15:50:52,592 WARN  [regionserver60020] regionserver.HRegionServer: Failed to close usertable,user7,1404228265526.667c1f2c34fe37d48ba25a789d8aba98. - ignoring and continuing
org.apache.hadoop.hbase.regionserver.RegionAlreadyInTransitionException: The region 667c1f2c34fe37d48ba25a789d8aba98 was already closing. New CLOSE request is ignored.
	at org.apache.hadoop.hbase.regionserver.HRegionServer.closeRegion(HRegionServer.java:2586)
	at org.apache.hadoop.hbase.regionserver.HRegionServer.closeRegionIgnoreErrors(HRegionServer.java:2508)
	at org.apache.hadoop.hbase.regionserver.HRegionServer.closeUserRegions(HRegionServer.java:2098)
	at org.apache.hadoop.hbase.regionserver.HRegionServer.run(HRegionServer.java:894)
	at java.lang.Thread.run(Thread.java:701)
2014-07-01 15:50:52,592 DEBUG [regionserver60020] regionserver.HRegionServer: Waiting on 122e92eabfab38cb15a9965b21db25c5, bb79ae773f71289e0745068f68a0abd9, 9cef9e7d34f96f1d9b111be50040a208, 93253e37c284f3174309fede3f32339f, 667c1f2c34fe37d48ba25a789d8aba98
2014-07-01 15:50:52,737 INFO  [DataStreamer for file /hbase/WALs/slave1,60020,1404253868846/slave1%2C60020%2C1404253868846.1404254547570 block blk_-3161611795271398729_29985] ipc.Client: Retrying connect to server: sceplus-vm49.almaden.ibm.com/9.1.143.59:50020. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1 SECONDS)
2014-07-01 15:50:53,236 INFO  [regionserver60020-smallCompactions-1404253964856] ipc.Client: Retrying connect to server: master/9.1.143.58:54310. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1 SECONDS)
2014-07-01 15:50:53,738 INFO  [DataStreamer for file /hbase/WALs/slave1,60020,1404253868846/slave1%2C60020%2C1404253868846.1404254547570 block blk_-3161611795271398729_29985] ipc.Client: Retrying connect to server: sceplus-vm49.almaden.ibm.com/9.1.143.59:50020. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1 SECONDS)
2014-07-01 15:50:54,237 INFO  [regionserver60020-smallCompactions-1404253964856] ipc.Client: Retrying connect to server: master/9.1.143.58:54310. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1 SECONDS)
2014-07-01 15:50:54,241 WARN  [regionserver60020-smallCompactions-1404253964856] hdfs.DFSClient: DFS Read: java.net.ConnectException: Call to master/9.1.143.58:54310 failed on connection exception: java.net.ConnectException: Connection refused
	at org.apache.hadoop.ipc.Client.wrapException(Client.java:1142)
	at org.apache.hadoop.ipc.Client.call(Client.java:1118)
	at org.apache.hadoop.ipc.RPC$Invoker.invoke(RPC.java:229)
	at com.sun.proxy.$Proxy8.getBlockLocations(Unknown Source)
	at sun.reflect.GeneratedMethodAccessor10.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:622)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:85)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:62)
	at com.sun.proxy.$Proxy8.getBlockLocations(Unknown Source)
	at org.apache.hadoop.hdfs.DFSClient.callGetBlockLocations(DFSClient.java:695)
	at org.apache.hadoop.hdfs.DFSClient$DFSInputStream.fetchLocatedBlocks(DFSClient.java:2003)
	at org.apache.hadoop.hdfs.DFSClient$DFSInputStream.openInfo(DFSClient.java:1975)
	at org.apache.hadoop.hdfs.DFSClient$DFSInputStream.chooseDataNode(DFSClient.java:2474)
	at org.apache.hadoop.hdfs.DFSClient$DFSInputStream.blockSeekTo(DFSClient.java:2252)
	at org.apache.hadoop.hdfs.DFSClient$DFSInputStream.read(DFSClient.java:2415)
	at java.io.DataInputStream.read(DataInputStream.java:149)
	at org.apache.hadoop.io.IOUtils.readFully(IOUtils.java:175)
	at org.apache.hadoop.hbase.io.hfile.HFileBlock$AbstractFSReader.readAtOffset(HFileBlock.java:1210)
	at org.apache.hadoop.hbase.io.hfile.HFileBlock$FSReaderV2.readBlockDataInternal(HFileBlock.java:1483)
	at org.apache.hadoop.hbase.io.hfile.HFileBlock$FSReaderV2.readBlockData(HFileBlock.java:1314)
	at org.apache.hadoop.hbase.io.hfile.HFileReaderV2.readBlock(HFileReaderV2.java:392)
	at org.apache.hadoop.hbase.io.hfile.HFileReaderV2$ScannerV2.seekTo(HFileReaderV2.java:803)
	at org.apache.hadoop.hbase.regionserver.StoreFileScanner.seekAtOrAfter(StoreFileScanner.java:244)
	at org.apache.hadoop.hbase.regionserver.StoreFileScanner.seek(StoreFileScanner.java:152)
	at org.apache.hadoop.hbase.regionserver.StoreScanner.seekScanners(StoreScanner.java:317)
	at org.apache.hadoop.hbase.regionserver.StoreScanner.<init>(StoreScanner.java:240)
	at org.apache.hadoop.hbase.regionserver.StoreScanner.<init>(StoreScanner.java:202)
	at org.apache.hadoop.hbase.regionserver.compactions.Compactor.createScanner(Compactor.java:257)
	at org.apache.hadoop.hbase.regionserver.compactions.DefaultCompactor.compact(DefaultCompactor.java:65)
	at org.apache.hadoop.hbase.regionserver.DefaultStoreEngine$DefaultCompactionContext.compact(DefaultStoreEngine.java:109)
	at org.apache.hadoop.hbase.regionserver.HStore.compact(HStore.java:1086)
	at org.apache.hadoop.hbase.regionserver.HRegion.compact(HRegion.java:1481)
	at org.apache.hadoop.hbase.regionserver.CompactSplitThread$CompactionRunner.run(CompactSplitThread.java:475)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1146)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:701)
Caused by: java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:601)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:511)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:481)
	at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:457)
	at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:583)
	at org.apache.hadoop.ipc.Client$Connection.access$2200(Client.java:205)
	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1249)
	at org.apache.hadoop.ipc.Client.call(Client.java:1093)
	... 35 more

2014-07-01 15:50:54,242 ERROR [regionserver60020-smallCompactions-1404253964856] regionserver.CompactSplitThread: Compaction failed Request = regionName=usertable,user7,1404228265526.667c1f2c34fe37d48ba25a789d8aba98., storeName=family, fileCount=6, fileSize=891.0m (388.9m, 105.0m, 111.0m, 125.7m, 94.4m, 66.0m), priority=14, time=75125578705567
java.io.IOException: Could not seek StoreFileScanner[HFileScanner for reader reader=hdfs://master:54310/hbase/data/default/usertable/667c1f2c34fe37d48ba25a789d8aba98/family/56200eedd338491786b04080265aa716, compression=gz, cacheConf=CacheConfig:enabled [cacheDataOnRead=true] [cacheDataOnWrite=false] [cacheIndexesOnWrite=false] [cacheBloomsOnWrite=false] [cacheEvictOnClose=false] [cacheCompressed=false][prefetchOnOpen=false], firstKey=user7000001172980900254/family:field0/1404254132804/Put, lastKey=user7999998402207300106/family:field9/1404254151918/Put, avgKeyLen=46, avgValueLen=100, entries=5457920, length=407788494, cur=null] to key /family:/LATEST_TIMESTAMP/DeleteFamily/vlen=0/mvcc=0
	at org.apache.hadoop.hbase.regionserver.StoreFileScanner.seek(StoreFileScanner.java:164)
	at org.apache.hadoop.hbase.regionserver.StoreScanner.seekScanners(StoreScanner.java:317)
	at org.apache.hadoop.hbase.regionserver.StoreScanner.<init>(StoreScanner.java:240)
	at org.apache.hadoop.hbase.regionserver.StoreScanner.<init>(StoreScanner.java:202)
	at org.apache.hadoop.hbase.regionserver.compactions.Compactor.createScanner(Compactor.java:257)
	at org.apache.hadoop.hbase.regionserver.compactions.DefaultCompactor.compact(DefaultCompactor.java:65)
	at org.apache.hadoop.hbase.regionserver.DefaultStoreEngine$DefaultCompactionContext.compact(DefaultStoreEngine.java:109)
	at org.apache.hadoop.hbase.regionserver.HStore.compact(HStore.java:1086)
	at org.apache.hadoop.hbase.regionserver.HRegion.compact(HRegion.java:1481)
	at org.apache.hadoop.hbase.regionserver.CompactSplitThread$CompactionRunner.run(CompactSplitThread.java:475)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1146)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:701)
Caused by: java.net.ConnectException: Call to master/9.1.143.58:54310 failed on connection exception: java.net.ConnectException: Connection refused
	at org.apache.hadoop.ipc.Client.wrapException(Client.java:1142)
	at org.apache.hadoop.ipc.Client.call(Client.java:1118)
	at org.apache.hadoop.ipc.RPC$Invoker.invoke(RPC.java:229)
	at com.sun.proxy.$Proxy8.getBlockLocations(Unknown Source)
	at sun.reflect.GeneratedMethodAccessor10.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:622)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:85)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:62)
	at com.sun.proxy.$Proxy8.getBlockLocations(Unknown Source)
	at org.apache.hadoop.hdfs.DFSClient.callGetBlockLocations(DFSClient.java:695)
	at org.apache.hadoop.hdfs.DFSClient$DFSInputStream.fetchLocatedBlocks(DFSClient.java:2003)
	at org.apache.hadoop.hdfs.DFSClient$DFSInputStream.openInfo(DFSClient.java:1975)
	at org.apache.hadoop.hdfs.DFSClient$DFSInputStream.chooseDataNode(DFSClient.java:2474)
	at org.apache.hadoop.hdfs.DFSClient$DFSInputStream.blockSeekTo(DFSClient.java:2252)
	at org.apache.hadoop.hdfs.DFSClient$DFSInputStream.read(DFSClient.java:2415)
	at java.io.DataInputStream.read(DataInputStream.java:149)
	at org.apache.hadoop.io.IOUtils.readFully(IOUtils.java:175)
	at org.apache.hadoop.hbase.io.hfile.HFileBlock$AbstractFSReader.readAtOffset(HFileBlock.java:1210)
	at org.apache.hadoop.hbase.io.hfile.HFileBlock$FSReaderV2.readBlockDataInternal(HFileBlock.java:1483)
	at org.apache.hadoop.hbase.io.hfile.HFileBlock$FSReaderV2.readBlockData(HFileBlock.java:1314)
	at org.apache.hadoop.hbase.io.hfile.HFileReaderV2.readBlock(HFileReaderV2.java:392)
	at org.apache.hadoop.hbase.io.hfile.HFileReaderV2$ScannerV2.seekTo(HFileReaderV2.java:803)
	at org.apache.hadoop.hbase.regionserver.StoreFileScanner.seekAtOrAfter(StoreFileScanner.java:244)
	at org.apache.hadoop.hbase.regionserver.StoreFileScanner.seek(StoreFileScanner.java:152)
	... 12 more
Caused by: java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:601)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:511)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:481)
	at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:457)
	at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:583)
	at org.apache.hadoop.ipc.Client$Connection.access$2200(Client.java:205)
	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1249)
	at org.apache.hadoop.ipc.Client.call(Client.java:1093)
	... 35 more
2014-07-01 15:50:54,243 DEBUG [regionserver60020-smallCompactions-1404253964856] regionserver.CompactSplitThread: CompactSplitThread Status: compaction_queue=(0:26), split_queue=0, merge_queue=0
2014-07-01 15:50:54,244 DEBUG [regionserver60020-smallCompactions-1404253964856] compactions.RatioBasedCompactionPolicy: Selecting compaction from 6 store files, 0 compacting, 6 eligible, 20 blocking
2014-07-01 15:50:54,244 DEBUG [regionserver60020-smallCompactions-1404253964856] compactions.ExploringCompactionPolicy: Exploring compaction algorithm has selected 6 files of size 934259017 starting at candidate #0 after considering 10 permutations with 9 in ratio
2014-07-01 15:50:54,244 DEBUG [regionserver60020-smallCompactions-1404253964856] regionserver.HStore: 667c1f2c34fe37d48ba25a789d8aba98 - family: Initiating major compaction
2014-07-01 15:50:54,245 INFO  [regionserver60020-smallCompactions-1404253964856] regionserver.HRegion: Starting compaction on family in region usertable,user7,1404228265526.667c1f2c34fe37d48ba25a789d8aba98.
2014-07-01 15:50:54,245 INFO  [regionserver60020-smallCompactions-1404253964856] regionserver.HStore: Starting compaction of 6 file(s) in family of usertable,user7,1404228265526.667c1f2c34fe37d48ba25a789d8aba98. into tmpdir=hdfs://master:54310/hbase/data/default/usertable/667c1f2c34fe37d48ba25a789d8aba98/.tmp, totalSize=891.0m
2014-07-01 15:50:54,245 DEBUG [regionserver60020-smallCompactions-1404253964856] compactions.Compactor: Compacting hdfs://master:54310/hbase/data/default/usertable/667c1f2c34fe37d48ba25a789d8aba98/family/56200eedd338491786b04080265aa716, keycount=545792, bloomtype=ROW, size=388.9m, encoding=NONE, seqNum=2915, earliestPutTs=1404228281826
2014-07-01 15:50:54,246 DEBUG [regionserver60020-smallCompactions-1404253964856] compactions.Compactor: Compacting hdfs://master:54310/hbase/data/default/usertable/667c1f2c34fe37d48ba25a789d8aba98/family/50f4de22ccd04828b18b93cc72b88964, keycount=147565, bloomtype=ROW, size=105.0m, encoding=NONE, seqNum=3442, earliestPutTs=1404254293026
2014-07-01 15:50:54,246 DEBUG [regionserver60020-smallCompactions-1404253964856] compactions.Compactor: Compacting hdfs://master:54310/hbase/data/default/usertable/667c1f2c34fe37d48ba25a789d8aba98/family/244e3443161840b5a95504a408c4edc2, keycount=155927, bloomtype=ROW, size=111.0m, encoding=NONE, seqNum=3881, earliestPutTs=1404254357203
2014-07-01 15:50:54,246 DEBUG [regionserver60020-smallCompactions-1404253964856] compactions.Compactor: Compacting hdfs://master:54310/hbase/data/default/usertable/667c1f2c34fe37d48ba25a789d8aba98/family/a5468df3646d43fda8dcfd231b11facb, keycount=176456, bloomtype=ROW, size=125.7m, encoding=NONE, seqNum=4315, earliestPutTs=1404254389941
2014-07-01 15:50:54,246 DEBUG [regionserver60020-smallCompactions-1404253964856] compactions.Compactor: Compacting hdfs://master:54310/hbase/data/default/usertable/667c1f2c34fe37d48ba25a789d8aba98/family/5c4ecd1e5f5f40a68639ae8651cdf308, keycount=132606, bloomtype=ROW, size=94.4m, encoding=NONE, seqNum=4527, earliestPutTs=1404254439322
2014-07-01 15:50:54,246 DEBUG [regionserver60020-smallCompactions-1404253964856] compactions.Compactor: Compacting hdfs://master:54310/hbase/data/default/usertable/667c1f2c34fe37d48ba25a789d8aba98/family/6c4e4b0a6c594b6db087ac5f958bca9c, keycount=92565, bloomtype=ROW, size=66.0m, encoding=NONE, seqNum=4678, earliestPutTs=1404254473587
2014-07-01 15:50:54,248 WARN  [regionserver60020-smallCompactions-1404253964856] hdfs.DFSClient: Failed to connect to /9.1.143.58:50010, add to deadNodes and continuejava.net.ConnectException: Connection refused
2014-07-01 15:50:54,248 INFO  [regionserver60020-smallCompactions-1404253964856] hdfs.DFSClient: Could not obtain blk_221234652107626381_29849 from any node: java.io.IOException: No live nodes contain current block. Will get new block locations from namenode and retry...
2014-07-01 15:50:54,738 INFO  [DataStreamer for file /hbase/WALs/slave1,60020,1404253868846/slave1%2C60020%2C1404253868846.1404254547570 block blk_-3161611795271398729_29985] ipc.Client: Retrying connect to server: sceplus-vm49.almaden.ibm.com/9.1.143.59:50020. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1 SECONDS)
2014-07-01 15:50:55,240 INFO  [org.apache.hadoop.hdfs.LeaseRenewer$1@4fae46d5] ipc.Client: Retrying connect to server: master/9.1.143.58:54310. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1 SECONDS)
2014-07-01 15:50:55,595 INFO  [regionserver60020] regionserver.HRegionServer: Received CLOSE for the region: 122e92eabfab38cb15a9965b21db25c5 ,which we are already trying to CLOSE, but not completed yet
2014-07-01 15:50:55,595 WARN  [regionserver60020] regionserver.HRegionServer: Failed to close usertable,user5,1404228265526.122e92eabfab38cb15a9965b21db25c5. - ignoring and continuing
org.apache.hadoop.hbase.regionserver.RegionAlreadyInTransitionException: The region 122e92eabfab38cb15a9965b21db25c5 was already closing. New CLOSE request is ignored.
	at org.apache.hadoop.hbase.regionserver.HRegionServer.closeRegion(HRegionServer.java:2586)
	at org.apache.hadoop.hbase.regionserver.HRegionServer.closeRegionIgnoreErrors(HRegionServer.java:2508)
	at org.apache.hadoop.hbase.regionserver.HRegionServer.closeUserRegions(HRegionServer.java:2098)
	at org.apache.hadoop.hbase.regionserver.HRegionServer.run(HRegionServer.java:894)
	at java.lang.Thread.run(Thread.java:701)
2014-07-01 15:50:55,596 INFO  [regionserver60020] regionserver.HRegionServer: Received CLOSE for the region: bb79ae773f71289e0745068f68a0abd9 ,which we are already trying to CLOSE, but not completed yet
2014-07-01 15:50:55,596 WARN  [regionserver60020] regionserver.HRegionServer: Failed to close usertable,user9,1404228265526.bb79ae773f71289e0745068f68a0abd9. - ignoring and continuing
org.apache.hadoop.hbase.regionserver.RegionAlreadyInTransitionException: The region bb79ae773f71289e0745068f68a0abd9 was already closing. New CLOSE request is ignored.
	at org.apache.hadoop.hbase.regionserver.HRegionServer.closeRegion(HRegionServer.java:2586)
	at org.apache.hadoop.hbase.regionserver.HRegionServer.closeRegionIgnoreErrors(HRegionServer.java:2508)
	at org.apache.hadoop.hbase.regionserver.HRegionServer.closeUserRegions(HRegionServer.java:2098)
	at org.apache.hadoop.hbase.regionserver.HRegionServer.run(HRegionServer.java:894)
	at java.lang.Thread.run(Thread.java:701)
2014-07-01 15:50:55,596 INFO  [regionserver60020] regionserver.HRegionServer: Received CLOSE for the region: 9cef9e7d34f96f1d9b111be50040a208 ,which we are already trying to CLOSE, but not completed yet
2014-07-01 15:50:55,596 WARN  [regionserver60020] regionserver.HRegionServer: Failed to close usertable,,1404228265525.9cef9e7d34f96f1d9b111be50040a208. - ignoring and continuing
org.apache.hadoop.hbase.regionserver.RegionAlreadyInTransitionException: The region 9cef9e7d34f96f1d9b111be50040a208 was already closing. New CLOSE request is ignored.
	at org.apache.hadoop.hbase.regionserver.HRegionServer.closeRegion(HRegionServer.java:2586)
	at org.apache.hadoop.hbase.regionserver.HRegionServer.closeRegionIgnoreErrors(HRegionServer.java:2508)
	at org.apache.hadoop.hbase.regionserver.HRegionServer.closeUserRegions(HRegionServer.java:2098)
	at org.apache.hadoop.hbase.regionserver.HRegionServer.run(HRegionServer.java:894)
	at java.lang.Thread.run(Thread.java:701)
2014-07-01 15:50:55,597 INFO  [regionserver60020] regionserver.HRegionServer: Received CLOSE for the region: 93253e37c284f3174309fede3f32339f ,which we are already trying to CLOSE, but not completed yet
2014-07-01 15:50:55,597 WARN  [regionserver60020] regionserver.HRegionServer: Failed to close usertable,user3,1404228265526.93253e37c284f3174309fede3f32339f. - ignoring and continuing
org.apache.hadoop.hbase.regionserver.RegionAlreadyInTransitionException: The region 93253e37c284f3174309fede3f32339f was already closing. New CLOSE request is ignored.
	at org.apache.hadoop.hbase.regionserver.HRegionServer.closeRegion(HRegionServer.java:2586)
	at org.apache.hadoop.hbase.regionserver.HRegionServer.closeRegionIgnoreErrors(HRegionServer.java:2508)
	at org.apache.hadoop.hbase.regionserver.HRegionServer.closeUserRegions(HRegionServer.java:2098)
	at org.apache.hadoop.hbase.regionserver.HRegionServer.run(HRegionServer.java:894)
	at java.lang.Thread.run(Thread.java:701)
2014-07-01 15:50:55,597 INFO  [regionserver60020] regionserver.HRegionServer: Received CLOSE for the region: 667c1f2c34fe37d48ba25a789d8aba98 ,which we are already trying to CLOSE, but not completed yet
2014-07-01 15:50:55,597 WARN  [regionserver60020] regionserver.HRegionServer: Failed to close usertable,user7,1404228265526.667c1f2c34fe37d48ba25a789d8aba98. - ignoring and continuing
org.apache.hadoop.hbase.regionserver.RegionAlreadyInTransitionException: The region 667c1f2c34fe37d48ba25a789d8aba98 was already closing. New CLOSE request is ignored.
	at org.apache.hadoop.hbase.regionserver.HRegionServer.closeRegion(HRegionServer.java:2586)
	at org.apache.hadoop.hbase.regionserver.HRegionServer.closeRegionIgnoreErrors(HRegionServer.java:2508)
	at org.apache.hadoop.hbase.regionserver.HRegionServer.closeUserRegions(HRegionServer.java:2098)
	at org.apache.hadoop.hbase.regionserver.HRegionServer.run(HRegionServer.java:894)
	at java.lang.Thread.run(Thread.java:701)
2014-07-01 15:50:55,597 DEBUG [regionserver60020] regionserver.HRegionServer: Waiting on 122e92eabfab38cb15a9965b21db25c5, bb79ae773f71289e0745068f68a0abd9, 9cef9e7d34f96f1d9b111be50040a208, 93253e37c284f3174309fede3f32339f, 667c1f2c34fe37d48ba25a789d8aba98
2014-07-01 15:50:55,740 INFO  [DataStreamer for file /hbase/WALs/slave1,60020,1404253868846/slave1%2C60020%2C1404253868846.1404254547570 block blk_-3161611795271398729_29985] ipc.Client: Retrying connect to server: sceplus-vm49.almaden.ibm.com/9.1.143.59:50020. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1 SECONDS)
2014-07-01 15:50:56,241 INFO  [org.apache.hadoop.hdfs.LeaseRenewer$1@4fae46d5] ipc.Client: Retrying connect to server: master/9.1.143.58:54310. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1 SECONDS)
2014-07-01 15:50:56,741 INFO  [DataStreamer for file /hbase/WALs/slave1,60020,1404253868846/slave1%2C60020%2C1404253868846.1404254547570 block blk_-3161611795271398729_29985] ipc.Client: Retrying connect to server: sceplus-vm49.almaden.ibm.com/9.1.143.59:50020. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1 SECONDS)
2014-07-01 15:50:57,242 INFO  [org.apache.hadoop.hdfs.LeaseRenewer$1@4fae46d5] ipc.Client: Retrying connect to server: master/9.1.143.58:54310. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1 SECONDS)
2014-07-01 15:50:57,741 INFO  [DataStreamer for file /hbase/WALs/slave1,60020,1404253868846/slave1%2C60020%2C1404253868846.1404254547570 block blk_-3161611795271398729_29985] ipc.Client: Retrying connect to server: sceplus-vm49.almaden.ibm.com/9.1.143.59:50020. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1 SECONDS)
2014-07-01 15:50:58,243 INFO  [org.apache.hadoop.hdfs.LeaseRenewer$1@4fae46d5] ipc.Client: Retrying connect to server: master/9.1.143.58:54310. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1 SECONDS)
2014-07-01 15:50:58,600 INFO  [regionserver60020] regionserver.HRegionServer: Received CLOSE for the region: 122e92eabfab38cb15a9965b21db25c5 ,which we are already trying to CLOSE, but not completed yet
2014-07-01 15:50:58,600 WARN  [regionserver60020] regionserver.HRegionServer: Failed to close usertable,user5,1404228265526.122e92eabfab38cb15a9965b21db25c5. - ignoring and continuing
org.apache.hadoop.hbase.regionserver.RegionAlreadyInTransitionException: The region 122e92eabfab38cb15a9965b21db25c5 was already closing. New CLOSE request is ignored.
	at org.apache.hadoop.hbase.regionserver.HRegionServer.closeRegion(HRegionServer.java:2586)
	at org.apache.hadoop.hbase.regionserver.HRegionServer.closeRegionIgnoreErrors(HRegionServer.java:2508)
	at org.apache.hadoop.hbase.regionserver.HRegionServer.closeUserRegions(HRegionServer.java:2098)
	at org.apache.hadoop.hbase.regionserver.HRegionServer.run(HRegionServer.java:894)
	at java.lang.Thread.run(Thread.java:701)
2014-07-01 15:50:58,601 INFO  [regionserver60020] regionserver.HRegionServer: Received CLOSE for the region: bb79ae773f71289e0745068f68a0abd9 ,which we are already trying to CLOSE, but not completed yet
2014-07-01 15:50:58,601 WARN  [regionserver60020] regionserver.HRegionServer: Failed to close usertable,user9,1404228265526.bb79ae773f71289e0745068f68a0abd9. - ignoring and continuing
org.apache.hadoop.hbase.regionserver.RegionAlreadyInTransitionException: The region bb79ae773f71289e0745068f68a0abd9 was already closing. New CLOSE request is ignored.
	at org.apache.hadoop.hbase.regionserver.HRegionServer.closeRegion(HRegionServer.java:2586)
	at org.apache.hadoop.hbase.regionserver.HRegionServer.closeRegionIgnoreErrors(HRegionServer.java:2508)
	at org.apache.hadoop.hbase.regionserver.HRegionServer.closeUserRegions(HRegionServer.java:2098)
	at org.apache.hadoop.hbase.regionserver.HRegionServer.run(HRegionServer.java:894)
	at java.lang.Thread.run(Thread.java:701)
2014-07-01 15:50:58,601 INFO  [regionserver60020] regionserver.HRegionServer: Received CLOSE for the region: 9cef9e7d34f96f1d9b111be50040a208 ,which we are already trying to CLOSE, but not completed yet
2014-07-01 15:50:58,601 WARN  [regionserver60020] regionserver.HRegionServer: Failed to close usertable,,1404228265525.9cef9e7d34f96f1d9b111be50040a208. - ignoring and continuing
org.apache.hadoop.hbase.regionserver.RegionAlreadyInTransitionException: The region 9cef9e7d34f96f1d9b111be50040a208 was already closing. New CLOSE request is ignored.
	at org.apache.hadoop.hbase.regionserver.HRegionServer.closeRegion(HRegionServer.java:2586)
	at org.apache.hadoop.hbase.regionserver.HRegionServer.closeRegionIgnoreErrors(HRegionServer.java:2508)
	at org.apache.hadoop.hbase.regionserver.HRegionServer.closeUserRegions(HRegionServer.java:2098)
	at org.apache.hadoop.hbase.regionserver.HRegionServer.run(HRegionServer.java:894)
	at java.lang.Thread.run(Thread.java:701)
2014-07-01 15:50:58,601 INFO  [regionserver60020] regionserver.HRegionServer: Received CLOSE for the region: 93253e37c284f3174309fede3f32339f ,which we are already trying to CLOSE, but not completed yet
2014-07-01 15:50:58,601 WARN  [regionserver60020] regionserver.HRegionServer: Failed to close usertable,user3,1404228265526.93253e37c284f3174309fede3f32339f. - ignoring and continuing
org.apache.hadoop.hbase.regionserver.RegionAlreadyInTransitionException: The region 93253e37c284f3174309fede3f32339f was already closing. New CLOSE request is ignored.
	at org.apache.hadoop.hbase.regionserver.HRegionServer.closeRegion(HRegionServer.java:2586)
	at org.apache.hadoop.hbase.regionserver.HRegionServer.closeRegionIgnoreErrors(HRegionServer.java:2508)
	at org.apache.hadoop.hbase.regionserver.HRegionServer.closeUserRegions(HRegionServer.java:2098)
	at org.apache.hadoop.hbase.regionserver.HRegionServer.run(HRegionServer.java:894)
	at java.lang.Thread.run(Thread.java:701)
2014-07-01 15:50:58,602 INFO  [regionserver60020] regionserver.HRegionServer: Received CLOSE for the region: 667c1f2c34fe37d48ba25a789d8aba98 ,which we are already trying to CLOSE, but not completed yet
2014-07-01 15:50:58,602 WARN  [regionserver60020] regionserver.HRegionServer: Failed to close usertable,user7,1404228265526.667c1f2c34fe37d48ba25a789d8aba98. - ignoring and continuing
org.apache.hadoop.hbase.regionserver.RegionAlreadyInTransitionException: The region 667c1f2c34fe37d48ba25a789d8aba98 was already closing. New CLOSE request is ignored.
	at org.apache.hadoop.hbase.regionserver.HRegionServer.closeRegion(HRegionServer.java:2586)
	at org.apache.hadoop.hbase.regionserver.HRegionServer.closeRegionIgnoreErrors(HRegionServer.java:2508)
	at org.apache.hadoop.hbase.regionserver.HRegionServer.closeUserRegions(HRegionServer.java:2098)
	at org.apache.hadoop.hbase.regionserver.HRegionServer.run(HRegionServer.java:894)
	at java.lang.Thread.run(Thread.java:701)
2014-07-01 15:50:58,602 DEBUG [regionserver60020] regionserver.HRegionServer: Waiting on 122e92eabfab38cb15a9965b21db25c5, bb79ae773f71289e0745068f68a0abd9, 9cef9e7d34f96f1d9b111be50040a208, 93253e37c284f3174309fede3f32339f, 667c1f2c34fe37d48ba25a789d8aba98
2014-07-01 15:50:58,742 INFO  [DataStreamer for file /hbase/WALs/slave1,60020,1404253868846/slave1%2C60020%2C1404253868846.1404254547570 block blk_-3161611795271398729_29985] ipc.Client: Retrying connect to server: sceplus-vm49.almaden.ibm.com/9.1.143.59:50020. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1 SECONDS)
2014-07-01 15:50:59,245 INFO  [org.apache.hadoop.hdfs.LeaseRenewer$1@4fae46d5] ipc.Client: Retrying connect to server: master/9.1.143.58:54310. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1 SECONDS)
2014-07-01 15:50:59,742 INFO  [DataStreamer for file /hbase/WALs/slave1,60020,1404253868846/slave1%2C60020%2C1404253868846.1404254547570 block blk_-3161611795271398729_29985] ipc.Client: Retrying connect to server: sceplus-vm49.almaden.ibm.com/9.1.143.59:50020. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1 SECONDS)
2014-07-01 15:51:00,246 INFO  [org.apache.hadoop.hdfs.LeaseRenewer$1@4fae46d5] ipc.Client: Retrying connect to server: master/9.1.143.58:54310. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1 SECONDS)
2014-07-01 15:51:00,743 INFO  [DataStreamer for file /hbase/WALs/slave1,60020,1404253868846/slave1%2C60020%2C1404253868846.1404254547570 block blk_-3161611795271398729_29985] ipc.Client: Retrying connect to server: sceplus-vm49.almaden.ibm.com/9.1.143.59:50020. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1 SECONDS)
2014-07-01 15:51:00,744 WARN  [DataStreamer for file /hbase/WALs/slave1,60020,1404253868846/slave1%2C60020%2C1404253868846.1404254547570 block blk_-3161611795271398729_29985] hdfs.DFSClient: Failed recovery attempt #3 from primary datanode 9.1.143.59:50010
java.net.ConnectException: Call to sceplus-vm49.almaden.ibm.com/9.1.143.59:50020 failed on connection exception: java.net.ConnectException: Connection refused
	at org.apache.hadoop.ipc.Client.wrapException(Client.java:1142)
	at org.apache.hadoop.ipc.Client.call(Client.java:1118)
	at org.apache.hadoop.ipc.RPC$Invoker.invoke(RPC.java:229)
	at com.sun.proxy.$Proxy13.getProtocolVersion(Unknown Source)
	at org.apache.hadoop.ipc.RPC.checkVersion(RPC.java:422)
	at org.apache.hadoop.ipc.RPC.getProxy(RPC.java:414)
	at org.apache.hadoop.ipc.RPC.getProxy(RPC.java:392)
	at org.apache.hadoop.hdfs.DFSClient.createClientDatanodeProtocolProxy(DFSClient.java:201)
	at org.apache.hadoop.hdfs.DFSClient$DFSOutputStream.processDatanodeError(DFSClient.java:3317)
	at org.apache.hadoop.hdfs.DFSClient$DFSOutputStream.access$2200(DFSClient.java:2783)
	at org.apache.hadoop.hdfs.DFSClient$DFSOutputStream$DataStreamer.run(DFSClient.java:2987)
Caused by: java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:601)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:511)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:481)
	at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:457)
	at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:583)
	at org.apache.hadoop.ipc.Client$Connection.access$2200(Client.java:205)
	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1249)
	at org.apache.hadoop.ipc.Client.call(Client.java:1093)
	... 9 more
2014-07-01 15:51:00,744 WARN  [DataStreamer for file /hbase/WALs/slave1,60020,1404253868846/slave1%2C60020%2C1404253868846.1404254547570 block blk_-3161611795271398729_29985] hdfs.DFSClient: Error Recovery for block blk_-3161611795271398729_29985 failed  because recovery from primary datanode 9.1.143.59:50010 failed 4 times.  Pipeline was 9.1.143.59:50010. Will retry...
2014-07-01 15:51:01,247 INFO  [org.apache.hadoop.hdfs.LeaseRenewer$1@4fae46d5] ipc.Client: Retrying connect to server: master/9.1.143.58:54310. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1 SECONDS)
2014-07-01 15:51:01,605 INFO  [regionserver60020] regionserver.HRegionServer: Received CLOSE for the region: 122e92eabfab38cb15a9965b21db25c5 ,which we are already trying to CLOSE, but not completed yet
2014-07-01 15:51:01,605 WARN  [regionserver60020] regionserver.HRegionServer: Failed to close usertable,user5,1404228265526.122e92eabfab38cb15a9965b21db25c5. - ignoring and continuing
org.apache.hadoop.hbase.regionserver.RegionAlreadyInTransitionException: The region 122e92eabfab38cb15a9965b21db25c5 was already closing. New CLOSE request is ignored.
	at org.apache.hadoop.hbase.regionserver.HRegionServer.closeRegion(HRegionServer.java:2586)
	at org.apache.hadoop.hbase.regionserver.HRegionServer.closeRegionIgnoreErrors(HRegionServer.java:2508)
	at org.apache.hadoop.hbase.regionserver.HRegionServer.closeUserRegions(HRegionServer.java:2098)
	at org.apache.hadoop.hbase.regionserver.HRegionServer.run(HRegionServer.java:894)
	at java.lang.Thread.run(Thread.java:701)
2014-07-01 15:51:01,606 INFO  [regionserver60020] regionserver.HRegionServer: Received CLOSE for the region: bb79ae773f71289e0745068f68a0abd9 ,which we are already trying to CLOSE, but not completed yet
2014-07-01 15:51:01,606 WARN  [regionserver60020] regionserver.HRegionServer: Failed to close usertable,user9,1404228265526.bb79ae773f71289e0745068f68a0abd9. - ignoring and continuing
org.apache.hadoop.hbase.regionserver.RegionAlreadyInTransitionException: The region bb79ae773f71289e0745068f68a0abd9 was already closing. New CLOSE request is ignored.
	at org.apache.hadoop.hbase.regionserver.HRegionServer.closeRegion(HRegionServer.java:2586)
	at org.apache.hadoop.hbase.regionserver.HRegionServer.closeRegionIgnoreErrors(HRegionServer.java:2508)
	at org.apache.hadoop.hbase.regionserver.HRegionServer.closeUserRegions(HRegionServer.java:2098)
	at org.apache.hadoop.hbase.regionserver.HRegionServer.run(HRegionServer.java:894)
	at java.lang.Thread.run(Thread.java:701)
2014-07-01 15:51:01,606 INFO  [regionserver60020] regionserver.HRegionServer: Received CLOSE for the region: 9cef9e7d34f96f1d9b111be50040a208 ,which we are already trying to CLOSE, but not completed yet
2014-07-01 15:51:01,606 WARN  [regionserver60020] regionserver.HRegionServer: Failed to close usertable,,1404228265525.9cef9e7d34f96f1d9b111be50040a208. - ignoring and continuing
org.apache.hadoop.hbase.regionserver.RegionAlreadyInTransitionException: The region 9cef9e7d34f96f1d9b111be50040a208 was already closing. New CLOSE request is ignored.
	at org.apache.hadoop.hbase.regionserver.HRegionServer.closeRegion(HRegionServer.java:2586)
	at org.apache.hadoop.hbase.regionserver.HRegionServer.closeRegionIgnoreErrors(HRegionServer.java:2508)
	at org.apache.hadoop.hbase.regionserver.HRegionServer.closeUserRegions(HRegionServer.java:2098)
	at org.apache.hadoop.hbase.regionserver.HRegionServer.run(HRegionServer.java:894)
	at java.lang.Thread.run(Thread.java:701)
2014-07-01 15:51:01,606 INFO  [regionserver60020] regionserver.HRegionServer: Received CLOSE for the region: 93253e37c284f3174309fede3f32339f ,which we are already trying to CLOSE, but not completed yet
2014-07-01 15:51:01,606 WARN  [regionserver60020] regionserver.HRegionServer: Failed to close usertable,user3,1404228265526.93253e37c284f3174309fede3f32339f. - ignoring and continuing
org.apache.hadoop.hbase.regionserver.RegionAlreadyInTransitionException: The region 93253e37c284f3174309fede3f32339f was already closing. New CLOSE request is ignored.
	at org.apache.hadoop.hbase.regionserver.HRegionServer.closeRegion(HRegionServer.java:2586)
	at org.apache.hadoop.hbase.regionserver.HRegionServer.closeRegionIgnoreErrors(HRegionServer.java:2508)
	at org.apache.hadoop.hbase.regionserver.HRegionServer.closeUserRegions(HRegionServer.java:2098)
	at org.apache.hadoop.hbase.regionserver.HRegionServer.run(HRegionServer.java:894)
	at java.lang.Thread.run(Thread.java:701)
2014-07-01 15:51:01,607 INFO  [regionserver60020] regionserver.HRegionServer: Received CLOSE for the region: 667c1f2c34fe37d48ba25a789d8aba98 ,which we are already trying to CLOSE, but not completed yet
2014-07-01 15:51:01,607 WARN  [regionserver60020] regionserver.HRegionServer: Failed to close usertable,user7,1404228265526.667c1f2c34fe37d48ba25a789d8aba98. - ignoring and continuing
org.apache.hadoop.hbase.regionserver.RegionAlreadyInTransitionException: The region 667c1f2c34fe37d48ba25a789d8aba98 was already closing. New CLOSE request is ignored.
	at org.apache.hadoop.hbase.regionserver.HRegionServer.closeRegion(HRegionServer.java:2586)
	at org.apache.hadoop.hbase.regionserver.HRegionServer.closeRegionIgnoreErrors(HRegionServer.java:2508)
	at org.apache.hadoop.hbase.regionserver.HRegionServer.closeUserRegions(HRegionServer.java:2098)
	at org.apache.hadoop.hbase.regionserver.HRegionServer.run(HRegionServer.java:894)
	at java.lang.Thread.run(Thread.java:701)
2014-07-01 15:51:01,607 DEBUG [regionserver60020] regionserver.HRegionServer: Waiting on 122e92eabfab38cb15a9965b21db25c5, bb79ae773f71289e0745068f68a0abd9, 9cef9e7d34f96f1d9b111be50040a208, 93253e37c284f3174309fede3f32339f, 667c1f2c34fe37d48ba25a789d8aba98
2014-07-01 15:51:02,248 INFO  [org.apache.hadoop.hdfs.LeaseRenewer$1@4fae46d5] ipc.Client: Retrying connect to server: master/9.1.143.58:54310. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1 SECONDS)
2014-07-01 15:51:02,746 INFO  [DataStreamer for file /hbase/WALs/slave1,60020,1404253868846/slave1%2C60020%2C1404253868846.1404254547570 block blk_-3161611795271398729_29985] ipc.Client: Retrying connect to server: sceplus-vm49.almaden.ibm.com/9.1.143.59:50020. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1 SECONDS)
2014-07-01 15:51:03,249 INFO  [org.apache.hadoop.hdfs.LeaseRenewer$1@4fae46d5] ipc.Client: Retrying connect to server: master/9.1.143.58:54310. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1 SECONDS)
2014-07-01 15:51:03,747 INFO  [DataStreamer for file /hbase/WALs/slave1,60020,1404253868846/slave1%2C60020%2C1404253868846.1404254547570 block blk_-3161611795271398729_29985] ipc.Client: Retrying connect to server: sceplus-vm49.almaden.ibm.com/9.1.143.59:50020. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1 SECONDS)
2014-07-01 15:51:04,250 INFO  [org.apache.hadoop.hdfs.LeaseRenewer$1@4fae46d5] ipc.Client: Retrying connect to server: master/9.1.143.58:54310. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1 SECONDS)
2014-07-01 15:51:04,251 WARN  [org.apache.hadoop.hdfs.LeaseRenewer$1@4fae46d5] hdfs.LeaseRenewer: Failed to renew lease for [DFSClient_hb_rs_slave1,60020,1404253868846_-130361673_31] for 30 seconds.  Will retry shortly ...
java.net.ConnectException: Call to master/9.1.143.58:54310 failed on connection exception: java.net.ConnectException: Connection refused
	at org.apache.hadoop.ipc.Client.wrapException(Client.java:1142)
	at org.apache.hadoop.ipc.Client.call(Client.java:1118)
	at org.apache.hadoop.ipc.RPC$Invoker.invoke(RPC.java:229)
	at com.sun.proxy.$Proxy8.renewLease(Unknown Source)
	at sun.reflect.GeneratedMethodAccessor29.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:622)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:85)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:62)
	at com.sun.proxy.$Proxy8.renewLease(Unknown Source)
	at sun.reflect.GeneratedMethodAccessor29.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:622)
	at org.apache.hadoop.hbase.fs.HFileSystem$1.invoke(HFileSystem.java:294)
	at com.sun.proxy.$Proxy9.renewLease(Unknown Source)
	at org.apache.hadoop.hdfs.DFSClient.renewLease(DFSClient.java:379)
	at org.apache.hadoop.hdfs.LeaseRenewer.renew(LeaseRenewer.java:378)
	at org.apache.hadoop.hdfs.LeaseRenewer.run(LeaseRenewer.java:400)
	at org.apache.hadoop.hdfs.LeaseRenewer.access$600(LeaseRenewer.java:69)
	at org.apache.hadoop.hdfs.LeaseRenewer$1.run(LeaseRenewer.java:273)
	at java.lang.Thread.run(Thread.java:701)
Caused by: java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:601)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:511)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:481)
	at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:457)
	at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:583)
	at org.apache.hadoop.ipc.Client$Connection.access$2200(Client.java:205)
	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1249)
	at org.apache.hadoop.ipc.Client.call(Client.java:1093)
	... 19 more
2014-07-01 15:51:04,251 WARN  [RS_CLOSE_REGION-slave1:60020-0] regionserver.HStore: Failed flushing store file, retrying num=0
java.net.ConnectException: Call to master/9.1.143.58:54310 failed on connection exception: java.net.ConnectException: Connection refused
	at org.apache.hadoop.ipc.Client.wrapException(Client.java:1142)
	at org.apache.hadoop.ipc.Client.call(Client.java:1118)
	at org.apache.hadoop.ipc.RPC$Invoker.invoke(RPC.java:229)
	at com.sun.proxy.$Proxy8.getFileInfo(Unknown Source)
	at sun.reflect.GeneratedMethodAccessor2.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:622)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:85)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:62)
	at com.sun.proxy.$Proxy8.getFileInfo(Unknown Source)
	at sun.reflect.GeneratedMethodAccessor2.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:622)
	at org.apache.hadoop.hbase.fs.HFileSystem$1.invoke(HFileSystem.java:294)
	at com.sun.proxy.$Proxy9.getFileInfo(Unknown Source)
	at org.apache.hadoop.hdfs.DFSClient.getFileInfo(DFSClient.java:1059)
	at org.apache.hadoop.hdfs.DistributedFileSystem.getFileStatus(DistributedFileSystem.java:554)
	at org.apache.hadoop.fs.FilterFileSystem.getFileStatus(FilterFileSystem.java:255)
	at org.apache.hadoop.fs.FileSystem.exists(FileSystem.java:788)
	at org.apache.hadoop.hbase.regionserver.StoreFile$WriterBuilder.build(StoreFile.java:603)
	at org.apache.hadoop.hbase.regionserver.HStore.createWriterInTmp(HStore.java:889)
	at org.apache.hadoop.hbase.regionserver.DefaultStoreFlusher.flushSnapshot(DefaultStoreFlusher.java:69)
	at org.apache.hadoop.hbase.regionserver.HStore.flushCache(HStore.java:798)
	at org.apache.hadoop.hbase.regionserver.HStore$StoreFlusherImpl.flushCache(HStore.java:1949)
	at org.apache.hadoop.hbase.regionserver.HRegion.internalFlushcache(HRegion.java:1776)
	at org.apache.hadoop.hbase.regionserver.HRegion.internalFlushcache(HRegion.java:1658)
	at org.apache.hadoop.hbase.regionserver.HRegion.doClose(HRegion.java:1119)
	at org.apache.hadoop.hbase.regionserver.HRegion.close(HRegion.java:1085)
	at org.apache.hadoop.hbase.regionserver.handler.CloseRegionHandler.process(CloseRegionHandler.java:147)
	at org.apache.hadoop.hbase.executor.EventHandler.run(EventHandler.java:128)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1146)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:701)
Caused by: java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:601)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:511)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:481)
	at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:457)
	at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:583)
	at org.apache.hadoop.ipc.Client$Connection.access$2200(Client.java:205)
	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1249)
	at org.apache.hadoop.ipc.Client.call(Client.java:1093)
	at org.apache.hadoop.ipc.RPC$Invoker.invoke(RPC.java:229)
	at com.sun.proxy.$Proxy8.renewLease(Unknown Source)
	at sun.reflect.GeneratedMethodAccessor29.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:622)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:85)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:62)
	at com.sun.proxy.$Proxy8.renewLease(Unknown Source)
	at sun.reflect.GeneratedMethodAccessor29.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:622)
	at org.apache.hadoop.hbase.fs.HFileSystem$1.invoke(HFileSystem.java:294)
	at com.sun.proxy.$Proxy9.renewLease(Unknown Source)
	at org.apache.hadoop.hdfs.DFSClient.renewLease(DFSClient.java:379)
	at org.apache.hadoop.hdfs.LeaseRenewer.renew(LeaseRenewer.java:378)
	at org.apache.hadoop.hdfs.LeaseRenewer.run(LeaseRenewer.java:400)
	at org.apache.hadoop.hdfs.LeaseRenewer.access$600(LeaseRenewer.java:69)
	at org.apache.hadoop.hdfs.LeaseRenewer$1.run(LeaseRenewer.java:273)
	... 1 more
2014-07-01 15:51:04,251 WARN  [RS_CLOSE_REGION-slave1:60020-2] regionserver.HStore: Failed flushing store file, retrying num=2
java.net.ConnectException: Call to master/9.1.143.58:54310 failed on connection exception: java.net.ConnectException: Connection refused
	at org.apache.hadoop.ipc.Client.wrapException(Client.java:1142)
	at org.apache.hadoop.ipc.Client.call(Client.java:1118)
	at org.apache.hadoop.ipc.RPC$Invoker.invoke(RPC.java:229)
	at com.sun.proxy.$Proxy8.getFileInfo(Unknown Source)
	at sun.reflect.GeneratedMethodAccessor2.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:622)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:85)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:62)
	at com.sun.proxy.$Proxy8.getFileInfo(Unknown Source)
	at sun.reflect.GeneratedMethodAccessor2.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:622)
	at org.apache.hadoop.hbase.fs.HFileSystem$1.invoke(HFileSystem.java:294)
	at com.sun.proxy.$Proxy9.getFileInfo(Unknown Source)
	at org.apache.hadoop.hdfs.DFSClient.getFileInfo(DFSClient.java:1059)
	at org.apache.hadoop.hdfs.DistributedFileSystem.getFileStatus(DistributedFileSystem.java:554)
	at org.apache.hadoop.fs.FilterFileSystem.getFileStatus(FilterFileSystem.java:255)
	at org.apache.hadoop.fs.FileSystem.exists(FileSystem.java:788)
	at org.apache.hadoop.hbase.regionserver.StoreFile$WriterBuilder.build(StoreFile.java:603)
	at org.apache.hadoop.hbase.regionserver.HStore.createWriterInTmp(HStore.java:889)
	at org.apache.hadoop.hbase.regionserver.DefaultStoreFlusher.flushSnapshot(DefaultStoreFlusher.java:69)
	at org.apache.hadoop.hbase.regionserver.HStore.flushCache(HStore.java:798)
	at org.apache.hadoop.hbase.regionserver.HStore$StoreFlusherImpl.flushCache(HStore.java:1949)
	at org.apache.hadoop.hbase.regionserver.HRegion.internalFlushcache(HRegion.java:1776)
	at org.apache.hadoop.hbase.regionserver.HRegion.internalFlushcache(HRegion.java:1658)
	at org.apache.hadoop.hbase.regionserver.HRegion.doClose(HRegion.java:1119)
	at org.apache.hadoop.hbase.regionserver.HRegion.close(HRegion.java:1085)
	at org.apache.hadoop.hbase.regionserver.handler.CloseRegionHandler.process(CloseRegionHandler.java:147)
	at org.apache.hadoop.hbase.executor.EventHandler.run(EventHandler.java:128)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1146)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:701)
Caused by: java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:601)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:511)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:481)
	at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:457)
	at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:583)
	at org.apache.hadoop.ipc.Client$Connection.access$2200(Client.java:205)
	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1249)
	at org.apache.hadoop.ipc.Client.call(Client.java:1093)
	at org.apache.hadoop.ipc.RPC$Invoker.invoke(RPC.java:229)
	at com.sun.proxy.$Proxy8.renewLease(Unknown Source)
	at sun.reflect.GeneratedMethodAccessor29.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:622)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:85)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:62)
	at com.sun.proxy.$Proxy8.renewLease(Unknown Source)
	at sun.reflect.GeneratedMethodAccessor29.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:622)
	at org.apache.hadoop.hbase.fs.HFileSystem$1.invoke(HFileSystem.java:294)
	at com.sun.proxy.$Proxy9.renewLease(Unknown Source)
	at org.apache.hadoop.hdfs.DFSClient.renewLease(DFSClient.java:379)
	at org.apache.hadoop.hdfs.LeaseRenewer.renew(LeaseRenewer.java:378)
	at org.apache.hadoop.hdfs.LeaseRenewer.run(LeaseRenewer.java:400)
	at org.apache.hadoop.hdfs.LeaseRenewer.access$600(LeaseRenewer.java:69)
	at org.apache.hadoop.hdfs.LeaseRenewer$1.run(LeaseRenewer.java:273)
	... 1 more
2014-07-01 15:51:04,610 INFO  [regionserver60020] regionserver.HRegionServer: Received CLOSE for the region: 122e92eabfab38cb15a9965b21db25c5 ,which we are already trying to CLOSE, but not completed yet
2014-07-01 15:51:04,610 WARN  [regionserver60020] regionserver.HRegionServer: Failed to close usertable,user5,1404228265526.122e92eabfab38cb15a9965b21db25c5. - ignoring and continuing
org.apache.hadoop.hbase.regionserver.RegionAlreadyInTransitionException: The region 122e92eabfab38cb15a9965b21db25c5 was already closing. New CLOSE request is ignored.
	at org.apache.hadoop.hbase.regionserver.HRegionServer.closeRegion(HRegionServer.java:2586)
	at org.apache.hadoop.hbase.regionserver.HRegionServer.closeRegionIgnoreErrors(HRegionServer.java:2508)
	at org.apache.hadoop.hbase.regionserver.HRegionServer.closeUserRegions(HRegionServer.java:2098)
	at org.apache.hadoop.hbase.regionserver.HRegionServer.run(HRegionServer.java:894)
	at java.lang.Thread.run(Thread.java:701)
2014-07-01 15:51:04,611 INFO  [regionserver60020] regionserver.HRegionServer: Received CLOSE for the region: bb79ae773f71289e0745068f68a0abd9 ,which we are already trying to CLOSE, but not completed yet
2014-07-01 15:51:04,612 WARN  [regionserver60020] regionserver.HRegionServer: Failed to close usertable,user9,1404228265526.bb79ae773f71289e0745068f68a0abd9. - ignoring and continuing
org.apache.hadoop.hbase.regionserver.RegionAlreadyInTransitionException: The region bb79ae773f71289e0745068f68a0abd9 was already closing. New CLOSE request is ignored.
	at org.apache.hadoop.hbase.regionserver.HRegionServer.closeRegion(HRegionServer.java:2586)
	at org.apache.hadoop.hbase.regionserver.HRegionServer.closeRegionIgnoreErrors(HRegionServer.java:2508)
	at org.apache.hadoop.hbase.regionserver.HRegionServer.closeUserRegions(HRegionServer.java:2098)
	at org.apache.hadoop.hbase.regionserver.HRegionServer.run(HRegionServer.java:894)
	at java.lang.Thread.run(Thread.java:701)
2014-07-01 15:51:04,613 INFO  [regionserver60020] regionserver.HRegionServer: Received CLOSE for the region: 9cef9e7d34f96f1d9b111be50040a208 ,which we are already trying to CLOSE, but not completed yet
2014-07-01 15:51:04,613 WARN  [regionserver60020] regionserver.HRegionServer: Failed to close usertable,,1404228265525.9cef9e7d34f96f1d9b111be50040a208. - ignoring and continuing
org.apache.hadoop.hbase.regionserver.RegionAlreadyInTransitionException: The region 9cef9e7d34f96f1d9b111be50040a208 was already closing. New CLOSE request is ignored.
	at org.apache.hadoop.hbase.regionserver.HRegionServer.closeRegion(HRegionServer.java:2586)
	at org.apache.hadoop.hbase.regionserver.HRegionServer.closeRegionIgnoreErrors(HRegionServer.java:2508)
	at org.apache.hadoop.hbase.regionserver.HRegionServer.closeUserRegions(HRegionServer.java:2098)
	at org.apache.hadoop.hbase.regionserver.HRegionServer.run(HRegionServer.java:894)
	at java.lang.Thread.run(Thread.java:701)
2014-07-01 15:51:04,613 INFO  [regionserver60020] regionserver.HRegionServer: Received CLOSE for the region: 93253e37c284f3174309fede3f32339f ,which we are already trying to CLOSE, but not completed yet
2014-07-01 15:51:04,613 WARN  [regionserver60020] regionserver.HRegionServer: Failed to close usertable,user3,1404228265526.93253e37c284f3174309fede3f32339f. - ignoring and continuing
org.apache.hadoop.hbase.regionserver.RegionAlreadyInTransitionException: The region 93253e37c284f3174309fede3f32339f was already closing. New CLOSE request is ignored.
	at org.apache.hadoop.hbase.regionserver.HRegionServer.closeRegion(HRegionServer.java:2586)
	at org.apache.hadoop.hbase.regionserver.HRegionServer.closeRegionIgnoreErrors(HRegionServer.java:2508)
	at org.apache.hadoop.hbase.regionserver.HRegionServer.closeUserRegions(HRegionServer.java:2098)
	at org.apache.hadoop.hbase.regionserver.HRegionServer.run(HRegionServer.java:894)
	at java.lang.Thread.run(Thread.java:701)
2014-07-01 15:51:04,613 INFO  [regionserver60020] regionserver.HRegionServer: Received CLOSE for the region: 667c1f2c34fe37d48ba25a789d8aba98 ,which we are already trying to CLOSE, but not completed yet
2014-07-01 15:51:04,614 WARN  [regionserver60020] regionserver.HRegionServer: Failed to close usertable,user7,1404228265526.667c1f2c34fe37d48ba25a789d8aba98. - ignoring and continuing
org.apache.hadoop.hbase.regionserver.RegionAlreadyInTransitionException: The region 667c1f2c34fe37d48ba25a789d8aba98 was already closing. New CLOSE request is ignored.
	at org.apache.hadoop.hbase.regionserver.HRegionServer.closeRegion(HRegionServer.java:2586)
	at org.apache.hadoop.hbase.regionserver.HRegionServer.closeRegionIgnoreErrors(HRegionServer.java:2508)
	at org.apache.hadoop.hbase.regionserver.HRegionServer.closeUserRegions(HRegionServer.java:2098)
	at org.apache.hadoop.hbase.regionserver.HRegionServer.run(HRegionServer.java:894)
	at java.lang.Thread.run(Thread.java:701)
2014-07-01 15:51:04,614 DEBUG [regionserver60020] regionserver.HRegionServer: Waiting on 122e92eabfab38cb15a9965b21db25c5, bb79ae773f71289e0745068f68a0abd9, 9cef9e7d34f96f1d9b111be50040a208, 93253e37c284f3174309fede3f32339f, 667c1f2c34fe37d48ba25a789d8aba98
2014-07-01 15:51:04,747 INFO  [DataStreamer for file /hbase/WALs/slave1,60020,1404253868846/slave1%2C60020%2C1404253868846.1404254547570 block blk_-3161611795271398729_29985] ipc.Client: Retrying connect to server: sceplus-vm49.almaden.ibm.com/9.1.143.59:50020. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1 SECONDS)
2014-07-01 15:51:05,252 INFO  [RS_CLOSE_REGION-slave1:60020-1] ipc.Client: Retrying connect to server: master/9.1.143.58:54310. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1 SECONDS)
2014-07-01 15:51:05,748 INFO  [DataStreamer for file /hbase/WALs/slave1,60020,1404253868846/slave1%2C60020%2C1404253868846.1404254547570 block blk_-3161611795271398729_29985] ipc.Client: Retrying connect to server: sceplus-vm49.almaden.ibm.com/9.1.143.59:50020. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1 SECONDS)
2014-07-01 15:51:06,253 INFO  [RS_CLOSE_REGION-slave1:60020-1] ipc.Client: Retrying connect to server: master/9.1.143.58:54310. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1 SECONDS)
2014-07-01 15:51:06,749 INFO  [DataStreamer for file /hbase/WALs/slave1,60020,1404253868846/slave1%2C60020%2C1404253868846.1404254547570 block blk_-3161611795271398729_29985] ipc.Client: Retrying connect to server: sceplus-vm49.almaden.ibm.com/9.1.143.59:50020. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1 SECONDS)
2014-07-01 15:51:07,254 INFO  [RS_CLOSE_REGION-slave1:60020-1] ipc.Client: Retrying connect to server: master/9.1.143.58:54310. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1 SECONDS)
2014-07-01 15:51:07,617 INFO  [regionserver60020] regionserver.HRegionServer: Received CLOSE for the region: 122e92eabfab38cb15a9965b21db25c5 ,which we are already trying to CLOSE, but not completed yet
2014-07-01 15:51:07,618 WARN  [regionserver60020] regionserver.HRegionServer: Failed to close usertable,user5,1404228265526.122e92eabfab38cb15a9965b21db25c5. - ignoring and continuing
org.apache.hadoop.hbase.regionserver.RegionAlreadyInTransitionException: The region 122e92eabfab38cb15a9965b21db25c5 was already closing. New CLOSE request is ignored.
	at org.apache.hadoop.hbase.regionserver.HRegionServer.closeRegion(HRegionServer.java:2586)
	at org.apache.hadoop.hbase.regionserver.HRegionServer.closeRegionIgnoreErrors(HRegionServer.java:2508)
	at org.apache.hadoop.hbase.regionserver.HRegionServer.closeUserRegions(HRegionServer.java:2098)
	at org.apache.hadoop.hbase.regionserver.HRegionServer.run(HRegionServer.java:894)
	at java.lang.Thread.run(Thread.java:701)
2014-07-01 15:51:07,618 INFO  [regionserver60020] regionserver.HRegionServer: Received CLOSE for the region: bb79ae773f71289e0745068f68a0abd9 ,which we are already trying to CLOSE, but not completed yet
2014-07-01 15:51:07,618 WARN  [regionserver60020] regionserver.HRegionServer: Failed to close usertable,user9,1404228265526.bb79ae773f71289e0745068f68a0abd9. - ignoring and continuing
org.apache.hadoop.hbase.regionserver.RegionAlreadyInTransitionException: The region bb79ae773f71289e0745068f68a0abd9 was already closing. New CLOSE request is ignored.
	at org.apache.hadoop.hbase.regionserver.HRegionServer.closeRegion(HRegionServer.java:2586)
	at org.apache.hadoop.hbase.regionserver.HRegionServer.closeRegionIgnoreErrors(HRegionServer.java:2508)
	at org.apache.hadoop.hbase.regionserver.HRegionServer.closeUserRegions(HRegionServer.java:2098)
	at org.apache.hadoop.hbase.regionserver.HRegionServer.run(HRegionServer.java:894)
	at java.lang.Thread.run(Thread.java:701)
2014-07-01 15:51:07,618 INFO  [regionserver60020] regionserver.HRegionServer: Received CLOSE for the region: 9cef9e7d34f96f1d9b111be50040a208 ,which we are already trying to CLOSE, but not completed yet
2014-07-01 15:51:07,618 WARN  [regionserver60020] regionserver.HRegionServer: Failed to close usertable,,1404228265525.9cef9e7d34f96f1d9b111be50040a208. - ignoring and continuing
org.apache.hadoop.hbase.regionserver.RegionAlreadyInTransitionException: The region 9cef9e7d34f96f1d9b111be50040a208 was already closing. New CLOSE request is ignored.
	at org.apache.hadoop.hbase.regionserver.HRegionServer.closeRegion(HRegionServer.java:2586)
	at org.apache.hadoop.hbase.regionserver.HRegionServer.closeRegionIgnoreErrors(HRegionServer.java:2508)
	at org.apache.hadoop.hbase.regionserver.HRegionServer.closeUserRegions(HRegionServer.java:2098)
	at org.apache.hadoop.hbase.regionserver.HRegionServer.run(HRegionServer.java:894)
	at java.lang.Thread.run(Thread.java:701)
2014-07-01 15:51:07,620 INFO  [regionserver60020] regionserver.HRegionServer: Received CLOSE for the region: 93253e37c284f3174309fede3f32339f ,which we are already trying to CLOSE, but not completed yet
2014-07-01 15:51:07,620 WARN  [regionserver60020] regionserver.HRegionServer: Failed to close usertable,user3,1404228265526.93253e37c284f3174309fede3f32339f. - ignoring and continuing
org.apache.hadoop.hbase.regionserver.RegionAlreadyInTransitionException: The region 93253e37c284f3174309fede3f32339f was already closing. New CLOSE request is ignored.
	at org.apache.hadoop.hbase.regionserver.HRegionServer.closeRegion(HRegionServer.java:2586)
	at org.apache.hadoop.hbase.regionserver.HRegionServer.closeRegionIgnoreErrors(HRegionServer.java:2508)
	at org.apache.hadoop.hbase.regionserver.HRegionServer.closeUserRegions(HRegionServer.java:2098)
	at org.apache.hadoop.hbase.regionserver.HRegionServer.run(HRegionServer.java:894)
	at java.lang.Thread.run(Thread.java:701)
2014-07-01 15:51:07,621 INFO  [regionserver60020] regionserver.HRegionServer: Received CLOSE for the region: 667c1f2c34fe37d48ba25a789d8aba98 ,which we are already trying to CLOSE, but not completed yet
2014-07-01 15:51:07,621 WARN  [regionserver60020] regionserver.HRegionServer: Failed to close usertable,user7,1404228265526.667c1f2c34fe37d48ba25a789d8aba98. - ignoring and continuing
org.apache.hadoop.hbase.regionserver.RegionAlreadyInTransitionException: The region 667c1f2c34fe37d48ba25a789d8aba98 was already closing. New CLOSE request is ignored.
	at org.apache.hadoop.hbase.regionserver.HRegionServer.closeRegion(HRegionServer.java:2586)
	at org.apache.hadoop.hbase.regionserver.HRegionServer.closeRegionIgnoreErrors(HRegionServer.java:2508)
	at org.apache.hadoop.hbase.regionserver.HRegionServer.closeUserRegions(HRegionServer.java:2098)
	at org.apache.hadoop.hbase.regionserver.HRegionServer.run(HRegionServer.java:894)
	at java.lang.Thread.run(Thread.java:701)
2014-07-01 15:51:07,621 DEBUG [regionserver60020] regionserver.HRegionServer: Waiting on 122e92eabfab38cb15a9965b21db25c5, bb79ae773f71289e0745068f68a0abd9, 9cef9e7d34f96f1d9b111be50040a208, 93253e37c284f3174309fede3f32339f, 667c1f2c34fe37d48ba25a789d8aba98
2014-07-01 15:51:07,750 INFO  [DataStreamer for file /hbase/WALs/slave1,60020,1404253868846/slave1%2C60020%2C1404253868846.1404254547570 block blk_-3161611795271398729_29985] ipc.Client: Retrying connect to server: sceplus-vm49.almaden.ibm.com/9.1.143.59:50020. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1 SECONDS)
2014-07-01 15:51:08,255 INFO  [RS_CLOSE_REGION-slave1:60020-1] ipc.Client: Retrying connect to server: master/9.1.143.58:54310. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1 SECONDS)
2014-07-01 15:51:08,751 INFO  [DataStreamer for file /hbase/WALs/slave1,60020,1404253868846/slave1%2C60020%2C1404253868846.1404254547570 block blk_-3161611795271398729_29985] ipc.Client: Retrying connect to server: sceplus-vm49.almaden.ibm.com/9.1.143.59:50020. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1 SECONDS)
2014-07-01 15:51:08,940 DEBUG [LruStats #0] hfile.LruBlockCache: Total=669.64 MB, free=3.3 GB, max=3.96 GB, blocks=10463, accesses=3336967, hits=3159836, hitRatio=94.69%, , cachingAccesses=3169694, cachingHits=3152608, cachingHitsRatio=99.46%, evictions=0, evicted=6358, evictedPerRun=Infinity
2014-07-01 15:51:09,256 INFO  [RS_CLOSE_REGION-slave1:60020-1] ipc.Client: Retrying connect to server: master/9.1.143.58:54310. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1 SECONDS)
2014-07-01 15:51:09,751 INFO  [DataStreamer for file /hbase/WALs/slave1,60020,1404253868846/slave1%2C60020%2C1404253868846.1404254547570 block blk_-3161611795271398729_29985] ipc.Client: Retrying connect to server: sceplus-vm49.almaden.ibm.com/9.1.143.59:50020. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1 SECONDS)
2014-07-01 15:51:10,257 INFO  [RS_CLOSE_REGION-slave1:60020-1] ipc.Client: Retrying connect to server: master/9.1.143.58:54310. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1 SECONDS)
2014-07-01 15:51:10,625 INFO  [regionserver60020] regionserver.HRegionServer: Received CLOSE for the region: 122e92eabfab38cb15a9965b21db25c5 ,which we are already trying to CLOSE, but not completed yet
2014-07-01 15:51:10,625 WARN  [regionserver60020] regionserver.HRegionServer: Failed to close usertable,user5,1404228265526.122e92eabfab38cb15a9965b21db25c5. - ignoring and continuing
org.apache.hadoop.hbase.regionserver.RegionAlreadyInTransitionException: The region 122e92eabfab38cb15a9965b21db25c5 was already closing. New CLOSE request is ignored.
	at org.apache.hadoop.hbase.regionserver.HRegionServer.closeRegion(HRegionServer.java:2586)
	at org.apache.hadoop.hbase.regionserver.HRegionServer.closeRegionIgnoreErrors(HRegionServer.java:2508)
	at org.apache.hadoop.hbase.regionserver.HRegionServer.closeUserRegions(HRegionServer.java:2098)
	at org.apache.hadoop.hbase.regionserver.HRegionServer.run(HRegionServer.java:894)
	at java.lang.Thread.run(Thread.java:701)
2014-07-01 15:51:10,625 INFO  [regionserver60020] regionserver.HRegionServer: Received CLOSE for the region: bb79ae773f71289e0745068f68a0abd9 ,which we are already trying to CLOSE, but not completed yet
2014-07-01 15:51:10,625 WARN  [regionserver60020] regionserver.HRegionServer: Failed to close usertable,user9,1404228265526.bb79ae773f71289e0745068f68a0abd9. - ignoring and continuing
org.apache.hadoop.hbase.regionserver.RegionAlreadyInTransitionException: The region bb79ae773f71289e0745068f68a0abd9 was already closing. New CLOSE request is ignored.
	at org.apache.hadoop.hbase.regionserver.HRegionServer.closeRegion(HRegionServer.java:2586)
	at org.apache.hadoop.hbase.regionserver.HRegionServer.closeRegionIgnoreErrors(HRegionServer.java:2508)
	at org.apache.hadoop.hbase.regionserver.HRegionServer.closeUserRegions(HRegionServer.java:2098)
	at org.apache.hadoop.hbase.regionserver.HRegionServer.run(HRegionServer.java:894)
	at java.lang.Thread.run(Thread.java:701)
2014-07-01 15:51:10,625 INFO  [regionserver60020] regionserver.HRegionServer: Received CLOSE for the region: 9cef9e7d34f96f1d9b111be50040a208 ,which we are already trying to CLOSE, but not completed yet
2014-07-01 15:51:10,626 WARN  [regionserver60020] regionserver.HRegionServer: Failed to close usertable,,1404228265525.9cef9e7d34f96f1d9b111be50040a208. - ignoring and continuing
org.apache.hadoop.hbase.regionserver.RegionAlreadyInTransitionException: The region 9cef9e7d34f96f1d9b111be50040a208 was already closing. New CLOSE request is ignored.
	at org.apache.hadoop.hbase.regionserver.HRegionServer.closeRegion(HRegionServer.java:2586)
	at org.apache.hadoop.hbase.regionserver.HRegionServer.closeRegionIgnoreErrors(HRegionServer.java:2508)
	at org.apache.hadoop.hbase.regionserver.HRegionServer.closeUserRegions(HRegionServer.java:2098)
	at org.apache.hadoop.hbase.regionserver.HRegionServer.run(HRegionServer.java:894)
	at java.lang.Thread.run(Thread.java:701)
2014-07-01 15:51:10,626 INFO  [regionserver60020] regionserver.HRegionServer: Received CLOSE for the region: 93253e37c284f3174309fede3f32339f ,which we are already trying to CLOSE, but not completed yet
2014-07-01 15:51:10,626 WARN  [regionserver60020] regionserver.HRegionServer: Failed to close usertable,user3,1404228265526.93253e37c284f3174309fede3f32339f. - ignoring and continuing
org.apache.hadoop.hbase.regionserver.RegionAlreadyInTransitionException: The region 93253e37c284f3174309fede3f32339f was already closing. New CLOSE request is ignored.
	at org.apache.hadoop.hbase.regionserver.HRegionServer.closeRegion(HRegionServer.java:2586)
	at org.apache.hadoop.hbase.regionserver.HRegionServer.closeRegionIgnoreErrors(HRegionServer.java:2508)
	at org.apache.hadoop.hbase.regionserver.HRegionServer.closeUserRegions(HRegionServer.java:2098)
	at org.apache.hadoop.hbase.regionserver.HRegionServer.run(HRegionServer.java:894)
	at java.lang.Thread.run(Thread.java:701)
2014-07-01 15:51:10,626 INFO  [regionserver60020] regionserver.HRegionServer: Received CLOSE for the region: 667c1f2c34fe37d48ba25a789d8aba98 ,which we are already trying to CLOSE, but not completed yet
2014-07-01 15:51:10,626 WARN  [regionserver60020] regionserver.HRegionServer: Failed to close usertable,user7,1404228265526.667c1f2c34fe37d48ba25a789d8aba98. - ignoring and continuing
org.apache.hadoop.hbase.regionserver.RegionAlreadyInTransitionException: The region 667c1f2c34fe37d48ba25a789d8aba98 was already closing. New CLOSE request is ignored.
	at org.apache.hadoop.hbase.regionserver.HRegionServer.closeRegion(HRegionServer.java:2586)
	at org.apache.hadoop.hbase.regionserver.HRegionServer.closeRegionIgnoreErrors(HRegionServer.java:2508)
	at org.apache.hadoop.hbase.regionserver.HRegionServer.closeUserRegions(HRegionServer.java:2098)
	at org.apache.hadoop.hbase.regionserver.HRegionServer.run(HRegionServer.java:894)
	at java.lang.Thread.run(Thread.java:701)
2014-07-01 15:51:10,628 DEBUG [regionserver60020] regionserver.HRegionServer: Waiting on 122e92eabfab38cb15a9965b21db25c5, bb79ae773f71289e0745068f68a0abd9, 9cef9e7d34f96f1d9b111be50040a208, 93253e37c284f3174309fede3f32339f, 667c1f2c34fe37d48ba25a789d8aba98
2014-07-01 15:51:10,752 INFO  [DataStreamer for file /hbase/WALs/slave1,60020,1404253868846/slave1%2C60020%2C1404253868846.1404254547570 block blk_-3161611795271398729_29985] ipc.Client: Retrying connect to server: sceplus-vm49.almaden.ibm.com/9.1.143.59:50020. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1 SECONDS)
2014-07-01 15:51:11,258 INFO  [RS_CLOSE_REGION-slave1:60020-1] ipc.Client: Retrying connect to server: master/9.1.143.58:54310. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1 SECONDS)
2014-07-01 15:51:11,753 INFO  [DataStreamer for file /hbase/WALs/slave1,60020,1404253868846/slave1%2C60020%2C1404253868846.1404254547570 block blk_-3161611795271398729_29985] ipc.Client: Retrying connect to server: sceplus-vm49.almaden.ibm.com/9.1.143.59:50020. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1 SECONDS)
2014-07-01 15:51:11,753 WARN  [DataStreamer for file /hbase/WALs/slave1,60020,1404253868846/slave1%2C60020%2C1404253868846.1404254547570 block blk_-3161611795271398729_29985] hdfs.DFSClient: Failed recovery attempt #4 from primary datanode 9.1.143.59:50010
java.net.ConnectException: Call to sceplus-vm49.almaden.ibm.com/9.1.143.59:50020 failed on connection exception: java.net.ConnectException: Connection refused
	at org.apache.hadoop.ipc.Client.wrapException(Client.java:1142)
	at org.apache.hadoop.ipc.Client.call(Client.java:1118)
	at org.apache.hadoop.ipc.RPC$Invoker.invoke(RPC.java:229)
	at com.sun.proxy.$Proxy13.getProtocolVersion(Unknown Source)
	at org.apache.hadoop.ipc.RPC.checkVersion(RPC.java:422)
	at org.apache.hadoop.ipc.RPC.getProxy(RPC.java:414)
	at org.apache.hadoop.ipc.RPC.getProxy(RPC.java:392)
	at org.apache.hadoop.hdfs.DFSClient.createClientDatanodeProtocolProxy(DFSClient.java:201)
	at org.apache.hadoop.hdfs.DFSClient$DFSOutputStream.processDatanodeError(DFSClient.java:3317)
	at org.apache.hadoop.hdfs.DFSClient$DFSOutputStream.access$2200(DFSClient.java:2783)
	at org.apache.hadoop.hdfs.DFSClient$DFSOutputStream$DataStreamer.run(DFSClient.java:2987)
Caused by: java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:601)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:511)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:481)
	at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:457)
	at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:583)
	at org.apache.hadoop.ipc.Client$Connection.access$2200(Client.java:205)
	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1249)
	at org.apache.hadoop.ipc.Client.call(Client.java:1093)
	... 9 more
2014-07-01 15:51:11,754 WARN  [DataStreamer for file /hbase/WALs/slave1,60020,1404253868846/slave1%2C60020%2C1404253868846.1404254547570 block blk_-3161611795271398729_29985] hdfs.DFSClient: Error Recovery for block blk_-3161611795271398729_29985 failed  because recovery from primary datanode 9.1.143.59:50010 failed 5 times.  Pipeline was 9.1.143.59:50010. Will retry...
2014-07-01 15:51:12,258 INFO  [RS_CLOSE_REGION-slave1:60020-1] ipc.Client: Retrying connect to server: master/9.1.143.58:54310. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1 SECONDS)
2014-07-01 15:51:13,261 INFO  [RS_CLOSE_REGION-slave1:60020-1] ipc.Client: Retrying connect to server: master/9.1.143.58:54310. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1 SECONDS)
2014-07-01 15:51:13,632 INFO  [regionserver60020] regionserver.HRegionServer: Received CLOSE for the region: 122e92eabfab38cb15a9965b21db25c5 ,which we are already trying to CLOSE, but not completed yet
2014-07-01 15:51:13,632 WARN  [regionserver60020] regionserver.HRegionServer: Failed to close usertable,user5,1404228265526.122e92eabfab38cb15a9965b21db25c5. - ignoring and continuing
org.apache.hadoop.hbase.regionserver.RegionAlreadyInTransitionException: The region 122e92eabfab38cb15a9965b21db25c5 was already closing. New CLOSE request is ignored.
	at org.apache.hadoop.hbase.regionserver.HRegionServer.closeRegion(HRegionServer.java:2586)
	at org.apache.hadoop.hbase.regionserver.HRegionServer.closeRegionIgnoreErrors(HRegionServer.java:2508)
	at org.apache.hadoop.hbase.regionserver.HRegionServer.closeUserRegions(HRegionServer.java:2098)
	at org.apache.hadoop.hbase.regionserver.HRegionServer.run(HRegionServer.java:894)
	at java.lang.Thread.run(Thread.java:701)
2014-07-01 15:51:13,632 INFO  [regionserver60020] regionserver.HRegionServer: Received CLOSE for the region: bb79ae773f71289e0745068f68a0abd9 ,which we are already trying to CLOSE, but not completed yet
2014-07-01 15:51:13,632 WARN  [regionserver60020] regionserver.HRegionServer: Failed to close usertable,user9,1404228265526.bb79ae773f71289e0745068f68a0abd9. - ignoring and continuing
org.apache.hadoop.hbase.regionserver.RegionAlreadyInTransitionException: The region bb79ae773f71289e0745068f68a0abd9 was already closing. New CLOSE request is ignored.
	at org.apache.hadoop.hbase.regionserver.HRegionServer.closeRegion(HRegionServer.java:2586)
	at org.apache.hadoop.hbase.regionserver.HRegionServer.closeRegionIgnoreErrors(HRegionServer.java:2508)
	at org.apache.hadoop.hbase.regionserver.HRegionServer.closeUserRegions(HRegionServer.java:2098)
	at org.apache.hadoop.hbase.regionserver.HRegionServer.run(HRegionServer.java:894)
	at java.lang.Thread.run(Thread.java:701)
2014-07-01 15:51:13,632 INFO  [regionserver60020] regionserver.HRegionServer: Received CLOSE for the region: 9cef9e7d34f96f1d9b111be50040a208 ,which we are already trying to CLOSE, but not completed yet
2014-07-01 15:51:13,632 WARN  [regionserver60020] regionserver.HRegionServer: Failed to close usertable,,1404228265525.9cef9e7d34f96f1d9b111be50040a208. - ignoring and continuing
org.apache.hadoop.hbase.regionserver.RegionAlreadyInTransitionException: The region 9cef9e7d34f96f1d9b111be50040a208 was already closing. New CLOSE request is ignored.
	at org.apache.hadoop.hbase.regionserver.HRegionServer.closeRegion(HRegionServer.java:2586)
	at org.apache.hadoop.hbase.regionserver.HRegionServer.closeRegionIgnoreErrors(HRegionServer.java:2508)
	at org.apache.hadoop.hbase.regionserver.HRegionServer.closeUserRegions(HRegionServer.java:2098)
	at org.apache.hadoop.hbase.regionserver.HRegionServer.run(HRegionServer.java:894)
	at java.lang.Thread.run(Thread.java:701)
2014-07-01 15:51:13,633 INFO  [regionserver60020] regionserver.HRegionServer: Received CLOSE for the region: 93253e37c284f3174309fede3f32339f ,which we are already trying to CLOSE, but not completed yet
2014-07-01 15:51:13,633 WARN  [regionserver60020] regionserver.HRegionServer: Failed to close usertable,user3,1404228265526.93253e37c284f3174309fede3f32339f. - ignoring and continuing
org.apache.hadoop.hbase.regionserver.RegionAlreadyInTransitionException: The region 93253e37c284f3174309fede3f32339f was already closing. New CLOSE request is ignored.
	at org.apache.hadoop.hbase.regionserver.HRegionServer.closeRegion(HRegionServer.java:2586)
	at org.apache.hadoop.hbase.regionserver.HRegionServer.closeRegionIgnoreErrors(HRegionServer.java:2508)
	at org.apache.hadoop.hbase.regionserver.HRegionServer.closeUserRegions(HRegionServer.java:2098)
	at org.apache.hadoop.hbase.regionserver.HRegionServer.run(HRegionServer.java:894)
	at java.lang.Thread.run(Thread.java:701)
2014-07-01 15:51:13,633 INFO  [regionserver60020] regionserver.HRegionServer: Received CLOSE for the region: 667c1f2c34fe37d48ba25a789d8aba98 ,which we are already trying to CLOSE, but not completed yet
2014-07-01 15:51:13,633 WARN  [regionserver60020] regionserver.HRegionServer: Failed to close usertable,user7,1404228265526.667c1f2c34fe37d48ba25a789d8aba98. - ignoring and continuing
org.apache.hadoop.hbase.regionserver.RegionAlreadyInTransitionException: The region 667c1f2c34fe37d48ba25a789d8aba98 was already closing. New CLOSE request is ignored.
	at org.apache.hadoop.hbase.regionserver.HRegionServer.closeRegion(HRegionServer.java:2586)
	at org.apache.hadoop.hbase.regionserver.HRegionServer.closeRegionIgnoreErrors(HRegionServer.java:2508)
	at org.apache.hadoop.hbase.regionserver.HRegionServer.closeUserRegions(HRegionServer.java:2098)
	at org.apache.hadoop.hbase.regionserver.HRegionServer.run(HRegionServer.java:894)
	at java.lang.Thread.run(Thread.java:701)
2014-07-01 15:51:13,633 DEBUG [regionserver60020] regionserver.HRegionServer: Waiting on 122e92eabfab38cb15a9965b21db25c5, bb79ae773f71289e0745068f68a0abd9, 9cef9e7d34f96f1d9b111be50040a208, 93253e37c284f3174309fede3f32339f, 667c1f2c34fe37d48ba25a789d8aba98
2014-07-01 15:51:13,756 INFO  [DataStreamer for file /hbase/WALs/slave1,60020,1404253868846/slave1%2C60020%2C1404253868846.1404254547570 block blk_-3161611795271398729_29985] ipc.Client: Retrying connect to server: sceplus-vm49.almaden.ibm.com/9.1.143.59:50020. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1 SECONDS)
2014-07-01 15:51:14,262 INFO  [RS_CLOSE_REGION-slave1:60020-1] ipc.Client: Retrying connect to server: master/9.1.143.58:54310. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1 SECONDS)
2014-07-01 15:51:14,262 WARN  [RS_CLOSE_REGION-slave1:60020-1] regionserver.HStore: Failed flushing store file, retrying num=2
java.net.ConnectException: Call to master/9.1.143.58:54310 failed on connection exception: java.net.ConnectException: Connection refused
	at org.apache.hadoop.ipc.Client.wrapException(Client.java:1142)
	at org.apache.hadoop.ipc.Client.call(Client.java:1118)
	at org.apache.hadoop.ipc.RPC$Invoker.invoke(RPC.java:229)
	at com.sun.proxy.$Proxy8.getFileInfo(Unknown Source)
	at sun.reflect.GeneratedMethodAccessor2.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:622)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:85)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:62)
	at com.sun.proxy.$Proxy8.getFileInfo(Unknown Source)
	at sun.reflect.GeneratedMethodAccessor2.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:622)
	at org.apache.hadoop.hbase.fs.HFileSystem$1.invoke(HFileSystem.java:294)
	at com.sun.proxy.$Proxy9.getFileInfo(Unknown Source)
	at org.apache.hadoop.hdfs.DFSClient.getFileInfo(DFSClient.java:1059)
	at org.apache.hadoop.hdfs.DistributedFileSystem.getFileStatus(DistributedFileSystem.java:554)
	at org.apache.hadoop.fs.FilterFileSystem.getFileStatus(FilterFileSystem.java:255)
	at org.apache.hadoop.fs.FileSystem.exists(FileSystem.java:788)
	at org.apache.hadoop.hbase.regionserver.StoreFile$WriterBuilder.build(StoreFile.java:603)
	at org.apache.hadoop.hbase.regionserver.HStore.createWriterInTmp(HStore.java:889)
	at org.apache.hadoop.hbase.regionserver.DefaultStoreFlusher.flushSnapshot(DefaultStoreFlusher.java:69)
	at org.apache.hadoop.hbase.regionserver.HStore.flushCache(HStore.java:798)
	at org.apache.hadoop.hbase.regionserver.HStore$StoreFlusherImpl.flushCache(HStore.java:1949)
	at org.apache.hadoop.hbase.regionserver.HRegion.internalFlushcache(HRegion.java:1776)
	at org.apache.hadoop.hbase.regionserver.HRegion.internalFlushcache(HRegion.java:1658)
	at org.apache.hadoop.hbase.regionserver.HRegion.doClose(HRegion.java:1119)
	at org.apache.hadoop.hbase.regionserver.HRegion.close(HRegion.java:1085)
	at org.apache.hadoop.hbase.regionserver.handler.CloseRegionHandler.process(CloseRegionHandler.java:147)
	at org.apache.hadoop.hbase.executor.EventHandler.run(EventHandler.java:128)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1146)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:701)
Caused by: java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:601)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:511)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:481)
	at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:457)
	at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:583)
	at org.apache.hadoop.ipc.Client$Connection.access$2200(Client.java:205)
	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1249)
	at org.apache.hadoop.ipc.Client.call(Client.java:1093)
	... 31 more
2014-07-01 15:51:14,756 INFO  [DataStreamer for file /hbase/WALs/slave1,60020,1404253868846/slave1%2C60020%2C1404253868846.1404254547570 block blk_-3161611795271398729_29985] ipc.Client: Retrying connect to server: sceplus-vm49.almaden.ibm.com/9.1.143.59:50020. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1 SECONDS)
2014-07-01 15:51:15,264 INFO  [RS_CLOSE_REGION-slave1:60020-2] ipc.Client: Retrying connect to server: master/9.1.143.58:54310. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1 SECONDS)
2014-07-01 15:51:15,757 INFO  [DataStreamer for file /hbase/WALs/slave1,60020,1404253868846/slave1%2C60020%2C1404253868846.1404254547570 block blk_-3161611795271398729_29985] ipc.Client: Retrying connect to server: sceplus-vm49.almaden.ibm.com/9.1.143.59:50020. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1 SECONDS)
2014-07-01 15:51:16,265 INFO  [RS_CLOSE_REGION-slave1:60020-2] ipc.Client: Retrying connect to server: master/9.1.143.58:54310. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1 SECONDS)
2014-07-01 15:51:16,635 INFO  [regionserver60020] regionserver.HRegionServer: Received CLOSE for the region: 122e92eabfab38cb15a9965b21db25c5 ,which we are already trying to CLOSE, but not completed yet
2014-07-01 15:51:16,636 WARN  [regionserver60020] regionserver.HRegionServer: Failed to close usertable,user5,1404228265526.122e92eabfab38cb15a9965b21db25c5. - ignoring and continuing
org.apache.hadoop.hbase.regionserver.RegionAlreadyInTransitionException: The region 122e92eabfab38cb15a9965b21db25c5 was already closing. New CLOSE request is ignored.
	at org.apache.hadoop.hbase.regionserver.HRegionServer.closeRegion(HRegionServer.java:2586)
	at org.apache.hadoop.hbase.regionserver.HRegionServer.closeRegionIgnoreErrors(HRegionServer.java:2508)
	at org.apache.hadoop.hbase.regionserver.HRegionServer.closeUserRegions(HRegionServer.java:2098)
	at org.apache.hadoop.hbase.regionserver.HRegionServer.run(HRegionServer.java:894)
	at java.lang.Thread.run(Thread.java:701)
2014-07-01 15:51:16,636 INFO  [regionserver60020] regionserver.HRegionServer: Received CLOSE for the region: bb79ae773f71289e0745068f68a0abd9 ,which we are already trying to CLOSE, but not completed yet
2014-07-01 15:51:16,636 WARN  [regionserver60020] regionserver.HRegionServer: Failed to close usertable,user9,1404228265526.bb79ae773f71289e0745068f68a0abd9. - ignoring and continuing
org.apache.hadoop.hbase.regionserver.RegionAlreadyInTransitionException: The region bb79ae773f71289e0745068f68a0abd9 was already closing. New CLOSE request is ignored.
	at org.apache.hadoop.hbase.regionserver.HRegionServer.closeRegion(HRegionServer.java:2586)
	at org.apache.hadoop.hbase.regionserver.HRegionServer.closeRegionIgnoreErrors(HRegionServer.java:2508)
	at org.apache.hadoop.hbase.regionserver.HRegionServer.closeUserRegions(HRegionServer.java:2098)
	at org.apache.hadoop.hbase.regionserver.HRegionServer.run(HRegionServer.java:894)
	at java.lang.Thread.run(Thread.java:701)
2014-07-01 15:51:16,636 INFO  [regionserver60020] regionserver.HRegionServer: Received CLOSE for the region: 9cef9e7d34f96f1d9b111be50040a208 ,which we are already trying to CLOSE, but not completed yet
2014-07-01 15:51:16,636 WARN  [regionserver60020] regionserver.HRegionServer: Failed to close usertable,,1404228265525.9cef9e7d34f96f1d9b111be50040a208. - ignoring and continuing
org.apache.hadoop.hbase.regionserver.RegionAlreadyInTransitionException: The region 9cef9e7d34f96f1d9b111be50040a208 was already closing. New CLOSE request is ignored.
	at org.apache.hadoop.hbase.regionserver.HRegionServer.closeRegion(HRegionServer.java:2586)
	at org.apache.hadoop.hbase.regionserver.HRegionServer.closeRegionIgnoreErrors(HRegionServer.java:2508)
	at org.apache.hadoop.hbase.regionserver.HRegionServer.closeUserRegions(HRegionServer.java:2098)
	at org.apache.hadoop.hbase.regionserver.HRegionServer.run(HRegionServer.java:894)
	at java.lang.Thread.run(Thread.java:701)
2014-07-01 15:51:16,637 INFO  [regionserver60020] regionserver.HRegionServer: Received CLOSE for the region: 93253e37c284f3174309fede3f32339f ,which we are already trying to CLOSE, but not completed yet
2014-07-01 15:51:16,637 WARN  [regionserver60020] regionserver.HRegionServer: Failed to close usertable,user3,1404228265526.93253e37c284f3174309fede3f32339f. - ignoring and continuing
org.apache.hadoop.hbase.regionserver.RegionAlreadyInTransitionException: The region 93253e37c284f3174309fede3f32339f was already closing. New CLOSE request is ignored.
	at org.apache.hadoop.hbase.regionserver.HRegionServer.closeRegion(HRegionServer.java:2586)
	at org.apache.hadoop.hbase.regionserver.HRegionServer.closeRegionIgnoreErrors(HRegionServer.java:2508)
	at org.apache.hadoop.hbase.regionserver.HRegionServer.closeUserRegions(HRegionServer.java:2098)
	at org.apache.hadoop.hbase.regionserver.HRegionServer.run(HRegionServer.java:894)
	at java.lang.Thread.run(Thread.java:701)
2014-07-01 15:51:16,637 INFO  [regionserver60020] regionserver.HRegionServer: Received CLOSE for the region: 667c1f2c34fe37d48ba25a789d8aba98 ,which we are already trying to CLOSE, but not completed yet
2014-07-01 15:51:16,637 WARN  [regionserver60020] regionserver.HRegionServer: Failed to close usertable,user7,1404228265526.667c1f2c34fe37d48ba25a789d8aba98. - ignoring and continuing
org.apache.hadoop.hbase.regionserver.RegionAlreadyInTransitionException: The region 667c1f2c34fe37d48ba25a789d8aba98 was already closing. New CLOSE request is ignored.
	at org.apache.hadoop.hbase.regionserver.HRegionServer.closeRegion(HRegionServer.java:2586)
	at org.apache.hadoop.hbase.regionserver.HRegionServer.closeRegionIgnoreErrors(HRegionServer.java:2508)
	at org.apache.hadoop.hbase.regionserver.HRegionServer.closeUserRegions(HRegionServer.java:2098)
	at org.apache.hadoop.hbase.regionserver.HRegionServer.run(HRegionServer.java:894)
	at java.lang.Thread.run(Thread.java:701)
2014-07-01 15:51:16,638 DEBUG [regionserver60020] regionserver.HRegionServer: Waiting on 122e92eabfab38cb15a9965b21db25c5, bb79ae773f71289e0745068f68a0abd9, 9cef9e7d34f96f1d9b111be50040a208, 93253e37c284f3174309fede3f32339f, 667c1f2c34fe37d48ba25a789d8aba98
2014-07-01 15:51:16,758 INFO  [DataStreamer for file /hbase/WALs/slave1,60020,1404253868846/slave1%2C60020%2C1404253868846.1404254547570 block blk_-3161611795271398729_29985] ipc.Client: Retrying connect to server: sceplus-vm49.almaden.ibm.com/9.1.143.59:50020. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1 SECONDS)
2014-07-01 15:51:17,266 INFO  [RS_CLOSE_REGION-slave1:60020-2] ipc.Client: Retrying connect to server: master/9.1.143.58:54310. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1 SECONDS)
2014-07-01 15:51:17,758 INFO  [DataStreamer for file /hbase/WALs/slave1,60020,1404253868846/slave1%2C60020%2C1404253868846.1404254547570 block blk_-3161611795271398729_29985] ipc.Client: Retrying connect to server: sceplus-vm49.almaden.ibm.com/9.1.143.59:50020. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1 SECONDS)
2014-07-01 15:51:18,269 INFO  [RS_CLOSE_REGION-slave1:60020-2] ipc.Client: Retrying connect to server: master/9.1.143.58:54310. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1 SECONDS)
2014-07-01 15:51:18,760 INFO  [DataStreamer for file /hbase/WALs/slave1,60020,1404253868846/slave1%2C60020%2C1404253868846.1404254547570 block blk_-3161611795271398729_29985] ipc.Client: Retrying connect to server: sceplus-vm49.almaden.ibm.com/9.1.143.59:50020. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1 SECONDS)
2014-07-01 15:51:19,270 INFO  [RS_CLOSE_REGION-slave1:60020-2] ipc.Client: Retrying connect to server: master/9.1.143.58:54310. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1 SECONDS)
2014-07-01 15:51:19,641 INFO  [regionserver60020] regionserver.HRegionServer: Received CLOSE for the region: 122e92eabfab38cb15a9965b21db25c5 ,which we are already trying to CLOSE, but not completed yet
2014-07-01 15:51:19,641 WARN  [regionserver60020] regionserver.HRegionServer: Failed to close usertable,user5,1404228265526.122e92eabfab38cb15a9965b21db25c5. - ignoring and continuing
org.apache.hadoop.hbase.regionserver.RegionAlreadyInTransitionException: The region 122e92eabfab38cb15a9965b21db25c5 was already closing. New CLOSE request is ignored.
	at org.apache.hadoop.hbase.regionserver.HRegionServer.closeRegion(HRegionServer.java:2586)
	at org.apache.hadoop.hbase.regionserver.HRegionServer.closeRegionIgnoreErrors(HRegionServer.java:2508)
	at org.apache.hadoop.hbase.regionserver.HRegionServer.closeUserRegions(HRegionServer.java:2098)
	at org.apache.hadoop.hbase.regionserver.HRegionServer.run(HRegionServer.java:894)
	at java.lang.Thread.run(Thread.java:701)
2014-07-01 15:51:19,642 INFO  [regionserver60020] regionserver.HRegionServer: Received CLOSE for the region: bb79ae773f71289e0745068f68a0abd9 ,which we are already trying to CLOSE, but not completed yet
2014-07-01 15:51:19,642 WARN  [regionserver60020] regionserver.HRegionServer: Failed to close usertable,user9,1404228265526.bb79ae773f71289e0745068f68a0abd9. - ignoring and continuing
org.apache.hadoop.hbase.regionserver.RegionAlreadyInTransitionException: The region bb79ae773f71289e0745068f68a0abd9 was already closing. New CLOSE request is ignored.
	at org.apache.hadoop.hbase.regionserver.HRegionServer.closeRegion(HRegionServer.java:2586)
	at org.apache.hadoop.hbase.regionserver.HRegionServer.closeRegionIgnoreErrors(HRegionServer.java:2508)
	at org.apache.hadoop.hbase.regionserver.HRegionServer.closeUserRegions(HRegionServer.java:2098)
	at org.apache.hadoop.hbase.regionserver.HRegionServer.run(HRegionServer.java:894)
	at java.lang.Thread.run(Thread.java:701)
2014-07-01 15:51:19,642 INFO  [regionserver60020] regionserver.HRegionServer: Received CLOSE for the region: 9cef9e7d34f96f1d9b111be50040a208 ,which we are already trying to CLOSE, but not completed yet
2014-07-01 15:51:19,642 WARN  [regionserver60020] regionserver.HRegionServer: Failed to close usertable,,1404228265525.9cef9e7d34f96f1d9b111be50040a208. - ignoring and continuing
org.apache.hadoop.hbase.regionserver.RegionAlreadyInTransitionException: The region 9cef9e7d34f96f1d9b111be50040a208 was already closing. New CLOSE request is ignored.
	at org.apache.hadoop.hbase.regionserver.HRegionServer.closeRegion(HRegionServer.java:2586)
	at org.apache.hadoop.hbase.regionserver.HRegionServer.closeRegionIgnoreErrors(HRegionServer.java:2508)
	at org.apache.hadoop.hbase.regionserver.HRegionServer.closeUserRegions(HRegionServer.java:2098)
	at org.apache.hadoop.hbase.regionserver.HRegionServer.run(HRegionServer.java:894)
	at java.lang.Thread.run(Thread.java:701)
2014-07-01 15:51:19,642 INFO  [regionserver60020] regionserver.HRegionServer: Received CLOSE for the region: 93253e37c284f3174309fede3f32339f ,which we are already trying to CLOSE, but not completed yet
2014-07-01 15:51:19,643 WARN  [regionserver60020] regionserver.HRegionServer: Failed to close usertable,user3,1404228265526.93253e37c284f3174309fede3f32339f. - ignoring and continuing
org.apache.hadoop.hbase.regionserver.RegionAlreadyInTransitionException: The region 93253e37c284f3174309fede3f32339f was already closing. New CLOSE request is ignored.
	at org.apache.hadoop.hbase.regionserver.HRegionServer.closeRegion(HRegionServer.java:2586)
	at org.apache.hadoop.hbase.regionserver.HRegionServer.closeRegionIgnoreErrors(HRegionServer.java:2508)
	at org.apache.hadoop.hbase.regionserver.HRegionServer.closeUserRegions(HRegionServer.java:2098)
	at org.apache.hadoop.hbase.regionserver.HRegionServer.run(HRegionServer.java:894)
	at java.lang.Thread.run(Thread.java:701)
2014-07-01 15:51:19,644 INFO  [regionserver60020] regionserver.HRegionServer: Received CLOSE for the region: 667c1f2c34fe37d48ba25a789d8aba98 ,which we are already trying to CLOSE, but not completed yet
2014-07-01 15:51:19,645 WARN  [regionserver60020] regionserver.HRegionServer: Failed to close usertable,user7,1404228265526.667c1f2c34fe37d48ba25a789d8aba98. - ignoring and continuing
org.apache.hadoop.hbase.regionserver.RegionAlreadyInTransitionException: The region 667c1f2c34fe37d48ba25a789d8aba98 was already closing. New CLOSE request is ignored.
	at org.apache.hadoop.hbase.regionserver.HRegionServer.closeRegion(HRegionServer.java:2586)
	at org.apache.hadoop.hbase.regionserver.HRegionServer.closeRegionIgnoreErrors(HRegionServer.java:2508)
	at org.apache.hadoop.hbase.regionserver.HRegionServer.closeUserRegions(HRegionServer.java:2098)
	at org.apache.hadoop.hbase.regionserver.HRegionServer.run(HRegionServer.java:894)
	at java.lang.Thread.run(Thread.java:701)
2014-07-01 15:51:19,645 DEBUG [regionserver60020] regionserver.HRegionServer: Waiting on 122e92eabfab38cb15a9965b21db25c5, bb79ae773f71289e0745068f68a0abd9, 9cef9e7d34f96f1d9b111be50040a208, 93253e37c284f3174309fede3f32339f, 667c1f2c34fe37d48ba25a789d8aba98
2014-07-01 15:51:19,760 INFO  [DataStreamer for file /hbase/WALs/slave1,60020,1404253868846/slave1%2C60020%2C1404253868846.1404254547570 block blk_-3161611795271398729_29985] ipc.Client: Retrying connect to server: sceplus-vm49.almaden.ibm.com/9.1.143.59:50020. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1 SECONDS)
2014-07-01 15:51:20,271 INFO  [RS_CLOSE_REGION-slave1:60020-2] ipc.Client: Retrying connect to server: master/9.1.143.58:54310. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1 SECONDS)
2014-07-01 15:51:20,761 INFO  [DataStreamer for file /hbase/WALs/slave1,60020,1404253868846/slave1%2C60020%2C1404253868846.1404254547570 block blk_-3161611795271398729_29985] ipc.Client: Retrying connect to server: sceplus-vm49.almaden.ibm.com/9.1.143.59:50020. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1 SECONDS)
2014-07-01 15:51:21,272 INFO  [RS_CLOSE_REGION-slave1:60020-2] ipc.Client: Retrying connect to server: master/9.1.143.58:54310. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1 SECONDS)
2014-07-01 15:51:21,762 INFO  [DataStreamer for file /hbase/WALs/slave1,60020,1404253868846/slave1%2C60020%2C1404253868846.1404254547570 block blk_-3161611795271398729_29985] ipc.Client: Retrying connect to server: sceplus-vm49.almaden.ibm.com/9.1.143.59:50020. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1 SECONDS)
2014-07-01 15:51:22,273 INFO  [RS_CLOSE_REGION-slave1:60020-2] ipc.Client: Retrying connect to server: master/9.1.143.58:54310. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1 SECONDS)
2014-07-01 15:51:22,648 INFO  [regionserver60020] regionserver.HRegionServer: Received CLOSE for the region: 122e92eabfab38cb15a9965b21db25c5 ,which we are already trying to CLOSE, but not completed yet
2014-07-01 15:51:22,648 WARN  [regionserver60020] regionserver.HRegionServer: Failed to close usertable,user5,1404228265526.122e92eabfab38cb15a9965b21db25c5. - ignoring and continuing
org.apache.hadoop.hbase.regionserver.RegionAlreadyInTransitionException: The region 122e92eabfab38cb15a9965b21db25c5 was already closing. New CLOSE request is ignored.
	at org.apache.hadoop.hbase.regionserver.HRegionServer.closeRegion(HRegionServer.java:2586)
	at org.apache.hadoop.hbase.regionserver.HRegionServer.closeRegionIgnoreErrors(HRegionServer.java:2508)
	at org.apache.hadoop.hbase.regionserver.HRegionServer.closeUserRegions(HRegionServer.java:2098)
	at org.apache.hadoop.hbase.regionserver.HRegionServer.run(HRegionServer.java:894)
	at java.lang.Thread.run(Thread.java:701)
2014-07-01 15:51:22,649 INFO  [regionserver60020] regionserver.HRegionServer: Received CLOSE for the region: bb79ae773f71289e0745068f68a0abd9 ,which we are already trying to CLOSE, but not completed yet
2014-07-01 15:51:22,649 WARN  [regionserver60020] regionserver.HRegionServer: Failed to close usertable,user9,1404228265526.bb79ae773f71289e0745068f68a0abd9. - ignoring and continuing
org.apache.hadoop.hbase.regionserver.RegionAlreadyInTransitionException: The region bb79ae773f71289e0745068f68a0abd9 was already closing. New CLOSE request is ignored.
	at org.apache.hadoop.hbase.regionserver.HRegionServer.closeRegion(HRegionServer.java:2586)
	at org.apache.hadoop.hbase.regionserver.HRegionServer.closeRegionIgnoreErrors(HRegionServer.java:2508)
	at org.apache.hadoop.hbase.regionserver.HRegionServer.closeUserRegions(HRegionServer.java:2098)
	at org.apache.hadoop.hbase.regionserver.HRegionServer.run(HRegionServer.java:894)
	at java.lang.Thread.run(Thread.java:701)
2014-07-01 15:51:22,649 INFO  [regionserver60020] regionserver.HRegionServer: Received CLOSE for the region: 9cef9e7d34f96f1d9b111be50040a208 ,which we are already trying to CLOSE, but not completed yet
2014-07-01 15:51:22,649 WARN  [regionserver60020] regionserver.HRegionServer: Failed to close usertable,,1404228265525.9cef9e7d34f96f1d9b111be50040a208. - ignoring and continuing
org.apache.hadoop.hbase.regionserver.RegionAlreadyInTransitionException: The region 9cef9e7d34f96f1d9b111be50040a208 was already closing. New CLOSE request is ignored.
	at org.apache.hadoop.hbase.regionserver.HRegionServer.closeRegion(HRegionServer.java:2586)
	at org.apache.hadoop.hbase.regionserver.HRegionServer.closeRegionIgnoreErrors(HRegionServer.java:2508)
	at org.apache.hadoop.hbase.regionserver.HRegionServer.closeUserRegions(HRegionServer.java:2098)
	at org.apache.hadoop.hbase.regionserver.HRegionServer.run(HRegionServer.java:894)
	at java.lang.Thread.run(Thread.java:701)
2014-07-01 15:51:22,649 INFO  [regionserver60020] regionserver.HRegionServer: Received CLOSE for the region: 93253e37c284f3174309fede3f32339f ,which we are already trying to CLOSE, but not completed yet
2014-07-01 15:51:22,650 WARN  [regionserver60020] regionserver.HRegionServer: Failed to close usertable,user3,1404228265526.93253e37c284f3174309fede3f32339f. - ignoring and continuing
org.apache.hadoop.hbase.regionserver.RegionAlreadyInTransitionException: The region 93253e37c284f3174309fede3f32339f was already closing. New CLOSE request is ignored.
	at org.apache.hadoop.hbase.regionserver.HRegionServer.closeRegion(HRegionServer.java:2586)
	at org.apache.hadoop.hbase.regionserver.HRegionServer.closeRegionIgnoreErrors(HRegionServer.java:2508)
	at org.apache.hadoop.hbase.regionserver.HRegionServer.closeUserRegions(HRegionServer.java:2098)
	at org.apache.hadoop.hbase.regionserver.HRegionServer.run(HRegionServer.java:894)
	at java.lang.Thread.run(Thread.java:701)
2014-07-01 15:51:22,650 INFO  [regionserver60020] regionserver.HRegionServer: Received CLOSE for the region: 667c1f2c34fe37d48ba25a789d8aba98 ,which we are already trying to CLOSE, but not completed yet
2014-07-01 15:51:22,650 WARN  [regionserver60020] regionserver.HRegionServer: Failed to close usertable,user7,1404228265526.667c1f2c34fe37d48ba25a789d8aba98. - ignoring and continuing
org.apache.hadoop.hbase.regionserver.RegionAlreadyInTransitionException: The region 667c1f2c34fe37d48ba25a789d8aba98 was already closing. New CLOSE request is ignored.
	at org.apache.hadoop.hbase.regionserver.HRegionServer.closeRegion(HRegionServer.java:2586)
	at org.apache.hadoop.hbase.regionserver.HRegionServer.closeRegionIgnoreErrors(HRegionServer.java:2508)
	at org.apache.hadoop.hbase.regionserver.HRegionServer.closeUserRegions(HRegionServer.java:2098)
	at org.apache.hadoop.hbase.regionserver.HRegionServer.run(HRegionServer.java:894)
	at java.lang.Thread.run(Thread.java:701)
2014-07-01 15:51:22,650 DEBUG [regionserver60020] regionserver.HRegionServer: Waiting on 122e92eabfab38cb15a9965b21db25c5, bb79ae773f71289e0745068f68a0abd9, 9cef9e7d34f96f1d9b111be50040a208, 93253e37c284f3174309fede3f32339f, 667c1f2c34fe37d48ba25a789d8aba98
2014-07-01 15:51:22,762 INFO  [DataStreamer for file /hbase/WALs/slave1,60020,1404253868846/slave1%2C60020%2C1404253868846.1404254547570 block blk_-3161611795271398729_29985] ipc.Client: Retrying connect to server: sceplus-vm49.almaden.ibm.com/9.1.143.59:50020. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1 SECONDS)
2014-07-01 15:51:22,763 WARN  [DataStreamer for file /hbase/WALs/slave1,60020,1404253868846/slave1%2C60020%2C1404253868846.1404254547570 block blk_-3161611795271398729_29985] hdfs.DFSClient: Failed recovery attempt #5 from primary datanode 9.1.143.59:50010
java.net.ConnectException: Call to sceplus-vm49.almaden.ibm.com/9.1.143.59:50020 failed on connection exception: java.net.ConnectException: Connection refused
	at org.apache.hadoop.ipc.Client.wrapException(Client.java:1142)
	at org.apache.hadoop.ipc.Client.call(Client.java:1118)
	at org.apache.hadoop.ipc.RPC$Invoker.invoke(RPC.java:229)
	at com.sun.proxy.$Proxy13.getProtocolVersion(Unknown Source)
	at org.apache.hadoop.ipc.RPC.checkVersion(RPC.java:422)
	at org.apache.hadoop.ipc.RPC.getProxy(RPC.java:414)
	at org.apache.hadoop.ipc.RPC.getProxy(RPC.java:392)
	at org.apache.hadoop.hdfs.DFSClient.createClientDatanodeProtocolProxy(DFSClient.java:201)
	at org.apache.hadoop.hdfs.DFSClient$DFSOutputStream.processDatanodeError(DFSClient.java:3317)
	at org.apache.hadoop.hdfs.DFSClient$DFSOutputStream.access$2200(DFSClient.java:2783)
	at org.apache.hadoop.hdfs.DFSClient$DFSOutputStream$DataStreamer.run(DFSClient.java:2987)
Caused by: java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:601)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:511)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:481)
	at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:457)
	at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:583)
	at org.apache.hadoop.ipc.Client$Connection.access$2200(Client.java:205)
	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1249)
	at org.apache.hadoop.ipc.Client.call(Client.java:1093)
	... 9 more
2014-07-01 15:51:22,764 WARN  [DataStreamer for file /hbase/WALs/slave1,60020,1404253868846/slave1%2C60020%2C1404253868846.1404254547570 block blk_-3161611795271398729_29985] hdfs.DFSClient: Error Recovery for block blk_-3161611795271398729_29985 failed  because recovery from primary datanode 9.1.143.59:50010 failed 6 times.  Pipeline was 9.1.143.59:50010. Aborting...
2014-07-01 15:51:23,274 INFO  [RS_CLOSE_REGION-slave1:60020-2] ipc.Client: Retrying connect to server: master/9.1.143.58:54310. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1 SECONDS)
2014-07-01 15:51:24,277 INFO  [RS_CLOSE_REGION-slave1:60020-2] ipc.Client: Retrying connect to server: master/9.1.143.58:54310. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1 SECONDS)
2014-07-01 15:51:24,278 WARN  [RS_CLOSE_REGION-slave1:60020-2] regionserver.HStore: Failed flushing store file, retrying num=3
java.net.ConnectException: Call to master/9.1.143.58:54310 failed on connection exception: java.net.ConnectException: Connection refused
	at org.apache.hadoop.ipc.Client.wrapException(Client.java:1142)
	at org.apache.hadoop.ipc.Client.call(Client.java:1118)
	at org.apache.hadoop.ipc.RPC$Invoker.invoke(RPC.java:229)
	at com.sun.proxy.$Proxy8.getFileInfo(Unknown Source)
	at sun.reflect.GeneratedMethodAccessor2.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:622)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:85)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:62)
	at com.sun.proxy.$Proxy8.getFileInfo(Unknown Source)
	at sun.reflect.GeneratedMethodAccessor2.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:622)
	at org.apache.hadoop.hbase.fs.HFileSystem$1.invoke(HFileSystem.java:294)
	at com.sun.proxy.$Proxy9.getFileInfo(Unknown Source)
	at org.apache.hadoop.hdfs.DFSClient.getFileInfo(DFSClient.java:1059)
	at org.apache.hadoop.hdfs.DistributedFileSystem.getFileStatus(DistributedFileSystem.java:554)
	at org.apache.hadoop.fs.FilterFileSystem.getFileStatus(FilterFileSystem.java:255)
	at org.apache.hadoop.fs.FileSystem.exists(FileSystem.java:788)
	at org.apache.hadoop.hbase.regionserver.StoreFile$WriterBuilder.build(StoreFile.java:603)
	at org.apache.hadoop.hbase.regionserver.HStore.createWriterInTmp(HStore.java:889)
	at org.apache.hadoop.hbase.regionserver.DefaultStoreFlusher.flushSnapshot(DefaultStoreFlusher.java:69)
	at org.apache.hadoop.hbase.regionserver.HStore.flushCache(HStore.java:798)
	at org.apache.hadoop.hbase.regionserver.HStore$StoreFlusherImpl.flushCache(HStore.java:1949)
	at org.apache.hadoop.hbase.regionserver.HRegion.internalFlushcache(HRegion.java:1776)
	at org.apache.hadoop.hbase.regionserver.HRegion.internalFlushcache(HRegion.java:1658)
	at org.apache.hadoop.hbase.regionserver.HRegion.doClose(HRegion.java:1119)
	at org.apache.hadoop.hbase.regionserver.HRegion.close(HRegion.java:1085)
	at org.apache.hadoop.hbase.regionserver.handler.CloseRegionHandler.process(CloseRegionHandler.java:147)
	at org.apache.hadoop.hbase.executor.EventHandler.run(EventHandler.java:128)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1146)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:701)
Caused by: java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:601)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:511)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:481)
	at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:457)
	at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:583)
	at org.apache.hadoop.ipc.Client$Connection.access$2200(Client.java:205)
	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1249)
	at org.apache.hadoop.ipc.Client.call(Client.java:1093)
	... 31 more
2014-07-01 15:51:25,282 INFO  [RS_CLOSE_REGION-slave1:60020-1] ipc.Client: Retrying connect to server: master/9.1.143.58:54310. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1 SECONDS)
2014-07-01 15:51:25,655 INFO  [regionserver60020] regionserver.HRegionServer: Received CLOSE for the region: 122e92eabfab38cb15a9965b21db25c5 ,which we are already trying to CLOSE, but not completed yet
2014-07-01 15:51:25,655 WARN  [regionserver60020] regionserver.HRegionServer: Failed to close usertable,user5,1404228265526.122e92eabfab38cb15a9965b21db25c5. - ignoring and continuing
org.apache.hadoop.hbase.regionserver.RegionAlreadyInTransitionException: The region 122e92eabfab38cb15a9965b21db25c5 was already closing. New CLOSE request is ignored.
	at org.apache.hadoop.hbase.regionserver.HRegionServer.closeRegion(HRegionServer.java:2586)
	at org.apache.hadoop.hbase.regionserver.HRegionServer.closeRegionIgnoreErrors(HRegionServer.java:2508)
	at org.apache.hadoop.hbase.regionserver.HRegionServer.closeUserRegions(HRegionServer.java:2098)
	at org.apache.hadoop.hbase.regionserver.HRegionServer.run(HRegionServer.java:894)
	at java.lang.Thread.run(Thread.java:701)
2014-07-01 15:51:25,656 INFO  [regionserver60020] regionserver.HRegionServer: Received CLOSE for the region: bb79ae773f71289e0745068f68a0abd9 ,which we are already trying to CLOSE, but not completed yet
2014-07-01 15:51:25,656 WARN  [regionserver60020] regionserver.HRegionServer: Failed to close usertable,user9,1404228265526.bb79ae773f71289e0745068f68a0abd9. - ignoring and continuing
org.apache.hadoop.hbase.regionserver.RegionAlreadyInTransitionException: The region bb79ae773f71289e0745068f68a0abd9 was already closing. New CLOSE request is ignored.
	at org.apache.hadoop.hbase.regionserver.HRegionServer.closeRegion(HRegionServer.java:2586)
	at org.apache.hadoop.hbase.regionserver.HRegionServer.closeRegionIgnoreErrors(HRegionServer.java:2508)
	at org.apache.hadoop.hbase.regionserver.HRegionServer.closeUserRegions(HRegionServer.java:2098)
	at org.apache.hadoop.hbase.regionserver.HRegionServer.run(HRegionServer.java:894)
	at java.lang.Thread.run(Thread.java:701)
2014-07-01 15:51:25,656 INFO  [regionserver60020] regionserver.HRegionServer: Received CLOSE for the region: 9cef9e7d34f96f1d9b111be50040a208 ,which we are already trying to CLOSE, but not completed yet
2014-07-01 15:51:25,656 WARN  [regionserver60020] regionserver.HRegionServer: Failed to close usertable,,1404228265525.9cef9e7d34f96f1d9b111be50040a208. - ignoring and continuing
org.apache.hadoop.hbase.regionserver.RegionAlreadyInTransitionException: The region 9cef9e7d34f96f1d9b111be50040a208 was already closing. New CLOSE request is ignored.
	at org.apache.hadoop.hbase.regionserver.HRegionServer.closeRegion(HRegionServer.java:2586)
	at org.apache.hadoop.hbase.regionserver.HRegionServer.closeRegionIgnoreErrors(HRegionServer.java:2508)
	at org.apache.hadoop.hbase.regionserver.HRegionServer.closeUserRegions(HRegionServer.java:2098)
	at org.apache.hadoop.hbase.regionserver.HRegionServer.run(HRegionServer.java:894)
	at java.lang.Thread.run(Thread.java:701)
2014-07-01 15:51:25,656 INFO  [regionserver60020] regionserver.HRegionServer: Received CLOSE for the region: 93253e37c284f3174309fede3f32339f ,which we are already trying to CLOSE, but not completed yet
2014-07-01 15:51:25,656 WARN  [regionserver60020] regionserver.HRegionServer: Failed to close usertable,user3,1404228265526.93253e37c284f3174309fede3f32339f. - ignoring and continuing
org.apache.hadoop.hbase.regionserver.RegionAlreadyInTransitionException: The region 93253e37c284f3174309fede3f32339f was already closing. New CLOSE request is ignored.
	at org.apache.hadoop.hbase.regionserver.HRegionServer.closeRegion(HRegionServer.java:2586)
	at org.apache.hadoop.hbase.regionserver.HRegionServer.closeRegionIgnoreErrors(HRegionServer.java:2508)
	at org.apache.hadoop.hbase.regionserver.HRegionServer.closeUserRegions(HRegionServer.java:2098)
	at org.apache.hadoop.hbase.regionserver.HRegionServer.run(HRegionServer.java:894)
	at java.lang.Thread.run(Thread.java:701)
2014-07-01 15:51:25,657 INFO  [regionserver60020] regionserver.HRegionServer: Received CLOSE for the region: 667c1f2c34fe37d48ba25a789d8aba98 ,which we are already trying to CLOSE, but not completed yet
2014-07-01 15:51:25,657 WARN  [regionserver60020] regionserver.HRegionServer: Failed to close usertable,user7,1404228265526.667c1f2c34fe37d48ba25a789d8aba98. - ignoring and continuing
org.apache.hadoop.hbase.regionserver.RegionAlreadyInTransitionException: The region 667c1f2c34fe37d48ba25a789d8aba98 was already closing. New CLOSE request is ignored.
	at org.apache.hadoop.hbase.regionserver.HRegionServer.closeRegion(HRegionServer.java:2586)
	at org.apache.hadoop.hbase.regionserver.HRegionServer.closeRegionIgnoreErrors(HRegionServer.java:2508)
	at org.apache.hadoop.hbase.regionserver.HRegionServer.closeUserRegions(HRegionServer.java:2098)
	at org.apache.hadoop.hbase.regionserver.HRegionServer.run(HRegionServer.java:894)
	at java.lang.Thread.run(Thread.java:701)
2014-07-01 15:51:25,657 DEBUG [regionserver60020] regionserver.HRegionServer: Waiting on 122e92eabfab38cb15a9965b21db25c5, bb79ae773f71289e0745068f68a0abd9, 9cef9e7d34f96f1d9b111be50040a208, 93253e37c284f3174309fede3f32339f, 667c1f2c34fe37d48ba25a789d8aba98
2014-07-01 15:51:26,283 INFO  [RS_CLOSE_REGION-slave1:60020-1] ipc.Client: Retrying connect to server: master/9.1.143.58:54310. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1 SECONDS)
2014-07-01 15:51:27,284 INFO  [RS_CLOSE_REGION-slave1:60020-1] ipc.Client: Retrying connect to server: master/9.1.143.58:54310. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1 SECONDS)
2014-07-01 15:51:28,285 INFO  [RS_CLOSE_REGION-slave1:60020-1] ipc.Client: Retrying connect to server: master/9.1.143.58:54310. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1 SECONDS)
2014-07-01 15:51:28,661 INFO  [regionserver60020] regionserver.HRegionServer: Received CLOSE for the region: 122e92eabfab38cb15a9965b21db25c5 ,which we are already trying to CLOSE, but not completed yet
2014-07-01 15:51:28,661 WARN  [regionserver60020] regionserver.HRegionServer: Failed to close usertable,user5,1404228265526.122e92eabfab38cb15a9965b21db25c5. - ignoring and continuing
org.apache.hadoop.hbase.regionserver.RegionAlreadyInTransitionException: The region 122e92eabfab38cb15a9965b21db25c5 was already closing. New CLOSE request is ignored.
	at org.apache.hadoop.hbase.regionserver.HRegionServer.closeRegion(HRegionServer.java:2586)
	at org.apache.hadoop.hbase.regionserver.HRegionServer.closeRegionIgnoreErrors(HRegionServer.java:2508)
	at org.apache.hadoop.hbase.regionserver.HRegionServer.closeUserRegions(HRegionServer.java:2098)
	at org.apache.hadoop.hbase.regionserver.HRegionServer.run(HRegionServer.java:894)
	at java.lang.Thread.run(Thread.java:701)
2014-07-01 15:51:28,661 INFO  [regionserver60020] regionserver.HRegionServer: Received CLOSE for the region: bb79ae773f71289e0745068f68a0abd9 ,which we are already trying to CLOSE, but not completed yet
2014-07-01 15:51:28,661 WARN  [regionserver60020] regionserver.HRegionServer: Failed to close usertable,user9,1404228265526.bb79ae773f71289e0745068f68a0abd9. - ignoring and continuing
org.apache.hadoop.hbase.regionserver.RegionAlreadyInTransitionException: The region bb79ae773f71289e0745068f68a0abd9 was already closing. New CLOSE request is ignored.
	at org.apache.hadoop.hbase.regionserver.HRegionServer.closeRegion(HRegionServer.java:2586)
	at org.apache.hadoop.hbase.regionserver.HRegionServer.closeRegionIgnoreErrors(HRegionServer.java:2508)
	at org.apache.hadoop.hbase.regionserver.HRegionServer.closeUserRegions(HRegionServer.java:2098)
	at org.apache.hadoop.hbase.regionserver.HRegionServer.run(HRegionServer.java:894)
	at java.lang.Thread.run(Thread.java:701)
2014-07-01 15:51:28,661 INFO  [regionserver60020] regionserver.HRegionServer: Received CLOSE for the region: 9cef9e7d34f96f1d9b111be50040a208 ,which we are already trying to CLOSE, but not completed yet
2014-07-01 15:51:28,661 WARN  [regionserver60020] regionserver.HRegionServer: Failed to close usertable,,1404228265525.9cef9e7d34f96f1d9b111be50040a208. - ignoring and continuing
org.apache.hadoop.hbase.regionserver.RegionAlreadyInTransitionException: The region 9cef9e7d34f96f1d9b111be50040a208 was already closing. New CLOSE request is ignored.
	at org.apache.hadoop.hbase.regionserver.HRegionServer.closeRegion(HRegionServer.java:2586)
	at org.apache.hadoop.hbase.regionserver.HRegionServer.closeRegionIgnoreErrors(HRegionServer.java:2508)
	at org.apache.hadoop.hbase.regionserver.HRegionServer.closeUserRegions(HRegionServer.java:2098)
	at org.apache.hadoop.hbase.regionserver.HRegionServer.run(HRegionServer.java:894)
	at java.lang.Thread.run(Thread.java:701)
2014-07-01 15:51:28,661 INFO  [regionserver60020] regionserver.HRegionServer: Received CLOSE for the region: 93253e37c284f3174309fede3f32339f ,which we are already trying to CLOSE, but not completed yet
2014-07-01 15:51:28,661 WARN  [regionserver60020] regionserver.HRegionServer: Failed to close usertable,user3,1404228265526.93253e37c284f3174309fede3f32339f. - ignoring and continuing
org.apache.hadoop.hbase.regionserver.RegionAlreadyInTransitionException: The region 93253e37c284f3174309fede3f32339f was already closing. New CLOSE request is ignored.
	at org.apache.hadoop.hbase.regionserver.HRegionServer.closeRegion(HRegionServer.java:2586)
	at org.apache.hadoop.hbase.regionserver.HRegionServer.closeRegionIgnoreErrors(HRegionServer.java:2508)
	at org.apache.hadoop.hbase.regionserver.HRegionServer.closeUserRegions(HRegionServer.java:2098)
	at org.apache.hadoop.hbase.regionserver.HRegionServer.run(HRegionServer.java:894)
	at java.lang.Thread.run(Thread.java:701)
2014-07-01 15:51:28,662 INFO  [regionserver60020] regionserver.HRegionServer: Received CLOSE for the region: 667c1f2c34fe37d48ba25a789d8aba98 ,which we are already trying to CLOSE, but not completed yet
2014-07-01 15:51:28,662 WARN  [regionserver60020] regionserver.HRegionServer: Failed to close usertable,user7,1404228265526.667c1f2c34fe37d48ba25a789d8aba98. - ignoring and continuing
org.apache.hadoop.hbase.regionserver.RegionAlreadyInTransitionException: The region 667c1f2c34fe37d48ba25a789d8aba98 was already closing. New CLOSE request is ignored.
	at org.apache.hadoop.hbase.regionserver.HRegionServer.closeRegion(HRegionServer.java:2586)
	at org.apache.hadoop.hbase.regionserver.HRegionServer.closeRegionIgnoreErrors(HRegionServer.java:2508)
	at org.apache.hadoop.hbase.regionserver.HRegionServer.closeUserRegions(HRegionServer.java:2098)
	at org.apache.hadoop.hbase.regionserver.HRegionServer.run(HRegionServer.java:894)
	at java.lang.Thread.run(Thread.java:701)
2014-07-01 15:51:28,662 DEBUG [regionserver60020] regionserver.HRegionServer: Waiting on 122e92eabfab38cb15a9965b21db25c5, bb79ae773f71289e0745068f68a0abd9, 9cef9e7d34f96f1d9b111be50040a208, 93253e37c284f3174309fede3f32339f, 667c1f2c34fe37d48ba25a789d8aba98
2014-07-01 15:51:29,286 INFO  [RS_CLOSE_REGION-slave1:60020-1] ipc.Client: Retrying connect to server: master/9.1.143.58:54310. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1 SECONDS)
2014-07-01 15:51:30,287 INFO  [RS_CLOSE_REGION-slave1:60020-1] ipc.Client: Retrying connect to server: master/9.1.143.58:54310. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1 SECONDS)
2014-07-01 15:51:30,293 WARN  [org.apache.hadoop.hdfs.LeaseRenewer$1@4fae46d5] hdfs.LeaseRenewer: Failed to renew lease for [DFSClient_hb_rs_slave1,60020,1404253868846_-130361673_31] for 30 seconds.  Will retry shortly ...
org.apache.hadoop.ipc.RemoteException: org.apache.hadoop.hdfs.server.namenode.SafeModeException: Cannot renew lease for DFSClient_hb_rs_slave1,60020,1404253868846_-130361673_31. Name node is in safe mode.
The reported blocks is only 0 but the threshold is 0.9990 and the total blocks 481. Safe mode will be turned off automatically.
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.renewLease(FSNamesystem.java:2771)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.renewLease(NameNode.java:919)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:622)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:587)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1432)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1428)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:416)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1190)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:1426)

	at org.apache.hadoop.ipc.Client.call(Client.java:1113)
	at org.apache.hadoop.ipc.RPC$Invoker.invoke(RPC.java:229)
	at com.sun.proxy.$Proxy8.renewLease(Unknown Source)
	at sun.reflect.GeneratedMethodAccessor29.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:622)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:85)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:62)
	at com.sun.proxy.$Proxy8.renewLease(Unknown Source)
	at sun.reflect.GeneratedMethodAccessor29.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:622)
	at org.apache.hadoop.hbase.fs.HFileSystem$1.invoke(HFileSystem.java:294)
	at com.sun.proxy.$Proxy9.renewLease(Unknown Source)
	at org.apache.hadoop.hdfs.DFSClient.renewLease(DFSClient.java:379)
	at org.apache.hadoop.hdfs.LeaseRenewer.renew(LeaseRenewer.java:378)
	at org.apache.hadoop.hdfs.LeaseRenewer.run(LeaseRenewer.java:400)
	at org.apache.hadoop.hdfs.LeaseRenewer.access$600(LeaseRenewer.java:69)
	at org.apache.hadoop.hdfs.LeaseRenewer$1.run(LeaseRenewer.java:273)
	at java.lang.Thread.run(Thread.java:701)
2014-07-01 15:51:30,315 DEBUG [RS_CLOSE_REGION-slave1:60020-0] util.FSUtils: DFS Client does not support most favored nodes create; using default create
2014-07-01 15:51:30,317 DEBUG [RS_CLOSE_REGION-slave1:60020-1] util.FSUtils: DFS Client does not support most favored nodes create; using default create
2014-07-01 15:51:30,317 DEBUG [RS_CLOSE_REGION-slave1:60020-2] util.FSUtils: DFS Client does not support most favored nodes create; using default create
2014-07-01 15:51:30,323 WARN  [regionserver60020-smallCompactions-1404253964856] hdfs.DFSClient: Failed to connect to /9.1.143.58:50010, add to deadNodes and continuejava.net.ConnectException: Connection refused
2014-07-01 15:51:30,323 INFO  [regionserver60020-smallCompactions-1404253964856] hdfs.DFSClient: Could not obtain blk_221234652107626381_29849 from any node: java.io.IOException: No live nodes contain current block. Will get new block locations from namenode and retry...
2014-07-01 15:51:30,337 WARN  [RS_CLOSE_REGION-slave1:60020-0] regionserver.HStore: Failed flushing store file, retrying num=1
org.apache.hadoop.ipc.RemoteException: org.apache.hadoop.hdfs.server.namenode.SafeModeException: Cannot create /hbase/data/default/usertable/122e92eabfab38cb15a9965b21db25c5/.tmp/a90b43db00d44e32a26bddc4de24dfdd. Name node is in safe mode.
The reported blocks is only 0 but the threshold is 0.9990 and the total blocks 481. Safe mode will be turned off automatically.
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.startFileInternal(FSNamesystem.java:1561)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.startFile(FSNamesystem.java:1527)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.create(NameNode.java:710)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.create(NameNode.java:689)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:622)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:587)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1432)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1428)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:416)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1190)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:1426)

	at org.apache.hadoop.ipc.Client.call(Client.java:1113)
	at org.apache.hadoop.ipc.RPC$Invoker.invoke(RPC.java:229)
	at com.sun.proxy.$Proxy8.create(Unknown Source)
	at sun.reflect.GeneratedMethodAccessor26.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:622)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:85)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:62)
	at com.sun.proxy.$Proxy8.create(Unknown Source)
	at sun.reflect.GeneratedMethodAccessor26.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:622)
	at org.apache.hadoop.hbase.fs.HFileSystem$1.invoke(HFileSystem.java:294)
	at com.sun.proxy.$Proxy9.create(Unknown Source)
	at org.apache.hadoop.hdfs.DFSClient$DFSOutputStream.<init>(DFSClient.java:3451)
	at org.apache.hadoop.hdfs.DFSClient.create(DFSClient.java:870)
	at org.apache.hadoop.hdfs.DistributedFileSystem.create(DistributedFileSystem.java:205)
	at org.apache.hadoop.fs.FilterFileSystem.create(FilterFileSystem.java:126)
	at org.apache.hadoop.hbase.util.FSUtils.create(FSUtils.java:341)
	at org.apache.hadoop.hbase.util.FSUtils.create(FSUtils.java:316)
	at org.apache.hadoop.hbase.io.hfile.AbstractHFileWriter.createOutputStream(AbstractHFileWriter.java:266)
	at org.apache.hadoop.hbase.io.hfile.HFile$WriterFactory.create(HFile.java:391)
	at org.apache.hadoop.hbase.regionserver.StoreFile$Writer.<init>(StoreFile.java:728)
	at org.apache.hadoop.hbase.regionserver.StoreFile$Writer.<init>(StoreFile.java:679)
	at org.apache.hadoop.hbase.regionserver.StoreFile$WriterBuilder.build(StoreFile.java:617)
	at org.apache.hadoop.hbase.regionserver.HStore.createWriterInTmp(HStore.java:889)
	at org.apache.hadoop.hbase.regionserver.DefaultStoreFlusher.flushSnapshot(DefaultStoreFlusher.java:69)
	at org.apache.hadoop.hbase.regionserver.HStore.flushCache(HStore.java:798)
	at org.apache.hadoop.hbase.regionserver.HStore$StoreFlusherImpl.flushCache(HStore.java:1949)
	at org.apache.hadoop.hbase.regionserver.HRegion.internalFlushcache(HRegion.java:1776)
	at org.apache.hadoop.hbase.regionserver.HRegion.internalFlushcache(HRegion.java:1658)
	at org.apache.hadoop.hbase.regionserver.HRegion.doClose(HRegion.java:1119)
	at org.apache.hadoop.hbase.regionserver.HRegion.close(HRegion.java:1085)
	at org.apache.hadoop.hbase.regionserver.handler.CloseRegionHandler.process(CloseRegionHandler.java:147)
	at org.apache.hadoop.hbase.executor.EventHandler.run(EventHandler.java:128)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1146)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:701)
2014-07-01 15:51:30,337 WARN  [RS_CLOSE_REGION-slave1:60020-1] regionserver.HStore: Failed flushing store file, retrying num=3
org.apache.hadoop.ipc.RemoteException: org.apache.hadoop.hdfs.server.namenode.SafeModeException: Cannot create /hbase/data/default/usertable/bb79ae773f71289e0745068f68a0abd9/.tmp/2f4fbda0024c43d7b33857fcd0b68b22. Name node is in safe mode.
The reported blocks is only 0 but the threshold is 0.9990 and the total blocks 481. Safe mode will be turned off automatically.
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.startFileInternal(FSNamesystem.java:1561)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.startFile(FSNamesystem.java:1527)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.create(NameNode.java:710)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.create(NameNode.java:689)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:622)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:587)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1432)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1428)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:416)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1190)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:1426)

	at org.apache.hadoop.ipc.Client.call(Client.java:1113)
	at org.apache.hadoop.ipc.RPC$Invoker.invoke(RPC.java:229)
	at com.sun.proxy.$Proxy8.create(Unknown Source)
	at sun.reflect.GeneratedMethodAccessor26.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:622)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:85)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:62)
	at com.sun.proxy.$Proxy8.create(Unknown Source)
	at sun.reflect.GeneratedMethodAccessor26.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:622)
	at org.apache.hadoop.hbase.fs.HFileSystem$1.invoke(HFileSystem.java:294)
	at com.sun.proxy.$Proxy9.create(Unknown Source)
	at org.apache.hadoop.hdfs.DFSClient$DFSOutputStream.<init>(DFSClient.java:3451)
	at org.apache.hadoop.hdfs.DFSClient.create(DFSClient.java:870)
	at org.apache.hadoop.hdfs.DistributedFileSystem.create(DistributedFileSystem.java:205)
	at org.apache.hadoop.fs.FilterFileSystem.create(FilterFileSystem.java:126)
	at org.apache.hadoop.hbase.util.FSUtils.create(FSUtils.java:341)
	at org.apache.hadoop.hbase.util.FSUtils.create(FSUtils.java:316)
	at org.apache.hadoop.hbase.io.hfile.AbstractHFileWriter.createOutputStream(AbstractHFileWriter.java:266)
	at org.apache.hadoop.hbase.io.hfile.HFile$WriterFactory.create(HFile.java:391)
	at org.apache.hadoop.hbase.regionserver.StoreFile$Writer.<init>(StoreFile.java:728)
	at org.apache.hadoop.hbase.regionserver.StoreFile$Writer.<init>(StoreFile.java:679)
	at org.apache.hadoop.hbase.regionserver.StoreFile$WriterBuilder.build(StoreFile.java:617)
	at org.apache.hadoop.hbase.regionserver.HStore.createWriterInTmp(HStore.java:889)
	at org.apache.hadoop.hbase.regionserver.DefaultStoreFlusher.flushSnapshot(DefaultStoreFlusher.java:69)
	at org.apache.hadoop.hbase.regionserver.HStore.flushCache(HStore.java:798)
	at org.apache.hadoop.hbase.regionserver.HStore$StoreFlusherImpl.flushCache(HStore.java:1949)
	at org.apache.hadoop.hbase.regionserver.HRegion.internalFlushcache(HRegion.java:1776)
	at org.apache.hadoop.hbase.regionserver.HRegion.internalFlushcache(HRegion.java:1658)
	at org.apache.hadoop.hbase.regionserver.HRegion.doClose(HRegion.java:1119)
	at org.apache.hadoop.hbase.regionserver.HRegion.close(HRegion.java:1085)
	at org.apache.hadoop.hbase.regionserver.handler.CloseRegionHandler.process(CloseRegionHandler.java:147)
	at org.apache.hadoop.hbase.executor.EventHandler.run(EventHandler.java:128)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1146)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:701)
2014-07-01 15:51:30,337 WARN  [RS_CLOSE_REGION-slave1:60020-2] regionserver.HStore: Failed flushing store file, retrying num=4
org.apache.hadoop.ipc.RemoteException: org.apache.hadoop.hdfs.server.namenode.SafeModeException: Cannot create /hbase/data/default/usertable/9cef9e7d34f96f1d9b111be50040a208/.tmp/cb6b9281cb2942038fab0c684d2bac69. Name node is in safe mode.
The reported blocks is only 0 but the threshold is 0.9990 and the total blocks 481. Safe mode will be turned off automatically.
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.startFileInternal(FSNamesystem.java:1561)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.startFile(FSNamesystem.java:1527)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.create(NameNode.java:710)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.create(NameNode.java:689)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:622)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:587)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1432)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1428)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:416)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1190)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:1426)

	at org.apache.hadoop.ipc.Client.call(Client.java:1113)
	at org.apache.hadoop.ipc.RPC$Invoker.invoke(RPC.java:229)
	at com.sun.proxy.$Proxy8.create(Unknown Source)
	at sun.reflect.GeneratedMethodAccessor26.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:622)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:85)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:62)
	at com.sun.proxy.$Proxy8.create(Unknown Source)
	at sun.reflect.GeneratedMethodAccessor26.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:622)
	at org.apache.hadoop.hbase.fs.HFileSystem$1.invoke(HFileSystem.java:294)
	at com.sun.proxy.$Proxy9.create(Unknown Source)
	at org.apache.hadoop.hdfs.DFSClient$DFSOutputStream.<init>(DFSClient.java:3451)
	at org.apache.hadoop.hdfs.DFSClient.create(DFSClient.java:870)
	at org.apache.hadoop.hdfs.DistributedFileSystem.create(DistributedFileSystem.java:205)
	at org.apache.hadoop.fs.FilterFileSystem.create(FilterFileSystem.java:126)
	at org.apache.hadoop.hbase.util.FSUtils.create(FSUtils.java:341)
	at org.apache.hadoop.hbase.util.FSUtils.create(FSUtils.java:316)
	at org.apache.hadoop.hbase.io.hfile.AbstractHFileWriter.createOutputStream(AbstractHFileWriter.java:266)
	at org.apache.hadoop.hbase.io.hfile.HFile$WriterFactory.create(HFile.java:391)
	at org.apache.hadoop.hbase.regionserver.StoreFile$Writer.<init>(StoreFile.java:728)
	at org.apache.hadoop.hbase.regionserver.StoreFile$Writer.<init>(StoreFile.java:679)
	at org.apache.hadoop.hbase.regionserver.StoreFile$WriterBuilder.build(StoreFile.java:617)
	at org.apache.hadoop.hbase.regionserver.HStore.createWriterInTmp(HStore.java:889)
	at org.apache.hadoop.hbase.regionserver.DefaultStoreFlusher.flushSnapshot(DefaultStoreFlusher.java:69)
	at org.apache.hadoop.hbase.regionserver.HStore.flushCache(HStore.java:798)
	at org.apache.hadoop.hbase.regionserver.HStore$StoreFlusherImpl.flushCache(HStore.java:1949)
	at org.apache.hadoop.hbase.regionserver.HRegion.internalFlushcache(HRegion.java:1776)
	at org.apache.hadoop.hbase.regionserver.HRegion.internalFlushcache(HRegion.java:1658)
	at org.apache.hadoop.hbase.regionserver.HRegion.doClose(HRegion.java:1119)
	at org.apache.hadoop.hbase.regionserver.HRegion.close(HRegion.java:1085)
	at org.apache.hadoop.hbase.regionserver.handler.CloseRegionHandler.process(CloseRegionHandler.java:147)
	at org.apache.hadoop.hbase.executor.EventHandler.run(EventHandler.java:128)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1146)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:701)
2014-07-01 15:51:31,297 WARN  [org.apache.hadoop.hdfs.LeaseRenewer$1@4fae46d5] hdfs.LeaseRenewer: Failed to renew lease for [DFSClient_hb_rs_slave1,60020,1404253868846_-130361673_31] for 30 seconds.  Will retry shortly ...
org.apache.hadoop.ipc.RemoteException: org.apache.hadoop.hdfs.server.namenode.SafeModeException: Cannot renew lease for DFSClient_hb_rs_slave1,60020,1404253868846_-130361673_31. Name node is in safe mode.
The reported blocks is only 3 but the threshold is 0.9990 and the total blocks 481. Safe mode will be turned off automatically.
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.renewLease(FSNamesystem.java:2771)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.renewLease(NameNode.java:919)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:622)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:587)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1432)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1428)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:416)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1190)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:1426)

	at org.apache.hadoop.ipc.Client.call(Client.java:1113)
	at org.apache.hadoop.ipc.RPC$Invoker.invoke(RPC.java:229)
	at com.sun.proxy.$Proxy8.renewLease(Unknown Source)
	at sun.reflect.GeneratedMethodAccessor29.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:622)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:85)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:62)
	at com.sun.proxy.$Proxy8.renewLease(Unknown Source)
	at sun.reflect.GeneratedMethodAccessor29.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:622)
	at org.apache.hadoop.hbase.fs.HFileSystem$1.invoke(HFileSystem.java:294)
	at com.sun.proxy.$Proxy9.renewLease(Unknown Source)
	at org.apache.hadoop.hdfs.DFSClient.renewLease(DFSClient.java:379)
	at org.apache.hadoop.hdfs.LeaseRenewer.renew(LeaseRenewer.java:378)
	at org.apache.hadoop.hdfs.LeaseRenewer.run(LeaseRenewer.java:400)
	at org.apache.hadoop.hdfs.LeaseRenewer.access$600(LeaseRenewer.java:69)
	at org.apache.hadoop.hdfs.LeaseRenewer$1.run(LeaseRenewer.java:273)
	at java.lang.Thread.run(Thread.java:701)
2014-07-01 15:51:31,368 DEBUG [RS_CLOSE_REGION-slave1:60020-0] util.FSUtils: DFS Client does not support most favored nodes create; using default create
2014-07-01 15:51:31,371 WARN  [RS_CLOSE_REGION-slave1:60020-0] regionserver.HStore: Failed flushing store file, retrying num=2
org.apache.hadoop.ipc.RemoteException: org.apache.hadoop.hdfs.server.namenode.SafeModeException: Cannot create /hbase/data/default/usertable/122e92eabfab38cb15a9965b21db25c5/.tmp/87a072ac561449f097e0ae76a404478e. Name node is in safe mode.
The reported blocks is only 3 but the threshold is 0.9990 and the total blocks 481. Safe mode will be turned off automatically.
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.startFileInternal(FSNamesystem.java:1561)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.startFile(FSNamesystem.java:1527)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.create(NameNode.java:710)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.create(NameNode.java:689)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:622)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:587)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1432)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1428)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:416)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1190)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:1426)

	at org.apache.hadoop.ipc.Client.call(Client.java:1113)
	at org.apache.hadoop.ipc.RPC$Invoker.invoke(RPC.java:229)
	at com.sun.proxy.$Proxy8.create(Unknown Source)
	at sun.reflect.GeneratedMethodAccessor26.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:622)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:85)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:62)
	at com.sun.proxy.$Proxy8.create(Unknown Source)
	at sun.reflect.GeneratedMethodAccessor26.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:622)
	at org.apache.hadoop.hbase.fs.HFileSystem$1.invoke(HFileSystem.java:294)
	at com.sun.proxy.$Proxy9.create(Unknown Source)
	at org.apache.hadoop.hdfs.DFSClient$DFSOutputStream.<init>(DFSClient.java:3451)
	at org.apache.hadoop.hdfs.DFSClient.create(DFSClient.java:870)
	at org.apache.hadoop.hdfs.DistributedFileSystem.create(DistributedFileSystem.java:205)
	at org.apache.hadoop.fs.FilterFileSystem.create(FilterFileSystem.java:126)
	at org.apache.hadoop.hbase.util.FSUtils.create(FSUtils.java:341)
	at org.apache.hadoop.hbase.util.FSUtils.create(FSUtils.java:316)
	at org.apache.hadoop.hbase.io.hfile.AbstractHFileWriter.createOutputStream(AbstractHFileWriter.java:266)
	at org.apache.hadoop.hbase.io.hfile.HFile$WriterFactory.create(HFile.java:391)
	at org.apache.hadoop.hbase.regionserver.StoreFile$Writer.<init>(StoreFile.java:728)
	at org.apache.hadoop.hbase.regionserver.StoreFile$Writer.<init>(StoreFile.java:679)
	at org.apache.hadoop.hbase.regionserver.StoreFile$WriterBuilder.build(StoreFile.java:617)
	at org.apache.hadoop.hbase.regionserver.HStore.createWriterInTmp(HStore.java:889)
	at org.apache.hadoop.hbase.regionserver.DefaultStoreFlusher.flushSnapshot(DefaultStoreFlusher.java:69)
	at org.apache.hadoop.hbase.regionserver.HStore.flushCache(HStore.java:798)
	at org.apache.hadoop.hbase.regionserver.HStore$StoreFlusherImpl.flushCache(HStore.java:1949)
	at org.apache.hadoop.hbase.regionserver.HRegion.internalFlushcache(HRegion.java:1776)
	at org.apache.hadoop.hbase.regionserver.HRegion.internalFlushcache(HRegion.java:1658)
	at org.apache.hadoop.hbase.regionserver.HRegion.doClose(HRegion.java:1119)
	at org.apache.hadoop.hbase.regionserver.HRegion.close(HRegion.java:1085)
	at org.apache.hadoop.hbase.regionserver.handler.CloseRegionHandler.process(CloseRegionHandler.java:147)
	at org.apache.hadoop.hbase.executor.EventHandler.run(EventHandler.java:128)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1146)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:701)
2014-07-01 15:51:31,408 DEBUG [RS_CLOSE_REGION-slave1:60020-2] util.FSUtils: DFS Client does not support most favored nodes create; using default create
2014-07-01 15:51:31,411 WARN  [RS_CLOSE_REGION-slave1:60020-2] regionserver.HStore: Failed flushing store file, retrying num=5
org.apache.hadoop.ipc.RemoteException: org.apache.hadoop.hdfs.server.namenode.SafeModeException: Cannot create /hbase/data/default/usertable/9cef9e7d34f96f1d9b111be50040a208/.tmp/e22a78556b1447b0a908f49f7386b6f1. Name node is in safe mode.
The reported blocks is only 3 but the threshold is 0.9990 and the total blocks 481. Safe mode will be turned off automatically.
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.startFileInternal(FSNamesystem.java:1561)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.startFile(FSNamesystem.java:1527)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.create(NameNode.java:710)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.create(NameNode.java:689)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:622)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:587)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1432)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1428)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:416)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1190)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:1426)

	at org.apache.hadoop.ipc.Client.call(Client.java:1113)
	at org.apache.hadoop.ipc.RPC$Invoker.invoke(RPC.java:229)
	at com.sun.proxy.$Proxy8.create(Unknown Source)
	at sun.reflect.GeneratedMethodAccessor26.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:622)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:85)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:62)
	at com.sun.proxy.$Proxy8.create(Unknown Source)
	at sun.reflect.GeneratedMethodAccessor26.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:622)
	at org.apache.hadoop.hbase.fs.HFileSystem$1.invoke(HFileSystem.java:294)
	at com.sun.proxy.$Proxy9.create(Unknown Source)
	at org.apache.hadoop.hdfs.DFSClient$DFSOutputStream.<init>(DFSClient.java:3451)
	at org.apache.hadoop.hdfs.DFSClient.create(DFSClient.java:870)
	at org.apache.hadoop.hdfs.DistributedFileSystem.create(DistributedFileSystem.java:205)
	at org.apache.hadoop.fs.FilterFileSystem.create(FilterFileSystem.java:126)
	at org.apache.hadoop.hbase.util.FSUtils.create(FSUtils.java:341)
	at org.apache.hadoop.hbase.util.FSUtils.create(FSUtils.java:316)
	at org.apache.hadoop.hbase.io.hfile.AbstractHFileWriter.createOutputStream(AbstractHFileWriter.java:266)
	at org.apache.hadoop.hbase.io.hfile.HFile$WriterFactory.create(HFile.java:391)
	at org.apache.hadoop.hbase.regionserver.StoreFile$Writer.<init>(StoreFile.java:728)
	at org.apache.hadoop.hbase.regionserver.StoreFile$Writer.<init>(StoreFile.java:679)
	at org.apache.hadoop.hbase.regionserver.StoreFile$WriterBuilder.build(StoreFile.java:617)
	at org.apache.hadoop.hbase.regionserver.HStore.createWriterInTmp(HStore.java:889)
	at org.apache.hadoop.hbase.regionserver.DefaultStoreFlusher.flushSnapshot(DefaultStoreFlusher.java:69)
	at org.apache.hadoop.hbase.regionserver.HStore.flushCache(HStore.java:798)
	at org.apache.hadoop.hbase.regionserver.HStore$StoreFlusherImpl.flushCache(HStore.java:1949)
	at org.apache.hadoop.hbase.regionserver.HRegion.internalFlushcache(HRegion.java:1776)
	at org.apache.hadoop.hbase.regionserver.HRegion.internalFlushcache(HRegion.java:1658)
	at org.apache.hadoop.hbase.regionserver.HRegion.doClose(HRegion.java:1119)
	at org.apache.hadoop.hbase.regionserver.HRegion.close(HRegion.java:1085)
	at org.apache.hadoop.hbase.regionserver.handler.CloseRegionHandler.process(CloseRegionHandler.java:147)
	at org.apache.hadoop.hbase.executor.EventHandler.run(EventHandler.java:128)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1146)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:701)
2014-07-01 15:51:31,431 DEBUG [RS_CLOSE_REGION-slave1:60020-1] util.FSUtils: DFS Client does not support most favored nodes create; using default create
2014-07-01 15:51:31,433 WARN  [RS_CLOSE_REGION-slave1:60020-1] regionserver.HStore: Failed flushing store file, retrying num=4
org.apache.hadoop.ipc.RemoteException: org.apache.hadoop.hdfs.server.namenode.SafeModeException: Cannot create /hbase/data/default/usertable/bb79ae773f71289e0745068f68a0abd9/.tmp/6bbce9dda2504e2f9c32ccb3f2b8630d. Name node is in safe mode.
The reported blocks is only 3 but the threshold is 0.9990 and the total blocks 481. Safe mode will be turned off automatically.
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.startFileInternal(FSNamesystem.java:1561)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.startFile(FSNamesystem.java:1527)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.create(NameNode.java:710)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.create(NameNode.java:689)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:622)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:587)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1432)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1428)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:416)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1190)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:1426)

	at org.apache.hadoop.ipc.Client.call(Client.java:1113)
	at org.apache.hadoop.ipc.RPC$Invoker.invoke(RPC.java:229)
	at com.sun.proxy.$Proxy8.create(Unknown Source)
	at sun.reflect.GeneratedMethodAccessor26.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:622)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:85)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:62)
	at com.sun.proxy.$Proxy8.create(Unknown Source)
	at sun.reflect.GeneratedMethodAccessor26.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:622)
	at org.apache.hadoop.hbase.fs.HFileSystem$1.invoke(HFileSystem.java:294)
	at com.sun.proxy.$Proxy9.create(Unknown Source)
	at org.apache.hadoop.hdfs.DFSClient$DFSOutputStream.<init>(DFSClient.java:3451)
	at org.apache.hadoop.hdfs.DFSClient.create(DFSClient.java:870)
	at org.apache.hadoop.hdfs.DistributedFileSystem.create(DistributedFileSystem.java:205)
	at org.apache.hadoop.fs.FilterFileSystem.create(FilterFileSystem.java:126)
	at org.apache.hadoop.hbase.util.FSUtils.create(FSUtils.java:341)
	at org.apache.hadoop.hbase.util.FSUtils.create(FSUtils.java:316)
	at org.apache.hadoop.hbase.io.hfile.AbstractHFileWriter.createOutputStream(AbstractHFileWriter.java:266)
	at org.apache.hadoop.hbase.io.hfile.HFile$WriterFactory.create(HFile.java:391)
	at org.apache.hadoop.hbase.regionserver.StoreFile$Writer.<init>(StoreFile.java:728)
	at org.apache.hadoop.hbase.regionserver.StoreFile$Writer.<init>(StoreFile.java:679)
	at org.apache.hadoop.hbase.regionserver.StoreFile$WriterBuilder.build(StoreFile.java:617)
	at org.apache.hadoop.hbase.regionserver.HStore.createWriterInTmp(HStore.java:889)
	at org.apache.hadoop.hbase.regionserver.DefaultStoreFlusher.flushSnapshot(DefaultStoreFlusher.java:69)
	at org.apache.hadoop.hbase.regionserver.HStore.flushCache(HStore.java:798)
	at org.apache.hadoop.hbase.regionserver.HStore$StoreFlusherImpl.flushCache(HStore.java:1949)
	at org.apache.hadoop.hbase.regionserver.HRegion.internalFlushcache(HRegion.java:1776)
	at org.apache.hadoop.hbase.regionserver.HRegion.internalFlushcache(HRegion.java:1658)
	at org.apache.hadoop.hbase.regionserver.HRegion.doClose(HRegion.java:1119)
	at org.apache.hadoop.hbase.regionserver.HRegion.close(HRegion.java:1085)
	at org.apache.hadoop.hbase.regionserver.handler.CloseRegionHandler.process(CloseRegionHandler.java:147)
	at org.apache.hadoop.hbase.executor.EventHandler.run(EventHandler.java:128)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1146)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:701)
2014-07-01 15:51:31,664 INFO  [regionserver60020] regionserver.HRegionServer: Received CLOSE for the region: 122e92eabfab38cb15a9965b21db25c5 ,which we are already trying to CLOSE, but not completed yet
2014-07-01 15:51:31,664 WARN  [regionserver60020] regionserver.HRegionServer: Failed to close usertable,user5,1404228265526.122e92eabfab38cb15a9965b21db25c5. - ignoring and continuing
org.apache.hadoop.hbase.regionserver.RegionAlreadyInTransitionException: The region 122e92eabfab38cb15a9965b21db25c5 was already closing. New CLOSE request is ignored.
	at org.apache.hadoop.hbase.regionserver.HRegionServer.closeRegion(HRegionServer.java:2586)
	at org.apache.hadoop.hbase.regionserver.HRegionServer.closeRegionIgnoreErrors(HRegionServer.java:2508)
	at org.apache.hadoop.hbase.regionserver.HRegionServer.closeUserRegions(HRegionServer.java:2098)
	at org.apache.hadoop.hbase.regionserver.HRegionServer.run(HRegionServer.java:894)
	at java.lang.Thread.run(Thread.java:701)
2014-07-01 15:51:31,665 INFO  [regionserver60020] regionserver.HRegionServer: Received CLOSE for the region: bb79ae773f71289e0745068f68a0abd9 ,which we are already trying to CLOSE, but not completed yet
2014-07-01 15:51:31,665 WARN  [regionserver60020] regionserver.HRegionServer: Failed to close usertable,user9,1404228265526.bb79ae773f71289e0745068f68a0abd9. - ignoring and continuing
org.apache.hadoop.hbase.regionserver.RegionAlreadyInTransitionException: The region bb79ae773f71289e0745068f68a0abd9 was already closing. New CLOSE request is ignored.
	at org.apache.hadoop.hbase.regionserver.HRegionServer.closeRegion(HRegionServer.java:2586)
	at org.apache.hadoop.hbase.regionserver.HRegionServer.closeRegionIgnoreErrors(HRegionServer.java:2508)
	at org.apache.hadoop.hbase.regionserver.HRegionServer.closeUserRegions(HRegionServer.java:2098)
	at org.apache.hadoop.hbase.regionserver.HRegionServer.run(HRegionServer.java:894)
	at java.lang.Thread.run(Thread.java:701)
2014-07-01 15:51:31,665 INFO  [regionserver60020] regionserver.HRegionServer: Received CLOSE for the region: 9cef9e7d34f96f1d9b111be50040a208 ,which we are already trying to CLOSE, but not completed yet
2014-07-01 15:51:31,665 WARN  [regionserver60020] regionserver.HRegionServer: Failed to close usertable,,1404228265525.9cef9e7d34f96f1d9b111be50040a208. - ignoring and continuing
org.apache.hadoop.hbase.regionserver.RegionAlreadyInTransitionException: The region 9cef9e7d34f96f1d9b111be50040a208 was already closing. New CLOSE request is ignored.
	at org.apache.hadoop.hbase.regionserver.HRegionServer.closeRegion(HRegionServer.java:2586)
	at org.apache.hadoop.hbase.regionserver.HRegionServer.closeRegionIgnoreErrors(HRegionServer.java:2508)
	at org.apache.hadoop.hbase.regionserver.HRegionServer.closeUserRegions(HRegionServer.java:2098)
	at org.apache.hadoop.hbase.regionserver.HRegionServer.run(HRegionServer.java:894)
	at java.lang.Thread.run(Thread.java:701)
2014-07-01 15:51:31,666 INFO  [regionserver60020] regionserver.HRegionServer: Received CLOSE for the region: 93253e37c284f3174309fede3f32339f ,which we are already trying to CLOSE, but not completed yet
2014-07-01 15:51:31,666 WARN  [regionserver60020] regionserver.HRegionServer: Failed to close usertable,user3,1404228265526.93253e37c284f3174309fede3f32339f. - ignoring and continuing
org.apache.hadoop.hbase.regionserver.RegionAlreadyInTransitionException: The region 93253e37c284f3174309fede3f32339f was already closing. New CLOSE request is ignored.
	at org.apache.hadoop.hbase.regionserver.HRegionServer.closeRegion(HRegionServer.java:2586)
	at org.apache.hadoop.hbase.regionserver.HRegionServer.closeRegionIgnoreErrors(HRegionServer.java:2508)
	at org.apache.hadoop.hbase.regionserver.HRegionServer.closeUserRegions(HRegionServer.java:2098)
	at org.apache.hadoop.hbase.regionserver.HRegionServer.run(HRegionServer.java:894)
	at java.lang.Thread.run(Thread.java:701)
2014-07-01 15:51:31,666 INFO  [regionserver60020] regionserver.HRegionServer: Received CLOSE for the region: 667c1f2c34fe37d48ba25a789d8aba98 ,which we are already trying to CLOSE, but not completed yet
2014-07-01 15:51:31,666 WARN  [regionserver60020] regionserver.HRegionServer: Failed to close usertable,user7,1404228265526.667c1f2c34fe37d48ba25a789d8aba98. - ignoring and continuing
org.apache.hadoop.hbase.regionserver.RegionAlreadyInTransitionException: The region 667c1f2c34fe37d48ba25a789d8aba98 was already closing. New CLOSE request is ignored.
	at org.apache.hadoop.hbase.regionserver.HRegionServer.closeRegion(HRegionServer.java:2586)
	at org.apache.hadoop.hbase.regionserver.HRegionServer.closeRegionIgnoreErrors(HRegionServer.java:2508)
	at org.apache.hadoop.hbase.regionserver.HRegionServer.closeUserRegions(HRegionServer.java:2098)
	at org.apache.hadoop.hbase.regionserver.HRegionServer.run(HRegionServer.java:894)
	at java.lang.Thread.run(Thread.java:701)
2014-07-01 15:51:31,666 DEBUG [regionserver60020] regionserver.HRegionServer: Waiting on 122e92eabfab38cb15a9965b21db25c5, bb79ae773f71289e0745068f68a0abd9, 9cef9e7d34f96f1d9b111be50040a208, 93253e37c284f3174309fede3f32339f, 667c1f2c34fe37d48ba25a789d8aba98
2014-07-01 15:51:32,301 WARN  [org.apache.hadoop.hdfs.LeaseRenewer$1@4fae46d5] hdfs.LeaseRenewer: Failed to renew lease for [DFSClient_hb_rs_slave1,60020,1404253868846_-130361673_31] for 30 seconds.  Will retry shortly ...
org.apache.hadoop.ipc.RemoteException: org.apache.hadoop.hdfs.server.namenode.SafeModeException: Cannot renew lease for DFSClient_hb_rs_slave1,60020,1404253868846_-130361673_31. Name node is in safe mode.
The reported blocks is only 3 but the threshold is 0.9990 and the total blocks 481. Safe mode will be turned off automatically.
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.renewLease(FSNamesystem.java:2771)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.renewLease(NameNode.java:919)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:622)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:587)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1432)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1428)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:416)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1190)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:1426)

	at org.apache.hadoop.ipc.Client.call(Client.java:1113)
	at org.apache.hadoop.ipc.RPC$Invoker.invoke(RPC.java:229)
	at com.sun.proxy.$Proxy8.renewLease(Unknown Source)
	at sun.reflect.GeneratedMethodAccessor29.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:622)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:85)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:62)
	at com.sun.proxy.$Proxy8.renewLease(Unknown Source)
	at sun.reflect.GeneratedMethodAccessor29.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:622)
	at org.apache.hadoop.hbase.fs.HFileSystem$1.invoke(HFileSystem.java:294)
	at com.sun.proxy.$Proxy9.renewLease(Unknown Source)
	at org.apache.hadoop.hdfs.DFSClient.renewLease(DFSClient.java:379)
	at org.apache.hadoop.hdfs.LeaseRenewer.renew(LeaseRenewer.java:378)
	at org.apache.hadoop.hdfs.LeaseRenewer.run(LeaseRenewer.java:400)
	at org.apache.hadoop.hdfs.LeaseRenewer.access$600(LeaseRenewer.java:69)
	at org.apache.hadoop.hdfs.LeaseRenewer$1.run(LeaseRenewer.java:273)
	at java.lang.Thread.run(Thread.java:701)
2014-07-01 15:51:32,394 DEBUG [RS_CLOSE_REGION-slave1:60020-0] util.FSUtils: DFS Client does not support most favored nodes create; using default create
2014-07-01 15:51:32,397 WARN  [RS_CLOSE_REGION-slave1:60020-0] regionserver.HStore: Failed flushing store file, retrying num=3
org.apache.hadoop.ipc.RemoteException: org.apache.hadoop.hdfs.server.namenode.SafeModeException: Cannot create /hbase/data/default/usertable/122e92eabfab38cb15a9965b21db25c5/.tmp/2b9f6f96ccd244a6971cbf5328790fa0. Name node is in safe mode.
The reported blocks is only 3 but the threshold is 0.9990 and the total blocks 481. Safe mode will be turned off automatically.
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.startFileInternal(FSNamesystem.java:1561)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.startFile(FSNamesystem.java:1527)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.create(NameNode.java:710)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.create(NameNode.java:689)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:622)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:587)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1432)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1428)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:416)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1190)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:1426)

	at org.apache.hadoop.ipc.Client.call(Client.java:1113)
	at org.apache.hadoop.ipc.RPC$Invoker.invoke(RPC.java:229)
	at com.sun.proxy.$Proxy8.create(Unknown Source)
	at sun.reflect.GeneratedMethodAccessor26.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:622)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:85)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:62)
	at com.sun.proxy.$Proxy8.create(Unknown Source)
	at sun.reflect.GeneratedMethodAccessor26.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:622)
	at org.apache.hadoop.hbase.fs.HFileSystem$1.invoke(HFileSystem.java:294)
	at com.sun.proxy.$Proxy9.create(Unknown Source)
	at org.apache.hadoop.hdfs.DFSClient$DFSOutputStream.<init>(DFSClient.java:3451)
	at org.apache.hadoop.hdfs.DFSClient.create(DFSClient.java:870)
	at org.apache.hadoop.hdfs.DistributedFileSystem.create(DistributedFileSystem.java:205)
	at org.apache.hadoop.fs.FilterFileSystem.create(FilterFileSystem.java:126)
	at org.apache.hadoop.hbase.util.FSUtils.create(FSUtils.java:341)
	at org.apache.hadoop.hbase.util.FSUtils.create(FSUtils.java:316)
	at org.apache.hadoop.hbase.io.hfile.AbstractHFileWriter.createOutputStream(AbstractHFileWriter.java:266)
	at org.apache.hadoop.hbase.io.hfile.HFile$WriterFactory.create(HFile.java:391)
	at org.apache.hadoop.hbase.regionserver.StoreFile$Writer.<init>(StoreFile.java:728)
	at org.apache.hadoop.hbase.regionserver.StoreFile$Writer.<init>(StoreFile.java:679)
	at org.apache.hadoop.hbase.regionserver.StoreFile$WriterBuilder.build(StoreFile.java:617)
	at org.apache.hadoop.hbase.regionserver.HStore.createWriterInTmp(HStore.java:889)
	at org.apache.hadoop.hbase.regionserver.DefaultStoreFlusher.flushSnapshot(DefaultStoreFlusher.java:69)
	at org.apache.hadoop.hbase.regionserver.HStore.flushCache(HStore.java:798)
	at org.apache.hadoop.hbase.regionserver.HStore$StoreFlusherImpl.flushCache(HStore.java:1949)
	at org.apache.hadoop.hbase.regionserver.HRegion.internalFlushcache(HRegion.java:1776)
	at org.apache.hadoop.hbase.regionserver.HRegion.internalFlushcache(HRegion.java:1658)
	at org.apache.hadoop.hbase.regionserver.HRegion.doClose(HRegion.java:1119)
	at org.apache.hadoop.hbase.regionserver.HRegion.close(HRegion.java:1085)
	at org.apache.hadoop.hbase.regionserver.handler.CloseRegionHandler.process(CloseRegionHandler.java:147)
	at org.apache.hadoop.hbase.executor.EventHandler.run(EventHandler.java:128)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1146)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:701)
2014-07-01 15:51:32,480 DEBUG [RS_CLOSE_REGION-slave1:60020-2] util.FSUtils: DFS Client does not support most favored nodes create; using default create
2014-07-01 15:51:32,484 WARN  [RS_CLOSE_REGION-slave1:60020-2] regionserver.HStore: Failed flushing store file, retrying num=6
org.apache.hadoop.ipc.RemoteException: org.apache.hadoop.hdfs.server.namenode.SafeModeException: Cannot create /hbase/data/default/usertable/9cef9e7d34f96f1d9b111be50040a208/.tmp/6c6b9fef791b4dcda18d7b15a6f60b88. Name node is in safe mode.
The reported blocks is only 3 but the threshold is 0.9990 and the total blocks 481. Safe mode will be turned off automatically.
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.startFileInternal(FSNamesystem.java:1561)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.startFile(FSNamesystem.java:1527)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.create(NameNode.java:710)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.create(NameNode.java:689)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:622)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:587)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1432)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1428)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:416)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1190)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:1426)

	at org.apache.hadoop.ipc.Client.call(Client.java:1113)
	at org.apache.hadoop.ipc.RPC$Invoker.invoke(RPC.java:229)
	at com.sun.proxy.$Proxy8.create(Unknown Source)
	at sun.reflect.GeneratedMethodAccessor26.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:622)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:85)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:62)
	at com.sun.proxy.$Proxy8.create(Unknown Source)
	at sun.reflect.GeneratedMethodAccessor26.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:622)
	at org.apache.hadoop.hbase.fs.HFileSystem$1.invoke(HFileSystem.java:294)
	at com.sun.proxy.$Proxy9.create(Unknown Source)
	at org.apache.hadoop.hdfs.DFSClient$DFSOutputStream.<init>(DFSClient.java:3451)
	at org.apache.hadoop.hdfs.DFSClient.create(DFSClient.java:870)
	at org.apache.hadoop.hdfs.DistributedFileSystem.create(DistributedFileSystem.java:205)
	at org.apache.hadoop.fs.FilterFileSystem.create(FilterFileSystem.java:126)
	at org.apache.hadoop.hbase.util.FSUtils.create(FSUtils.java:341)
	at org.apache.hadoop.hbase.util.FSUtils.create(FSUtils.java:316)
	at org.apache.hadoop.hbase.io.hfile.AbstractHFileWriter.createOutputStream(AbstractHFileWriter.java:266)
	at org.apache.hadoop.hbase.io.hfile.HFile$WriterFactory.create(HFile.java:391)
	at org.apache.hadoop.hbase.regionserver.StoreFile$Writer.<init>(StoreFile.java:728)
	at org.apache.hadoop.hbase.regionserver.StoreFile$Writer.<init>(StoreFile.java:679)
	at org.apache.hadoop.hbase.regionserver.StoreFile$WriterBuilder.build(StoreFile.java:617)
	at org.apache.hadoop.hbase.regionserver.HStore.createWriterInTmp(HStore.java:889)
	at org.apache.hadoop.hbase.regionserver.DefaultStoreFlusher.flushSnapshot(DefaultStoreFlusher.java:69)
	at org.apache.hadoop.hbase.regionserver.HStore.flushCache(HStore.java:798)
	at org.apache.hadoop.hbase.regionserver.HStore$StoreFlusherImpl.flushCache(HStore.java:1949)
	at org.apache.hadoop.hbase.regionserver.HRegion.internalFlushcache(HRegion.java:1776)
	at org.apache.hadoop.hbase.regionserver.HRegion.internalFlushcache(HRegion.java:1658)
	at org.apache.hadoop.hbase.regionserver.HRegion.doClose(HRegion.java:1119)
	at org.apache.hadoop.hbase.regionserver.HRegion.close(HRegion.java:1085)
	at org.apache.hadoop.hbase.regionserver.handler.CloseRegionHandler.process(CloseRegionHandler.java:147)
	at org.apache.hadoop.hbase.executor.EventHandler.run(EventHandler.java:128)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1146)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:701)
2014-07-01 15:51:32,509 DEBUG [RS_CLOSE_REGION-slave1:60020-1] util.FSUtils: DFS Client does not support most favored nodes create; using default create
2014-07-01 15:51:32,512 WARN  [RS_CLOSE_REGION-slave1:60020-1] regionserver.HStore: Failed flushing store file, retrying num=5
org.apache.hadoop.ipc.RemoteException: org.apache.hadoop.hdfs.server.namenode.SafeModeException: Cannot create /hbase/data/default/usertable/bb79ae773f71289e0745068f68a0abd9/.tmp/a1d8399014914e65a766cfdb8eb4d4fa. Name node is in safe mode.
The reported blocks is only 3 but the threshold is 0.9990 and the total blocks 481. Safe mode will be turned off automatically.
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.startFileInternal(FSNamesystem.java:1561)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.startFile(FSNamesystem.java:1527)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.create(NameNode.java:710)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.create(NameNode.java:689)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:622)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:587)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1432)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1428)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:416)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1190)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:1426)

	at org.apache.hadoop.ipc.Client.call(Client.java:1113)
	at org.apache.hadoop.ipc.RPC$Invoker.invoke(RPC.java:229)
	at com.sun.proxy.$Proxy8.create(Unknown Source)
	at sun.reflect.GeneratedMethodAccessor26.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:622)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:85)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:62)
	at com.sun.proxy.$Proxy8.create(Unknown Source)
	at sun.reflect.GeneratedMethodAccessor26.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:622)
	at org.apache.hadoop.hbase.fs.HFileSystem$1.invoke(HFileSystem.java:294)
	at com.sun.proxy.$Proxy9.create(Unknown Source)
	at org.apache.hadoop.hdfs.DFSClient$DFSOutputStream.<init>(DFSClient.java:3451)
	at org.apache.hadoop.hdfs.DFSClient.create(DFSClient.java:870)
	at org.apache.hadoop.hdfs.DistributedFileSystem.create(DistributedFileSystem.java:205)
	at org.apache.hadoop.fs.FilterFileSystem.create(FilterFileSystem.java:126)
	at org.apache.hadoop.hbase.util.FSUtils.create(FSUtils.java:341)
	at org.apache.hadoop.hbase.util.FSUtils.create(FSUtils.java:316)
	at org.apache.hadoop.hbase.io.hfile.AbstractHFileWriter.createOutputStream(AbstractHFileWriter.java:266)
	at org.apache.hadoop.hbase.io.hfile.HFile$WriterFactory.create(HFile.java:391)
	at org.apache.hadoop.hbase.regionserver.StoreFile$Writer.<init>(StoreFile.java:728)
	at org.apache.hadoop.hbase.regionserver.StoreFile$Writer.<init>(StoreFile.java:679)
	at org.apache.hadoop.hbase.regionserver.StoreFile$WriterBuilder.build(StoreFile.java:617)
	at org.apache.hadoop.hbase.regionserver.HStore.createWriterInTmp(HStore.java:889)
	at org.apache.hadoop.hbase.regionserver.DefaultStoreFlusher.flushSnapshot(DefaultStoreFlusher.java:69)
	at org.apache.hadoop.hbase.regionserver.HStore.flushCache(HStore.java:798)
	at org.apache.hadoop.hbase.regionserver.HStore$StoreFlusherImpl.flushCache(HStore.java:1949)
	at org.apache.hadoop.hbase.regionserver.HRegion.internalFlushcache(HRegion.java:1776)
	at org.apache.hadoop.hbase.regionserver.HRegion.internalFlushcache(HRegion.java:1658)
	at org.apache.hadoop.hbase.regionserver.HRegion.doClose(HRegion.java:1119)
	at org.apache.hadoop.hbase.regionserver.HRegion.close(HRegion.java:1085)
	at org.apache.hadoop.hbase.regionserver.handler.CloseRegionHandler.process(CloseRegionHandler.java:147)
	at org.apache.hadoop.hbase.executor.EventHandler.run(EventHandler.java:128)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1146)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:701)
2014-07-01 15:51:33,305 WARN  [org.apache.hadoop.hdfs.LeaseRenewer$1@4fae46d5] hdfs.LeaseRenewer: Failed to renew lease for [DFSClient_hb_rs_slave1,60020,1404253868846_-130361673_31] for 30 seconds.  Will retry shortly ...
org.apache.hadoop.ipc.RemoteException: org.apache.hadoop.hdfs.server.namenode.SafeModeException: Cannot renew lease for DFSClient_hb_rs_slave1,60020,1404253868846_-130361673_31. Name node is in safe mode.
The reported blocks is only 3 but the threshold is 0.9990 and the total blocks 481. Safe mode will be turned off automatically.
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.renewLease(FSNamesystem.java:2771)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.renewLease(NameNode.java:919)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:622)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:587)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1432)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1428)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:416)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1190)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:1426)

	at org.apache.hadoop.ipc.Client.call(Client.java:1113)
	at org.apache.hadoop.ipc.RPC$Invoker.invoke(RPC.java:229)
	at com.sun.proxy.$Proxy8.renewLease(Unknown Source)
	at sun.reflect.GeneratedMethodAccessor29.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:622)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:85)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:62)
	at com.sun.proxy.$Proxy8.renewLease(Unknown Source)
	at sun.reflect.GeneratedMethodAccessor29.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:622)
	at org.apache.hadoop.hbase.fs.HFileSystem$1.invoke(HFileSystem.java:294)
	at com.sun.proxy.$Proxy9.renewLease(Unknown Source)
	at org.apache.hadoop.hdfs.DFSClient.renewLease(DFSClient.java:379)
	at org.apache.hadoop.hdfs.LeaseRenewer.renew(LeaseRenewer.java:378)
	at org.apache.hadoop.hdfs.LeaseRenewer.run(LeaseRenewer.java:400)
	at org.apache.hadoop.hdfs.LeaseRenewer.access$600(LeaseRenewer.java:69)
	at org.apache.hadoop.hdfs.LeaseRenewer$1.run(LeaseRenewer.java:273)
	at java.lang.Thread.run(Thread.java:701)
2014-07-01 15:51:33,329 WARN  [regionserver60020-smallCompactions-1404253964856] hdfs.DFSClient: DFS Read: org.apache.hadoop.ipc.RemoteException: org.apache.hadoop.hdfs.server.namenode.SafeModeException: Zero blocklocations for /hbase/data/default/usertable/667c1f2c34fe37d48ba25a789d8aba98/family/56200eedd338491786b04080265aa716. Name node is in safe mode.
The reported blocks is only 3 but the threshold is 0.9990 and the total blocks 481. Safe mode will be turned off automatically.
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getBlockLocations(FSNamesystem.java:1108)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getBlockLocations(FSNamesystem.java:1037)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.getBlockLocations(NameNode.java:666)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:622)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:587)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1432)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1428)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:416)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1190)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:1426)

	at org.apache.hadoop.ipc.Client.call(Client.java:1113)
	at org.apache.hadoop.ipc.RPC$Invoker.invoke(RPC.java:229)
	at com.sun.proxy.$Proxy8.getBlockLocations(Unknown Source)
	at sun.reflect.GeneratedMethodAccessor10.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:622)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:85)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:62)
	at com.sun.proxy.$Proxy8.getBlockLocations(Unknown Source)
	at org.apache.hadoop.hdfs.DFSClient.callGetBlockLocations(DFSClient.java:695)
	at org.apache.hadoop.hdfs.DFSClient$DFSInputStream.fetchLocatedBlocks(DFSClient.java:2003)
	at org.apache.hadoop.hdfs.DFSClient$DFSInputStream.openInfo(DFSClient.java:1975)
	at org.apache.hadoop.hdfs.DFSClient$DFSInputStream.chooseDataNode(DFSClient.java:2474)
	at org.apache.hadoop.hdfs.DFSClient$DFSInputStream.blockSeekTo(DFSClient.java:2252)
	at org.apache.hadoop.hdfs.DFSClient$DFSInputStream.read(DFSClient.java:2415)
	at java.io.DataInputStream.read(DataInputStream.java:149)
	at org.apache.hadoop.io.IOUtils.readFully(IOUtils.java:175)
	at org.apache.hadoop.hbase.io.hfile.HFileBlock$AbstractFSReader.readAtOffset(HFileBlock.java:1210)
	at org.apache.hadoop.hbase.io.hfile.HFileBlock$FSReaderV2.readBlockDataInternal(HFileBlock.java:1483)
	at org.apache.hadoop.hbase.io.hfile.HFileBlock$FSReaderV2.readBlockData(HFileBlock.java:1314)
	at org.apache.hadoop.hbase.io.hfile.HFileReaderV2.readBlock(HFileReaderV2.java:392)
	at org.apache.hadoop.hbase.io.hfile.HFileReaderV2$ScannerV2.seekTo(HFileReaderV2.java:803)
	at org.apache.hadoop.hbase.regionserver.StoreFileScanner.seekAtOrAfter(StoreFileScanner.java:244)
	at org.apache.hadoop.hbase.regionserver.StoreFileScanner.seek(StoreFileScanner.java:152)
	at org.apache.hadoop.hbase.regionserver.StoreScanner.seekScanners(StoreScanner.java:317)
	at org.apache.hadoop.hbase.regionserver.StoreScanner.<init>(StoreScanner.java:240)
	at org.apache.hadoop.hbase.regionserver.StoreScanner.<init>(StoreScanner.java:202)
	at org.apache.hadoop.hbase.regionserver.compactions.Compactor.createScanner(Compactor.java:257)
	at org.apache.hadoop.hbase.regionserver.compactions.DefaultCompactor.compact(DefaultCompactor.java:65)
	at org.apache.hadoop.hbase.regionserver.DefaultStoreEngine$DefaultCompactionContext.compact(DefaultStoreEngine.java:109)
	at org.apache.hadoop.hbase.regionserver.HStore.compact(HStore.java:1086)
	at org.apache.hadoop.hbase.regionserver.HRegion.compact(HRegion.java:1481)
	at org.apache.hadoop.hbase.regionserver.CompactSplitThread$CompactionRunner.run(CompactSplitThread.java:475)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1146)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:701)

2014-07-01 15:51:33,330 ERROR [regionserver60020-smallCompactions-1404253964856] regionserver.CompactSplitThread: Compaction failed Request = regionName=usertable,user7,1404228265526.667c1f2c34fe37d48ba25a789d8aba98., storeName=family, fileCount=6, fileSize=891.0m (388.9m, 105.0m, 111.0m, 125.7m, 94.4m, 66.0m), priority=14, time=75165633683428
java.io.IOException: Could not seek StoreFileScanner[HFileScanner for reader reader=hdfs://master:54310/hbase/data/default/usertable/667c1f2c34fe37d48ba25a789d8aba98/family/56200eedd338491786b04080265aa716, compression=gz, cacheConf=CacheConfig:enabled [cacheDataOnRead=true] [cacheDataOnWrite=false] [cacheIndexesOnWrite=false] [cacheBloomsOnWrite=false] [cacheEvictOnClose=false] [cacheCompressed=false][prefetchOnOpen=false], firstKey=user7000001172980900254/family:field0/1404254132804/Put, lastKey=user7999998402207300106/family:field9/1404254151918/Put, avgKeyLen=46, avgValueLen=100, entries=5457920, length=407788494, cur=null] to key /family:/LATEST_TIMESTAMP/DeleteFamily/vlen=0/mvcc=0
	at org.apache.hadoop.hbase.regionserver.StoreFileScanner.seek(StoreFileScanner.java:164)
	at org.apache.hadoop.hbase.regionserver.StoreScanner.seekScanners(StoreScanner.java:317)
	at org.apache.hadoop.hbase.regionserver.StoreScanner.<init>(StoreScanner.java:240)
	at org.apache.hadoop.hbase.regionserver.StoreScanner.<init>(StoreScanner.java:202)
	at org.apache.hadoop.hbase.regionserver.compactions.Compactor.createScanner(Compactor.java:257)
	at org.apache.hadoop.hbase.regionserver.compactions.DefaultCompactor.compact(DefaultCompactor.java:65)
	at org.apache.hadoop.hbase.regionserver.DefaultStoreEngine$DefaultCompactionContext.compact(DefaultStoreEngine.java:109)
	at org.apache.hadoop.hbase.regionserver.HStore.compact(HStore.java:1086)
	at org.apache.hadoop.hbase.regionserver.HRegion.compact(HRegion.java:1481)
	at org.apache.hadoop.hbase.regionserver.CompactSplitThread$CompactionRunner.run(CompactSplitThread.java:475)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1146)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:701)
Caused by: org.apache.hadoop.ipc.RemoteException: org.apache.hadoop.hdfs.server.namenode.SafeModeException: Zero blocklocations for /hbase/data/default/usertable/667c1f2c34fe37d48ba25a789d8aba98/family/56200eedd338491786b04080265aa716. Name node is in safe mode.
The reported blocks is only 3 but the threshold is 0.9990 and the total blocks 481. Safe mode will be turned off automatically.
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getBlockLocations(FSNamesystem.java:1108)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getBlockLocations(FSNamesystem.java:1037)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.getBlockLocations(NameNode.java:666)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:622)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:587)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1432)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1428)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:416)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1190)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:1426)

	at org.apache.hadoop.ipc.Client.call(Client.java:1113)
	at org.apache.hadoop.ipc.RPC$Invoker.invoke(RPC.java:229)
	at com.sun.proxy.$Proxy8.getBlockLocations(Unknown Source)
	at sun.reflect.GeneratedMethodAccessor10.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:622)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:85)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:62)
	at com.sun.proxy.$Proxy8.getBlockLocations(Unknown Source)
	at org.apache.hadoop.hdfs.DFSClient.callGetBlockLocations(DFSClient.java:695)
	at org.apache.hadoop.hdfs.DFSClient$DFSInputStream.fetchLocatedBlocks(DFSClient.java:2003)
	at org.apache.hadoop.hdfs.DFSClient$DFSInputStream.openInfo(DFSClient.java:1975)
	at org.apache.hadoop.hdfs.DFSClient$DFSInputStream.chooseDataNode(DFSClient.java:2474)
	at org.apache.hadoop.hdfs.DFSClient$DFSInputStream.blockSeekTo(DFSClient.java:2252)
	at org.apache.hadoop.hdfs.DFSClient$DFSInputStream.read(DFSClient.java:2415)
	at java.io.DataInputStream.read(DataInputStream.java:149)
	at org.apache.hadoop.io.IOUtils.readFully(IOUtils.java:175)
	at org.apache.hadoop.hbase.io.hfile.HFileBlock$AbstractFSReader.readAtOffset(HFileBlock.java:1210)
	at org.apache.hadoop.hbase.io.hfile.HFileBlock$FSReaderV2.readBlockDataInternal(HFileBlock.java:1483)
	at org.apache.hadoop.hbase.io.hfile.HFileBlock$FSReaderV2.readBlockData(HFileBlock.java:1314)
	at org.apache.hadoop.hbase.io.hfile.HFileReaderV2.readBlock(HFileReaderV2.java:392)
	at org.apache.hadoop.hbase.io.hfile.HFileReaderV2$ScannerV2.seekTo(HFileReaderV2.java:803)
	at org.apache.hadoop.hbase.regionserver.StoreFileScanner.seekAtOrAfter(StoreFileScanner.java:244)
	at org.apache.hadoop.hbase.regionserver.StoreFileScanner.seek(StoreFileScanner.java:152)
	... 12 more
2014-07-01 15:51:33,331 DEBUG [regionserver60020-smallCompactions-1404253964856] regionserver.CompactSplitThread: CompactSplitThread Status: compaction_queue=(0:25), split_queue=0, merge_queue=0
2014-07-01 15:51:33,332 DEBUG [regionserver60020-smallCompactions-1404253964856] compactions.RatioBasedCompactionPolicy: Selecting compaction from 3 store files, 0 compacting, 3 eligible, 20 blocking
2014-07-01 15:51:33,332 DEBUG [regionserver60020-smallCompactions-1404253964856] compactions.ExploringCompactionPolicy: Exploring compaction algorithm has selected 0 files of size 0 starting at candidate #-1 after considering 1 permutations with 0 in ratio
2014-07-01 15:51:33,332 DEBUG [regionserver60020-smallCompactions-1404253964856] compactions.RatioBasedCompactionPolicy: Not compacting files because we only have 0 files ready for compaction. Need 3 to initiate.
2014-07-01 15:51:33,332 DEBUG [regionserver60020-smallCompactions-1404253964856] regionserver.CompactSplitThread: Not compacting usertable,user3,1404228265526.93253e37c284f3174309fede3f32339f. because compaction request was cancelled
2014-07-01 15:51:33,332 DEBUG [regionserver60020-smallCompactions-1404253964856] compactions.RatioBasedCompactionPolicy: Selecting compaction from 6 store files, 0 compacting, 6 eligible, 20 blocking
2014-07-01 15:51:33,333 DEBUG [regionserver60020-smallCompactions-1404253964856] compactions.ExploringCompactionPolicy: Exploring compaction algorithm has selected 6 files of size 934259017 starting at candidate #0 after considering 10 permutations with 9 in ratio
2014-07-01 15:51:33,333 DEBUG [regionserver60020-smallCompactions-1404253964856] regionserver.HStore: 667c1f2c34fe37d48ba25a789d8aba98 - family: Initiating major compaction
2014-07-01 15:51:33,333 INFO  [regionserver60020-smallCompactions-1404253964856] regionserver.HRegion: Starting compaction on family in region usertable,user7,1404228265526.667c1f2c34fe37d48ba25a789d8aba98.
2014-07-01 15:51:33,334 INFO  [regionserver60020-smallCompactions-1404253964856] regionserver.HStore: Starting compaction of 6 file(s) in family of usertable,user7,1404228265526.667c1f2c34fe37d48ba25a789d8aba98. into tmpdir=hdfs://master:54310/hbase/data/default/usertable/667c1f2c34fe37d48ba25a789d8aba98/.tmp, totalSize=891.0m
2014-07-01 15:51:33,334 DEBUG [regionserver60020-smallCompactions-1404253964856] compactions.Compactor: Compacting hdfs://master:54310/hbase/data/default/usertable/667c1f2c34fe37d48ba25a789d8aba98/family/56200eedd338491786b04080265aa716, keycount=545792, bloomtype=ROW, size=388.9m, encoding=NONE, seqNum=2915, earliestPutTs=1404228281826
2014-07-01 15:51:33,334 DEBUG [regionserver60020-smallCompactions-1404253964856] compactions.Compactor: Compacting hdfs://master:54310/hbase/data/default/usertable/667c1f2c34fe37d48ba25a789d8aba98/family/50f4de22ccd04828b18b93cc72b88964, keycount=147565, bloomtype=ROW, size=105.0m, encoding=NONE, seqNum=3442, earliestPutTs=1404254293026
2014-07-01 15:51:33,334 DEBUG [regionserver60020-smallCompactions-1404253964856] compactions.Compactor: Compacting hdfs://master:54310/hbase/data/default/usertable/667c1f2c34fe37d48ba25a789d8aba98/family/244e3443161840b5a95504a408c4edc2, keycount=155927, bloomtype=ROW, size=111.0m, encoding=NONE, seqNum=3881, earliestPutTs=1404254357203
2014-07-01 15:51:33,335 DEBUG [regionserver60020-smallCompactions-1404253964856] compactions.Compactor: Compacting hdfs://master:54310/hbase/data/default/usertable/667c1f2c34fe37d48ba25a789d8aba98/family/a5468df3646d43fda8dcfd231b11facb, keycount=176456, bloomtype=ROW, size=125.7m, encoding=NONE, seqNum=4315, earliestPutTs=1404254389941
2014-07-01 15:51:33,335 DEBUG [regionserver60020-smallCompactions-1404253964856] compactions.Compactor: Compacting hdfs://master:54310/hbase/data/default/usertable/667c1f2c34fe37d48ba25a789d8aba98/family/5c4ecd1e5f5f40a68639ae8651cdf308, keycount=132606, bloomtype=ROW, size=94.4m, encoding=NONE, seqNum=4527, earliestPutTs=1404254439322
2014-07-01 15:51:33,335 DEBUG [regionserver60020-smallCompactions-1404253964856] compactions.Compactor: Compacting hdfs://master:54310/hbase/data/default/usertable/667c1f2c34fe37d48ba25a789d8aba98/family/6c4e4b0a6c594b6db087ac5f958bca9c, keycount=92565, bloomtype=ROW, size=66.0m, encoding=NONE, seqNum=4678, earliestPutTs=1404254473587
2014-07-01 15:51:33,396 DEBUG [regionserver60020-smallCompactions-1404253964856] util.FSUtils: DFS Client does not support most favored nodes create; using default create
2014-07-01 15:51:33,415 ERROR [regionserver60020-smallCompactions-1404253964856] regionserver.CompactSplitThread: Compaction failed Request = regionName=usertable,user7,1404228265526.667c1f2c34fe37d48ba25a789d8aba98., storeName=family, fileCount=6, fileSize=891.0m (388.9m, 105.0m, 111.0m, 125.7m, 94.4m, 66.0m), priority=14, time=75204722279156
org.apache.hadoop.ipc.RemoteException: org.apache.hadoop.hdfs.server.namenode.SafeModeException: Cannot create /hbase/data/default/usertable/667c1f2c34fe37d48ba25a789d8aba98/.tmp/28b1eb45e71e4324863a2d1afbc563fc. Name node is in safe mode.
The reported blocks is only 3 but the threshold is 0.9990 and the total blocks 481. Safe mode will be turned off automatically.
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.startFileInternal(FSNamesystem.java:1561)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.startFile(FSNamesystem.java:1527)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.create(NameNode.java:710)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.create(NameNode.java:689)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:622)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:587)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1432)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1428)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:416)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1190)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:1426)

	at org.apache.hadoop.ipc.Client.call(Client.java:1113)
	at org.apache.hadoop.ipc.RPC$Invoker.invoke(RPC.java:229)
	at com.sun.proxy.$Proxy8.create(Unknown Source)
	at sun.reflect.GeneratedMethodAccessor26.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:622)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:85)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:62)
	at com.sun.proxy.$Proxy8.create(Unknown Source)
	at sun.reflect.GeneratedMethodAccessor26.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:622)
	at org.apache.hadoop.hbase.fs.HFileSystem$1.invoke(HFileSystem.java:294)
	at com.sun.proxy.$Proxy9.create(Unknown Source)
	at org.apache.hadoop.hdfs.DFSClient$DFSOutputStream.<init>(DFSClient.java:3451)
	at org.apache.hadoop.hdfs.DFSClient.create(DFSClient.java:870)
	at org.apache.hadoop.hdfs.DistributedFileSystem.create(DistributedFileSystem.java:205)
	at org.apache.hadoop.fs.FilterFileSystem.create(FilterFileSystem.java:126)
	at org.apache.hadoop.hbase.util.FSUtils.create(FSUtils.java:341)
	at org.apache.hadoop.hbase.util.FSUtils.create(FSUtils.java:316)
	at org.apache.hadoop.hbase.io.hfile.AbstractHFileWriter.createOutputStream(AbstractHFileWriter.java:266)
	at org.apache.hadoop.hbase.io.hfile.HFile$WriterFactory.create(HFile.java:391)
	at org.apache.hadoop.hbase.regionserver.StoreFile$Writer.<init>(StoreFile.java:728)
	at org.apache.hadoop.hbase.regionserver.StoreFile$Writer.<init>(StoreFile.java:679)
	at org.apache.hadoop.hbase.regionserver.StoreFile$WriterBuilder.build(StoreFile.java:617)
	at org.apache.hadoop.hbase.regionserver.HStore.createWriterInTmp(HStore.java:889)
	at org.apache.hadoop.hbase.regionserver.compactions.DefaultCompactor.compact(DefaultCompactor.java:74)
	at org.apache.hadoop.hbase.regionserver.DefaultStoreEngine$DefaultCompactionContext.compact(DefaultStoreEngine.java:109)
	at org.apache.hadoop.hbase.regionserver.HStore.compact(HStore.java:1086)
	at org.apache.hadoop.hbase.regionserver.HRegion.compact(HRegion.java:1481)
	at org.apache.hadoop.hbase.regionserver.CompactSplitThread$CompactionRunner.run(CompactSplitThread.java:475)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1146)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:701)
2014-07-01 15:51:33,415 DEBUG [regionserver60020-smallCompactions-1404253964856] regionserver.CompactSplitThread: CompactSplitThread Status: compaction_queue=(0:23), split_queue=0, merge_queue=0
2014-07-01 15:51:33,415 DEBUG [regionserver60020-smallCompactions-1404253964856] compactions.RatioBasedCompactionPolicy: Selecting compaction from 3 store files, 0 compacting, 3 eligible, 20 blocking
2014-07-01 15:51:33,415 DEBUG [regionserver60020-smallCompactions-1404253964856] compactions.ExploringCompactionPolicy: Exploring compaction algorithm has selected 0 files of size 0 starting at candidate #-1 after considering 1 permutations with 0 in ratio
2014-07-01 15:51:33,415 DEBUG [regionserver60020-smallCompactions-1404253964856] compactions.RatioBasedCompactionPolicy: Not compacting files because we only have 0 files ready for compaction. Need 3 to initiate.
2014-07-01 15:51:33,415 DEBUG [regionserver60020-smallCompactions-1404253964856] regionserver.CompactSplitThread: Not compacting usertable,user3,1404228265526.93253e37c284f3174309fede3f32339f. because compaction request was cancelled
2014-07-01 15:51:33,416 DEBUG [regionserver60020-smallCompactions-1404253964856] compactions.RatioBasedCompactionPolicy: Selecting compaction from 6 store files, 0 compacting, 6 eligible, 20 blocking
2014-07-01 15:51:33,416 DEBUG [regionserver60020-smallCompactions-1404253964856] compactions.ExploringCompactionPolicy: Exploring compaction algorithm has selected 6 files of size 934259017 starting at candidate #0 after considering 10 permutations with 9 in ratio
2014-07-01 15:51:33,416 DEBUG [regionserver60020-smallCompactions-1404253964856] regionserver.HStore: 667c1f2c34fe37d48ba25a789d8aba98 - family: Initiating major compaction
2014-07-01 15:51:33,416 INFO  [regionserver60020-smallCompactions-1404253964856] regionserver.HRegion: Starting compaction on family in region usertable,user7,1404228265526.667c1f2c34fe37d48ba25a789d8aba98.
2014-07-01 15:51:33,416 INFO  [regionserver60020-smallCompactions-1404253964856] regionserver.HStore: Starting compaction of 6 file(s) in family of usertable,user7,1404228265526.667c1f2c34fe37d48ba25a789d8aba98. into tmpdir=hdfs://master:54310/hbase/data/default/usertable/667c1f2c34fe37d48ba25a789d8aba98/.tmp, totalSize=891.0m
2014-07-01 15:51:33,416 DEBUG [regionserver60020-smallCompactions-1404253964856] compactions.Compactor: Compacting hdfs://master:54310/hbase/data/default/usertable/667c1f2c34fe37d48ba25a789d8aba98/family/56200eedd338491786b04080265aa716, keycount=545792, bloomtype=ROW, size=388.9m, encoding=NONE, seqNum=2915, earliestPutTs=1404228281826
2014-07-01 15:51:33,416 DEBUG [regionserver60020-smallCompactions-1404253964856] compactions.Compactor: Compacting hdfs://master:54310/hbase/data/default/usertable/667c1f2c34fe37d48ba25a789d8aba98/family/50f4de22ccd04828b18b93cc72b88964, keycount=147565, bloomtype=ROW, size=105.0m, encoding=NONE, seqNum=3442, earliestPutTs=1404254293026
2014-07-01 15:51:33,417 DEBUG [regionserver60020-smallCompactions-1404253964856] compactions.Compactor: Compacting hdfs://master:54310/hbase/data/default/usertable/667c1f2c34fe37d48ba25a789d8aba98/family/244e3443161840b5a95504a408c4edc2, keycount=155927, bloomtype=ROW, size=111.0m, encoding=NONE, seqNum=3881, earliestPutTs=1404254357203
2014-07-01 15:51:33,417 DEBUG [regionserver60020-smallCompactions-1404253964856] compactions.Compactor: Compacting hdfs://master:54310/hbase/data/default/usertable/667c1f2c34fe37d48ba25a789d8aba98/family/a5468df3646d43fda8dcfd231b11facb, keycount=176456, bloomtype=ROW, size=125.7m, encoding=NONE, seqNum=4315, earliestPutTs=1404254389941
2014-07-01 15:51:33,417 DEBUG [regionserver60020-smallCompactions-1404253964856] compactions.Compactor: Compacting hdfs://master:54310/hbase/data/default/usertable/667c1f2c34fe37d48ba25a789d8aba98/family/5c4ecd1e5f5f40a68639ae8651cdf308, keycount=132606, bloomtype=ROW, size=94.4m, encoding=NONE, seqNum=4527, earliestPutTs=1404254439322
2014-07-01 15:51:33,417 DEBUG [regionserver60020-smallCompactions-1404253964856] compactions.Compactor: Compacting hdfs://master:54310/hbase/data/default/usertable/667c1f2c34fe37d48ba25a789d8aba98/family/6c4e4b0a6c594b6db087ac5f958bca9c, keycount=92565, bloomtype=ROW, size=66.0m, encoding=NONE, seqNum=4678, earliestPutTs=1404254473587
2014-07-01 15:51:33,423 DEBUG [RS_CLOSE_REGION-slave1:60020-0] util.FSUtils: DFS Client does not support most favored nodes create; using default create
2014-07-01 15:51:33,427 WARN  [RS_CLOSE_REGION-slave1:60020-0] regionserver.HStore: Failed flushing store file, retrying num=4
org.apache.hadoop.ipc.RemoteException: org.apache.hadoop.hdfs.server.namenode.SafeModeException: Cannot create /hbase/data/default/usertable/122e92eabfab38cb15a9965b21db25c5/.tmp/9e9f23510c7b400fa413a99bb57be5e8. Name node is in safe mode.
The reported blocks is only 3 but the threshold is 0.9990 and the total blocks 481. Safe mode will be turned off automatically.
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.startFileInternal(FSNamesystem.java:1561)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.startFile(FSNamesystem.java:1527)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.create(NameNode.java:710)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.create(NameNode.java:689)
	at sun.reflect.GeneratedMethodAccessor3.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:622)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:587)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1432)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1428)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:416)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1190)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:1426)

	at org.apache.hadoop.ipc.Client.call(Client.java:1113)
	at org.apache.hadoop.ipc.RPC$Invoker.invoke(RPC.java:229)
	at com.sun.proxy.$Proxy8.create(Unknown Source)
	at sun.reflect.GeneratedMethodAccessor26.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:622)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:85)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:62)
	at com.sun.proxy.$Proxy8.create(Unknown Source)
	at sun.reflect.GeneratedMethodAccessor26.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:622)
	at org.apache.hadoop.hbase.fs.HFileSystem$1.invoke(HFileSystem.java:294)
	at com.sun.proxy.$Proxy9.create(Unknown Source)
	at org.apache.hadoop.hdfs.DFSClient$DFSOutputStream.<init>(DFSClient.java:3451)
	at org.apache.hadoop.hdfs.DFSClient.create(DFSClient.java:870)
	at org.apache.hadoop.hdfs.DistributedFileSystem.create(DistributedFileSystem.java:205)
	at org.apache.hadoop.fs.FilterFileSystem.create(FilterFileSystem.java:126)
	at org.apache.hadoop.hbase.util.FSUtils.create(FSUtils.java:341)
	at org.apache.hadoop.hbase.util.FSUtils.create(FSUtils.java:316)
	at org.apache.hadoop.hbase.io.hfile.AbstractHFileWriter.createOutputStream(AbstractHFileWriter.java:266)
	at org.apache.hadoop.hbase.io.hfile.HFile$WriterFactory.create(HFile.java:391)
	at org.apache.hadoop.hbase.regionserver.StoreFile$Writer.<init>(StoreFile.java:728)
	at org.apache.hadoop.hbase.regionserver.StoreFile$Writer.<init>(StoreFile.java:679)
	at org.apache.hadoop.hbase.regionserver.StoreFile$WriterBuilder.build(StoreFile.java:617)
	at org.apache.hadoop.hbase.regionserver.HStore.createWriterInTmp(HStore.java:889)
	at org.apache.hadoop.hbase.regionserver.DefaultStoreFlusher.flushSnapshot(DefaultStoreFlusher.java:69)
	at org.apache.hadoop.hbase.regionserver.HStore.flushCache(HStore.java:798)
	at org.apache.hadoop.hbase.regionserver.HStore$StoreFlusherImpl.flushCache(HStore.java:1949)
	at org.apache.hadoop.hbase.regionserver.HRegion.internalFlushcache(HRegion.java:1776)
	at org.apache.hadoop.hbase.regionserver.HRegion.internalFlushcache(HRegion.java:1658)
	at org.apache.hadoop.hbase.regionserver.HRegion.doClose(HRegion.java:1119)
	at org.apache.hadoop.hbase.regionserver.HRegion.close(HRegion.java:1085)
	at org.apache.hadoop.hbase.regionserver.handler.CloseRegionHandler.process(CloseRegionHandler.java:147)
	at org.apache.hadoop.hbase.executor.EventHandler.run(EventHandler.java:128)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1146)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:701)
2014-07-01 15:51:33,446 DEBUG [regionserver60020-smallCompactions-1404253964856] util.FSUtils: DFS Client does not support most favored nodes create; using default create
2014-07-01 15:51:33,449 ERROR [regionserver60020-smallCompactions-1404253964856] regionserver.CompactSplitThread: Compaction failed Request = regionName=usertable,user7,1404228265526.667c1f2c34fe37d48ba25a789d8aba98., storeName=family, fileCount=6, fileSize=891.0m (388.9m, 105.0m, 111.0m, 125.7m, 94.4m, 66.0m), priority=14, time=75204805251022
org.apache.hadoop.ipc.RemoteException: org.apache.hadoop.hdfs.server.namenode.SafeModeException: Cannot create /hbase/data/default/usertable/667c1f2c34fe37d48ba25a789d8aba98/.tmp/0ccd0bd140ea420496284d19dcb83159. Name node is in safe mode.
The reported blocks is only 3 but the threshold is 0.9990 and the total blocks 481. Safe mode will be turned off automatically.
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.startFileInternal(FSNamesystem.java:1561)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.startFile(FSNamesystem.java:1527)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.create(NameNode.java:710)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.create(NameNode.java:689)
	at sun.reflect.GeneratedMethodAccessor3.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:622)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:587)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1432)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1428)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:416)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1190)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:1426)

	at org.apache.hadoop.ipc.Client.call(Client.java:1113)
	at org.apache.hadoop.ipc.RPC$Invoker.invoke(RPC.java:229)
	at com.sun.proxy.$Proxy8.create(Unknown Source)
	at sun.reflect.GeneratedMethodAccessor26.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:622)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:85)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:62)
	at com.sun.proxy.$Proxy8.create(Unknown Source)
	at sun.reflect.GeneratedMethodAccessor26.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:622)
	at org.apache.hadoop.hbase.fs.HFileSystem$1.invoke(HFileSystem.java:294)
	at com.sun.proxy.$Proxy9.create(Unknown Source)
	at org.apache.hadoop.hdfs.DFSClient$DFSOutputStream.<init>(DFSClient.java:3451)
	at org.apache.hadoop.hdfs.DFSClient.create(DFSClient.java:870)
	at org.apache.hadoop.hdfs.DistributedFileSystem.create(DistributedFileSystem.java:205)
	at org.apache.hadoop.fs.FilterFileSystem.create(FilterFileSystem.java:126)
	at org.apache.hadoop.hbase.util.FSUtils.create(FSUtils.java:341)
	at org.apache.hadoop.hbase.util.FSUtils.create(FSUtils.java:316)
	at org.apache.hadoop.hbase.io.hfile.AbstractHFileWriter.createOutputStream(AbstractHFileWriter.java:266)
	at org.apache.hadoop.hbase.io.hfile.HFile$WriterFactory.create(HFile.java:391)
	at org.apache.hadoop.hbase.regionserver.StoreFile$Writer.<init>(StoreFile.java:728)
	at org.apache.hadoop.hbase.regionserver.StoreFile$Writer.<init>(StoreFile.java:679)
	at org.apache.hadoop.hbase.regionserver.StoreFile$WriterBuilder.build(StoreFile.java:617)
	at org.apache.hadoop.hbase.regionserver.HStore.createWriterInTmp(HStore.java:889)
	at org.apache.hadoop.hbase.regionserver.compactions.DefaultCompactor.compact(DefaultCompactor.java:74)
	at org.apache.hadoop.hbase.regionserver.DefaultStoreEngine$DefaultCompactionContext.compact(DefaultStoreEngine.java:109)
	at org.apache.hadoop.hbase.regionserver.HStore.compact(HStore.java:1086)
	at org.apache.hadoop.hbase.regionserver.HRegion.compact(HRegion.java:1481)
	at org.apache.hadoop.hbase.regionserver.CompactSplitThread$CompactionRunner.run(CompactSplitThread.java:475)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1146)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:701)
2014-07-01 15:51:33,450 DEBUG [regionserver60020-smallCompactions-1404253964856] regionserver.CompactSplitThread: CompactSplitThread Status: compaction_queue=(0:21), split_queue=0, merge_queue=0
2014-07-01 15:51:33,450 DEBUG [regionserver60020-smallCompactions-1404253964856] compactions.RatioBasedCompactionPolicy: Selecting compaction from 3 store files, 0 compacting, 3 eligible, 20 blocking
2014-07-01 15:51:33,450 DEBUG [regionserver60020-smallCompactions-1404253964856] compactions.ExploringCompactionPolicy: Exploring compaction algorithm has selected 0 files of size 0 starting at candidate #-1 after considering 1 permutations with 0 in ratio
2014-07-01 15:51:33,450 DEBUG [regionserver60020-smallCompactions-1404253964856] compactions.RatioBasedCompactionPolicy: Not compacting files because we only have 0 files ready for compaction. Need 3 to initiate.
2014-07-01 15:51:33,450 DEBUG [regionserver60020-smallCompactions-1404253964856] regionserver.CompactSplitThread: Not compacting usertable,user3,1404228265526.93253e37c284f3174309fede3f32339f. because compaction request was cancelled
2014-07-01 15:51:33,450 DEBUG [regionserver60020-smallCompactions-1404253964856] compactions.RatioBasedCompactionPolicy: Selecting compaction from 3 store files, 0 compacting, 3 eligible, 20 blocking
2014-07-01 15:51:33,450 DEBUG [regionserver60020-smallCompactions-1404253964856] compactions.ExploringCompactionPolicy: Exploring compaction algorithm has selected 0 files of size 0 starting at candidate #-1 after considering 1 permutations with 0 in ratio
2014-07-01 15:51:33,450 DEBUG [regionserver60020-smallCompactions-1404253964856] compactions.RatioBasedCompactionPolicy: Not compacting files because we only have 0 files ready for compaction. Need 3 to initiate.
2014-07-01 15:51:33,450 DEBUG [regionserver60020-smallCompactions-1404253964856] regionserver.CompactSplitThread: Not compacting usertable,user3,1404228265526.93253e37c284f3174309fede3f32339f. because compaction request was cancelled
2014-07-01 15:51:33,451 DEBUG [regionserver60020-smallCompactions-1404253964856] compactions.RatioBasedCompactionPolicy: Selecting compaction from 3 store files, 0 compacting, 3 eligible, 20 blocking
2014-07-01 15:51:33,451 DEBUG [regionserver60020-smallCompactions-1404253964856] compactions.ExploringCompactionPolicy: Exploring compaction algorithm has selected 0 files of size 0 starting at candidate #-1 after considering 1 permutations with 0 in ratio
2014-07-01 15:51:33,451 DEBUG [regionserver60020-smallCompactions-1404253964856] compactions.RatioBasedCompactionPolicy: Not compacting files because we only have 0 files ready for compaction. Need 3 to initiate.
2014-07-01 15:51:33,451 DEBUG [regionserver60020-smallCompactions-1404253964856] regionserver.CompactSplitThread: Not compacting usertable,user3,1404228265526.93253e37c284f3174309fede3f32339f. because compaction request was cancelled
2014-07-01 15:51:33,451 DEBUG [regionserver60020-smallCompactions-1404253964856] compactions.RatioBasedCompactionPolicy: Selecting compaction from 3 store files, 0 compacting, 3 eligible, 20 blocking
2014-07-01 15:51:33,451 DEBUG [regionserver60020-smallCompactions-1404253964856] compactions.ExploringCompactionPolicy: Exploring compaction algorithm has selected 0 files of size 0 starting at candidate #-1 after considering 1 permutations with 0 in ratio
2014-07-01 15:51:33,451 DEBUG [regionserver60020-smallCompactions-1404253964856] compactions.RatioBasedCompactionPolicy: Not compacting files because we only have 0 files ready for compaction. Need 3 to initiate.
2014-07-01 15:51:33,451 DEBUG [regionserver60020-smallCompactions-1404253964856] regionserver.CompactSplitThread: Not compacting usertable,user3,1404228265526.93253e37c284f3174309fede3f32339f. because compaction request was cancelled
2014-07-01 15:51:33,451 DEBUG [regionserver60020-smallCompactions-1404253964856] regionserver.CompactSplitThread: Not compacting usertable,user9,1404228265526.bb79ae773f71289e0745068f68a0abd9. because compaction request was cancelled
2014-07-01 15:51:33,451 DEBUG [regionserver60020-smallCompactions-1404253964856] regionserver.CompactSplitThread: Not compacting usertable,,1404228265525.9cef9e7d34f96f1d9b111be50040a208. because compaction request was cancelled
2014-07-01 15:51:33,451 DEBUG [regionserver60020-smallCompactions-1404253964856] regionserver.CompactSplitThread: Not compacting usertable,user5,1404228265526.122e92eabfab38cb15a9965b21db25c5. because compaction request was cancelled
2014-07-01 15:51:33,451 DEBUG [regionserver60020-smallCompactions-1404253964856] regionserver.CompactSplitThread: Not compacting usertable,,1404228265525.9cef9e7d34f96f1d9b111be50040a208. because compaction request was cancelled
2014-07-01 15:51:33,451 DEBUG [regionserver60020-smallCompactions-1404253964856] regionserver.CompactSplitThread: Not compacting usertable,user5,1404228265526.122e92eabfab38cb15a9965b21db25c5. because compaction request was cancelled
2014-07-01 15:51:33,451 DEBUG [regionserver60020-smallCompactions-1404253964856] regionserver.CompactSplitThread: Not compacting usertable,user5,1404228265526.122e92eabfab38cb15a9965b21db25c5. because compaction request was cancelled
2014-07-01 15:51:33,452 DEBUG [regionserver60020-smallCompactions-1404253964856] regionserver.CompactSplitThread: Not compacting usertable,user5,1404228265526.122e92eabfab38cb15a9965b21db25c5. because compaction request was cancelled
2014-07-01 15:51:33,452 DEBUG [regionserver60020-smallCompactions-1404253964856] regionserver.CompactSplitThread: Not compacting usertable,user5,1404228265526.122e92eabfab38cb15a9965b21db25c5. because compaction request was cancelled
2014-07-01 15:51:33,452 DEBUG [regionserver60020-smallCompactions-1404253964856] regionserver.CompactSplitThread: Not compacting usertable,,1404228265525.9cef9e7d34f96f1d9b111be50040a208. because compaction request was cancelled
2014-07-01 15:51:33,452 DEBUG [regionserver60020-smallCompactions-1404253964856] regionserver.CompactSplitThread: Not compacting usertable,user9,1404228265526.bb79ae773f71289e0745068f68a0abd9. because compaction request was cancelled
2014-07-01 15:51:33,452 DEBUG [regionserver60020-smallCompactions-1404253964856] regionserver.CompactSplitThread: Not compacting usertable,user5,1404228265526.122e92eabfab38cb15a9965b21db25c5. because compaction request was cancelled
2014-07-01 15:51:33,452 DEBUG [regionserver60020-smallCompactions-1404253964856] regionserver.CompactSplitThread: Not compacting usertable,user9,1404228265526.bb79ae773f71289e0745068f68a0abd9. because compaction request was cancelled
2014-07-01 15:51:33,452 DEBUG [regionserver60020-smallCompactions-1404253964856] regionserver.CompactSplitThread: Not compacting usertable,,1404228265525.9cef9e7d34f96f1d9b111be50040a208. because compaction request was cancelled
2014-07-01 15:51:33,452 DEBUG [regionserver60020-smallCompactions-1404253964856] regionserver.CompactSplitThread: Not compacting usertable,,1404228265525.9cef9e7d34f96f1d9b111be50040a208. because compaction request was cancelled
2014-07-01 15:51:33,452 DEBUG [regionserver60020-smallCompactions-1404253964856] regionserver.CompactSplitThread: Not compacting usertable,,1404228265525.9cef9e7d34f96f1d9b111be50040a208. because compaction request was cancelled
2014-07-01 15:51:33,452 DEBUG [regionserver60020-smallCompactions-1404253964856] regionserver.CompactSplitThread: Not compacting usertable,user9,1404228265526.bb79ae773f71289e0745068f68a0abd9. because compaction request was cancelled
2014-07-01 15:51:33,452 DEBUG [regionserver60020-smallCompactions-1404253964856] regionserver.CompactSplitThread: Not compacting usertable,user5,1404228265526.122e92eabfab38cb15a9965b21db25c5. because compaction request was cancelled
2014-07-01 15:51:33,556 DEBUG [RS_CLOSE_REGION-slave1:60020-2] util.FSUtils: DFS Client does not support most favored nodes create; using default create
2014-07-01 15:51:33,558 WARN  [RS_CLOSE_REGION-slave1:60020-2] regionserver.HStore: Failed flushing store file, retrying num=7
org.apache.hadoop.ipc.RemoteException: org.apache.hadoop.hdfs.server.namenode.SafeModeException: Cannot create /hbase/data/default/usertable/9cef9e7d34f96f1d9b111be50040a208/.tmp/523e55a4ccc64b1eb6570d8f014ba647. Name node is in safe mode.
The reported blocks is only 3 but the threshold is 0.9990 and the total blocks 481. Safe mode will be turned off automatically.
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.startFileInternal(FSNamesystem.java:1561)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.startFile(FSNamesystem.java:1527)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.create(NameNode.java:710)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.create(NameNode.java:689)
	at sun.reflect.GeneratedMethodAccessor3.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:622)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:587)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1432)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1428)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:416)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1190)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:1426)

	at org.apache.hadoop.ipc.Client.call(Client.java:1113)
	at org.apache.hadoop.ipc.RPC$Invoker.invoke(RPC.java:229)
	at com.sun.proxy.$Proxy8.create(Unknown Source)
	at sun.reflect.GeneratedMethodAccessor26.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:622)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:85)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:62)
	at com.sun.proxy.$Proxy8.create(Unknown Source)
	at sun.reflect.GeneratedMethodAccessor26.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:622)
	at org.apache.hadoop.hbase.fs.HFileSystem$1.invoke(HFileSystem.java:294)
	at com.sun.proxy.$Proxy9.create(Unknown Source)
	at org.apache.hadoop.hdfs.DFSClient$DFSOutputStream.<init>(DFSClient.java:3451)
	at org.apache.hadoop.hdfs.DFSClient.create(DFSClient.java:870)
	at org.apache.hadoop.hdfs.DistributedFileSystem.create(DistributedFileSystem.java:205)
	at org.apache.hadoop.fs.FilterFileSystem.create(FilterFileSystem.java:126)
	at org.apache.hadoop.hbase.util.FSUtils.create(FSUtils.java:341)
	at org.apache.hadoop.hbase.util.FSUtils.create(FSUtils.java:316)
	at org.apache.hadoop.hbase.io.hfile.AbstractHFileWriter.createOutputStream(AbstractHFileWriter.java:266)
	at org.apache.hadoop.hbase.io.hfile.HFile$WriterFactory.create(HFile.java:391)
	at org.apache.hadoop.hbase.regionserver.StoreFile$Writer.<init>(StoreFile.java:728)
	at org.apache.hadoop.hbase.regionserver.StoreFile$Writer.<init>(StoreFile.java:679)
	at org.apache.hadoop.hbase.regionserver.StoreFile$WriterBuilder.build(StoreFile.java:617)
	at org.apache.hadoop.hbase.regionserver.HStore.createWriterInTmp(HStore.java:889)
	at org.apache.hadoop.hbase.regionserver.DefaultStoreFlusher.flushSnapshot(DefaultStoreFlusher.java:69)
	at org.apache.hadoop.hbase.regionserver.HStore.flushCache(HStore.java:798)
	at org.apache.hadoop.hbase.regionserver.HStore$StoreFlusherImpl.flushCache(HStore.java:1949)
	at org.apache.hadoop.hbase.regionserver.HRegion.internalFlushcache(HRegion.java:1776)
	at org.apache.hadoop.hbase.regionserver.HRegion.internalFlushcache(HRegion.java:1658)
	at org.apache.hadoop.hbase.regionserver.HRegion.doClose(HRegion.java:1119)
	at org.apache.hadoop.hbase.regionserver.HRegion.close(HRegion.java:1085)
	at org.apache.hadoop.hbase.regionserver.handler.CloseRegionHandler.process(CloseRegionHandler.java:147)
	at org.apache.hadoop.hbase.executor.EventHandler.run(EventHandler.java:128)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1146)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:701)
2014-07-01 15:51:33,586 DEBUG [RS_CLOSE_REGION-slave1:60020-1] util.FSUtils: DFS Client does not support most favored nodes create; using default create
2014-07-01 15:51:33,589 WARN  [RS_CLOSE_REGION-slave1:60020-1] regionserver.HStore: Failed flushing store file, retrying num=6
org.apache.hadoop.ipc.RemoteException: org.apache.hadoop.hdfs.server.namenode.SafeModeException: Cannot create /hbase/data/default/usertable/bb79ae773f71289e0745068f68a0abd9/.tmp/0638855eaab44c1eb9a06814173b3d93. Name node is in safe mode.
The reported blocks is only 3 but the threshold is 0.9990 and the total blocks 481. Safe mode will be turned off automatically.
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.startFileInternal(FSNamesystem.java:1561)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.startFile(FSNamesystem.java:1527)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.create(NameNode.java:710)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.create(NameNode.java:689)
	at sun.reflect.GeneratedMethodAccessor3.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:622)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:587)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1432)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1428)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:416)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1190)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:1426)

	at org.apache.hadoop.ipc.Client.call(Client.java:1113)
	at org.apache.hadoop.ipc.RPC$Invoker.invoke(RPC.java:229)
	at com.sun.proxy.$Proxy8.create(Unknown Source)
	at sun.reflect.GeneratedMethodAccessor26.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:622)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:85)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:62)
	at com.sun.proxy.$Proxy8.create(Unknown Source)
	at sun.reflect.GeneratedMethodAccessor26.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:622)
	at org.apache.hadoop.hbase.fs.HFileSystem$1.invoke(HFileSystem.java:294)
	at com.sun.proxy.$Proxy9.create(Unknown Source)
	at org.apache.hadoop.hdfs.DFSClient$DFSOutputStream.<init>(DFSClient.java:3451)
	at org.apache.hadoop.hdfs.DFSClient.create(DFSClient.java:870)
	at org.apache.hadoop.hdfs.DistributedFileSystem.create(DistributedFileSystem.java:205)
	at org.apache.hadoop.fs.FilterFileSystem.create(FilterFileSystem.java:126)
	at org.apache.hadoop.hbase.util.FSUtils.create(FSUtils.java:341)
	at org.apache.hadoop.hbase.util.FSUtils.create(FSUtils.java:316)
	at org.apache.hadoop.hbase.io.hfile.AbstractHFileWriter.createOutputStream(AbstractHFileWriter.java:266)
	at org.apache.hadoop.hbase.io.hfile.HFile$WriterFactory.create(HFile.java:391)
	at org.apache.hadoop.hbase.regionserver.StoreFile$Writer.<init>(StoreFile.java:728)
	at org.apache.hadoop.hbase.regionserver.StoreFile$Writer.<init>(StoreFile.java:679)
	at org.apache.hadoop.hbase.regionserver.StoreFile$WriterBuilder.build(StoreFile.java:617)
	at org.apache.hadoop.hbase.regionserver.HStore.createWriterInTmp(HStore.java:889)
	at org.apache.hadoop.hbase.regionserver.DefaultStoreFlusher.flushSnapshot(DefaultStoreFlusher.java:69)
	at org.apache.hadoop.hbase.regionserver.HStore.flushCache(HStore.java:798)
	at org.apache.hadoop.hbase.regionserver.HStore$StoreFlusherImpl.flushCache(HStore.java:1949)
	at org.apache.hadoop.hbase.regionserver.HRegion.internalFlushcache(HRegion.java:1776)
	at org.apache.hadoop.hbase.regionserver.HRegion.internalFlushcache(HRegion.java:1658)
	at org.apache.hadoop.hbase.regionserver.HRegion.doClose(HRegion.java:1119)
	at org.apache.hadoop.hbase.regionserver.HRegion.close(HRegion.java:1085)
	at org.apache.hadoop.hbase.regionserver.handler.CloseRegionHandler.process(CloseRegionHandler.java:147)
	at org.apache.hadoop.hbase.executor.EventHandler.run(EventHandler.java:128)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1146)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:701)
2014-07-01 15:51:34,310 WARN  [org.apache.hadoop.hdfs.LeaseRenewer$1@4fae46d5] hdfs.LeaseRenewer: Failed to renew lease for [DFSClient_hb_rs_slave1,60020,1404253868846_-130361673_31] for 30 seconds.  Will retry shortly ...
org.apache.hadoop.ipc.RemoteException: org.apache.hadoop.hdfs.server.namenode.SafeModeException: Cannot renew lease for DFSClient_hb_rs_slave1,60020,1404253868846_-130361673_31. Name node is in safe mode.
The reported blocks 486 has reached the threshold 0.9990 of total blocks 481. Safe mode will be turned off automatically in 29 seconds.
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.renewLease(FSNamesystem.java:2771)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.renewLease(NameNode.java:919)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:622)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:587)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1432)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1428)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:416)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1190)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:1426)

	at org.apache.hadoop.ipc.Client.call(Client.java:1113)
	at org.apache.hadoop.ipc.RPC$Invoker.invoke(RPC.java:229)
	at com.sun.proxy.$Proxy8.renewLease(Unknown Source)
	at sun.reflect.GeneratedMethodAccessor29.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:622)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:85)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:62)
	at com.sun.proxy.$Proxy8.renewLease(Unknown Source)
	at sun.reflect.GeneratedMethodAccessor29.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:622)
	at org.apache.hadoop.hbase.fs.HFileSystem$1.invoke(HFileSystem.java:294)
	at com.sun.proxy.$Proxy9.renewLease(Unknown Source)
	at org.apache.hadoop.hdfs.DFSClient.renewLease(DFSClient.java:379)
	at org.apache.hadoop.hdfs.LeaseRenewer.renew(LeaseRenewer.java:378)
	at org.apache.hadoop.hdfs.LeaseRenewer.run(LeaseRenewer.java:400)
	at org.apache.hadoop.hdfs.LeaseRenewer.access$600(LeaseRenewer.java:69)
	at org.apache.hadoop.hdfs.LeaseRenewer$1.run(LeaseRenewer.java:273)
	at java.lang.Thread.run(Thread.java:701)
2014-07-01 15:51:34,455 DEBUG [RS_CLOSE_REGION-slave1:60020-0] util.FSUtils: DFS Client does not support most favored nodes create; using default create
2014-07-01 15:51:34,458 WARN  [RS_CLOSE_REGION-slave1:60020-0] regionserver.HStore: Failed flushing store file, retrying num=5
org.apache.hadoop.ipc.RemoteException: org.apache.hadoop.hdfs.server.namenode.SafeModeException: Cannot create /hbase/data/default/usertable/122e92eabfab38cb15a9965b21db25c5/.tmp/f175a66be5d848deaede076490d8e480. Name node is in safe mode.
The reported blocks 486 has reached the threshold 0.9990 of total blocks 481. Safe mode will be turned off automatically in 29 seconds.
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.startFileInternal(FSNamesystem.java:1561)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.startFile(FSNamesystem.java:1527)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.create(NameNode.java:710)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.create(NameNode.java:689)
	at sun.reflect.GeneratedMethodAccessor3.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:622)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:587)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1432)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1428)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:416)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1190)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:1426)

	at org.apache.hadoop.ipc.Client.call(Client.java:1113)
	at org.apache.hadoop.ipc.RPC$Invoker.invoke(RPC.java:229)
	at com.sun.proxy.$Proxy8.create(Unknown Source)
	at sun.reflect.GeneratedMethodAccessor26.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:622)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:85)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:62)
	at com.sun.proxy.$Proxy8.create(Unknown Source)
	at sun.reflect.GeneratedMethodAccessor26.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:622)
	at org.apache.hadoop.hbase.fs.HFileSystem$1.invoke(HFileSystem.java:294)
	at com.sun.proxy.$Proxy9.create(Unknown Source)
	at org.apache.hadoop.hdfs.DFSClient$DFSOutputStream.<init>(DFSClient.java:3451)
	at org.apache.hadoop.hdfs.DFSClient.create(DFSClient.java:870)
	at org.apache.hadoop.hdfs.DistributedFileSystem.create(DistributedFileSystem.java:205)
	at org.apache.hadoop.fs.FilterFileSystem.create(FilterFileSystem.java:126)
	at org.apache.hadoop.hbase.util.FSUtils.create(FSUtils.java:341)
	at org.apache.hadoop.hbase.util.FSUtils.create(FSUtils.java:316)
	at org.apache.hadoop.hbase.io.hfile.AbstractHFileWriter.createOutputStream(AbstractHFileWriter.java:266)
	at org.apache.hadoop.hbase.io.hfile.HFile$WriterFactory.create(HFile.java:391)
	at org.apache.hadoop.hbase.regionserver.StoreFile$Writer.<init>(StoreFile.java:728)
	at org.apache.hadoop.hbase.regionserver.StoreFile$Writer.<init>(StoreFile.java:679)
	at org.apache.hadoop.hbase.regionserver.StoreFile$WriterBuilder.build(StoreFile.java:617)
	at org.apache.hadoop.hbase.regionserver.HStore.createWriterInTmp(HStore.java:889)
	at org.apache.hadoop.hbase.regionserver.DefaultStoreFlusher.flushSnapshot(DefaultStoreFlusher.java:69)
	at org.apache.hadoop.hbase.regionserver.HStore.flushCache(HStore.java:798)
	at org.apache.hadoop.hbase.regionserver.HStore$StoreFlusherImpl.flushCache(HStore.java:1949)
	at org.apache.hadoop.hbase.regionserver.HRegion.internalFlushcache(HRegion.java:1776)
	at org.apache.hadoop.hbase.regionserver.HRegion.internalFlushcache(HRegion.java:1658)
	at org.apache.hadoop.hbase.regionserver.HRegion.doClose(HRegion.java:1119)
	at org.apache.hadoop.hbase.regionserver.HRegion.close(HRegion.java:1085)
	at org.apache.hadoop.hbase.regionserver.handler.CloseRegionHandler.process(CloseRegionHandler.java:147)
	at org.apache.hadoop.hbase.executor.EventHandler.run(EventHandler.java:128)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1146)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:701)
2014-07-01 15:51:34,626 DEBUG [RS_CLOSE_REGION-slave1:60020-2] util.FSUtils: DFS Client does not support most favored nodes create; using default create
2014-07-01 15:51:34,630 WARN  [RS_CLOSE_REGION-slave1:60020-2] regionserver.HStore: Failed flushing store file, retrying num=8
org.apache.hadoop.ipc.RemoteException: org.apache.hadoop.hdfs.server.namenode.SafeModeException: Cannot create /hbase/data/default/usertable/9cef9e7d34f96f1d9b111be50040a208/.tmp/2b7344fe56744d0aa089729eabf3de0d. Name node is in safe mode.
The reported blocks 486 has reached the threshold 0.9990 of total blocks 481. Safe mode will be turned off automatically in 29 seconds.
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.startFileInternal(FSNamesystem.java:1561)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.startFile(FSNamesystem.java:1527)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.create(NameNode.java:710)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.create(NameNode.java:689)
	at sun.reflect.GeneratedMethodAccessor3.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:622)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:587)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1432)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1428)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:416)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1190)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:1426)

	at org.apache.hadoop.ipc.Client.call(Client.java:1113)
	at org.apache.hadoop.ipc.RPC$Invoker.invoke(RPC.java:229)
	at com.sun.proxy.$Proxy8.create(Unknown Source)
	at sun.reflect.GeneratedMethodAccessor26.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:622)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:85)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:62)
	at com.sun.proxy.$Proxy8.create(Unknown Source)
	at sun.reflect.GeneratedMethodAccessor26.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:622)
	at org.apache.hadoop.hbase.fs.HFileSystem$1.invoke(HFileSystem.java:294)
	at com.sun.proxy.$Proxy9.create(Unknown Source)
	at org.apache.hadoop.hdfs.DFSClient$DFSOutputStream.<init>(DFSClient.java:3451)
	at org.apache.hadoop.hdfs.DFSClient.create(DFSClient.java:870)
	at org.apache.hadoop.hdfs.DistributedFileSystem.create(DistributedFileSystem.java:205)
	at org.apache.hadoop.fs.FilterFileSystem.create(FilterFileSystem.java:126)
	at org.apache.hadoop.hbase.util.FSUtils.create(FSUtils.java:341)
	at org.apache.hadoop.hbase.util.FSUtils.create(FSUtils.java:316)
	at org.apache.hadoop.hbase.io.hfile.AbstractHFileWriter.createOutputStream(AbstractHFileWriter.java:266)
	at org.apache.hadoop.hbase.io.hfile.HFile$WriterFactory.create(HFile.java:391)
	at org.apache.hadoop.hbase.regionserver.StoreFile$Writer.<init>(StoreFile.java:728)
	at org.apache.hadoop.hbase.regionserver.StoreFile$Writer.<init>(StoreFile.java:679)
	at org.apache.hadoop.hbase.regionserver.StoreFile$WriterBuilder.build(StoreFile.java:617)
	at org.apache.hadoop.hbase.regionserver.HStore.createWriterInTmp(HStore.java:889)
	at org.apache.hadoop.hbase.regionserver.DefaultStoreFlusher.flushSnapshot(DefaultStoreFlusher.java:69)
	at org.apache.hadoop.hbase.regionserver.HStore.flushCache(HStore.java:798)
	at org.apache.hadoop.hbase.regionserver.HStore$StoreFlusherImpl.flushCache(HStore.java:1949)
	at org.apache.hadoop.hbase.regionserver.HRegion.internalFlushcache(HRegion.java:1776)
	at org.apache.hadoop.hbase.regionserver.HRegion.internalFlushcache(HRegion.java:1658)
	at org.apache.hadoop.hbase.regionserver.HRegion.doClose(HRegion.java:1119)
	at org.apache.hadoop.hbase.regionserver.HRegion.close(HRegion.java:1085)
	at org.apache.hadoop.hbase.regionserver.handler.CloseRegionHandler.process(CloseRegionHandler.java:147)
	at org.apache.hadoop.hbase.executor.EventHandler.run(EventHandler.java:128)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1146)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:701)
2014-07-01 15:51:34,657 DEBUG [RS_CLOSE_REGION-slave1:60020-1] util.FSUtils: DFS Client does not support most favored nodes create; using default create
2014-07-01 15:51:34,661 WARN  [RS_CLOSE_REGION-slave1:60020-1] regionserver.HStore: Failed flushing store file, retrying num=7
org.apache.hadoop.ipc.RemoteException: org.apache.hadoop.hdfs.server.namenode.SafeModeException: Cannot create /hbase/data/default/usertable/bb79ae773f71289e0745068f68a0abd9/.tmp/262a48c9794f4ae5b1bc66ff5e0db082. Name node is in safe mode.
The reported blocks 486 has reached the threshold 0.9990 of total blocks 481. Safe mode will be turned off automatically in 29 seconds.
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.startFileInternal(FSNamesystem.java:1561)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.startFile(FSNamesystem.java:1527)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.create(NameNode.java:710)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.create(NameNode.java:689)
	at sun.reflect.GeneratedMethodAccessor3.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:622)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:587)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1432)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1428)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:416)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1190)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:1426)

	at org.apache.hadoop.ipc.Client.call(Client.java:1113)
	at org.apache.hadoop.ipc.RPC$Invoker.invoke(RPC.java:229)
	at com.sun.proxy.$Proxy8.create(Unknown Source)
	at sun.reflect.GeneratedMethodAccessor26.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:622)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:85)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:62)
	at com.sun.proxy.$Proxy8.create(Unknown Source)
	at sun.reflect.GeneratedMethodAccessor26.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:622)
	at org.apache.hadoop.hbase.fs.HFileSystem$1.invoke(HFileSystem.java:294)
	at com.sun.proxy.$Proxy9.create(Unknown Source)
	at org.apache.hadoop.hdfs.DFSClient$DFSOutputStream.<init>(DFSClient.java:3451)
	at org.apache.hadoop.hdfs.DFSClient.create(DFSClient.java:870)
	at org.apache.hadoop.hdfs.DistributedFileSystem.create(DistributedFileSystem.java:205)
	at org.apache.hadoop.fs.FilterFileSystem.create(FilterFileSystem.java:126)
	at org.apache.hadoop.hbase.util.FSUtils.create(FSUtils.java:341)
	at org.apache.hadoop.hbase.util.FSUtils.create(FSUtils.java:316)
	at org.apache.hadoop.hbase.io.hfile.AbstractHFileWriter.createOutputStream(AbstractHFileWriter.java:266)
	at org.apache.hadoop.hbase.io.hfile.HFile$WriterFactory.create(HFile.java:391)
	at org.apache.hadoop.hbase.regionserver.StoreFile$Writer.<init>(StoreFile.java:728)
	at org.apache.hadoop.hbase.regionserver.StoreFile$Writer.<init>(StoreFile.java:679)
	at org.apache.hadoop.hbase.regionserver.StoreFile$WriterBuilder.build(StoreFile.java:617)
	at org.apache.hadoop.hbase.regionserver.HStore.createWriterInTmp(HStore.java:889)
	at org.apache.hadoop.hbase.regionserver.DefaultStoreFlusher.flushSnapshot(DefaultStoreFlusher.java:69)
	at org.apache.hadoop.hbase.regionserver.HStore.flushCache(HStore.java:798)
	at org.apache.hadoop.hbase.regionserver.HStore$StoreFlusherImpl.flushCache(HStore.java:1949)
	at org.apache.hadoop.hbase.regionserver.HRegion.internalFlushcache(HRegion.java:1776)
	at org.apache.hadoop.hbase.regionserver.HRegion.internalFlushcache(HRegion.java:1658)
	at org.apache.hadoop.hbase.regionserver.HRegion.doClose(HRegion.java:1119)
	at org.apache.hadoop.hbase.regionserver.HRegion.close(HRegion.java:1085)
	at org.apache.hadoop.hbase.regionserver.handler.CloseRegionHandler.process(CloseRegionHandler.java:147)
	at org.apache.hadoop.hbase.executor.EventHandler.run(EventHandler.java:128)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1146)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:701)
2014-07-01 15:51:34,671 INFO  [regionserver60020] regionserver.HRegionServer: Received CLOSE for the region: 122e92eabfab38cb15a9965b21db25c5 ,which we are already trying to CLOSE, but not completed yet
2014-07-01 15:51:34,671 WARN  [regionserver60020] regionserver.HRegionServer: Failed to close usertable,user5,1404228265526.122e92eabfab38cb15a9965b21db25c5. - ignoring and continuing
org.apache.hadoop.hbase.regionserver.RegionAlreadyInTransitionException: The region 122e92eabfab38cb15a9965b21db25c5 was already closing. New CLOSE request is ignored.
	at org.apache.hadoop.hbase.regionserver.HRegionServer.closeRegion(HRegionServer.java:2586)
	at org.apache.hadoop.hbase.regionserver.HRegionServer.closeRegionIgnoreErrors(HRegionServer.java:2508)
	at org.apache.hadoop.hbase.regionserver.HRegionServer.closeUserRegions(HRegionServer.java:2098)
	at org.apache.hadoop.hbase.regionserver.HRegionServer.run(HRegionServer.java:894)
	at java.lang.Thread.run(Thread.java:701)
2014-07-01 15:51:34,672 INFO  [regionserver60020] regionserver.HRegionServer: Received CLOSE for the region: bb79ae773f71289e0745068f68a0abd9 ,which we are already trying to CLOSE, but not completed yet
2014-07-01 15:51:34,672 WARN  [regionserver60020] regionserver.HRegionServer: Failed to close usertable,user9,1404228265526.bb79ae773f71289e0745068f68a0abd9. - ignoring and continuing
org.apache.hadoop.hbase.regionserver.RegionAlreadyInTransitionException: The region bb79ae773f71289e0745068f68a0abd9 was already closing. New CLOSE request is ignored.
	at org.apache.hadoop.hbase.regionserver.HRegionServer.closeRegion(HRegionServer.java:2586)
	at org.apache.hadoop.hbase.regionserver.HRegionServer.closeRegionIgnoreErrors(HRegionServer.java:2508)
	at org.apache.hadoop.hbase.regionserver.HRegionServer.closeUserRegions(HRegionServer.java:2098)
	at org.apache.hadoop.hbase.regionserver.HRegionServer.run(HRegionServer.java:894)
	at java.lang.Thread.run(Thread.java:701)
2014-07-01 15:51:34,672 INFO  [regionserver60020] regionserver.HRegionServer: Received CLOSE for the region: 9cef9e7d34f96f1d9b111be50040a208 ,which we are already trying to CLOSE, but not completed yet
2014-07-01 15:51:34,672 WARN  [regionserver60020] regionserver.HRegionServer: Failed to close usertable,,1404228265525.9cef9e7d34f96f1d9b111be50040a208. - ignoring and continuing
org.apache.hadoop.hbase.regionserver.RegionAlreadyInTransitionException: The region 9cef9e7d34f96f1d9b111be50040a208 was already closing. New CLOSE request is ignored.
	at org.apache.hadoop.hbase.regionserver.HRegionServer.closeRegion(HRegionServer.java:2586)
	at org.apache.hadoop.hbase.regionserver.HRegionServer.closeRegionIgnoreErrors(HRegionServer.java:2508)
	at org.apache.hadoop.hbase.regionserver.HRegionServer.closeUserRegions(HRegionServer.java:2098)
	at org.apache.hadoop.hbase.regionserver.HRegionServer.run(HRegionServer.java:894)
	at java.lang.Thread.run(Thread.java:701)
2014-07-01 15:51:34,672 INFO  [regionserver60020] regionserver.HRegionServer: Received CLOSE for the region: 93253e37c284f3174309fede3f32339f ,which we are already trying to CLOSE, but not completed yet
2014-07-01 15:51:34,672 WARN  [regionserver60020] regionserver.HRegionServer: Failed to close usertable,user3,1404228265526.93253e37c284f3174309fede3f32339f. - ignoring and continuing
org.apache.hadoop.hbase.regionserver.RegionAlreadyInTransitionException: The region 93253e37c284f3174309fede3f32339f was already closing. New CLOSE request is ignored.
	at org.apache.hadoop.hbase.regionserver.HRegionServer.closeRegion(HRegionServer.java:2586)
	at org.apache.hadoop.hbase.regionserver.HRegionServer.closeRegionIgnoreErrors(HRegionServer.java:2508)
	at org.apache.hadoop.hbase.regionserver.HRegionServer.closeUserRegions(HRegionServer.java:2098)
	at org.apache.hadoop.hbase.regionserver.HRegionServer.run(HRegionServer.java:894)
	at java.lang.Thread.run(Thread.java:701)
2014-07-01 15:51:34,673 INFO  [regionserver60020] regionserver.HRegionServer: Received CLOSE for the region: 667c1f2c34fe37d48ba25a789d8aba98 ,which we are already trying to CLOSE, but not completed yet
2014-07-01 15:51:34,673 WARN  [regionserver60020] regionserver.HRegionServer: Failed to close usertable,user7,1404228265526.667c1f2c34fe37d48ba25a789d8aba98. - ignoring and continuing
org.apache.hadoop.hbase.regionserver.RegionAlreadyInTransitionException: The region 667c1f2c34fe37d48ba25a789d8aba98 was already closing. New CLOSE request is ignored.
	at org.apache.hadoop.hbase.regionserver.HRegionServer.closeRegion(HRegionServer.java:2586)
	at org.apache.hadoop.hbase.regionserver.HRegionServer.closeRegionIgnoreErrors(HRegionServer.java:2508)
	at org.apache.hadoop.hbase.regionserver.HRegionServer.closeUserRegions(HRegionServer.java:2098)
	at org.apache.hadoop.hbase.regionserver.HRegionServer.run(HRegionServer.java:894)
	at java.lang.Thread.run(Thread.java:701)
2014-07-01 15:51:34,673 DEBUG [regionserver60020] regionserver.HRegionServer: Waiting on 122e92eabfab38cb15a9965b21db25c5, bb79ae773f71289e0745068f68a0abd9, 9cef9e7d34f96f1d9b111be50040a208, 93253e37c284f3174309fede3f32339f, 667c1f2c34fe37d48ba25a789d8aba98
2014-07-01 15:51:35,315 WARN  [org.apache.hadoop.hdfs.LeaseRenewer$1@4fae46d5] hdfs.LeaseRenewer: Failed to renew lease for [DFSClient_hb_rs_slave1,60020,1404253868846_-130361673_31] for 30 seconds.  Will retry shortly ...
org.apache.hadoop.ipc.RemoteException: org.apache.hadoop.hdfs.server.namenode.SafeModeException: Cannot renew lease for DFSClient_hb_rs_slave1,60020,1404253868846_-130361673_31. Name node is in safe mode.
The reported blocks 486 has reached the threshold 0.9990 of total blocks 481. Safe mode will be turned off automatically in 28 seconds.
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.renewLease(FSNamesystem.java:2771)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.renewLease(NameNode.java:919)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:622)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:587)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1432)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1428)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:416)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1190)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:1426)

	at org.apache.hadoop.ipc.Client.call(Client.java:1113)
	at org.apache.hadoop.ipc.RPC$Invoker.invoke(RPC.java:229)
	at com.sun.proxy.$Proxy8.renewLease(Unknown Source)
	at sun.reflect.GeneratedMethodAccessor29.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:622)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:85)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:62)
	at com.sun.proxy.$Proxy8.renewLease(Unknown Source)
	at sun.reflect.GeneratedMethodAccessor29.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:622)
	at org.apache.hadoop.hbase.fs.HFileSystem$1.invoke(HFileSystem.java:294)
	at com.sun.proxy.$Proxy9.renewLease(Unknown Source)
	at org.apache.hadoop.hdfs.DFSClient.renewLease(DFSClient.java:379)
	at org.apache.hadoop.hdfs.LeaseRenewer.renew(LeaseRenewer.java:378)
	at org.apache.hadoop.hdfs.LeaseRenewer.run(LeaseRenewer.java:400)
	at org.apache.hadoop.hdfs.LeaseRenewer.access$600(LeaseRenewer.java:69)
	at org.apache.hadoop.hdfs.LeaseRenewer$1.run(LeaseRenewer.java:273)
	at java.lang.Thread.run(Thread.java:701)
2014-07-01 15:51:35,488 DEBUG [RS_CLOSE_REGION-slave1:60020-0] util.FSUtils: DFS Client does not support most favored nodes create; using default create
2014-07-01 15:51:35,491 WARN  [RS_CLOSE_REGION-slave1:60020-0] regionserver.HStore: Failed flushing store file, retrying num=6
org.apache.hadoop.ipc.RemoteException: org.apache.hadoop.hdfs.server.namenode.SafeModeException: Cannot create /hbase/data/default/usertable/122e92eabfab38cb15a9965b21db25c5/.tmp/73ced97dae6242fc97b147abb336dbeb. Name node is in safe mode.
The reported blocks 486 has reached the threshold 0.9990 of total blocks 481. Safe mode will be turned off automatically in 28 seconds.
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.startFileInternal(FSNamesystem.java:1561)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.startFile(FSNamesystem.java:1527)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.create(NameNode.java:710)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.create(NameNode.java:689)
	at sun.reflect.GeneratedMethodAccessor3.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:622)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:587)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1432)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1428)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:416)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1190)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:1426)

	at org.apache.hadoop.ipc.Client.call(Client.java:1113)
	at org.apache.hadoop.ipc.RPC$Invoker.invoke(RPC.java:229)
	at com.sun.proxy.$Proxy8.create(Unknown Source)
	at sun.reflect.GeneratedMethodAccessor26.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:622)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:85)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:62)
	at com.sun.proxy.$Proxy8.create(Unknown Source)
	at sun.reflect.GeneratedMethodAccessor26.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:622)
	at org.apache.hadoop.hbase.fs.HFileSystem$1.invoke(HFileSystem.java:294)
	at com.sun.proxy.$Proxy9.create(Unknown Source)
	at org.apache.hadoop.hdfs.DFSClient$DFSOutputStream.<init>(DFSClient.java:3451)
	at org.apache.hadoop.hdfs.DFSClient.create(DFSClient.java:870)
	at org.apache.hadoop.hdfs.DistributedFileSystem.create(DistributedFileSystem.java:205)
	at org.apache.hadoop.fs.FilterFileSystem.create(FilterFileSystem.java:126)
	at org.apache.hadoop.hbase.util.FSUtils.create(FSUtils.java:341)
	at org.apache.hadoop.hbase.util.FSUtils.create(FSUtils.java:316)
	at org.apache.hadoop.hbase.io.hfile.AbstractHFileWriter.createOutputStream(AbstractHFileWriter.java:266)
	at org.apache.hadoop.hbase.io.hfile.HFile$WriterFactory.create(HFile.java:391)
	at org.apache.hadoop.hbase.regionserver.StoreFile$Writer.<init>(StoreFile.java:728)
	at org.apache.hadoop.hbase.regionserver.StoreFile$Writer.<init>(StoreFile.java:679)
	at org.apache.hadoop.hbase.regionserver.StoreFile$WriterBuilder.build(StoreFile.java:617)
	at org.apache.hadoop.hbase.regionserver.HStore.createWriterInTmp(HStore.java:889)
	at org.apache.hadoop.hbase.regionserver.DefaultStoreFlusher.flushSnapshot(DefaultStoreFlusher.java:69)
	at org.apache.hadoop.hbase.regionserver.HStore.flushCache(HStore.java:798)
	at org.apache.hadoop.hbase.regionserver.HStore$StoreFlusherImpl.flushCache(HStore.java:1949)
	at org.apache.hadoop.hbase.regionserver.HRegion.internalFlushcache(HRegion.java:1776)
	at org.apache.hadoop.hbase.regionserver.HRegion.internalFlushcache(HRegion.java:1658)
	at org.apache.hadoop.hbase.regionserver.HRegion.doClose(HRegion.java:1119)
	at org.apache.hadoop.hbase.regionserver.HRegion.close(HRegion.java:1085)
	at org.apache.hadoop.hbase.regionserver.handler.CloseRegionHandler.process(CloseRegionHandler.java:147)
	at org.apache.hadoop.hbase.executor.EventHandler.run(EventHandler.java:128)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1146)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:701)
2014-07-01 15:51:35,696 DEBUG [RS_CLOSE_REGION-slave1:60020-2] util.FSUtils: DFS Client does not support most favored nodes create; using default create
2014-07-01 15:51:35,699 WARN  [RS_CLOSE_REGION-slave1:60020-2] regionserver.HStore: Failed flushing store file, retrying num=9
org.apache.hadoop.ipc.RemoteException: org.apache.hadoop.hdfs.server.namenode.SafeModeException: Cannot create /hbase/data/default/usertable/9cef9e7d34f96f1d9b111be50040a208/.tmp/4657b934eb5e44e99edf6f0e35425d56. Name node is in safe mode.
The reported blocks 486 has reached the threshold 0.9990 of total blocks 481. Safe mode will be turned off automatically in 28 seconds.
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.startFileInternal(FSNamesystem.java:1561)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.startFile(FSNamesystem.java:1527)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.create(NameNode.java:710)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.create(NameNode.java:689)
	at sun.reflect.GeneratedMethodAccessor3.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:622)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:587)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1432)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1428)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:416)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1190)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:1426)

	at org.apache.hadoop.ipc.Client.call(Client.java:1113)
	at org.apache.hadoop.ipc.RPC$Invoker.invoke(RPC.java:229)
	at com.sun.proxy.$Proxy8.create(Unknown Source)
	at sun.reflect.GeneratedMethodAccessor26.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:622)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:85)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:62)
	at com.sun.proxy.$Proxy8.create(Unknown Source)
	at sun.reflect.GeneratedMethodAccessor26.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:622)
	at org.apache.hadoop.hbase.fs.HFileSystem$1.invoke(HFileSystem.java:294)
	at com.sun.proxy.$Proxy9.create(Unknown Source)
	at org.apache.hadoop.hdfs.DFSClient$DFSOutputStream.<init>(DFSClient.java:3451)
	at org.apache.hadoop.hdfs.DFSClient.create(DFSClient.java:870)
	at org.apache.hadoop.hdfs.DistributedFileSystem.create(DistributedFileSystem.java:205)
	at org.apache.hadoop.fs.FilterFileSystem.create(FilterFileSystem.java:126)
	at org.apache.hadoop.hbase.util.FSUtils.create(FSUtils.java:341)
	at org.apache.hadoop.hbase.util.FSUtils.create(FSUtils.java:316)
	at org.apache.hadoop.hbase.io.hfile.AbstractHFileWriter.createOutputStream(AbstractHFileWriter.java:266)
	at org.apache.hadoop.hbase.io.hfile.HFile$WriterFactory.create(HFile.java:391)
	at org.apache.hadoop.hbase.regionserver.StoreFile$Writer.<init>(StoreFile.java:728)
	at org.apache.hadoop.hbase.regionserver.StoreFile$Writer.<init>(StoreFile.java:679)
	at org.apache.hadoop.hbase.regionserver.StoreFile$WriterBuilder.build(StoreFile.java:617)
	at org.apache.hadoop.hbase.regionserver.HStore.createWriterInTmp(HStore.java:889)
	at org.apache.hadoop.hbase.regionserver.DefaultStoreFlusher.flushSnapshot(DefaultStoreFlusher.java:69)
	at org.apache.hadoop.hbase.regionserver.HStore.flushCache(HStore.java:798)
	at org.apache.hadoop.hbase.regionserver.HStore$StoreFlusherImpl.flushCache(HStore.java:1949)
	at org.apache.hadoop.hbase.regionserver.HRegion.internalFlushcache(HRegion.java:1776)
	at org.apache.hadoop.hbase.regionserver.HRegion.internalFlushcache(HRegion.java:1658)
	at org.apache.hadoop.hbase.regionserver.HRegion.doClose(HRegion.java:1119)
	at org.apache.hadoop.hbase.regionserver.HRegion.close(HRegion.java:1085)
	at org.apache.hadoop.hbase.regionserver.handler.CloseRegionHandler.process(CloseRegionHandler.java:147)
	at org.apache.hadoop.hbase.executor.EventHandler.run(EventHandler.java:128)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1146)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:701)
2014-07-01 15:51:35,701 DEBUG [RS_CLOSE_REGION-slave1:60020-2] regionserver.HRegion: Updates disabled for region usertable,,1404228265525.9cef9e7d34f96f1d9b111be50040a208.
2014-07-01 15:51:35,701 DEBUG [RS_CLOSE_REGION-slave1:60020-2] regionserver.HRegion: Started memstore flush for usertable,,1404228265525.9cef9e7d34f96f1d9b111be50040a208., current region memstore size 150.4m
2014-07-01 15:51:35,701 WARN  [RS_CLOSE_REGION-slave1:60020-2] regionserver.MemStore: Snapshot called again without clearing previous. Doing nothing. Another ongoing flush or did we fail last attempt?
2014-07-01 15:51:35,738 DEBUG [RS_CLOSE_REGION-slave1:60020-1] util.FSUtils: DFS Client does not support most favored nodes create; using default create
2014-07-01 15:51:35,740 WARN  [RS_CLOSE_REGION-slave1:60020-1] regionserver.HStore: Failed flushing store file, retrying num=8
org.apache.hadoop.ipc.RemoteException: org.apache.hadoop.hdfs.server.namenode.SafeModeException: Cannot create /hbase/data/default/usertable/bb79ae773f71289e0745068f68a0abd9/.tmp/44ebd804d6d24f4592f9b5f082bc0e2b. Name node is in safe mode.
The reported blocks 486 has reached the threshold 0.9990 of total blocks 481. Safe mode will be turned off automatically in 28 seconds.
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.startFileInternal(FSNamesystem.java:1561)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.startFile(FSNamesystem.java:1527)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.create(NameNode.java:710)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.create(NameNode.java:689)
	at sun.reflect.GeneratedMethodAccessor3.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:622)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:587)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1432)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1428)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:416)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1190)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:1426)

	at org.apache.hadoop.ipc.Client.call(Client.java:1113)
	at org.apache.hadoop.ipc.RPC$Invoker.invoke(RPC.java:229)
	at com.sun.proxy.$Proxy8.create(Unknown Source)
	at sun.reflect.GeneratedMethodAccessor26.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:622)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:85)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:62)
	at com.sun.proxy.$Proxy8.create(Unknown Source)
	at sun.reflect.GeneratedMethodAccessor26.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:622)
	at org.apache.hadoop.hbase.fs.HFileSystem$1.invoke(HFileSystem.java:294)
	at com.sun.proxy.$Proxy9.create(Unknown Source)
	at org.apache.hadoop.hdfs.DFSClient$DFSOutputStream.<init>(DFSClient.java:3451)
	at org.apache.hadoop.hdfs.DFSClient.create(DFSClient.java:870)
	at org.apache.hadoop.hdfs.DistributedFileSystem.create(DistributedFileSystem.java:205)
	at org.apache.hadoop.fs.FilterFileSystem.create(FilterFileSystem.java:126)
	at org.apache.hadoop.hbase.util.FSUtils.create(FSUtils.java:341)
	at org.apache.hadoop.hbase.util.FSUtils.create(FSUtils.java:316)
	at org.apache.hadoop.hbase.io.hfile.AbstractHFileWriter.createOutputStream(AbstractHFileWriter.java:266)
	at org.apache.hadoop.hbase.io.hfile.HFile$WriterFactory.create(HFile.java:391)
	at org.apache.hadoop.hbase.regionserver.StoreFile$Writer.<init>(StoreFile.java:728)
	at org.apache.hadoop.hbase.regionserver.StoreFile$Writer.<init>(StoreFile.java:679)
	at org.apache.hadoop.hbase.regionserver.StoreFile$WriterBuilder.build(StoreFile.java:617)
	at org.apache.hadoop.hbase.regionserver.HStore.createWriterInTmp(HStore.java:889)
	at org.apache.hadoop.hbase.regionserver.DefaultStoreFlusher.flushSnapshot(DefaultStoreFlusher.java:69)
	at org.apache.hadoop.hbase.regionserver.HStore.flushCache(HStore.java:798)
	at org.apache.hadoop.hbase.regionserver.HStore$StoreFlusherImpl.flushCache(HStore.java:1949)
	at org.apache.hadoop.hbase.regionserver.HRegion.internalFlushcache(HRegion.java:1776)
	at org.apache.hadoop.hbase.regionserver.HRegion.internalFlushcache(HRegion.java:1658)
	at org.apache.hadoop.hbase.regionserver.HRegion.doClose(HRegion.java:1119)
	at org.apache.hadoop.hbase.regionserver.HRegion.close(HRegion.java:1085)
	at org.apache.hadoop.hbase.regionserver.handler.CloseRegionHandler.process(CloseRegionHandler.java:147)
	at org.apache.hadoop.hbase.executor.EventHandler.run(EventHandler.java:128)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1146)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:701)
2014-07-01 15:51:35,760 DEBUG [RS_CLOSE_REGION-slave1:60020-2] util.FSUtils: DFS Client does not support most favored nodes create; using default create
2014-07-01 15:51:35,762 WARN  [RS_CLOSE_REGION-slave1:60020-2] regionserver.HStore: Failed flushing store file, retrying num=0
org.apache.hadoop.ipc.RemoteException: org.apache.hadoop.hdfs.server.namenode.SafeModeException: Cannot create /hbase/data/default/usertable/9cef9e7d34f96f1d9b111be50040a208/.tmp/db61c3954f2d43159940becefd48a45a. Name node is in safe mode.
The reported blocks 486 has reached the threshold 0.9990 of total blocks 481. Safe mode will be turned off automatically in 28 seconds.
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.startFileInternal(FSNamesystem.java:1561)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.startFile(FSNamesystem.java:1527)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.create(NameNode.java:710)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.create(NameNode.java:689)
	at sun.reflect.GeneratedMethodAccessor3.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:622)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:587)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1432)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1428)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:416)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1190)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:1426)

	at org.apache.hadoop.ipc.Client.call(Client.java:1113)
	at org.apache.hadoop.ipc.RPC$Invoker.invoke(RPC.java:229)
	at com.sun.proxy.$Proxy8.create(Unknown Source)
	at sun.reflect.GeneratedMethodAccessor26.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:622)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:85)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:62)
	at com.sun.proxy.$Proxy8.create(Unknown Source)
	at sun.reflect.GeneratedMethodAccessor26.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:622)
	at org.apache.hadoop.hbase.fs.HFileSystem$1.invoke(HFileSystem.java:294)
	at com.sun.proxy.$Proxy9.create(Unknown Source)
	at org.apache.hadoop.hdfs.DFSClient$DFSOutputStream.<init>(DFSClient.java:3451)
	at org.apache.hadoop.hdfs.DFSClient.create(DFSClient.java:870)
	at org.apache.hadoop.hdfs.DistributedFileSystem.create(DistributedFileSystem.java:205)
	at org.apache.hadoop.fs.FilterFileSystem.create(FilterFileSystem.java:126)
	at org.apache.hadoop.hbase.util.FSUtils.create(FSUtils.java:341)
	at org.apache.hadoop.hbase.util.FSUtils.create(FSUtils.java:316)
	at org.apache.hadoop.hbase.io.hfile.AbstractHFileWriter.createOutputStream(AbstractHFileWriter.java:266)
	at org.apache.hadoop.hbase.io.hfile.HFile$WriterFactory.create(HFile.java:391)
	at org.apache.hadoop.hbase.regionserver.StoreFile$Writer.<init>(StoreFile.java:728)
	at org.apache.hadoop.hbase.regionserver.StoreFile$Writer.<init>(StoreFile.java:679)
	at org.apache.hadoop.hbase.regionserver.StoreFile$WriterBuilder.build(StoreFile.java:617)
	at org.apache.hadoop.hbase.regionserver.HStore.createWriterInTmp(HStore.java:889)
	at org.apache.hadoop.hbase.regionserver.DefaultStoreFlusher.flushSnapshot(DefaultStoreFlusher.java:69)
	at org.apache.hadoop.hbase.regionserver.HStore.flushCache(HStore.java:798)
	at org.apache.hadoop.hbase.regionserver.HStore$StoreFlusherImpl.flushCache(HStore.java:1949)
	at org.apache.hadoop.hbase.regionserver.HRegion.internalFlushcache(HRegion.java:1776)
	at org.apache.hadoop.hbase.regionserver.HRegion.internalFlushcache(HRegion.java:1658)
	at org.apache.hadoop.hbase.regionserver.HRegion.doClose(HRegion.java:1153)
	at org.apache.hadoop.hbase.regionserver.HRegion.close(HRegion.java:1085)
	at org.apache.hadoop.hbase.regionserver.handler.CloseRegionHandler.process(CloseRegionHandler.java:147)
	at org.apache.hadoop.hbase.executor.EventHandler.run(EventHandler.java:128)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1146)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:701)
2014-07-01 15:51:36,318 WARN  [org.apache.hadoop.hdfs.LeaseRenewer$1@4fae46d5] hdfs.LeaseRenewer: Failed to renew lease for [DFSClient_hb_rs_slave1,60020,1404253868846_-130361673_31] for 30 seconds.  Will retry shortly ...
org.apache.hadoop.ipc.RemoteException: org.apache.hadoop.hdfs.server.namenode.SafeModeException: Cannot renew lease for DFSClient_hb_rs_slave1,60020,1404253868846_-130361673_31. Name node is in safe mode.
The reported blocks 486 has reached the threshold 0.9990 of total blocks 481. Safe mode will be turned off automatically in 27 seconds.
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.renewLease(FSNamesystem.java:2771)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.renewLease(NameNode.java:919)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:622)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:587)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1432)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1428)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:416)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1190)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:1426)

	at org.apache.hadoop.ipc.Client.call(Client.java:1113)
	at org.apache.hadoop.ipc.RPC$Invoker.invoke(RPC.java:229)
	at com.sun.proxy.$Proxy8.renewLease(Unknown Source)
	at sun.reflect.GeneratedMethodAccessor29.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:622)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:85)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:62)
	at com.sun.proxy.$Proxy8.renewLease(Unknown Source)
	at sun.reflect.GeneratedMethodAccessor29.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:622)
	at org.apache.hadoop.hbase.fs.HFileSystem$1.invoke(HFileSystem.java:294)
	at com.sun.proxy.$Proxy9.renewLease(Unknown Source)
	at org.apache.hadoop.hdfs.DFSClient.renewLease(DFSClient.java:379)
	at org.apache.hadoop.hdfs.LeaseRenewer.renew(LeaseRenewer.java:378)
	at org.apache.hadoop.hdfs.LeaseRenewer.run(LeaseRenewer.java:400)
	at org.apache.hadoop.hdfs.LeaseRenewer.access$600(LeaseRenewer.java:69)
	at org.apache.hadoop.hdfs.LeaseRenewer$1.run(LeaseRenewer.java:273)
	at java.lang.Thread.run(Thread.java:701)
2014-07-01 15:51:36,519 DEBUG [RS_CLOSE_REGION-slave1:60020-0] util.FSUtils: DFS Client does not support most favored nodes create; using default create
2014-07-01 15:51:36,522 WARN  [RS_CLOSE_REGION-slave1:60020-0] regionserver.HStore: Failed flushing store file, retrying num=7
org.apache.hadoop.ipc.RemoteException: org.apache.hadoop.hdfs.server.namenode.SafeModeException: Cannot create /hbase/data/default/usertable/122e92eabfab38cb15a9965b21db25c5/.tmp/2beafecbe9b3475da5432d37d07526d6. Name node is in safe mode.
The reported blocks 486 has reached the threshold 0.9990 of total blocks 481. Safe mode will be turned off automatically in 27 seconds.
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.startFileInternal(FSNamesystem.java:1561)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.startFile(FSNamesystem.java:1527)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.create(NameNode.java:710)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.create(NameNode.java:689)
	at sun.reflect.GeneratedMethodAccessor3.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:622)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:587)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1432)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1428)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:416)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1190)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:1426)

	at org.apache.hadoop.ipc.Client.call(Client.java:1113)
	at org.apache.hadoop.ipc.RPC$Invoker.invoke(RPC.java:229)
	at com.sun.proxy.$Proxy8.create(Unknown Source)
	at sun.reflect.GeneratedMethodAccessor26.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:622)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:85)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:62)
	at com.sun.proxy.$Proxy8.create(Unknown Source)
	at sun.reflect.GeneratedMethodAccessor26.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:622)
	at org.apache.hadoop.hbase.fs.HFileSystem$1.invoke(HFileSystem.java:294)
	at com.sun.proxy.$Proxy9.create(Unknown Source)
	at org.apache.hadoop.hdfs.DFSClient$DFSOutputStream.<init>(DFSClient.java:3451)
	at org.apache.hadoop.hdfs.DFSClient.create(DFSClient.java:870)
	at org.apache.hadoop.hdfs.DistributedFileSystem.create(DistributedFileSystem.java:205)
	at org.apache.hadoop.fs.FilterFileSystem.create(FilterFileSystem.java:126)
	at org.apache.hadoop.hbase.util.FSUtils.create(FSUtils.java:341)
	at org.apache.hadoop.hbase.util.FSUtils.create(FSUtils.java:316)
	at org.apache.hadoop.hbase.io.hfile.AbstractHFileWriter.createOutputStream(AbstractHFileWriter.java:266)
	at org.apache.hadoop.hbase.io.hfile.HFile$WriterFactory.create(HFile.java:391)
	at org.apache.hadoop.hbase.regionserver.StoreFile$Writer.<init>(StoreFile.java:728)
	at org.apache.hadoop.hbase.regionserver.StoreFile$Writer.<init>(StoreFile.java:679)
	at org.apache.hadoop.hbase.regionserver.StoreFile$WriterBuilder.build(StoreFile.java:617)
	at org.apache.hadoop.hbase.regionserver.HStore.createWriterInTmp(HStore.java:889)
	at org.apache.hadoop.hbase.regionserver.DefaultStoreFlusher.flushSnapshot(DefaultStoreFlusher.java:69)
	at org.apache.hadoop.hbase.regionserver.HStore.flushCache(HStore.java:798)
	at org.apache.hadoop.hbase.regionserver.HStore$StoreFlusherImpl.flushCache(HStore.java:1949)
	at org.apache.hadoop.hbase.regionserver.HRegion.internalFlushcache(HRegion.java:1776)
	at org.apache.hadoop.hbase.regionserver.HRegion.internalFlushcache(HRegion.java:1658)
	at org.apache.hadoop.hbase.regionserver.HRegion.doClose(HRegion.java:1119)
	at org.apache.hadoop.hbase.regionserver.HRegion.close(HRegion.java:1085)
	at org.apache.hadoop.hbase.regionserver.handler.CloseRegionHandler.process(CloseRegionHandler.java:147)
	at org.apache.hadoop.hbase.executor.EventHandler.run(EventHandler.java:128)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1146)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:701)
2014-07-01 15:51:36,816 DEBUG [RS_CLOSE_REGION-slave1:60020-1] util.FSUtils: DFS Client does not support most favored nodes create; using default create
2014-07-01 15:51:36,819 WARN  [RS_CLOSE_REGION-slave1:60020-1] regionserver.HStore: Failed flushing store file, retrying num=9
org.apache.hadoop.ipc.RemoteException: org.apache.hadoop.hdfs.server.namenode.SafeModeException: Cannot create /hbase/data/default/usertable/bb79ae773f71289e0745068f68a0abd9/.tmp/2c1dd78b033845e9b67fbb2e6263cf12. Name node is in safe mode.
The reported blocks 486 has reached the threshold 0.9990 of total blocks 481. Safe mode will be turned off automatically in 27 seconds.
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.startFileInternal(FSNamesystem.java:1561)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.startFile(FSNamesystem.java:1527)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.create(NameNode.java:710)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.create(NameNode.java:689)
	at sun.reflect.GeneratedMethodAccessor3.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:622)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:587)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1432)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1428)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:416)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1190)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:1426)

	at org.apache.hadoop.ipc.Client.call(Client.java:1113)
	at org.apache.hadoop.ipc.RPC$Invoker.invoke(RPC.java:229)
	at com.sun.proxy.$Proxy8.create(Unknown Source)
	at sun.reflect.GeneratedMethodAccessor26.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:622)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:85)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:62)
	at com.sun.proxy.$Proxy8.create(Unknown Source)
	at sun.reflect.GeneratedMethodAccessor26.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:622)
	at org.apache.hadoop.hbase.fs.HFileSystem$1.invoke(HFileSystem.java:294)
	at com.sun.proxy.$Proxy9.create(Unknown Source)
	at org.apache.hadoop.hdfs.DFSClient$DFSOutputStream.<init>(DFSClient.java:3451)
	at org.apache.hadoop.hdfs.DFSClient.create(DFSClient.java:870)
	at org.apache.hadoop.hdfs.DistributedFileSystem.create(DistributedFileSystem.java:205)
	at org.apache.hadoop.fs.FilterFileSystem.create(FilterFileSystem.java:126)
	at org.apache.hadoop.hbase.util.FSUtils.create(FSUtils.java:341)
	at org.apache.hadoop.hbase.util.FSUtils.create(FSUtils.java:316)
	at org.apache.hadoop.hbase.io.hfile.AbstractHFileWriter.createOutputStream(AbstractHFileWriter.java:266)
	at org.apache.hadoop.hbase.io.hfile.HFile$WriterFactory.create(HFile.java:391)
	at org.apache.hadoop.hbase.regionserver.StoreFile$Writer.<init>(StoreFile.java:728)
	at org.apache.hadoop.hbase.regionserver.StoreFile$Writer.<init>(StoreFile.java:679)
	at org.apache.hadoop.hbase.regionserver.StoreFile$WriterBuilder.build(StoreFile.java:617)
	at org.apache.hadoop.hbase.regionserver.HStore.createWriterInTmp(HStore.java:889)
	at org.apache.hadoop.hbase.regionserver.DefaultStoreFlusher.flushSnapshot(DefaultStoreFlusher.java:69)
	at org.apache.hadoop.hbase.regionserver.HStore.flushCache(HStore.java:798)
	at org.apache.hadoop.hbase.regionserver.HStore$StoreFlusherImpl.flushCache(HStore.java:1949)
	at org.apache.hadoop.hbase.regionserver.HRegion.internalFlushcache(HRegion.java:1776)
	at org.apache.hadoop.hbase.regionserver.HRegion.internalFlushcache(HRegion.java:1658)
	at org.apache.hadoop.hbase.regionserver.HRegion.doClose(HRegion.java:1119)
	at org.apache.hadoop.hbase.regionserver.HRegion.close(HRegion.java:1085)
	at org.apache.hadoop.hbase.regionserver.handler.CloseRegionHandler.process(CloseRegionHandler.java:147)
	at org.apache.hadoop.hbase.executor.EventHandler.run(EventHandler.java:128)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1146)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:701)
2014-07-01 15:51:36,820 DEBUG [RS_CLOSE_REGION-slave1:60020-1] regionserver.HRegion: Updates disabled for region usertable,user9,1404228265526.bb79ae773f71289e0745068f68a0abd9.
2014-07-01 15:51:36,820 DEBUG [RS_CLOSE_REGION-slave1:60020-1] regionserver.HRegion: Started memstore flush for usertable,user9,1404228265526.bb79ae773f71289e0745068f68a0abd9., current region memstore size 134.8m
2014-07-01 15:51:36,820 WARN  [RS_CLOSE_REGION-slave1:60020-1] regionserver.MemStore: Snapshot called again without clearing previous. Doing nothing. Another ongoing flush or did we fail last attempt?
2014-07-01 15:51:36,830 DEBUG [RS_CLOSE_REGION-slave1:60020-2] util.FSUtils: DFS Client does not support most favored nodes create; using default create
2014-07-01 15:51:36,832 WARN  [RS_CLOSE_REGION-slave1:60020-2] regionserver.HStore: Failed flushing store file, retrying num=1
org.apache.hadoop.ipc.RemoteException: org.apache.hadoop.hdfs.server.namenode.SafeModeException: Cannot create /hbase/data/default/usertable/9cef9e7d34f96f1d9b111be50040a208/.tmp/8f9a97f8f85543dba500e861d72644a0. Name node is in safe mode.
The reported blocks 486 has reached the threshold 0.9990 of total blocks 481. Safe mode will be turned off automatically in 27 seconds.
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.startFileInternal(FSNamesystem.java:1561)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.startFile(FSNamesystem.java:1527)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.create(NameNode.java:710)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.create(NameNode.java:689)
	at sun.reflect.GeneratedMethodAccessor3.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:622)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:587)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1432)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1428)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:416)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1190)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:1426)

	at org.apache.hadoop.ipc.Client.call(Client.java:1113)
	at org.apache.hadoop.ipc.RPC$Invoker.invoke(RPC.java:229)
	at com.sun.proxy.$Proxy8.create(Unknown Source)
	at sun.reflect.GeneratedMethodAccessor26.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:622)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:85)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:62)
	at com.sun.proxy.$Proxy8.create(Unknown Source)
	at sun.reflect.GeneratedMethodAccessor26.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:622)
	at org.apache.hadoop.hbase.fs.HFileSystem$1.invoke(HFileSystem.java:294)
	at com.sun.proxy.$Proxy9.create(Unknown Source)
	at org.apache.hadoop.hdfs.DFSClient$DFSOutputStream.<init>(DFSClient.java:3451)
	at org.apache.hadoop.hdfs.DFSClient.create(DFSClient.java:870)
	at org.apache.hadoop.hdfs.DistributedFileSystem.create(DistributedFileSystem.java:205)
	at org.apache.hadoop.fs.FilterFileSystem.create(FilterFileSystem.java:126)
	at org.apache.hadoop.hbase.util.FSUtils.create(FSUtils.java:341)
	at org.apache.hadoop.hbase.util.FSUtils.create(FSUtils.java:316)
	at org.apache.hadoop.hbase.io.hfile.AbstractHFileWriter.createOutputStream(AbstractHFileWriter.java:266)
	at org.apache.hadoop.hbase.io.hfile.HFile$WriterFactory.create(HFile.java:391)
	at org.apache.hadoop.hbase.regionserver.StoreFile$Writer.<init>(StoreFile.java:728)
	at org.apache.hadoop.hbase.regionserver.StoreFile$Writer.<init>(StoreFile.java:679)
	at org.apache.hadoop.hbase.regionserver.StoreFile$WriterBuilder.build(StoreFile.java:617)
	at org.apache.hadoop.hbase.regionserver.HStore.createWriterInTmp(HStore.java:889)
	at org.apache.hadoop.hbase.regionserver.DefaultStoreFlusher.flushSnapshot(DefaultStoreFlusher.java:69)
	at org.apache.hadoop.hbase.regionserver.HStore.flushCache(HStore.java:798)
	at org.apache.hadoop.hbase.regionserver.HStore$StoreFlusherImpl.flushCache(HStore.java:1949)
	at org.apache.hadoop.hbase.regionserver.HRegion.internalFlushcache(HRegion.java:1776)
	at org.apache.hadoop.hbase.regionserver.HRegion.internalFlushcache(HRegion.java:1658)
	at org.apache.hadoop.hbase.regionserver.HRegion.doClose(HRegion.java:1153)
	at org.apache.hadoop.hbase.regionserver.HRegion.close(HRegion.java:1085)
	at org.apache.hadoop.hbase.regionserver.handler.CloseRegionHandler.process(CloseRegionHandler.java:147)
	at org.apache.hadoop.hbase.executor.EventHandler.run(EventHandler.java:128)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1146)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:701)
2014-07-01 15:51:36,868 DEBUG [RS_CLOSE_REGION-slave1:60020-1] util.FSUtils: DFS Client does not support most favored nodes create; using default create
2014-07-01 15:51:36,870 WARN  [RS_CLOSE_REGION-slave1:60020-1] regionserver.HStore: Failed flushing store file, retrying num=0
org.apache.hadoop.ipc.RemoteException: org.apache.hadoop.hdfs.server.namenode.SafeModeException: Cannot create /hbase/data/default/usertable/bb79ae773f71289e0745068f68a0abd9/.tmp/e3cefc99d8164b25a99313e8ffe4fabb. Name node is in safe mode.
The reported blocks 486 has reached the threshold 0.9990 of total blocks 481. Safe mode will be turned off automatically in 27 seconds.
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.startFileInternal(FSNamesystem.java:1561)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.startFile(FSNamesystem.java:1527)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.create(NameNode.java:710)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.create(NameNode.java:689)
	at sun.reflect.GeneratedMethodAccessor3.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:622)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:587)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1432)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1428)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:416)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1190)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:1426)

	at org.apache.hadoop.ipc.Client.call(Client.java:1113)
	at org.apache.hadoop.ipc.RPC$Invoker.invoke(RPC.java:229)
	at com.sun.proxy.$Proxy8.create(Unknown Source)
	at sun.reflect.GeneratedMethodAccessor26.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:622)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:85)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:62)
	at com.sun.proxy.$Proxy8.create(Unknown Source)
	at sun.reflect.GeneratedMethodAccessor26.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:622)
	at org.apache.hadoop.hbase.fs.HFileSystem$1.invoke(HFileSystem.java:294)
	at com.sun.proxy.$Proxy9.create(Unknown Source)
	at org.apache.hadoop.hdfs.DFSClient$DFSOutputStream.<init>(DFSClient.java:3451)
	at org.apache.hadoop.hdfs.DFSClient.create(DFSClient.java:870)
	at org.apache.hadoop.hdfs.DistributedFileSystem.create(DistributedFileSystem.java:205)
	at org.apache.hadoop.fs.FilterFileSystem.create(FilterFileSystem.java:126)
	at org.apache.hadoop.hbase.util.FSUtils.create(FSUtils.java:341)
	at org.apache.hadoop.hbase.util.FSUtils.create(FSUtils.java:316)
	at org.apache.hadoop.hbase.io.hfile.AbstractHFileWriter.createOutputStream(AbstractHFileWriter.java:266)
	at org.apache.hadoop.hbase.io.hfile.HFile$WriterFactory.create(HFile.java:391)
	at org.apache.hadoop.hbase.regionserver.StoreFile$Writer.<init>(StoreFile.java:728)
	at org.apache.hadoop.hbase.regionserver.StoreFile$Writer.<init>(StoreFile.java:679)
	at org.apache.hadoop.hbase.regionserver.StoreFile$WriterBuilder.build(StoreFile.java:617)
	at org.apache.hadoop.hbase.regionserver.HStore.createWriterInTmp(HStore.java:889)
	at org.apache.hadoop.hbase.regionserver.DefaultStoreFlusher.flushSnapshot(DefaultStoreFlusher.java:69)
	at org.apache.hadoop.hbase.regionserver.HStore.flushCache(HStore.java:798)
	at org.apache.hadoop.hbase.regionserver.HStore$StoreFlusherImpl.flushCache(HStore.java:1949)
	at org.apache.hadoop.hbase.regionserver.HRegion.internalFlushcache(HRegion.java:1776)
	at org.apache.hadoop.hbase.regionserver.HRegion.internalFlushcache(HRegion.java:1658)
	at org.apache.hadoop.hbase.regionserver.HRegion.doClose(HRegion.java:1153)
	at org.apache.hadoop.hbase.regionserver.HRegion.close(HRegion.java:1085)
	at org.apache.hadoop.hbase.regionserver.handler.CloseRegionHandler.process(CloseRegionHandler.java:147)
	at org.apache.hadoop.hbase.executor.EventHandler.run(EventHandler.java:128)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1146)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:701)
2014-07-01 15:51:37,325 WARN  [org.apache.hadoop.hdfs.LeaseRenewer$1@4fae46d5] hdfs.LeaseRenewer: Failed to renew lease for [DFSClient_hb_rs_slave1,60020,1404253868846_-130361673_31] for 30 seconds.  Will retry shortly ...
org.apache.hadoop.ipc.RemoteException: org.apache.hadoop.hdfs.server.namenode.SafeModeException: Cannot renew lease for DFSClient_hb_rs_slave1,60020,1404253868846_-130361673_31. Name node is in safe mode.
The reported blocks 486 has reached the threshold 0.9990 of total blocks 481. Safe mode will be turned off automatically in 26 seconds.
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.renewLease(FSNamesystem.java:2771)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.renewLease(NameNode.java:919)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:622)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:587)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1432)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1428)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:416)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1190)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:1426)

	at org.apache.hadoop.ipc.Client.call(Client.java:1113)
	at org.apache.hadoop.ipc.RPC$Invoker.invoke(RPC.java:229)
	at com.sun.proxy.$Proxy8.renewLease(Unknown Source)
	at sun.reflect.GeneratedMethodAccessor29.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:622)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:85)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:62)
	at com.sun.proxy.$Proxy8.renewLease(Unknown Source)
	at sun.reflect.GeneratedMethodAccessor29.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:622)
	at org.apache.hadoop.hbase.fs.HFileSystem$1.invoke(HFileSystem.java:294)
	at com.sun.proxy.$Proxy9.renewLease(Unknown Source)
	at org.apache.hadoop.hdfs.DFSClient.renewLease(DFSClient.java:379)
	at org.apache.hadoop.hdfs.LeaseRenewer.renew(LeaseRenewer.java:378)
	at org.apache.hadoop.hdfs.LeaseRenewer.run(LeaseRenewer.java:400)
	at org.apache.hadoop.hdfs.LeaseRenewer.access$600(LeaseRenewer.java:69)
	at org.apache.hadoop.hdfs.LeaseRenewer$1.run(LeaseRenewer.java:273)
	at java.lang.Thread.run(Thread.java:701)
2014-07-01 15:51:37,548 DEBUG [RS_CLOSE_REGION-slave1:60020-0] util.FSUtils: DFS Client does not support most favored nodes create; using default create
2014-07-01 15:51:37,552 WARN  [RS_CLOSE_REGION-slave1:60020-0] regionserver.HStore: Failed flushing store file, retrying num=8
org.apache.hadoop.ipc.RemoteException: org.apache.hadoop.hdfs.server.namenode.SafeModeException: Cannot create /hbase/data/default/usertable/122e92eabfab38cb15a9965b21db25c5/.tmp/e1378331080245b3954519815436f354. Name node is in safe mode.
The reported blocks 486 has reached the threshold 0.9990 of total blocks 481. Safe mode will be turned off automatically in 26 seconds.
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.startFileInternal(FSNamesystem.java:1561)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.startFile(FSNamesystem.java:1527)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.create(NameNode.java:710)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.create(NameNode.java:689)
	at sun.reflect.GeneratedMethodAccessor3.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:622)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:587)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1432)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1428)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:416)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1190)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:1426)

	at org.apache.hadoop.ipc.Client.call(Client.java:1113)
	at org.apache.hadoop.ipc.RPC$Invoker.invoke(RPC.java:229)
	at com.sun.proxy.$Proxy8.create(Unknown Source)
	at sun.reflect.GeneratedMethodAccessor26.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:622)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:85)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:62)
	at com.sun.proxy.$Proxy8.create(Unknown Source)
	at sun.reflect.GeneratedMethodAccessor26.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:622)
	at org.apache.hadoop.hbase.fs.HFileSystem$1.invoke(HFileSystem.java:294)
	at com.sun.proxy.$Proxy9.create(Unknown Source)
	at org.apache.hadoop.hdfs.DFSClient$DFSOutputStream.<init>(DFSClient.java:3451)
	at org.apache.hadoop.hdfs.DFSClient.create(DFSClient.java:870)
	at org.apache.hadoop.hdfs.DistributedFileSystem.create(DistributedFileSystem.java:205)
	at org.apache.hadoop.fs.FilterFileSystem.create(FilterFileSystem.java:126)
	at org.apache.hadoop.hbase.util.FSUtils.create(FSUtils.java:341)
	at org.apache.hadoop.hbase.util.FSUtils.create(FSUtils.java:316)
	at org.apache.hadoop.hbase.io.hfile.AbstractHFileWriter.createOutputStream(AbstractHFileWriter.java:266)
	at org.apache.hadoop.hbase.io.hfile.HFile$WriterFactory.create(HFile.java:391)
	at org.apache.hadoop.hbase.regionserver.StoreFile$Writer.<init>(StoreFile.java:728)
	at org.apache.hadoop.hbase.regionserver.StoreFile$Writer.<init>(StoreFile.java:679)
	at org.apache.hadoop.hbase.regionserver.StoreFile$WriterBuilder.build(StoreFile.java:617)
	at org.apache.hadoop.hbase.regionserver.HStore.createWriterInTmp(HStore.java:889)
	at org.apache.hadoop.hbase.regionserver.DefaultStoreFlusher.flushSnapshot(DefaultStoreFlusher.java:69)
	at org.apache.hadoop.hbase.regionserver.HStore.flushCache(HStore.java:798)
	at org.apache.hadoop.hbase.regionserver.HStore$StoreFlusherImpl.flushCache(HStore.java:1949)
	at org.apache.hadoop.hbase.regionserver.HRegion.internalFlushcache(HRegion.java:1776)
	at org.apache.hadoop.hbase.regionserver.HRegion.internalFlushcache(HRegion.java:1658)
	at org.apache.hadoop.hbase.regionserver.HRegion.doClose(HRegion.java:1119)
	at org.apache.hadoop.hbase.regionserver.HRegion.close(HRegion.java:1085)
	at org.apache.hadoop.hbase.regionserver.handler.CloseRegionHandler.process(CloseRegionHandler.java:147)
	at org.apache.hadoop.hbase.executor.EventHandler.run(EventHandler.java:128)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1146)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:701)
2014-07-01 15:51:37,676 INFO  [regionserver60020] regionserver.HRegionServer: Received CLOSE for the region: 122e92eabfab38cb15a9965b21db25c5 ,which we are already trying to CLOSE, but not completed yet
2014-07-01 15:51:37,676 WARN  [regionserver60020] regionserver.HRegionServer: Failed to close usertable,user5,1404228265526.122e92eabfab38cb15a9965b21db25c5. - ignoring and continuing
org.apache.hadoop.hbase.regionserver.RegionAlreadyInTransitionException: The region 122e92eabfab38cb15a9965b21db25c5 was already closing. New CLOSE request is ignored.
	at org.apache.hadoop.hbase.regionserver.HRegionServer.closeRegion(HRegionServer.java:2586)
	at org.apache.hadoop.hbase.regionserver.HRegionServer.closeRegionIgnoreErrors(HRegionServer.java:2508)
	at org.apache.hadoop.hbase.regionserver.HRegionServer.closeUserRegions(HRegionServer.java:2098)
	at org.apache.hadoop.hbase.regionserver.HRegionServer.run(HRegionServer.java:894)
	at java.lang.Thread.run(Thread.java:701)
2014-07-01 15:51:37,677 INFO  [regionserver60020] regionserver.HRegionServer: Received CLOSE for the region: 93253e37c284f3174309fede3f32339f ,which we are already trying to CLOSE, but not completed yet
2014-07-01 15:51:37,677 WARN  [regionserver60020] regionserver.HRegionServer: Failed to close usertable,user3,1404228265526.93253e37c284f3174309fede3f32339f. - ignoring and continuing
org.apache.hadoop.hbase.regionserver.RegionAlreadyInTransitionException: The region 93253e37c284f3174309fede3f32339f was already closing. New CLOSE request is ignored.
	at org.apache.hadoop.hbase.regionserver.HRegionServer.closeRegion(HRegionServer.java:2586)
	at org.apache.hadoop.hbase.regionserver.HRegionServer.closeRegionIgnoreErrors(HRegionServer.java:2508)
	at org.apache.hadoop.hbase.regionserver.HRegionServer.closeUserRegions(HRegionServer.java:2098)
	at org.apache.hadoop.hbase.regionserver.HRegionServer.run(HRegionServer.java:894)
	at java.lang.Thread.run(Thread.java:701)
2014-07-01 15:51:37,677 INFO  [regionserver60020] regionserver.HRegionServer: Received CLOSE for the region: 667c1f2c34fe37d48ba25a789d8aba98 ,which we are already trying to CLOSE, but not completed yet
2014-07-01 15:51:37,677 WARN  [regionserver60020] regionserver.HRegionServer: Failed to close usertable,user7,1404228265526.667c1f2c34fe37d48ba25a789d8aba98. - ignoring and continuing
org.apache.hadoop.hbase.regionserver.RegionAlreadyInTransitionException: The region 667c1f2c34fe37d48ba25a789d8aba98 was already closing. New CLOSE request is ignored.
	at org.apache.hadoop.hbase.regionserver.HRegionServer.closeRegion(HRegionServer.java:2586)
	at org.apache.hadoop.hbase.regionserver.HRegionServer.closeRegionIgnoreErrors(HRegionServer.java:2508)
	at org.apache.hadoop.hbase.regionserver.HRegionServer.closeUserRegions(HRegionServer.java:2098)
	at org.apache.hadoop.hbase.regionserver.HRegionServer.run(HRegionServer.java:894)
	at java.lang.Thread.run(Thread.java:701)
2014-07-01 15:51:37,677 DEBUG [regionserver60020] regionserver.HRegionServer: Waiting on 122e92eabfab38cb15a9965b21db25c5, bb79ae773f71289e0745068f68a0abd9, 9cef9e7d34f96f1d9b111be50040a208, 93253e37c284f3174309fede3f32339f, 667c1f2c34fe37d48ba25a789d8aba98
2014-07-01 15:51:37,884 DEBUG [RS_CLOSE_REGION-slave1:60020-2] util.FSUtils: DFS Client does not support most favored nodes create; using default create
2014-07-01 15:51:37,887 WARN  [RS_CLOSE_REGION-slave1:60020-2] regionserver.HStore: Failed flushing store file, retrying num=2
org.apache.hadoop.ipc.RemoteException: org.apache.hadoop.hdfs.server.namenode.SafeModeException: Cannot create /hbase/data/default/usertable/9cef9e7d34f96f1d9b111be50040a208/.tmp/9012dba0d9ff4770b70a1daec3a70e79. Name node is in safe mode.
The reported blocks 486 has reached the threshold 0.9990 of total blocks 481. Safe mode will be turned off automatically in 26 seconds.
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.startFileInternal(FSNamesystem.java:1561)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.startFile(FSNamesystem.java:1527)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.create(NameNode.java:710)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.create(NameNode.java:689)
	at sun.reflect.GeneratedMethodAccessor3.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:622)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:587)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1432)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1428)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:416)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1190)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:1426)

	at org.apache.hadoop.ipc.Client.call(Client.java:1113)
	at org.apache.hadoop.ipc.RPC$Invoker.invoke(RPC.java:229)
	at com.sun.proxy.$Proxy8.create(Unknown Source)
	at sun.reflect.GeneratedMethodAccessor26.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:622)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:85)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:62)
	at com.sun.proxy.$Proxy8.create(Unknown Source)
	at sun.reflect.GeneratedMethodAccessor26.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:622)
	at org.apache.hadoop.hbase.fs.HFileSystem$1.invoke(HFileSystem.java:294)
	at com.sun.proxy.$Proxy9.create(Unknown Source)
	at org.apache.hadoop.hdfs.DFSClient$DFSOutputStream.<init>(DFSClient.java:3451)
	at org.apache.hadoop.hdfs.DFSClient.create(DFSClient.java:870)
	at org.apache.hadoop.hdfs.DistributedFileSystem.create(DistributedFileSystem.java:205)
	at org.apache.hadoop.fs.FilterFileSystem.create(FilterFileSystem.java:126)
	at org.apache.hadoop.hbase.util.FSUtils.create(FSUtils.java:341)
	at org.apache.hadoop.hbase.util.FSUtils.create(FSUtils.java:316)
	at org.apache.hadoop.hbase.io.hfile.AbstractHFileWriter.createOutputStream(AbstractHFileWriter.java:266)
	at org.apache.hadoop.hbase.io.hfile.HFile$WriterFactory.create(HFile.java:391)
	at org.apache.hadoop.hbase.regionserver.StoreFile$Writer.<init>(StoreFile.java:728)
	at org.apache.hadoop.hbase.regionserver.StoreFile$Writer.<init>(StoreFile.java:679)
	at org.apache.hadoop.hbase.regionserver.StoreFile$WriterBuilder.build(StoreFile.java:617)
	at org.apache.hadoop.hbase.regionserver.HStore.createWriterInTmp(HStore.java:889)
	at org.apache.hadoop.hbase.regionserver.DefaultStoreFlusher.flushSnapshot(DefaultStoreFlusher.java:69)
	at org.apache.hadoop.hbase.regionserver.HStore.flushCache(HStore.java:798)
	at org.apache.hadoop.hbase.regionserver.HStore$StoreFlusherImpl.flushCache(HStore.java:1949)
	at org.apache.hadoop.hbase.regionserver.HRegion.internalFlushcache(HRegion.java:1776)
	at org.apache.hadoop.hbase.regionserver.HRegion.internalFlushcache(HRegion.java:1658)
	at org.apache.hadoop.hbase.regionserver.HRegion.doClose(HRegion.java:1153)
	at org.apache.hadoop.hbase.regionserver.HRegion.close(HRegion.java:1085)
	at org.apache.hadoop.hbase.regionserver.handler.CloseRegionHandler.process(CloseRegionHandler.java:147)
	at org.apache.hadoop.hbase.executor.EventHandler.run(EventHandler.java:128)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1146)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:701)
2014-07-01 15:51:37,928 DEBUG [RS_CLOSE_REGION-slave1:60020-1] util.FSUtils: DFS Client does not support most favored nodes create; using default create
2014-07-01 15:51:37,931 WARN  [RS_CLOSE_REGION-slave1:60020-1] regionserver.HStore: Failed flushing store file, retrying num=1
org.apache.hadoop.ipc.RemoteException: org.apache.hadoop.hdfs.server.namenode.SafeModeException: Cannot create /hbase/data/default/usertable/bb79ae773f71289e0745068f68a0abd9/.tmp/0eca8d1ad4f445a68ec77c17615be1bf. Name node is in safe mode.
The reported blocks 486 has reached the threshold 0.9990 of total blocks 481. Safe mode will be turned off automatically in 26 seconds.
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.startFileInternal(FSNamesystem.java:1561)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.startFile(FSNamesystem.java:1527)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.create(NameNode.java:710)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.create(NameNode.java:689)
	at sun.reflect.GeneratedMethodAccessor3.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:622)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:587)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1432)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1428)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:416)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1190)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:1426)

	at org.apache.hadoop.ipc.Client.call(Client.java:1113)
	at org.apache.hadoop.ipc.RPC$Invoker.invoke(RPC.java:229)
	at com.sun.proxy.$Proxy8.create(Unknown Source)
	at sun.reflect.GeneratedMethodAccessor26.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:622)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:85)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:62)
	at com.sun.proxy.$Proxy8.create(Unknown Source)
	at sun.reflect.GeneratedMethodAccessor26.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:622)
	at org.apache.hadoop.hbase.fs.HFileSystem$1.invoke(HFileSystem.java:294)
	at com.sun.proxy.$Proxy9.create(Unknown Source)
	at org.apache.hadoop.hdfs.DFSClient$DFSOutputStream.<init>(DFSClient.java:3451)
	at org.apache.hadoop.hdfs.DFSClient.create(DFSClient.java:870)
	at org.apache.hadoop.hdfs.DistributedFileSystem.create(DistributedFileSystem.java:205)
	at org.apache.hadoop.fs.FilterFileSystem.create(FilterFileSystem.java:126)
	at org.apache.hadoop.hbase.util.FSUtils.create(FSUtils.java:341)
	at org.apache.hadoop.hbase.util.FSUtils.create(FSUtils.java:316)
	at org.apache.hadoop.hbase.io.hfile.AbstractHFileWriter.createOutputStream(AbstractHFileWriter.java:266)
	at org.apache.hadoop.hbase.io.hfile.HFile$WriterFactory.create(HFile.java:391)
	at org.apache.hadoop.hbase.regionserver.StoreFile$Writer.<init>(StoreFile.java:728)
	at org.apache.hadoop.hbase.regionserver.StoreFile$Writer.<init>(StoreFile.java:679)
	at org.apache.hadoop.hbase.regionserver.StoreFile$WriterBuilder.build(StoreFile.java:617)
	at org.apache.hadoop.hbase.regionserver.HStore.createWriterInTmp(HStore.java:889)
	at org.apache.hadoop.hbase.regionserver.DefaultStoreFlusher.flushSnapshot(DefaultStoreFlusher.java:69)
	at org.apache.hadoop.hbase.regionserver.HStore.flushCache(HStore.java:798)
	at org.apache.hadoop.hbase.regionserver.HStore$StoreFlusherImpl.flushCache(HStore.java:1949)
	at org.apache.hadoop.hbase.regionserver.HRegion.internalFlushcache(HRegion.java:1776)
	at org.apache.hadoop.hbase.regionserver.HRegion.internalFlushcache(HRegion.java:1658)
	at org.apache.hadoop.hbase.regionserver.HRegion.doClose(HRegion.java:1153)
	at org.apache.hadoop.hbase.regionserver.HRegion.close(HRegion.java:1085)
	at org.apache.hadoop.hbase.regionserver.handler.CloseRegionHandler.process(CloseRegionHandler.java:147)
	at org.apache.hadoop.hbase.executor.EventHandler.run(EventHandler.java:128)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1146)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:701)
2014-07-01 15:51:38,329 WARN  [org.apache.hadoop.hdfs.LeaseRenewer$1@4fae46d5] hdfs.LeaseRenewer: Failed to renew lease for [DFSClient_hb_rs_slave1,60020,1404253868846_-130361673_31] for 30 seconds.  Will retry shortly ...
org.apache.hadoop.ipc.RemoteException: org.apache.hadoop.hdfs.server.namenode.SafeModeException: Cannot renew lease for DFSClient_hb_rs_slave1,60020,1404253868846_-130361673_31. Name node is in safe mode.
The reported blocks 486 has reached the threshold 0.9990 of total blocks 481. Safe mode will be turned off automatically in 25 seconds.
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.renewLease(FSNamesystem.java:2771)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.renewLease(NameNode.java:919)
	at sun.reflect.GeneratedMethodAccessor4.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:622)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:587)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1432)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1428)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:416)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1190)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:1426)

	at org.apache.hadoop.ipc.Client.call(Client.java:1113)
	at org.apache.hadoop.ipc.RPC$Invoker.invoke(RPC.java:229)
	at com.sun.proxy.$Proxy8.renewLease(Unknown Source)
	at sun.reflect.GeneratedMethodAccessor29.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:622)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:85)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:62)
	at com.sun.proxy.$Proxy8.renewLease(Unknown Source)
	at sun.reflect.GeneratedMethodAccessor29.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:622)
	at org.apache.hadoop.hbase.fs.HFileSystem$1.invoke(HFileSystem.java:294)
	at com.sun.proxy.$Proxy9.renewLease(Unknown Source)
	at org.apache.hadoop.hdfs.DFSClient.renewLease(DFSClient.java:379)
	at org.apache.hadoop.hdfs.LeaseRenewer.renew(LeaseRenewer.java:378)
	at org.apache.hadoop.hdfs.LeaseRenewer.run(LeaseRenewer.java:400)
	at org.apache.hadoop.hdfs.LeaseRenewer.access$600(LeaseRenewer.java:69)
	at org.apache.hadoop.hdfs.LeaseRenewer$1.run(LeaseRenewer.java:273)
	at java.lang.Thread.run(Thread.java:701)
2014-07-01 15:51:38,575 DEBUG [RS_CLOSE_REGION-slave1:60020-0] util.FSUtils: DFS Client does not support most favored nodes create; using default create
2014-07-01 15:51:38,581 WARN  [RS_CLOSE_REGION-slave1:60020-0] regionserver.HStore: Failed flushing store file, retrying num=9
org.apache.hadoop.ipc.RemoteException: org.apache.hadoop.hdfs.server.namenode.SafeModeException: Cannot create /hbase/data/default/usertable/122e92eabfab38cb15a9965b21db25c5/.tmp/712ccb121b624c5cb1bab10d321302f9. Name node is in safe mode.
The reported blocks 486 has reached the threshold 0.9990 of total blocks 481. Safe mode will be turned off automatically in 25 seconds.
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.startFileInternal(FSNamesystem.java:1561)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.startFile(FSNamesystem.java:1527)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.create(NameNode.java:710)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.create(NameNode.java:689)
	at sun.reflect.GeneratedMethodAccessor3.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:622)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:587)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1432)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1428)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:416)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1190)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:1426)

	at org.apache.hadoop.ipc.Client.call(Client.java:1113)
	at org.apache.hadoop.ipc.RPC$Invoker.invoke(RPC.java:229)
	at com.sun.proxy.$Proxy8.create(Unknown Source)
	at sun.reflect.GeneratedMethodAccessor26.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:622)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:85)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:62)
	at com.sun.proxy.$Proxy8.create(Unknown Source)
	at sun.reflect.GeneratedMethodAccessor26.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:622)
	at org.apache.hadoop.hbase.fs.HFileSystem$1.invoke(HFileSystem.java:294)
	at com.sun.proxy.$Proxy9.create(Unknown Source)
	at org.apache.hadoop.hdfs.DFSClient$DFSOutputStream.<init>(DFSClient.java:3451)
	at org.apache.hadoop.hdfs.DFSClient.create(DFSClient.java:870)
	at org.apache.hadoop.hdfs.DistributedFileSystem.create(DistributedFileSystem.java:205)
	at org.apache.hadoop.fs.FilterFileSystem.create(FilterFileSystem.java:126)
	at org.apache.hadoop.hbase.util.FSUtils.create(FSUtils.java:341)
	at org.apache.hadoop.hbase.util.FSUtils.create(FSUtils.java:316)
	at org.apache.hadoop.hbase.io.hfile.AbstractHFileWriter.createOutputStream(AbstractHFileWriter.java:266)
	at org.apache.hadoop.hbase.io.hfile.HFile$WriterFactory.create(HFile.java:391)
	at org.apache.hadoop.hbase.regionserver.StoreFile$Writer.<init>(StoreFile.java:728)
	at org.apache.hadoop.hbase.regionserver.StoreFile$Writer.<init>(StoreFile.java:679)
	at org.apache.hadoop.hbase.regionserver.StoreFile$WriterBuilder.build(StoreFile.java:617)
	at org.apache.hadoop.hbase.regionserver.HStore.createWriterInTmp(HStore.java:889)
	at org.apache.hadoop.hbase.regionserver.DefaultStoreFlusher.flushSnapshot(DefaultStoreFlusher.java:69)
	at org.apache.hadoop.hbase.regionserver.HStore.flushCache(HStore.java:798)
	at org.apache.hadoop.hbase.regionserver.HStore$StoreFlusherImpl.flushCache(HStore.java:1949)
	at org.apache.hadoop.hbase.regionserver.HRegion.internalFlushcache(HRegion.java:1776)
	at org.apache.hadoop.hbase.regionserver.HRegion.internalFlushcache(HRegion.java:1658)
	at org.apache.hadoop.hbase.regionserver.HRegion.doClose(HRegion.java:1119)
	at org.apache.hadoop.hbase.regionserver.HRegion.close(HRegion.java:1085)
	at org.apache.hadoop.hbase.regionserver.handler.CloseRegionHandler.process(CloseRegionHandler.java:147)
	at org.apache.hadoop.hbase.executor.EventHandler.run(EventHandler.java:128)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1146)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:701)
2014-07-01 15:51:38,582 DEBUG [RS_CLOSE_REGION-slave1:60020-0] regionserver.HRegion: Updates disabled for region usertable,user5,1404228265526.122e92eabfab38cb15a9965b21db25c5.
2014-07-01 15:51:38,582 DEBUG [RS_CLOSE_REGION-slave1:60020-0] regionserver.HRegion: Started memstore flush for usertable,user5,1404228265526.122e92eabfab38cb15a9965b21db25c5., current region memstore size 84.0m
2014-07-01 15:51:38,582 WARN  [RS_CLOSE_REGION-slave1:60020-0] regionserver.MemStore: Snapshot called again without clearing previous. Doing nothing. Another ongoing flush or did we fail last attempt?
2014-07-01 15:51:38,593 DEBUG [RS_CLOSE_REGION-slave1:60020-0] util.FSUtils: DFS Client does not support most favored nodes create; using default create
2014-07-01 15:51:38,595 WARN  [RS_CLOSE_REGION-slave1:60020-0] regionserver.HStore: Failed flushing store file, retrying num=0
org.apache.hadoop.ipc.RemoteException: org.apache.hadoop.hdfs.server.namenode.SafeModeException: Cannot create /hbase/data/default/usertable/122e92eabfab38cb15a9965b21db25c5/.tmp/3acbd8c475dd486db00cb95bf88ccda4. Name node is in safe mode.
The reported blocks 486 has reached the threshold 0.9990 of total blocks 481. Safe mode will be turned off automatically in 25 seconds.
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.startFileInternal(FSNamesystem.java:1561)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.startFile(FSNamesystem.java:1527)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.create(NameNode.java:710)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.create(NameNode.java:689)
	at sun.reflect.GeneratedMethodAccessor3.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:622)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:587)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1432)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1428)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:416)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1190)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:1426)

	at org.apache.hadoop.ipc.Client.call(Client.java:1113)
	at org.apache.hadoop.ipc.RPC$Invoker.invoke(RPC.java:229)
	at com.sun.proxy.$Proxy8.create(Unknown Source)
	at sun.reflect.GeneratedMethodAccessor26.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:622)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:85)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:62)
	at com.sun.proxy.$Proxy8.create(Unknown Source)
	at sun.reflect.GeneratedMethodAccessor26.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:622)
	at org.apache.hadoop.hbase.fs.HFileSystem$1.invoke(HFileSystem.java:294)
	at com.sun.proxy.$Proxy9.create(Unknown Source)
	at org.apache.hadoop.hdfs.DFSClient$DFSOutputStream.<init>(DFSClient.java:3451)
	at org.apache.hadoop.hdfs.DFSClient.create(DFSClient.java:870)
	at org.apache.hadoop.hdfs.DistributedFileSystem.create(DistributedFileSystem.java:205)
	at org.apache.hadoop.fs.FilterFileSystem.create(FilterFileSystem.java:126)
	at org.apache.hadoop.hbase.util.FSUtils.create(FSUtils.java:341)
	at org.apache.hadoop.hbase.util.FSUtils.create(FSUtils.java:316)
	at org.apache.hadoop.hbase.io.hfile.AbstractHFileWriter.createOutputStream(AbstractHFileWriter.java:266)
	at org.apache.hadoop.hbase.io.hfile.HFile$WriterFactory.create(HFile.java:391)
	at org.apache.hadoop.hbase.regionserver.StoreFile$Writer.<init>(StoreFile.java:728)
	at org.apache.hadoop.hbase.regionserver.StoreFile$Writer.<init>(StoreFile.java:679)
	at org.apache.hadoop.hbase.regionserver.StoreFile$WriterBuilder.build(StoreFile.java:617)
	at org.apache.hadoop.hbase.regionserver.HStore.createWriterInTmp(HStore.java:889)
	at org.apache.hadoop.hbase.regionserver.DefaultStoreFlusher.flushSnapshot(DefaultStoreFlusher.java:69)
	at org.apache.hadoop.hbase.regionserver.HStore.flushCache(HStore.java:798)
	at org.apache.hadoop.hbase.regionserver.HStore$StoreFlusherImpl.flushCache(HStore.java:1949)
	at org.apache.hadoop.hbase.regionserver.HRegion.internalFlushcache(HRegion.java:1776)
	at org.apache.hadoop.hbase.regionserver.HRegion.internalFlushcache(HRegion.java:1658)
	at org.apache.hadoop.hbase.regionserver.HRegion.doClose(HRegion.java:1153)
	at org.apache.hadoop.hbase.regionserver.HRegion.close(HRegion.java:1085)
	at org.apache.hadoop.hbase.regionserver.handler.CloseRegionHandler.process(CloseRegionHandler.java:147)
	at org.apache.hadoop.hbase.executor.EventHandler.run(EventHandler.java:128)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1146)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:701)
2014-07-01 15:51:38,935 DEBUG [RS_CLOSE_REGION-slave1:60020-2] util.FSUtils: DFS Client does not support most favored nodes create; using default create
2014-07-01 15:51:38,937 WARN  [RS_CLOSE_REGION-slave1:60020-2] regionserver.HStore: Failed flushing store file, retrying num=3
org.apache.hadoop.ipc.RemoteException: org.apache.hadoop.hdfs.server.namenode.SafeModeException: Cannot create /hbase/data/default/usertable/9cef9e7d34f96f1d9b111be50040a208/.tmp/d315e3a18bf34a8ea8569bace8c372ef. Name node is in safe mode.
The reported blocks 486 has reached the threshold 0.9990 of total blocks 481. Safe mode will be turned off automatically in 25 seconds.
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.startFileInternal(FSNamesystem.java:1561)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.startFile(FSNamesystem.java:1527)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.create(NameNode.java:710)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.create(NameNode.java:689)
	at sun.reflect.GeneratedMethodAccessor3.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:622)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:587)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1432)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1428)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:416)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1190)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:1426)

	at org.apache.hadoop.ipc.Client.call(Client.java:1113)
	at org.apache.hadoop.ipc.RPC$Invoker.invoke(RPC.java:229)
	at com.sun.proxy.$Proxy8.create(Unknown Source)
	at sun.reflect.GeneratedMethodAccessor26.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:622)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:85)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:62)
	at com.sun.proxy.$Proxy8.create(Unknown Source)
	at sun.reflect.GeneratedMethodAccessor26.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:622)
	at org.apache.hadoop.hbase.fs.HFileSystem$1.invoke(HFileSystem.java:294)
	at com.sun.proxy.$Proxy9.create(Unknown Source)
	at org.apache.hadoop.hdfs.DFSClient$DFSOutputStream.<init>(DFSClient.java:3451)
	at org.apache.hadoop.hdfs.DFSClient.create(DFSClient.java:870)
	at org.apache.hadoop.hdfs.DistributedFileSystem.create(DistributedFileSystem.java:205)
	at org.apache.hadoop.fs.FilterFileSystem.create(FilterFileSystem.java:126)
	at org.apache.hadoop.hbase.util.FSUtils.create(FSUtils.java:341)
	at org.apache.hadoop.hbase.util.FSUtils.create(FSUtils.java:316)
	at org.apache.hadoop.hbase.io.hfile.AbstractHFileWriter.createOutputStream(AbstractHFileWriter.java:266)
	at org.apache.hadoop.hbase.io.hfile.HFile$WriterFactory.create(HFile.java:391)
	at org.apache.hadoop.hbase.regionserver.StoreFile$Writer.<init>(StoreFile.java:728)
	at org.apache.hadoop.hbase.regionserver.StoreFile$Writer.<init>(StoreFile.java:679)
	at org.apache.hadoop.hbase.regionserver.StoreFile$WriterBuilder.build(StoreFile.java:617)
	at org.apache.hadoop.hbase.regionserver.HStore.createWriterInTmp(HStore.java:889)
	at org.apache.hadoop.hbase.regionserver.DefaultStoreFlusher.flushSnapshot(DefaultStoreFlusher.java:69)
	at org.apache.hadoop.hbase.regionserver.HStore.flushCache(HStore.java:798)
	at org.apache.hadoop.hbase.regionserver.HStore$StoreFlusherImpl.flushCache(HStore.java:1949)
	at org.apache.hadoop.hbase.regionserver.HRegion.internalFlushcache(HRegion.java:1776)
	at org.apache.hadoop.hbase.regionserver.HRegion.internalFlushcache(HRegion.java:1658)
	at org.apache.hadoop.hbase.regionserver.HRegion.doClose(HRegion.java:1153)
	at org.apache.hadoop.hbase.regionserver.HRegion.close(HRegion.java:1085)
	at org.apache.hadoop.hbase.regionserver.handler.CloseRegionHandler.process(CloseRegionHandler.java:147)
	at org.apache.hadoop.hbase.executor.EventHandler.run(EventHandler.java:128)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1146)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:701)
2014-07-01 15:51:38,984 DEBUG [RS_CLOSE_REGION-slave1:60020-1] util.FSUtils: DFS Client does not support most favored nodes create; using default create
2014-07-01 15:51:38,986 WARN  [RS_CLOSE_REGION-slave1:60020-1] regionserver.HStore: Failed flushing store file, retrying num=2
org.apache.hadoop.ipc.RemoteException: org.apache.hadoop.hdfs.server.namenode.SafeModeException: Cannot create /hbase/data/default/usertable/bb79ae773f71289e0745068f68a0abd9/.tmp/e592926d99fd4fdeb63a36ae17bcb7df. Name node is in safe mode.
The reported blocks 486 has reached the threshold 0.9990 of total blocks 481. Safe mode will be turned off automatically in 25 seconds.
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.startFileInternal(FSNamesystem.java:1561)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.startFile(FSNamesystem.java:1527)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.create(NameNode.java:710)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.create(NameNode.java:689)
	at sun.reflect.GeneratedMethodAccessor3.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:622)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:587)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1432)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1428)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:416)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1190)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:1426)

	at org.apache.hadoop.ipc.Client.call(Client.java:1113)
	at org.apache.hadoop.ipc.RPC$Invoker.invoke(RPC.java:229)
	at com.sun.proxy.$Proxy8.create(Unknown Source)
	at sun.reflect.GeneratedMethodAccessor26.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:622)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:85)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:62)
	at com.sun.proxy.$Proxy8.create(Unknown Source)
	at sun.reflect.GeneratedMethodAccessor26.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:622)
	at org.apache.hadoop.hbase.fs.HFileSystem$1.invoke(HFileSystem.java:294)
	at com.sun.proxy.$Proxy9.create(Unknown Source)
	at org.apache.hadoop.hdfs.DFSClient$DFSOutputStream.<init>(DFSClient.java:3451)
	at org.apache.hadoop.hdfs.DFSClient.create(DFSClient.java:870)
	at org.apache.hadoop.hdfs.DistributedFileSystem.create(DistributedFileSystem.java:205)
	at org.apache.hadoop.fs.FilterFileSystem.create(FilterFileSystem.java:126)
	at org.apache.hadoop.hbase.util.FSUtils.create(FSUtils.java:341)
	at org.apache.hadoop.hbase.util.FSUtils.create(FSUtils.java:316)
	at org.apache.hadoop.hbase.io.hfile.AbstractHFileWriter.createOutputStream(AbstractHFileWriter.java:266)
	at org.apache.hadoop.hbase.io.hfile.HFile$WriterFactory.create(HFile.java:391)
	at org.apache.hadoop.hbase.regionserver.StoreFile$Writer.<init>(StoreFile.java:728)
	at org.apache.hadoop.hbase.regionserver.StoreFile$Writer.<init>(StoreFile.java:679)
	at org.apache.hadoop.hbase.regionserver.StoreFile$WriterBuilder.build(StoreFile.java:617)
	at org.apache.hadoop.hbase.regionserver.HStore.createWriterInTmp(HStore.java:889)
	at org.apache.hadoop.hbase.regionserver.DefaultStoreFlusher.flushSnapshot(DefaultStoreFlusher.java:69)
	at org.apache.hadoop.hbase.regionserver.HStore.flushCache(HStore.java:798)
	at org.apache.hadoop.hbase.regionserver.HStore$StoreFlusherImpl.flushCache(HStore.java:1949)
	at org.apache.hadoop.hbase.regionserver.HRegion.internalFlushcache(HRegion.java:1776)
	at org.apache.hadoop.hbase.regionserver.HRegion.internalFlushcache(HRegion.java:1658)
	at org.apache.hadoop.hbase.regionserver.HRegion.doClose(HRegion.java:1153)
	at org.apache.hadoop.hbase.regionserver.HRegion.close(HRegion.java:1085)
	at org.apache.hadoop.hbase.regionserver.handler.CloseRegionHandler.process(CloseRegionHandler.java:147)
	at org.apache.hadoop.hbase.executor.EventHandler.run(EventHandler.java:128)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1146)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:701)
2014-07-01 15:51:39,332 WARN  [org.apache.hadoop.hdfs.LeaseRenewer$1@4fae46d5] hdfs.LeaseRenewer: Failed to renew lease for [DFSClient_hb_rs_slave1,60020,1404253868846_-130361673_31] for 30 seconds.  Will retry shortly ...
org.apache.hadoop.ipc.RemoteException: org.apache.hadoop.hdfs.server.namenode.SafeModeException: Cannot renew lease for DFSClient_hb_rs_slave1,60020,1404253868846_-130361673_31. Name node is in safe mode.
The reported blocks 486 has reached the threshold 0.9990 of total blocks 481. Safe mode will be turned off automatically in 24 seconds.
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.renewLease(FSNamesystem.java:2771)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.renewLease(NameNode.java:919)
	at sun.reflect.GeneratedMethodAccessor4.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:622)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:587)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1432)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1428)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:416)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1190)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:1426)

	at org.apache.hadoop.ipc.Client.call(Client.java:1113)
	at org.apache.hadoop.ipc.RPC$Invoker.invoke(RPC.java:229)
	at com.sun.proxy.$Proxy8.renewLease(Unknown Source)
	at sun.reflect.GeneratedMethodAccessor29.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:622)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:85)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:62)
	at com.sun.proxy.$Proxy8.renewLease(Unknown Source)
	at sun.reflect.GeneratedMethodAccessor29.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:622)
	at org.apache.hadoop.hbase.fs.HFileSystem$1.invoke(HFileSystem.java:294)
	at com.sun.proxy.$Proxy9.renewLease(Unknown Source)
	at org.apache.hadoop.hdfs.DFSClient.renewLease(DFSClient.java:379)
	at org.apache.hadoop.hdfs.LeaseRenewer.renew(LeaseRenewer.java:378)
	at org.apache.hadoop.hdfs.LeaseRenewer.run(LeaseRenewer.java:400)
	at org.apache.hadoop.hdfs.LeaseRenewer.access$600(LeaseRenewer.java:69)
	at org.apache.hadoop.hdfs.LeaseRenewer$1.run(LeaseRenewer.java:273)
	at java.lang.Thread.run(Thread.java:701)
2014-07-01 15:51:39,621 DEBUG [RS_CLOSE_REGION-slave1:60020-0] util.FSUtils: DFS Client does not support most favored nodes create; using default create
2014-07-01 15:51:39,624 WARN  [RS_CLOSE_REGION-slave1:60020-0] regionserver.HStore: Failed flushing store file, retrying num=1
org.apache.hadoop.ipc.RemoteException: org.apache.hadoop.hdfs.server.namenode.SafeModeException: Cannot create /hbase/data/default/usertable/122e92eabfab38cb15a9965b21db25c5/.tmp/673e1010c7184b2a9a760d93be33293b. Name node is in safe mode.
The reported blocks 486 has reached the threshold 0.9990 of total blocks 481. Safe mode will be turned off automatically in 24 seconds.
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.startFileInternal(FSNamesystem.java:1561)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.startFile(FSNamesystem.java:1527)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.create(NameNode.java:710)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.create(NameNode.java:689)
	at sun.reflect.GeneratedMethodAccessor3.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:622)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:587)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1432)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1428)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:416)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1190)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:1426)

	at org.apache.hadoop.ipc.Client.call(Client.java:1113)
	at org.apache.hadoop.ipc.RPC$Invoker.invoke(RPC.java:229)
	at com.sun.proxy.$Proxy8.create(Unknown Source)
	at sun.reflect.GeneratedMethodAccessor26.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:622)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:85)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:62)
	at com.sun.proxy.$Proxy8.create(Unknown Source)
	at sun.reflect.GeneratedMethodAccessor26.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:622)
	at org.apache.hadoop.hbase.fs.HFileSystem$1.invoke(HFileSystem.java:294)
	at com.sun.proxy.$Proxy9.create(Unknown Source)
	at org.apache.hadoop.hdfs.DFSClient$DFSOutputStream.<init>(DFSClient.java:3451)
	at org.apache.hadoop.hdfs.DFSClient.create(DFSClient.java:870)
	at org.apache.hadoop.hdfs.DistributedFileSystem.create(DistributedFileSystem.java:205)
	at org.apache.hadoop.fs.FilterFileSystem.create(FilterFileSystem.java:126)
	at org.apache.hadoop.hbase.util.FSUtils.create(FSUtils.java:341)
	at org.apache.hadoop.hbase.util.FSUtils.create(FSUtils.java:316)
	at org.apache.hadoop.hbase.io.hfile.AbstractHFileWriter.createOutputStream(AbstractHFileWriter.java:266)
	at org.apache.hadoop.hbase.io.hfile.HFile$WriterFactory.create(HFile.java:391)
	at org.apache.hadoop.hbase.regionserver.StoreFile$Writer.<init>(StoreFile.java:728)
	at org.apache.hadoop.hbase.regionserver.StoreFile$Writer.<init>(StoreFile.java:679)
	at org.apache.hadoop.hbase.regionserver.StoreFile$WriterBuilder.build(StoreFile.java:617)
	at org.apache.hadoop.hbase.regionserver.HStore.createWriterInTmp(HStore.java:889)
	at org.apache.hadoop.hbase.regionserver.DefaultStoreFlusher.flushSnapshot(DefaultStoreFlusher.java:69)
	at org.apache.hadoop.hbase.regionserver.HStore.flushCache(HStore.java:798)
	at org.apache.hadoop.hbase.regionserver.HStore$StoreFlusherImpl.flushCache(HStore.java:1949)
	at org.apache.hadoop.hbase.regionserver.HRegion.internalFlushcache(HRegion.java:1776)
	at org.apache.hadoop.hbase.regionserver.HRegion.internalFlushcache(HRegion.java:1658)
	at org.apache.hadoop.hbase.regionserver.HRegion.doClose(HRegion.java:1153)
	at org.apache.hadoop.hbase.regionserver.HRegion.close(HRegion.java:1085)
	at org.apache.hadoop.hbase.regionserver.handler.CloseRegionHandler.process(CloseRegionHandler.java:147)
	at org.apache.hadoop.hbase.executor.EventHandler.run(EventHandler.java:128)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1146)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:701)
2014-07-01 15:51:39,982 DEBUG [RS_CLOSE_REGION-slave1:60020-2] util.FSUtils: DFS Client does not support most favored nodes create; using default create
2014-07-01 15:51:39,985 WARN  [RS_CLOSE_REGION-slave1:60020-2] regionserver.HStore: Failed flushing store file, retrying num=4
org.apache.hadoop.ipc.RemoteException: org.apache.hadoop.hdfs.server.namenode.SafeModeException: Cannot create /hbase/data/default/usertable/9cef9e7d34f96f1d9b111be50040a208/.tmp/4491e9823e0547bc82f66b15843f46e1. Name node is in safe mode.
The reported blocks 486 has reached the threshold 0.9990 of total blocks 481. Safe mode will be turned off automatically in 24 seconds.
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.startFileInternal(FSNamesystem.java:1561)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.startFile(FSNamesystem.java:1527)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.create(NameNode.java:710)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.create(NameNode.java:689)
	at sun.reflect.GeneratedMethodAccessor3.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:622)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:587)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1432)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1428)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:416)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1190)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:1426)

	at org.apache.hadoop.ipc.Client.call(Client.java:1113)
	at org.apache.hadoop.ipc.RPC$Invoker.invoke(RPC.java:229)
	at com.sun.proxy.$Proxy8.create(Unknown Source)
	at sun.reflect.GeneratedMethodAccessor26.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:622)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:85)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:62)
	at com.sun.proxy.$Proxy8.create(Unknown Source)
	at sun.reflect.GeneratedMethodAccessor26.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:622)
	at org.apache.hadoop.hbase.fs.HFileSystem$1.invoke(HFileSystem.java:294)
	at com.sun.proxy.$Proxy9.create(Unknown Source)
	at org.apache.hadoop.hdfs.DFSClient$DFSOutputStream.<init>(DFSClient.java:3451)
	at org.apache.hadoop.hdfs.DFSClient.create(DFSClient.java:870)
	at org.apache.hadoop.hdfs.DistributedFileSystem.create(DistributedFileSystem.java:205)
	at org.apache.hadoop.fs.FilterFileSystem.create(FilterFileSystem.java:126)
	at org.apache.hadoop.hbase.util.FSUtils.create(FSUtils.java:341)
	at org.apache.hadoop.hbase.util.FSUtils.create(FSUtils.java:316)
	at org.apache.hadoop.hbase.io.hfile.AbstractHFileWriter.createOutputStream(AbstractHFileWriter.java:266)
	at org.apache.hadoop.hbase.io.hfile.HFile$WriterFactory.create(HFile.java:391)
	at org.apache.hadoop.hbase.regionserver.StoreFile$Writer.<init>(StoreFile.java:728)
	at org.apache.hadoop.hbase.regionserver.StoreFile$Writer.<init>(StoreFile.java:679)
	at org.apache.hadoop.hbase.regionserver.StoreFile$WriterBuilder.build(StoreFile.java:617)
	at org.apache.hadoop.hbase.regionserver.HStore.createWriterInTmp(HStore.java:889)
	at org.apache.hadoop.hbase.regionserver.DefaultStoreFlusher.flushSnapshot(DefaultStoreFlusher.java:69)
	at org.apache.hadoop.hbase.regionserver.HStore.flushCache(HStore.java:798)
	at org.apache.hadoop.hbase.regionserver.HStore$StoreFlusherImpl.flushCache(HStore.java:1949)
	at org.apache.hadoop.hbase.regionserver.HRegion.internalFlushcache(HRegion.java:1776)
	at org.apache.hadoop.hbase.regionserver.HRegion.internalFlushcache(HRegion.java:1658)
	at org.apache.hadoop.hbase.regionserver.HRegion.doClose(HRegion.java:1153)
	at org.apache.hadoop.hbase.regionserver.HRegion.close(HRegion.java:1085)
	at org.apache.hadoop.hbase.regionserver.handler.CloseRegionHandler.process(CloseRegionHandler.java:147)
	at org.apache.hadoop.hbase.executor.EventHandler.run(EventHandler.java:128)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1146)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:701)
2014-07-01 15:51:40,039 DEBUG [RS_CLOSE_REGION-slave1:60020-1] util.FSUtils: DFS Client does not support most favored nodes create; using default create
2014-07-01 15:51:40,042 WARN  [RS_CLOSE_REGION-slave1:60020-1] regionserver.HStore: Failed flushing store file, retrying num=3
org.apache.hadoop.ipc.RemoteException: org.apache.hadoop.hdfs.server.namenode.SafeModeException: Cannot create /hbase/data/default/usertable/bb79ae773f71289e0745068f68a0abd9/.tmp/6c33d92541ad4227830830271935077d. Name node is in safe mode.
The reported blocks 486 has reached the threshold 0.9990 of total blocks 481. Safe mode will be turned off automatically in 24 seconds.
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.startFileInternal(FSNamesystem.java:1561)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.startFile(FSNamesystem.java:1527)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.create(NameNode.java:710)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.create(NameNode.java:689)
	at sun.reflect.GeneratedMethodAccessor3.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:622)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:587)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1432)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1428)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:416)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1190)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:1426)

	at org.apache.hadoop.ipc.Client.call(Client.java:1113)
	at org.apache.hadoop.ipc.RPC$Invoker.invoke(RPC.java:229)
	at com.sun.proxy.$Proxy8.create(Unknown Source)
	at sun.reflect.GeneratedMethodAccessor26.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:622)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:85)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:62)
	at com.sun.proxy.$Proxy8.create(Unknown Source)
	at sun.reflect.GeneratedMethodAccessor26.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:622)
	at org.apache.hadoop.hbase.fs.HFileSystem$1.invoke(HFileSystem.java:294)
	at com.sun.proxy.$Proxy9.create(Unknown Source)
	at org.apache.hadoop.hdfs.DFSClient$DFSOutputStream.<init>(DFSClient.java:3451)
	at org.apache.hadoop.hdfs.DFSClient.create(DFSClient.java:870)
	at org.apache.hadoop.hdfs.DistributedFileSystem.create(DistributedFileSystem.java:205)
	at org.apache.hadoop.fs.FilterFileSystem.create(FilterFileSystem.java:126)
	at org.apache.hadoop.hbase.util.FSUtils.create(FSUtils.java:341)
	at org.apache.hadoop.hbase.util.FSUtils.create(FSUtils.java:316)
	at org.apache.hadoop.hbase.io.hfile.AbstractHFileWriter.createOutputStream(AbstractHFileWriter.java:266)
	at org.apache.hadoop.hbase.io.hfile.HFile$WriterFactory.create(HFile.java:391)
	at org.apache.hadoop.hbase.regionserver.StoreFile$Writer.<init>(StoreFile.java:728)
	at org.apache.hadoop.hbase.regionserver.StoreFile$Writer.<init>(StoreFile.java:679)
	at org.apache.hadoop.hbase.regionserver.StoreFile$WriterBuilder.build(StoreFile.java:617)
	at org.apache.hadoop.hbase.regionserver.HStore.createWriterInTmp(HStore.java:889)
	at org.apache.hadoop.hbase.regionserver.DefaultStoreFlusher.flushSnapshot(DefaultStoreFlusher.java:69)
	at org.apache.hadoop.hbase.regionserver.HStore.flushCache(HStore.java:798)
	at org.apache.hadoop.hbase.regionserver.HStore$StoreFlusherImpl.flushCache(HStore.java:1949)
	at org.apache.hadoop.hbase.regionserver.HRegion.internalFlushcache(HRegion.java:1776)
	at org.apache.hadoop.hbase.regionserver.HRegion.internalFlushcache(HRegion.java:1658)
	at org.apache.hadoop.hbase.regionserver.HRegion.doClose(HRegion.java:1153)
	at org.apache.hadoop.hbase.regionserver.HRegion.close(HRegion.java:1085)
	at org.apache.hadoop.hbase.regionserver.handler.CloseRegionHandler.process(CloseRegionHandler.java:147)
	at org.apache.hadoop.hbase.executor.EventHandler.run(EventHandler.java:128)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1146)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:701)
2014-07-01 15:51:40,336 WARN  [org.apache.hadoop.hdfs.LeaseRenewer$1@4fae46d5] hdfs.LeaseRenewer: Failed to renew lease for [DFSClient_hb_rs_slave1,60020,1404253868846_-130361673_31] for 30 seconds.  Will retry shortly ...
org.apache.hadoop.ipc.RemoteException: org.apache.hadoop.hdfs.server.namenode.SafeModeException: Cannot renew lease for DFSClient_hb_rs_slave1,60020,1404253868846_-130361673_31. Name node is in safe mode.
The reported blocks 486 has reached the threshold 0.9990 of total blocks 481. Safe mode will be turned off automatically in 23 seconds.
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.renewLease(FSNamesystem.java:2771)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.renewLease(NameNode.java:919)
	at sun.reflect.GeneratedMethodAccessor4.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:622)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:587)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1432)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1428)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:416)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1190)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:1426)

	at org.apache.hadoop.ipc.Client.call(Client.java:1113)
	at org.apache.hadoop.ipc.RPC$Invoker.invoke(RPC.java:229)
	at com.sun.proxy.$Proxy8.renewLease(Unknown Source)
	at sun.reflect.GeneratedMethodAccessor29.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:622)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:85)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:62)
	at com.sun.proxy.$Proxy8.renewLease(Unknown Source)
	at sun.reflect.GeneratedMethodAccessor29.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:622)
	at org.apache.hadoop.hbase.fs.HFileSystem$1.invoke(HFileSystem.java:294)
	at com.sun.proxy.$Proxy9.renewLease(Unknown Source)
	at org.apache.hadoop.hdfs.DFSClient.renewLease(DFSClient.java:379)
	at org.apache.hadoop.hdfs.LeaseRenewer.renew(LeaseRenewer.java:378)
	at org.apache.hadoop.hdfs.LeaseRenewer.run(LeaseRenewer.java:400)
	at org.apache.hadoop.hdfs.LeaseRenewer.access$600(LeaseRenewer.java:69)
	at org.apache.hadoop.hdfs.LeaseRenewer$1.run(LeaseRenewer.java:273)
	at java.lang.Thread.run(Thread.java:701)
2014-07-01 15:51:40,649 DEBUG [RS_CLOSE_REGION-slave1:60020-0] util.FSUtils: DFS Client does not support most favored nodes create; using default create
2014-07-01 15:51:40,652 WARN  [RS_CLOSE_REGION-slave1:60020-0] regionserver.HStore: Failed flushing store file, retrying num=2
org.apache.hadoop.ipc.RemoteException: org.apache.hadoop.hdfs.server.namenode.SafeModeException: Cannot create /hbase/data/default/usertable/122e92eabfab38cb15a9965b21db25c5/.tmp/99955328c90f4b35999ed7aa28a57ea4. Name node is in safe mode.
The reported blocks 486 has reached the threshold 0.9990 of total blocks 481. Safe mode will be turned off automatically in 23 seconds.
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.startFileInternal(FSNamesystem.java:1561)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.startFile(FSNamesystem.java:1527)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.create(NameNode.java:710)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.create(NameNode.java:689)
	at sun.reflect.GeneratedMethodAccessor3.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:622)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:587)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1432)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1428)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:416)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1190)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:1426)

	at org.apache.hadoop.ipc.Client.call(Client.java:1113)
	at org.apache.hadoop.ipc.RPC$Invoker.invoke(RPC.java:229)
	at com.sun.proxy.$Proxy8.create(Unknown Source)
	at sun.reflect.GeneratedMethodAccessor26.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:622)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:85)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:62)
	at com.sun.proxy.$Proxy8.create(Unknown Source)
	at sun.reflect.GeneratedMethodAccessor26.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:622)
	at org.apache.hadoop.hbase.fs.HFileSystem$1.invoke(HFileSystem.java:294)
	at com.sun.proxy.$Proxy9.create(Unknown Source)
	at org.apache.hadoop.hdfs.DFSClient$DFSOutputStream.<init>(DFSClient.java:3451)
	at org.apache.hadoop.hdfs.DFSClient.create(DFSClient.java:870)
	at org.apache.hadoop.hdfs.DistributedFileSystem.create(DistributedFileSystem.java:205)
	at org.apache.hadoop.fs.FilterFileSystem.create(FilterFileSystem.java:126)
	at org.apache.hadoop.hbase.util.FSUtils.create(FSUtils.java:341)
	at org.apache.hadoop.hbase.util.FSUtils.create(FSUtils.java:316)
	at org.apache.hadoop.hbase.io.hfile.AbstractHFileWriter.createOutputStream(AbstractHFileWriter.java:266)
	at org.apache.hadoop.hbase.io.hfile.HFile$WriterFactory.create(HFile.java:391)
	at org.apache.hadoop.hbase.regionserver.StoreFile$Writer.<init>(StoreFile.java:728)
	at org.apache.hadoop.hbase.regionserver.StoreFile$Writer.<init>(StoreFile.java:679)
	at org.apache.hadoop.hbase.regionserver.StoreFile$WriterBuilder.build(StoreFile.java:617)
	at org.apache.hadoop.hbase.regionserver.HStore.createWriterInTmp(HStore.java:889)
	at org.apache.hadoop.hbase.regionserver.DefaultStoreFlusher.flushSnapshot(DefaultStoreFlusher.java:69)
	at org.apache.hadoop.hbase.regionserver.HStore.flushCache(HStore.java:798)
	at org.apache.hadoop.hbase.regionserver.HStore$StoreFlusherImpl.flushCache(HStore.java:1949)
	at org.apache.hadoop.hbase.regionserver.HRegion.internalFlushcache(HRegion.java:1776)
	at org.apache.hadoop.hbase.regionserver.HRegion.internalFlushcache(HRegion.java:1658)
	at org.apache.hadoop.hbase.regionserver.HRegion.doClose(HRegion.java:1153)
	at org.apache.hadoop.hbase.regionserver.HRegion.close(HRegion.java:1085)
	at org.apache.hadoop.hbase.regionserver.handler.CloseRegionHandler.process(CloseRegionHandler.java:147)
	at org.apache.hadoop.hbase.executor.EventHandler.run(EventHandler.java:128)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1146)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:701)
2014-07-01 15:51:40,681 INFO  [regionserver60020] regionserver.HRegionServer: Received CLOSE for the region: 93253e37c284f3174309fede3f32339f ,which we are already trying to CLOSE, but not completed yet
2014-07-01 15:51:40,681 WARN  [regionserver60020] regionserver.HRegionServer: Failed to close usertable,user3,1404228265526.93253e37c284f3174309fede3f32339f. - ignoring and continuing
org.apache.hadoop.hbase.regionserver.RegionAlreadyInTransitionException: The region 93253e37c284f3174309fede3f32339f was already closing. New CLOSE request is ignored.
	at org.apache.hadoop.hbase.regionserver.HRegionServer.closeRegion(HRegionServer.java:2586)
	at org.apache.hadoop.hbase.regionserver.HRegionServer.closeRegionIgnoreErrors(HRegionServer.java:2508)
	at org.apache.hadoop.hbase.regionserver.HRegionServer.closeUserRegions(HRegionServer.java:2098)
	at org.apache.hadoop.hbase.regionserver.HRegionServer.run(HRegionServer.java:894)
	at java.lang.Thread.run(Thread.java:701)
2014-07-01 15:51:40,681 INFO  [regionserver60020] regionserver.HRegionServer: Received CLOSE for the region: 667c1f2c34fe37d48ba25a789d8aba98 ,which we are already trying to CLOSE, but not completed yet
2014-07-01 15:51:40,681 WARN  [regionserver60020] regionserver.HRegionServer: Failed to close usertable,user7,1404228265526.667c1f2c34fe37d48ba25a789d8aba98. - ignoring and continuing
org.apache.hadoop.hbase.regionserver.RegionAlreadyInTransitionException: The region 667c1f2c34fe37d48ba25a789d8aba98 was already closing. New CLOSE request is ignored.
	at org.apache.hadoop.hbase.regionserver.HRegionServer.closeRegion(HRegionServer.java:2586)
	at org.apache.hadoop.hbase.regionserver.HRegionServer.closeRegionIgnoreErrors(HRegionServer.java:2508)
	at org.apache.hadoop.hbase.regionserver.HRegionServer.closeUserRegions(HRegionServer.java:2098)
	at org.apache.hadoop.hbase.regionserver.HRegionServer.run(HRegionServer.java:894)
	at java.lang.Thread.run(Thread.java:701)
2014-07-01 15:51:40,682 DEBUG [regionserver60020] regionserver.HRegionServer: Waiting on 122e92eabfab38cb15a9965b21db25c5, bb79ae773f71289e0745068f68a0abd9, 9cef9e7d34f96f1d9b111be50040a208, 93253e37c284f3174309fede3f32339f, 667c1f2c34fe37d48ba25a789d8aba98
2014-07-01 15:51:41,034 DEBUG [RS_CLOSE_REGION-slave1:60020-2] util.FSUtils: DFS Client does not support most favored nodes create; using default create
2014-07-01 15:51:41,037 WARN  [RS_CLOSE_REGION-slave1:60020-2] regionserver.HStore: Failed flushing store file, retrying num=5
org.apache.hadoop.ipc.RemoteException: org.apache.hadoop.hdfs.server.namenode.SafeModeException: Cannot create /hbase/data/default/usertable/9cef9e7d34f96f1d9b111be50040a208/.tmp/ba6a537400ba432f8cfb852ea07c5020. Name node is in safe mode.
The reported blocks 486 has reached the threshold 0.9990 of total blocks 481. Safe mode will be turned off automatically in 23 seconds.
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.startFileInternal(FSNamesystem.java:1561)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.startFile(FSNamesystem.java:1527)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.create(NameNode.java:710)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.create(NameNode.java:689)
	at sun.reflect.GeneratedMethodAccessor3.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:622)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:587)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1432)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1428)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:416)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1190)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:1426)

	at org.apache.hadoop.ipc.Client.call(Client.java:1113)
	at org.apache.hadoop.ipc.RPC$Invoker.invoke(RPC.java:229)
	at com.sun.proxy.$Proxy8.create(Unknown Source)
	at sun.reflect.GeneratedMethodAccessor26.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:622)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:85)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:62)
	at com.sun.proxy.$Proxy8.create(Unknown Source)
	at sun.reflect.GeneratedMethodAccessor26.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:622)
	at org.apache.hadoop.hbase.fs.HFileSystem$1.invoke(HFileSystem.java:294)
	at com.sun.proxy.$Proxy9.create(Unknown Source)
	at org.apache.hadoop.hdfs.DFSClient$DFSOutputStream.<init>(DFSClient.java:3451)
	at org.apache.hadoop.hdfs.DFSClient.create(DFSClient.java:870)
	at org.apache.hadoop.hdfs.DistributedFileSystem.create(DistributedFileSystem.java:205)
	at org.apache.hadoop.fs.FilterFileSystem.create(FilterFileSystem.java:126)
	at org.apache.hadoop.hbase.util.FSUtils.create(FSUtils.java:341)
	at org.apache.hadoop.hbase.util.FSUtils.create(FSUtils.java:316)
	at org.apache.hadoop.hbase.io.hfile.AbstractHFileWriter.createOutputStream(AbstractHFileWriter.java:266)
	at org.apache.hadoop.hbase.io.hfile.HFile$WriterFactory.create(HFile.java:391)
	at org.apache.hadoop.hbase.regionserver.StoreFile$Writer.<init>(StoreFile.java:728)
	at org.apache.hadoop.hbase.regionserver.StoreFile$Writer.<init>(StoreFile.java:679)
	at org.apache.hadoop.hbase.regionserver.StoreFile$WriterBuilder.build(StoreFile.java:617)
	at org.apache.hadoop.hbase.regionserver.HStore.createWriterInTmp(HStore.java:889)
	at org.apache.hadoop.hbase.regionserver.DefaultStoreFlusher.flushSnapshot(DefaultStoreFlusher.java:69)
	at org.apache.hadoop.hbase.regionserver.HStore.flushCache(HStore.java:798)
	at org.apache.hadoop.hbase.regionserver.HStore$StoreFlusherImpl.flushCache(HStore.java:1949)
	at org.apache.hadoop.hbase.regionserver.HRegion.internalFlushcache(HRegion.java:1776)
	at org.apache.hadoop.hbase.regionserver.HRegion.internalFlushcache(HRegion.java:1658)
	at org.apache.hadoop.hbase.regionserver.HRegion.doClose(HRegion.java:1153)
	at org.apache.hadoop.hbase.regionserver.HRegion.close(HRegion.java:1085)
	at org.apache.hadoop.hbase.regionserver.handler.CloseRegionHandler.process(CloseRegionHandler.java:147)
	at org.apache.hadoop.hbase.executor.EventHandler.run(EventHandler.java:128)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1146)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:701)
2014-07-01 15:51:41,096 DEBUG [RS_CLOSE_REGION-slave1:60020-1] util.FSUtils: DFS Client does not support most favored nodes create; using default create
2014-07-01 15:51:41,099 WARN  [RS_CLOSE_REGION-slave1:60020-1] regionserver.HStore: Failed flushing store file, retrying num=4
org.apache.hadoop.ipc.RemoteException: org.apache.hadoop.hdfs.server.namenode.SafeModeException: Cannot create /hbase/data/default/usertable/bb79ae773f71289e0745068f68a0abd9/.tmp/da211c2970534db78ec0bcc88026028f. Name node is in safe mode.
The reported blocks 486 has reached the threshold 0.9990 of total blocks 481. Safe mode will be turned off automatically in 23 seconds.
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.startFileInternal(FSNamesystem.java:1561)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.startFile(FSNamesystem.java:1527)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.create(NameNode.java:710)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.create(NameNode.java:689)
	at sun.reflect.GeneratedMethodAccessor3.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:622)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:587)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1432)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1428)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:416)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1190)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:1426)

	at org.apache.hadoop.ipc.Client.call(Client.java:1113)
	at org.apache.hadoop.ipc.RPC$Invoker.invoke(RPC.java:229)
	at com.sun.proxy.$Proxy8.create(Unknown Source)
	at sun.reflect.GeneratedMethodAccessor26.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:622)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:85)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:62)
	at com.sun.proxy.$Proxy8.create(Unknown Source)
	at sun.reflect.GeneratedMethodAccessor26.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:622)
	at org.apache.hadoop.hbase.fs.HFileSystem$1.invoke(HFileSystem.java:294)
	at com.sun.proxy.$Proxy9.create(Unknown Source)
	at org.apache.hadoop.hdfs.DFSClient$DFSOutputStream.<init>(DFSClient.java:3451)
	at org.apache.hadoop.hdfs.DFSClient.create(DFSClient.java:870)
	at org.apache.hadoop.hdfs.DistributedFileSystem.create(DistributedFileSystem.java:205)
	at org.apache.hadoop.fs.FilterFileSystem.create(FilterFileSystem.java:126)
	at org.apache.hadoop.hbase.util.FSUtils.create(FSUtils.java:341)
	at org.apache.hadoop.hbase.util.FSUtils.create(FSUtils.java:316)
	at org.apache.hadoop.hbase.io.hfile.AbstractHFileWriter.createOutputStream(AbstractHFileWriter.java:266)
	at org.apache.hadoop.hbase.io.hfile.HFile$WriterFactory.create(HFile.java:391)
	at org.apache.hadoop.hbase.regionserver.StoreFile$Writer.<init>(StoreFile.java:728)
	at org.apache.hadoop.hbase.regionserver.StoreFile$Writer.<init>(StoreFile.java:679)
	at org.apache.hadoop.hbase.regionserver.StoreFile$WriterBuilder.build(StoreFile.java:617)
	at org.apache.hadoop.hbase.regionserver.HStore.createWriterInTmp(HStore.java:889)
	at org.apache.hadoop.hbase.regionserver.DefaultStoreFlusher.flushSnapshot(DefaultStoreFlusher.java:69)
	at org.apache.hadoop.hbase.regionserver.HStore.flushCache(HStore.java:798)
	at org.apache.hadoop.hbase.regionserver.HStore$StoreFlusherImpl.flushCache(HStore.java:1949)
	at org.apache.hadoop.hbase.regionserver.HRegion.internalFlushcache(HRegion.java:1776)
	at org.apache.hadoop.hbase.regionserver.HRegion.internalFlushcache(HRegion.java:1658)
	at org.apache.hadoop.hbase.regionserver.HRegion.doClose(HRegion.java:1153)
	at org.apache.hadoop.hbase.regionserver.HRegion.close(HRegion.java:1085)
	at org.apache.hadoop.hbase.regionserver.handler.CloseRegionHandler.process(CloseRegionHandler.java:147)
	at org.apache.hadoop.hbase.executor.EventHandler.run(EventHandler.java:128)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1146)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:701)
2014-07-01 15:51:41,339 WARN  [org.apache.hadoop.hdfs.LeaseRenewer$1@4fae46d5] hdfs.LeaseRenewer: Failed to renew lease for [DFSClient_hb_rs_slave1,60020,1404253868846_-130361673_31] for 30 seconds.  Will retry shortly ...
org.apache.hadoop.ipc.RemoteException: org.apache.hadoop.hdfs.server.namenode.SafeModeException: Cannot renew lease for DFSClient_hb_rs_slave1,60020,1404253868846_-130361673_31. Name node is in safe mode.
The reported blocks 486 has reached the threshold 0.9990 of total blocks 481. Safe mode will be turned off automatically in 22 seconds.
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.renewLease(FSNamesystem.java:2771)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.renewLease(NameNode.java:919)
	at sun.reflect.GeneratedMethodAccessor4.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:622)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:587)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1432)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1428)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:416)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1190)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:1426)

	at org.apache.hadoop.ipc.Client.call(Client.java:1113)
	at org.apache.hadoop.ipc.RPC$Invoker.invoke(RPC.java:229)
	at com.sun.proxy.$Proxy8.renewLease(Unknown Source)
	at sun.reflect.GeneratedMethodAccessor29.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:622)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:85)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:62)
	at com.sun.proxy.$Proxy8.renewLease(Unknown Source)
	at sun.reflect.GeneratedMethodAccessor29.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:622)
	at org.apache.hadoop.hbase.fs.HFileSystem$1.invoke(HFileSystem.java:294)
	at com.sun.proxy.$Proxy9.renewLease(Unknown Source)
	at org.apache.hadoop.hdfs.DFSClient.renewLease(DFSClient.java:379)
	at org.apache.hadoop.hdfs.LeaseRenewer.renew(LeaseRenewer.java:378)
	at org.apache.hadoop.hdfs.LeaseRenewer.run(LeaseRenewer.java:400)
	at org.apache.hadoop.hdfs.LeaseRenewer.access$600(LeaseRenewer.java:69)
	at org.apache.hadoop.hdfs.LeaseRenewer$1.run(LeaseRenewer.java:273)
	at java.lang.Thread.run(Thread.java:701)
2014-07-01 15:51:41,676 DEBUG [RS_CLOSE_REGION-slave1:60020-0] util.FSUtils: DFS Client does not support most favored nodes create; using default create
2014-07-01 15:51:41,679 WARN  [RS_CLOSE_REGION-slave1:60020-0] regionserver.HStore: Failed flushing store file, retrying num=3
org.apache.hadoop.ipc.RemoteException: org.apache.hadoop.hdfs.server.namenode.SafeModeException: Cannot create /hbase/data/default/usertable/122e92eabfab38cb15a9965b21db25c5/.tmp/4b50fa92a7604e8894f81c437c9798dc. Name node is in safe mode.
The reported blocks 486 has reached the threshold 0.9990 of total blocks 481. Safe mode will be turned off automatically in 22 seconds.
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.startFileInternal(FSNamesystem.java:1561)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.startFile(FSNamesystem.java:1527)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.create(NameNode.java:710)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.create(NameNode.java:689)
	at sun.reflect.GeneratedMethodAccessor3.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:622)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:587)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1432)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1428)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:416)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1190)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:1426)

	at org.apache.hadoop.ipc.Client.call(Client.java:1113)
	at org.apache.hadoop.ipc.RPC$Invoker.invoke(RPC.java:229)
	at com.sun.proxy.$Proxy8.create(Unknown Source)
	at sun.reflect.GeneratedMethodAccessor26.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:622)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:85)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:62)
	at com.sun.proxy.$Proxy8.create(Unknown Source)
	at sun.reflect.GeneratedMethodAccessor26.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:622)
	at org.apache.hadoop.hbase.fs.HFileSystem$1.invoke(HFileSystem.java:294)
	at com.sun.proxy.$Proxy9.create(Unknown Source)
	at org.apache.hadoop.hdfs.DFSClient$DFSOutputStream.<init>(DFSClient.java:3451)
	at org.apache.hadoop.hdfs.DFSClient.create(DFSClient.java:870)
	at org.apache.hadoop.hdfs.DistributedFileSystem.create(DistributedFileSystem.java:205)
	at org.apache.hadoop.fs.FilterFileSystem.create(FilterFileSystem.java:126)
	at org.apache.hadoop.hbase.util.FSUtils.create(FSUtils.java:341)
	at org.apache.hadoop.hbase.util.FSUtils.create(FSUtils.java:316)
	at org.apache.hadoop.hbase.io.hfile.AbstractHFileWriter.createOutputStream(AbstractHFileWriter.java:266)
	at org.apache.hadoop.hbase.io.hfile.HFile$WriterFactory.create(HFile.java:391)
	at org.apache.hadoop.hbase.regionserver.StoreFile$Writer.<init>(StoreFile.java:728)
	at org.apache.hadoop.hbase.regionserver.StoreFile$Writer.<init>(StoreFile.java:679)
	at org.apache.hadoop.hbase.regionserver.StoreFile$WriterBuilder.build(StoreFile.java:617)
	at org.apache.hadoop.hbase.regionserver.HStore.createWriterInTmp(HStore.java:889)
	at org.apache.hadoop.hbase.regionserver.DefaultStoreFlusher.flushSnapshot(DefaultStoreFlusher.java:69)
	at org.apache.hadoop.hbase.regionserver.HStore.flushCache(HStore.java:798)
	at org.apache.hadoop.hbase.regionserver.HStore$StoreFlusherImpl.flushCache(HStore.java:1949)
	at org.apache.hadoop.hbase.regionserver.HRegion.internalFlushcache(HRegion.java:1776)
	at org.apache.hadoop.hbase.regionserver.HRegion.internalFlushcache(HRegion.java:1658)
	at org.apache.hadoop.hbase.regionserver.HRegion.doClose(HRegion.java:1153)
	at org.apache.hadoop.hbase.regionserver.HRegion.close(HRegion.java:1085)
	at org.apache.hadoop.hbase.regionserver.handler.CloseRegionHandler.process(CloseRegionHandler.java:147)
	at org.apache.hadoop.hbase.executor.EventHandler.run(EventHandler.java:128)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1146)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:701)
2014-07-01 15:51:42,085 DEBUG [RS_CLOSE_REGION-slave1:60020-2] util.FSUtils: DFS Client does not support most favored nodes create; using default create
2014-07-01 15:51:42,088 WARN  [RS_CLOSE_REGION-slave1:60020-2] regionserver.HStore: Failed flushing store file, retrying num=6
org.apache.hadoop.ipc.RemoteException: org.apache.hadoop.hdfs.server.namenode.SafeModeException: Cannot create /hbase/data/default/usertable/9cef9e7d34f96f1d9b111be50040a208/.tmp/57c187b6aedd408cb13fb13c52bd9974. Name node is in safe mode.
The reported blocks 486 has reached the threshold 0.9990 of total blocks 481. Safe mode will be turned off automatically in 22 seconds.
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.startFileInternal(FSNamesystem.java:1561)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.startFile(FSNamesystem.java:1527)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.create(NameNode.java:710)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.create(NameNode.java:689)
	at sun.reflect.GeneratedMethodAccessor3.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:622)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:587)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1432)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1428)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:416)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1190)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:1426)

	at org.apache.hadoop.ipc.Client.call(Client.java:1113)
	at org.apache.hadoop.ipc.RPC$Invoker.invoke(RPC.java:229)
	at com.sun.proxy.$Proxy8.create(Unknown Source)
	at sun.reflect.GeneratedMethodAccessor26.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:622)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:85)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:62)
	at com.sun.proxy.$Proxy8.create(Unknown Source)
	at sun.reflect.GeneratedMethodAccessor26.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:622)
	at org.apache.hadoop.hbase.fs.HFileSystem$1.invoke(HFileSystem.java:294)
	at com.sun.proxy.$Proxy9.create(Unknown Source)
	at org.apache.hadoop.hdfs.DFSClient$DFSOutputStream.<init>(DFSClient.java:3451)
	at org.apache.hadoop.hdfs.DFSClient.create(DFSClient.java:870)
	at org.apache.hadoop.hdfs.DistributedFileSystem.create(DistributedFileSystem.java:205)
	at org.apache.hadoop.fs.FilterFileSystem.create(FilterFileSystem.java:126)
	at org.apache.hadoop.hbase.util.FSUtils.create(FSUtils.java:341)
	at org.apache.hadoop.hbase.util.FSUtils.create(FSUtils.java:316)
	at org.apache.hadoop.hbase.io.hfile.AbstractHFileWriter.createOutputStream(AbstractHFileWriter.java:266)
	at org.apache.hadoop.hbase.io.hfile.HFile$WriterFactory.create(HFile.java:391)
	at org.apache.hadoop.hbase.regionserver.StoreFile$Writer.<init>(StoreFile.java:728)
	at org.apache.hadoop.hbase.regionserver.StoreFile$Writer.<init>(StoreFile.java:679)
	at org.apache.hadoop.hbase.regionserver.StoreFile$WriterBuilder.build(StoreFile.java:617)
	at org.apache.hadoop.hbase.regionserver.HStore.createWriterInTmp(HStore.java:889)
	at org.apache.hadoop.hbase.regionserver.DefaultStoreFlusher.flushSnapshot(DefaultStoreFlusher.java:69)
	at org.apache.hadoop.hbase.regionserver.HStore.flushCache(HStore.java:798)
	at org.apache.hadoop.hbase.regionserver.HStore$StoreFlusherImpl.flushCache(HStore.java:1949)
	at org.apache.hadoop.hbase.regionserver.HRegion.internalFlushcache(HRegion.java:1776)
	at org.apache.hadoop.hbase.regionserver.HRegion.internalFlushcache(HRegion.java:1658)
	at org.apache.hadoop.hbase.regionserver.HRegion.doClose(HRegion.java:1153)
	at org.apache.hadoop.hbase.regionserver.HRegion.close(HRegion.java:1085)
	at org.apache.hadoop.hbase.regionserver.handler.CloseRegionHandler.process(CloseRegionHandler.java:147)
	at org.apache.hadoop.hbase.executor.EventHandler.run(EventHandler.java:128)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1146)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:701)
2014-07-01 15:51:42,152 DEBUG [RS_CLOSE_REGION-slave1:60020-1] util.FSUtils: DFS Client does not support most favored nodes create; using default create
2014-07-01 15:51:42,155 WARN  [RS_CLOSE_REGION-slave1:60020-1] regionserver.HStore: Failed flushing store file, retrying num=5
org.apache.hadoop.ipc.RemoteException: org.apache.hadoop.hdfs.server.namenode.SafeModeException: Cannot create /hbase/data/default/usertable/bb79ae773f71289e0745068f68a0abd9/.tmp/a73d3b10affe45a19a92f13608ddc117. Name node is in safe mode.
The reported blocks 486 has reached the threshold 0.9990 of total blocks 481. Safe mode will be turned off automatically in 22 seconds.
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.startFileInternal(FSNamesystem.java:1561)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.startFile(FSNamesystem.java:1527)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.create(NameNode.java:710)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.create(NameNode.java:689)
	at sun.reflect.GeneratedMethodAccessor3.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:622)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:587)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1432)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1428)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:416)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1190)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:1426)

	at org.apache.hadoop.ipc.Client.call(Client.java:1113)
	at org.apache.hadoop.ipc.RPC$Invoker.invoke(RPC.java:229)
	at com.sun.proxy.$Proxy8.create(Unknown Source)
	at sun.reflect.GeneratedMethodAccessor26.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:622)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:85)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:62)
	at com.sun.proxy.$Proxy8.create(Unknown Source)
	at sun.reflect.GeneratedMethodAccessor26.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:622)
	at org.apache.hadoop.hbase.fs.HFileSystem$1.invoke(HFileSystem.java:294)
	at com.sun.proxy.$Proxy9.create(Unknown Source)
	at org.apache.hadoop.hdfs.DFSClient$DFSOutputStream.<init>(DFSClient.java:3451)
	at org.apache.hadoop.hdfs.DFSClient.create(DFSClient.java:870)
	at org.apache.hadoop.hdfs.DistributedFileSystem.create(DistributedFileSystem.java:205)
	at org.apache.hadoop.fs.FilterFileSystem.create(FilterFileSystem.java:126)
	at org.apache.hadoop.hbase.util.FSUtils.create(FSUtils.java:341)
	at org.apache.hadoop.hbase.util.FSUtils.create(FSUtils.java:316)
	at org.apache.hadoop.hbase.io.hfile.AbstractHFileWriter.createOutputStream(AbstractHFileWriter.java:266)
	at org.apache.hadoop.hbase.io.hfile.HFile$WriterFactory.create(HFile.java:391)
	at org.apache.hadoop.hbase.regionserver.StoreFile$Writer.<init>(StoreFile.java:728)
	at org.apache.hadoop.hbase.regionserver.StoreFile$Writer.<init>(StoreFile.java:679)
	at org.apache.hadoop.hbase.regionserver.StoreFile$WriterBuilder.build(StoreFile.java:617)
	at org.apache.hadoop.hbase.regionserver.HStore.createWriterInTmp(HStore.java:889)
	at org.apache.hadoop.hbase.regionserver.DefaultStoreFlusher.flushSnapshot(DefaultStoreFlusher.java:69)
	at org.apache.hadoop.hbase.regionserver.HStore.flushCache(HStore.java:798)
	at org.apache.hadoop.hbase.regionserver.HStore$StoreFlusherImpl.flushCache(HStore.java:1949)
	at org.apache.hadoop.hbase.regionserver.HRegion.internalFlushcache(HRegion.java:1776)
	at org.apache.hadoop.hbase.regionserver.HRegion.internalFlushcache(HRegion.java:1658)
	at org.apache.hadoop.hbase.regionserver.HRegion.doClose(HRegion.java:1153)
	at org.apache.hadoop.hbase.regionserver.HRegion.close(HRegion.java:1085)
	at org.apache.hadoop.hbase.regionserver.handler.CloseRegionHandler.process(CloseRegionHandler.java:147)
	at org.apache.hadoop.hbase.executor.EventHandler.run(EventHandler.java:128)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1146)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:701)
2014-07-01 15:51:42,342 WARN  [org.apache.hadoop.hdfs.LeaseRenewer$1@4fae46d5] hdfs.LeaseRenewer: Failed to renew lease for [DFSClient_hb_rs_slave1,60020,1404253868846_-130361673_31] for 30 seconds.  Will retry shortly ...
org.apache.hadoop.ipc.RemoteException: org.apache.hadoop.hdfs.server.namenode.SafeModeException: Cannot renew lease for DFSClient_hb_rs_slave1,60020,1404253868846_-130361673_31. Name node is in safe mode.
The reported blocks 486 has reached the threshold 0.9990 of total blocks 481. Safe mode will be turned off automatically in 21 seconds.
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.renewLease(FSNamesystem.java:2771)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.renewLease(NameNode.java:919)
	at sun.reflect.GeneratedMethodAccessor4.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:622)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:587)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1432)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1428)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:416)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1190)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:1426)

	at org.apache.hadoop.ipc.Client.call(Client.java:1113)
	at org.apache.hadoop.ipc.RPC$Invoker.invoke(RPC.java:229)
	at com.sun.proxy.$Proxy8.renewLease(Unknown Source)
	at sun.reflect.GeneratedMethodAccessor29.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:622)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:85)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:62)
	at com.sun.proxy.$Proxy8.renewLease(Unknown Source)
	at sun.reflect.GeneratedMethodAccessor29.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:622)
	at org.apache.hadoop.hbase.fs.HFileSystem$1.invoke(HFileSystem.java:294)
	at com.sun.proxy.$Proxy9.renewLease(Unknown Source)
	at org.apache.hadoop.hdfs.DFSClient.renewLease(DFSClient.java:379)
	at org.apache.hadoop.hdfs.LeaseRenewer.renew(LeaseRenewer.java:378)
	at org.apache.hadoop.hdfs.LeaseRenewer.run(LeaseRenewer.java:400)
	at org.apache.hadoop.hdfs.LeaseRenewer.access$600(LeaseRenewer.java:69)
	at org.apache.hadoop.hdfs.LeaseRenewer$1.run(LeaseRenewer.java:273)
	at java.lang.Thread.run(Thread.java:701)
2014-07-01 15:51:42,704 DEBUG [RS_CLOSE_REGION-slave1:60020-0] util.FSUtils: DFS Client does not support most favored nodes create; using default create
2014-07-01 15:51:42,707 WARN  [RS_CLOSE_REGION-slave1:60020-0] regionserver.HStore: Failed flushing store file, retrying num=4
org.apache.hadoop.ipc.RemoteException: org.apache.hadoop.hdfs.server.namenode.SafeModeException: Cannot create /hbase/data/default/usertable/122e92eabfab38cb15a9965b21db25c5/.tmp/1cbd0c037e954d2d944b399c8e98b7ac. Name node is in safe mode.
The reported blocks 486 has reached the threshold 0.9990 of total blocks 481. Safe mode will be turned off automatically in 21 seconds.
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.startFileInternal(FSNamesystem.java:1561)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.startFile(FSNamesystem.java:1527)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.create(NameNode.java:710)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.create(NameNode.java:689)
	at sun.reflect.GeneratedMethodAccessor3.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:622)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:587)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1432)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1428)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:416)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1190)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:1426)

	at org.apache.hadoop.ipc.Client.call(Client.java:1113)
	at org.apache.hadoop.ipc.RPC$Invoker.invoke(RPC.java:229)
	at com.sun.proxy.$Proxy8.create(Unknown Source)
	at sun.reflect.GeneratedMethodAccessor26.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:622)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:85)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:62)
	at com.sun.proxy.$Proxy8.create(Unknown Source)
	at sun.reflect.GeneratedMethodAccessor26.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:622)
	at org.apache.hadoop.hbase.fs.HFileSystem$1.invoke(HFileSystem.java:294)
	at com.sun.proxy.$Proxy9.create(Unknown Source)
	at org.apache.hadoop.hdfs.DFSClient$DFSOutputStream.<init>(DFSClient.java:3451)
	at org.apache.hadoop.hdfs.DFSClient.create(DFSClient.java:870)
	at org.apache.hadoop.hdfs.DistributedFileSystem.create(DistributedFileSystem.java:205)
	at org.apache.hadoop.fs.FilterFileSystem.create(FilterFileSystem.java:126)
	at org.apache.hadoop.hbase.util.FSUtils.create(FSUtils.java:341)
	at org.apache.hadoop.hbase.util.FSUtils.create(FSUtils.java:316)
	at org.apache.hadoop.hbase.io.hfile.AbstractHFileWriter.createOutputStream(AbstractHFileWriter.java:266)
	at org.apache.hadoop.hbase.io.hfile.HFile$WriterFactory.create(HFile.java:391)
	at org.apache.hadoop.hbase.regionserver.StoreFile$Writer.<init>(StoreFile.java:728)
	at org.apache.hadoop.hbase.regionserver.StoreFile$Writer.<init>(StoreFile.java:679)
	at org.apache.hadoop.hbase.regionserver.StoreFile$WriterBuilder.build(StoreFile.java:617)
	at org.apache.hadoop.hbase.regionserver.HStore.createWriterInTmp(HStore.java:889)
	at org.apache.hadoop.hbase.regionserver.DefaultStoreFlusher.flushSnapshot(DefaultStoreFlusher.java:69)
	at org.apache.hadoop.hbase.regionserver.HStore.flushCache(HStore.java:798)
	at org.apache.hadoop.hbase.regionserver.HStore$StoreFlusherImpl.flushCache(HStore.java:1949)
	at org.apache.hadoop.hbase.regionserver.HRegion.internalFlushcache(HRegion.java:1776)
	at org.apache.hadoop.hbase.regionserver.HRegion.internalFlushcache(HRegion.java:1658)
	at org.apache.hadoop.hbase.regionserver.HRegion.doClose(HRegion.java:1153)
	at org.apache.hadoop.hbase.regionserver.HRegion.close(HRegion.java:1085)
	at org.apache.hadoop.hbase.regionserver.handler.CloseRegionHandler.process(CloseRegionHandler.java:147)
	at org.apache.hadoop.hbase.executor.EventHandler.run(EventHandler.java:128)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1146)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:701)
2014-07-01 15:51:43,136 DEBUG [RS_CLOSE_REGION-slave1:60020-2] util.FSUtils: DFS Client does not support most favored nodes create; using default create
2014-07-01 15:51:43,139 WARN  [RS_CLOSE_REGION-slave1:60020-2] regionserver.HStore: Failed flushing store file, retrying num=7
org.apache.hadoop.ipc.RemoteException: org.apache.hadoop.hdfs.server.namenode.SafeModeException: Cannot create /hbase/data/default/usertable/9cef9e7d34f96f1d9b111be50040a208/.tmp/0a7d60bed1c04ee08dbba7f64868ff43. Name node is in safe mode.
The reported blocks 486 has reached the threshold 0.9990 of total blocks 481. Safe mode will be turned off automatically in 21 seconds.
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.startFileInternal(FSNamesystem.java:1561)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.startFile(FSNamesystem.java:1527)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.create(NameNode.java:710)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.create(NameNode.java:689)
	at sun.reflect.GeneratedMethodAccessor3.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:622)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:587)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1432)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1428)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:416)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1190)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:1426)

	at org.apache.hadoop.ipc.Client.call(Client.java:1113)
	at org.apache.hadoop.ipc.RPC$Invoker.invoke(RPC.java:229)
	at com.sun.proxy.$Proxy8.create(Unknown Source)
	at sun.reflect.GeneratedMethodAccessor26.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:622)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:85)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:62)
	at com.sun.proxy.$Proxy8.create(Unknown Source)
	at sun.reflect.GeneratedMethodAccessor26.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:622)
	at org.apache.hadoop.hbase.fs.HFileSystem$1.invoke(HFileSystem.java:294)
	at com.sun.proxy.$Proxy9.create(Unknown Source)
	at org.apache.hadoop.hdfs.DFSClient$DFSOutputStream.<init>(DFSClient.java:3451)
	at org.apache.hadoop.hdfs.DFSClient.create(DFSClient.java:870)
	at org.apache.hadoop.hdfs.DistributedFileSystem.create(DistributedFileSystem.java:205)
	at org.apache.hadoop.fs.FilterFileSystem.create(FilterFileSystem.java:126)
	at org.apache.hadoop.hbase.util.FSUtils.create(FSUtils.java:341)
	at org.apache.hadoop.hbase.util.FSUtils.create(FSUtils.java:316)
	at org.apache.hadoop.hbase.io.hfile.AbstractHFileWriter.createOutputStream(AbstractHFileWriter.java:266)
	at org.apache.hadoop.hbase.io.hfile.HFile$WriterFactory.create(HFile.java:391)
	at org.apache.hadoop.hbase.regionserver.StoreFile$Writer.<init>(StoreFile.java:728)
	at org.apache.hadoop.hbase.regionserver.StoreFile$Writer.<init>(StoreFile.java:679)
	at org.apache.hadoop.hbase.regionserver.StoreFile$WriterBuilder.build(StoreFile.java:617)
	at org.apache.hadoop.hbase.regionserver.HStore.createWriterInTmp(HStore.java:889)
	at org.apache.hadoop.hbase.regionserver.DefaultStoreFlusher.flushSnapshot(DefaultStoreFlusher.java:69)
	at org.apache.hadoop.hbase.regionserver.HStore.flushCache(HStore.java:798)
	at org.apache.hadoop.hbase.regionserver.HStore$StoreFlusherImpl.flushCache(HStore.java:1949)
	at org.apache.hadoop.hbase.regionserver.HRegion.internalFlushcache(HRegion.java:1776)
	at org.apache.hadoop.hbase.regionserver.HRegion.internalFlushcache(HRegion.java:1658)
	at org.apache.hadoop.hbase.regionserver.HRegion.doClose(HRegion.java:1153)
	at org.apache.hadoop.hbase.regionserver.HRegion.close(HRegion.java:1085)
	at org.apache.hadoop.hbase.regionserver.handler.CloseRegionHandler.process(CloseRegionHandler.java:147)
	at org.apache.hadoop.hbase.executor.EventHandler.run(EventHandler.java:128)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1146)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:701)
2014-07-01 15:51:43,210 DEBUG [RS_CLOSE_REGION-slave1:60020-1] util.FSUtils: DFS Client does not support most favored nodes create; using default create
2014-07-01 15:51:43,212 WARN  [RS_CLOSE_REGION-slave1:60020-1] regionserver.HStore: Failed flushing store file, retrying num=6
org.apache.hadoop.ipc.RemoteException: org.apache.hadoop.hdfs.server.namenode.SafeModeException: Cannot create /hbase/data/default/usertable/bb79ae773f71289e0745068f68a0abd9/.tmp/9cff066cc2ff4f7992a2cc5cb42c5c32. Name node is in safe mode.
The reported blocks 486 has reached the threshold 0.9990 of total blocks 481. Safe mode will be turned off automatically in 20 seconds.
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.startFileInternal(FSNamesystem.java:1561)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.startFile(FSNamesystem.java:1527)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.create(NameNode.java:710)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.create(NameNode.java:689)
	at sun.reflect.GeneratedMethodAccessor3.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:622)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:587)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1432)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1428)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:416)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1190)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:1426)

	at org.apache.hadoop.ipc.Client.call(Client.java:1113)
	at org.apache.hadoop.ipc.RPC$Invoker.invoke(RPC.java:229)
	at com.sun.proxy.$Proxy8.create(Unknown Source)
	at sun.reflect.GeneratedMethodAccessor26.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:622)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:85)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:62)
	at com.sun.proxy.$Proxy8.create(Unknown Source)
	at sun.reflect.GeneratedMethodAccessor26.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:622)
	at org.apache.hadoop.hbase.fs.HFileSystem$1.invoke(HFileSystem.java:294)
	at com.sun.proxy.$Proxy9.create(Unknown Source)
	at org.apache.hadoop.hdfs.DFSClient$DFSOutputStream.<init>(DFSClient.java:3451)
	at org.apache.hadoop.hdfs.DFSClient.create(DFSClient.java:870)
	at org.apache.hadoop.hdfs.DistributedFileSystem.create(DistributedFileSystem.java:205)
	at org.apache.hadoop.fs.FilterFileSystem.create(FilterFileSystem.java:126)
	at org.apache.hadoop.hbase.util.FSUtils.create(FSUtils.java:341)
	at org.apache.hadoop.hbase.util.FSUtils.create(FSUtils.java:316)
	at org.apache.hadoop.hbase.io.hfile.AbstractHFileWriter.createOutputStream(AbstractHFileWriter.java:266)
	at org.apache.hadoop.hbase.io.hfile.HFile$WriterFactory.create(HFile.java:391)
	at org.apache.hadoop.hbase.regionserver.StoreFile$Writer.<init>(StoreFile.java:728)
	at org.apache.hadoop.hbase.regionserver.StoreFile$Writer.<init>(StoreFile.java:679)
	at org.apache.hadoop.hbase.regionserver.StoreFile$WriterBuilder.build(StoreFile.java:617)
	at org.apache.hadoop.hbase.regionserver.HStore.createWriterInTmp(HStore.java:889)
	at org.apache.hadoop.hbase.regionserver.DefaultStoreFlusher.flushSnapshot(DefaultStoreFlusher.java:69)
	at org.apache.hadoop.hbase.regionserver.HStore.flushCache(HStore.java:798)
	at org.apache.hadoop.hbase.regionserver.HStore$StoreFlusherImpl.flushCache(HStore.java:1949)
	at org.apache.hadoop.hbase.regionserver.HRegion.internalFlushcache(HRegion.java:1776)
	at org.apache.hadoop.hbase.regionserver.HRegion.internalFlushcache(HRegion.java:1658)
	at org.apache.hadoop.hbase.regionserver.HRegion.doClose(HRegion.java:1153)
	at org.apache.hadoop.hbase.regionserver.HRegion.close(HRegion.java:1085)
	at org.apache.hadoop.hbase.regionserver.handler.CloseRegionHandler.process(CloseRegionHandler.java:147)
	at org.apache.hadoop.hbase.executor.EventHandler.run(EventHandler.java:128)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1146)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:701)
2014-07-01 15:51:43,345 WARN  [org.apache.hadoop.hdfs.LeaseRenewer$1@4fae46d5] hdfs.LeaseRenewer: Failed to renew lease for [DFSClient_hb_rs_slave1,60020,1404253868846_-130361673_31] for 30 seconds.  Will retry shortly ...
org.apache.hadoop.ipc.RemoteException: org.apache.hadoop.hdfs.server.namenode.SafeModeException: Cannot renew lease for DFSClient_hb_rs_slave1,60020,1404253868846_-130361673_31. Name node is in safe mode.
The reported blocks 486 has reached the threshold 0.9990 of total blocks 481. Safe mode will be turned off automatically in 20 seconds.
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.renewLease(FSNamesystem.java:2771)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.renewLease(NameNode.java:919)
	at sun.reflect.GeneratedMethodAccessor4.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:622)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:587)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1432)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1428)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:416)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1190)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:1426)

	at org.apache.hadoop.ipc.Client.call(Client.java:1113)
	at org.apache.hadoop.ipc.RPC$Invoker.invoke(RPC.java:229)
	at com.sun.proxy.$Proxy8.renewLease(Unknown Source)
	at sun.reflect.GeneratedMethodAccessor29.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:622)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:85)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:62)
	at com.sun.proxy.$Proxy8.renewLease(Unknown Source)
	at sun.reflect.GeneratedMethodAccessor29.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:622)
	at org.apache.hadoop.hbase.fs.HFileSystem$1.invoke(HFileSystem.java:294)
	at com.sun.proxy.$Proxy9.renewLease(Unknown Source)
	at org.apache.hadoop.hdfs.DFSClient.renewLease(DFSClient.java:379)
	at org.apache.hadoop.hdfs.LeaseRenewer.renew(LeaseRenewer.java:378)
	at org.apache.hadoop.hdfs.LeaseRenewer.run(LeaseRenewer.java:400)
	at org.apache.hadoop.hdfs.LeaseRenewer.access$600(LeaseRenewer.java:69)
	at org.apache.hadoop.hdfs.LeaseRenewer$1.run(LeaseRenewer.java:273)
	at java.lang.Thread.run(Thread.java:701)
2014-07-01 15:51:43,685 INFO  [regionserver60020] regionserver.HRegionServer: Received CLOSE for the region: 93253e37c284f3174309fede3f32339f ,which we are already trying to CLOSE, but not completed yet
2014-07-01 15:51:43,686 WARN  [regionserver60020] regionserver.HRegionServer: Failed to close usertable,user3,1404228265526.93253e37c284f3174309fede3f32339f. - ignoring and continuing
org.apache.hadoop.hbase.regionserver.RegionAlreadyInTransitionException: The region 93253e37c284f3174309fede3f32339f was already closing. New CLOSE request is ignored.
	at org.apache.hadoop.hbase.regionserver.HRegionServer.closeRegion(HRegionServer.java:2586)
	at org.apache.hadoop.hbase.regionserver.HRegionServer.closeRegionIgnoreErrors(HRegionServer.java:2508)
	at org.apache.hadoop.hbase.regionserver.HRegionServer.closeUserRegions(HRegionServer.java:2098)
	at org.apache.hadoop.hbase.regionserver.HRegionServer.run(HRegionServer.java:894)
	at java.lang.Thread.run(Thread.java:701)
2014-07-01 15:51:43,686 INFO  [regionserver60020] regionserver.HRegionServer: Received CLOSE for the region: 667c1f2c34fe37d48ba25a789d8aba98 ,which we are already trying to CLOSE, but not completed yet
2014-07-01 15:51:43,686 WARN  [regionserver60020] regionserver.HRegionServer: Failed to close usertable,user7,1404228265526.667c1f2c34fe37d48ba25a789d8aba98. - ignoring and continuing
org.apache.hadoop.hbase.regionserver.RegionAlreadyInTransitionException: The region 667c1f2c34fe37d48ba25a789d8aba98 was already closing. New CLOSE request is ignored.
	at org.apache.hadoop.hbase.regionserver.HRegionServer.closeRegion(HRegionServer.java:2586)
	at org.apache.hadoop.hbase.regionserver.HRegionServer.closeRegionIgnoreErrors(HRegionServer.java:2508)
	at org.apache.hadoop.hbase.regionserver.HRegionServer.closeUserRegions(HRegionServer.java:2098)
	at org.apache.hadoop.hbase.regionserver.HRegionServer.run(HRegionServer.java:894)
	at java.lang.Thread.run(Thread.java:701)
2014-07-01 15:51:43,686 DEBUG [regionserver60020] regionserver.HRegionServer: Waiting on 122e92eabfab38cb15a9965b21db25c5, bb79ae773f71289e0745068f68a0abd9, 9cef9e7d34f96f1d9b111be50040a208, 93253e37c284f3174309fede3f32339f, 667c1f2c34fe37d48ba25a789d8aba98
2014-07-01 15:51:43,731 DEBUG [RS_CLOSE_REGION-slave1:60020-0] util.FSUtils: DFS Client does not support most favored nodes create; using default create
2014-07-01 15:51:43,734 WARN  [RS_CLOSE_REGION-slave1:60020-0] regionserver.HStore: Failed flushing store file, retrying num=5
org.apache.hadoop.ipc.RemoteException: org.apache.hadoop.hdfs.server.namenode.SafeModeException: Cannot create /hbase/data/default/usertable/122e92eabfab38cb15a9965b21db25c5/.tmp/b6452f4667224bcfb918439192f8687e. Name node is in safe mode.
The reported blocks 486 has reached the threshold 0.9990 of total blocks 481. Safe mode will be turned off automatically in 20 seconds.
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.startFileInternal(FSNamesystem.java:1561)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.startFile(FSNamesystem.java:1527)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.create(NameNode.java:710)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.create(NameNode.java:689)
	at sun.reflect.GeneratedMethodAccessor3.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:622)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:587)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1432)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1428)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:416)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1190)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:1426)

	at org.apache.hadoop.ipc.Client.call(Client.java:1113)
	at org.apache.hadoop.ipc.RPC$Invoker.invoke(RPC.java:229)
	at com.sun.proxy.$Proxy8.create(Unknown Source)
	at sun.reflect.GeneratedMethodAccessor26.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:622)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:85)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:62)
	at com.sun.proxy.$Proxy8.create(Unknown Source)
	at sun.reflect.GeneratedMethodAccessor26.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:622)
	at org.apache.hadoop.hbase.fs.HFileSystem$1.invoke(HFileSystem.java:294)
	at com.sun.proxy.$Proxy9.create(Unknown Source)
	at org.apache.hadoop.hdfs.DFSClient$DFSOutputStream.<init>(DFSClient.java:3451)
	at org.apache.hadoop.hdfs.DFSClient.create(DFSClient.java:870)
	at org.apache.hadoop.hdfs.DistributedFileSystem.create(DistributedFileSystem.java:205)
	at org.apache.hadoop.fs.FilterFileSystem.create(FilterFileSystem.java:126)
	at org.apache.hadoop.hbase.util.FSUtils.create(FSUtils.java:341)
	at org.apache.hadoop.hbase.util.FSUtils.create(FSUtils.java:316)
	at org.apache.hadoop.hbase.io.hfile.AbstractHFileWriter.createOutputStream(AbstractHFileWriter.java:266)
	at org.apache.hadoop.hbase.io.hfile.HFile$WriterFactory.create(HFile.java:391)
	at org.apache.hadoop.hbase.regionserver.StoreFile$Writer.<init>(StoreFile.java:728)
	at org.apache.hadoop.hbase.regionserver.StoreFile$Writer.<init>(StoreFile.java:679)
	at org.apache.hadoop.hbase.regionserver.StoreFile$WriterBuilder.build(StoreFile.java:617)
	at org.apache.hadoop.hbase.regionserver.HStore.createWriterInTmp(HStore.java:889)
	at org.apache.hadoop.hbase.regionserver.DefaultStoreFlusher.flushSnapshot(DefaultStoreFlusher.java:69)
	at org.apache.hadoop.hbase.regionserver.HStore.flushCache(HStore.java:798)
	at org.apache.hadoop.hbase.regionserver.HStore$StoreFlusherImpl.flushCache(HStore.java:1949)
	at org.apache.hadoop.hbase.regionserver.HRegion.internalFlushcache(HRegion.java:1776)
	at org.apache.hadoop.hbase.regionserver.HRegion.internalFlushcache(HRegion.java:1658)
	at org.apache.hadoop.hbase.regionserver.HRegion.doClose(HRegion.java:1153)
	at org.apache.hadoop.hbase.regionserver.HRegion.close(HRegion.java:1085)
	at org.apache.hadoop.hbase.regionserver.handler.CloseRegionHandler.process(CloseRegionHandler.java:147)
	at org.apache.hadoop.hbase.executor.EventHandler.run(EventHandler.java:128)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1146)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:701)
2014-07-01 15:51:44,188 DEBUG [RS_CLOSE_REGION-slave1:60020-2] util.FSUtils: DFS Client does not support most favored nodes create; using default create
2014-07-01 15:51:44,191 WARN  [RS_CLOSE_REGION-slave1:60020-2] regionserver.HStore: Failed flushing store file, retrying num=8
org.apache.hadoop.ipc.RemoteException: org.apache.hadoop.hdfs.server.namenode.SafeModeException: Cannot create /hbase/data/default/usertable/9cef9e7d34f96f1d9b111be50040a208/.tmp/2809c93643f84e14a6459e857f0c2a70. Name node is in safe mode.
The reported blocks 486 has reached the threshold 0.9990 of total blocks 481. Safe mode will be turned off automatically in 20 seconds.
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.startFileInternal(FSNamesystem.java:1561)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.startFile(FSNamesystem.java:1527)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.create(NameNode.java:710)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.create(NameNode.java:689)
	at sun.reflect.GeneratedMethodAccessor3.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:622)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:587)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1432)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1428)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:416)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1190)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:1426)

	at org.apache.hadoop.ipc.Client.call(Client.java:1113)
	at org.apache.hadoop.ipc.RPC$Invoker.invoke(RPC.java:229)
	at com.sun.proxy.$Proxy8.create(Unknown Source)
	at sun.reflect.GeneratedMethodAccessor26.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:622)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:85)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:62)
	at com.sun.proxy.$Proxy8.create(Unknown Source)
	at sun.reflect.GeneratedMethodAccessor26.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:622)
	at org.apache.hadoop.hbase.fs.HFileSystem$1.invoke(HFileSystem.java:294)
	at com.sun.proxy.$Proxy9.create(Unknown Source)
	at org.apache.hadoop.hdfs.DFSClient$DFSOutputStream.<init>(DFSClient.java:3451)
	at org.apache.hadoop.hdfs.DFSClient.create(DFSClient.java:870)
	at org.apache.hadoop.hdfs.DistributedFileSystem.create(DistributedFileSystem.java:205)
	at org.apache.hadoop.fs.FilterFileSystem.create(FilterFileSystem.java:126)
	at org.apache.hadoop.hbase.util.FSUtils.create(FSUtils.java:341)
	at org.apache.hadoop.hbase.util.FSUtils.create(FSUtils.java:316)
	at org.apache.hadoop.hbase.io.hfile.AbstractHFileWriter.createOutputStream(AbstractHFileWriter.java:266)
	at org.apache.hadoop.hbase.io.hfile.HFile$WriterFactory.create(HFile.java:391)
	at org.apache.hadoop.hbase.regionserver.StoreFile$Writer.<init>(StoreFile.java:728)
	at org.apache.hadoop.hbase.regionserver.StoreFile$Writer.<init>(StoreFile.java:679)
	at org.apache.hadoop.hbase.regionserver.StoreFile$WriterBuilder.build(StoreFile.java:617)
	at org.apache.hadoop.hbase.regionserver.HStore.createWriterInTmp(HStore.java:889)
	at org.apache.hadoop.hbase.regionserver.DefaultStoreFlusher.flushSnapshot(DefaultStoreFlusher.java:69)
	at org.apache.hadoop.hbase.regionserver.HStore.flushCache(HStore.java:798)
	at org.apache.hadoop.hbase.regionserver.HStore$StoreFlusherImpl.flushCache(HStore.java:1949)
	at org.apache.hadoop.hbase.regionserver.HRegion.internalFlushcache(HRegion.java:1776)
	at org.apache.hadoop.hbase.regionserver.HRegion.internalFlushcache(HRegion.java:1658)
	at org.apache.hadoop.hbase.regionserver.HRegion.doClose(HRegion.java:1153)
	at org.apache.hadoop.hbase.regionserver.HRegion.close(HRegion.java:1085)
	at org.apache.hadoop.hbase.regionserver.handler.CloseRegionHandler.process(CloseRegionHandler.java:147)
	at org.apache.hadoop.hbase.executor.EventHandler.run(EventHandler.java:128)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1146)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:701)
2014-07-01 15:51:44,265 DEBUG [RS_CLOSE_REGION-slave1:60020-1] util.FSUtils: DFS Client does not support most favored nodes create; using default create
2014-07-01 15:51:44,268 WARN  [RS_CLOSE_REGION-slave1:60020-1] regionserver.HStore: Failed flushing store file, retrying num=7
org.apache.hadoop.ipc.RemoteException: org.apache.hadoop.hdfs.server.namenode.SafeModeException: Cannot create /hbase/data/default/usertable/bb79ae773f71289e0745068f68a0abd9/.tmp/24ddb42e701f42b685a12d94df48a383. Name node is in safe mode.
The reported blocks 486 has reached the threshold 0.9990 of total blocks 481. Safe mode will be turned off automatically in 19 seconds.
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.startFileInternal(FSNamesystem.java:1561)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.startFile(FSNamesystem.java:1527)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.create(NameNode.java:710)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.create(NameNode.java:689)
	at sun.reflect.GeneratedMethodAccessor3.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:622)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:587)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1432)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1428)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:416)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1190)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:1426)

	at org.apache.hadoop.ipc.Client.call(Client.java:1113)
	at org.apache.hadoop.ipc.RPC$Invoker.invoke(RPC.java:229)
	at com.sun.proxy.$Proxy8.create(Unknown Source)
	at sun.reflect.GeneratedMethodAccessor26.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:622)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:85)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:62)
	at com.sun.proxy.$Proxy8.create(Unknown Source)
	at sun.reflect.GeneratedMethodAccessor26.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:622)
	at org.apache.hadoop.hbase.fs.HFileSystem$1.invoke(HFileSystem.java:294)
	at com.sun.proxy.$Proxy9.create(Unknown Source)
	at org.apache.hadoop.hdfs.DFSClient$DFSOutputStream.<init>(DFSClient.java:3451)
	at org.apache.hadoop.hdfs.DFSClient.create(DFSClient.java:870)
	at org.apache.hadoop.hdfs.DistributedFileSystem.create(DistributedFileSystem.java:205)
	at org.apache.hadoop.fs.FilterFileSystem.create(FilterFileSystem.java:126)
	at org.apache.hadoop.hbase.util.FSUtils.create(FSUtils.java:341)
	at org.apache.hadoop.hbase.util.FSUtils.create(FSUtils.java:316)
	at org.apache.hadoop.hbase.io.hfile.AbstractHFileWriter.createOutputStream(AbstractHFileWriter.java:266)
	at org.apache.hadoop.hbase.io.hfile.HFile$WriterFactory.create(HFile.java:391)
	at org.apache.hadoop.hbase.regionserver.StoreFile$Writer.<init>(StoreFile.java:728)
	at org.apache.hadoop.hbase.regionserver.StoreFile$Writer.<init>(StoreFile.java:679)
	at org.apache.hadoop.hbase.regionserver.StoreFile$WriterBuilder.build(StoreFile.java:617)
	at org.apache.hadoop.hbase.regionserver.HStore.createWriterInTmp(HStore.java:889)
	at org.apache.hadoop.hbase.regionserver.DefaultStoreFlusher.flushSnapshot(DefaultStoreFlusher.java:69)
	at org.apache.hadoop.hbase.regionserver.HStore.flushCache(HStore.java:798)
	at org.apache.hadoop.hbase.regionserver.HStore$StoreFlusherImpl.flushCache(HStore.java:1949)
	at org.apache.hadoop.hbase.regionserver.HRegion.internalFlushcache(HRegion.java:1776)
	at org.apache.hadoop.hbase.regionserver.HRegion.internalFlushcache(HRegion.java:1658)
	at org.apache.hadoop.hbase.regionserver.HRegion.doClose(HRegion.java:1153)
	at org.apache.hadoop.hbase.regionserver.HRegion.close(HRegion.java:1085)
	at org.apache.hadoop.hbase.regionserver.handler.CloseRegionHandler.process(CloseRegionHandler.java:147)
	at org.apache.hadoop.hbase.executor.EventHandler.run(EventHandler.java:128)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1146)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:701)
2014-07-01 15:51:44,348 WARN  [org.apache.hadoop.hdfs.LeaseRenewer$1@4fae46d5] hdfs.LeaseRenewer: Failed to renew lease for [DFSClient_hb_rs_slave1,60020,1404253868846_-130361673_31] for 30 seconds.  Will retry shortly ...
org.apache.hadoop.ipc.RemoteException: org.apache.hadoop.hdfs.server.namenode.SafeModeException: Cannot renew lease for DFSClient_hb_rs_slave1,60020,1404253868846_-130361673_31. Name node is in safe mode.
The reported blocks 486 has reached the threshold 0.9990 of total blocks 481. Safe mode will be turned off automatically in 19 seconds.
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.renewLease(FSNamesystem.java:2771)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.renewLease(NameNode.java:919)
	at sun.reflect.GeneratedMethodAccessor4.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:622)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:587)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1432)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1428)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:416)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1190)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:1426)

	at org.apache.hadoop.ipc.Client.call(Client.java:1113)
	at org.apache.hadoop.ipc.RPC$Invoker.invoke(RPC.java:229)
	at com.sun.proxy.$Proxy8.renewLease(Unknown Source)
	at sun.reflect.GeneratedMethodAccessor29.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:622)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:85)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:62)
	at com.sun.proxy.$Proxy8.renewLease(Unknown Source)
	at sun.reflect.GeneratedMethodAccessor29.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:622)
	at org.apache.hadoop.hbase.fs.HFileSystem$1.invoke(HFileSystem.java:294)
	at com.sun.proxy.$Proxy9.renewLease(Unknown Source)
	at org.apache.hadoop.hdfs.DFSClient.renewLease(DFSClient.java:379)
	at org.apache.hadoop.hdfs.LeaseRenewer.renew(LeaseRenewer.java:378)
	at org.apache.hadoop.hdfs.LeaseRenewer.run(LeaseRenewer.java:400)
	at org.apache.hadoop.hdfs.LeaseRenewer.access$600(LeaseRenewer.java:69)
	at org.apache.hadoop.hdfs.LeaseRenewer$1.run(LeaseRenewer.java:273)
	at java.lang.Thread.run(Thread.java:701)
2014-07-01 15:51:44,760 DEBUG [RS_CLOSE_REGION-slave1:60020-0] util.FSUtils: DFS Client does not support most favored nodes create; using default create
2014-07-01 15:51:44,762 WARN  [RS_CLOSE_REGION-slave1:60020-0] regionserver.HStore: Failed flushing store file, retrying num=6
org.apache.hadoop.ipc.RemoteException: org.apache.hadoop.hdfs.server.namenode.SafeModeException: Cannot create /hbase/data/default/usertable/122e92eabfab38cb15a9965b21db25c5/.tmp/7c8fe03d09394e8880de076fe9ae23a4. Name node is in safe mode.
The reported blocks 486 has reached the threshold 0.9990 of total blocks 481. Safe mode will be turned off automatically in 19 seconds.
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.startFileInternal(FSNamesystem.java:1561)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.startFile(FSNamesystem.java:1527)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.create(NameNode.java:710)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.create(NameNode.java:689)
	at sun.reflect.GeneratedMethodAccessor3.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:622)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:587)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1432)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1428)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:416)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1190)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:1426)

	at org.apache.hadoop.ipc.Client.call(Client.java:1113)
	at org.apache.hadoop.ipc.RPC$Invoker.invoke(RPC.java:229)
	at com.sun.proxy.$Proxy8.create(Unknown Source)
	at sun.reflect.GeneratedMethodAccessor26.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:622)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:85)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:62)
	at com.sun.proxy.$Proxy8.create(Unknown Source)
	at sun.reflect.GeneratedMethodAccessor26.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:622)
	at org.apache.hadoop.hbase.fs.HFileSystem$1.invoke(HFileSystem.java:294)
	at com.sun.proxy.$Proxy9.create(Unknown Source)
	at org.apache.hadoop.hdfs.DFSClient$DFSOutputStream.<init>(DFSClient.java:3451)
	at org.apache.hadoop.hdfs.DFSClient.create(DFSClient.java:870)
	at org.apache.hadoop.hdfs.DistributedFileSystem.create(DistributedFileSystem.java:205)
	at org.apache.hadoop.fs.FilterFileSystem.create(FilterFileSystem.java:126)
	at org.apache.hadoop.hbase.util.FSUtils.create(FSUtils.java:341)
	at org.apache.hadoop.hbase.util.FSUtils.create(FSUtils.java:316)
	at org.apache.hadoop.hbase.io.hfile.AbstractHFileWriter.createOutputStream(AbstractHFileWriter.java:266)
	at org.apache.hadoop.hbase.io.hfile.HFile$WriterFactory.create(HFile.java:391)
	at org.apache.hadoop.hbase.regionserver.StoreFile$Writer.<init>(StoreFile.java:728)
	at org.apache.hadoop.hbase.regionserver.StoreFile$Writer.<init>(StoreFile.java:679)
	at org.apache.hadoop.hbase.regionserver.StoreFile$WriterBuilder.build(StoreFile.java:617)
	at org.apache.hadoop.hbase.regionserver.HStore.createWriterInTmp(HStore.java:889)
	at org.apache.hadoop.hbase.regionserver.DefaultStoreFlusher.flushSnapshot(DefaultStoreFlusher.java:69)
	at org.apache.hadoop.hbase.regionserver.HStore.flushCache(HStore.java:798)
	at org.apache.hadoop.hbase.regionserver.HStore$StoreFlusherImpl.flushCache(HStore.java:1949)
	at org.apache.hadoop.hbase.regionserver.HRegion.internalFlushcache(HRegion.java:1776)
	at org.apache.hadoop.hbase.regionserver.HRegion.internalFlushcache(HRegion.java:1658)
	at org.apache.hadoop.hbase.regionserver.HRegion.doClose(HRegion.java:1153)
	at org.apache.hadoop.hbase.regionserver.HRegion.close(HRegion.java:1085)
	at org.apache.hadoop.hbase.regionserver.handler.CloseRegionHandler.process(CloseRegionHandler.java:147)
	at org.apache.hadoop.hbase.executor.EventHandler.run(EventHandler.java:128)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1146)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:701)
2014-07-01 15:51:45,239 DEBUG [RS_CLOSE_REGION-slave1:60020-2] util.FSUtils: DFS Client does not support most favored nodes create; using default create
2014-07-01 15:51:45,242 WARN  [RS_CLOSE_REGION-slave1:60020-2] regionserver.HStore: Failed flushing store file, retrying num=9
org.apache.hadoop.ipc.RemoteException: org.apache.hadoop.hdfs.server.namenode.SafeModeException: Cannot create /hbase/data/default/usertable/9cef9e7d34f96f1d9b111be50040a208/.tmp/f4049263d57a4d2db84bb85a298ef661. Name node is in safe mode.
The reported blocks 486 has reached the threshold 0.9990 of total blocks 481. Safe mode will be turned off automatically in 18 seconds.
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.startFileInternal(FSNamesystem.java:1561)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.startFile(FSNamesystem.java:1527)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.create(NameNode.java:710)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.create(NameNode.java:689)
	at sun.reflect.GeneratedMethodAccessor3.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:622)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:587)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1432)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1428)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:416)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1190)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:1426)

	at org.apache.hadoop.ipc.Client.call(Client.java:1113)
	at org.apache.hadoop.ipc.RPC$Invoker.invoke(RPC.java:229)
	at com.sun.proxy.$Proxy8.create(Unknown Source)
	at sun.reflect.GeneratedMethodAccessor26.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:622)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:85)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:62)
	at com.sun.proxy.$Proxy8.create(Unknown Source)
	at sun.reflect.GeneratedMethodAccessor26.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:622)
	at org.apache.hadoop.hbase.fs.HFileSystem$1.invoke(HFileSystem.java:294)
	at com.sun.proxy.$Proxy9.create(Unknown Source)
	at org.apache.hadoop.hdfs.DFSClient$DFSOutputStream.<init>(DFSClient.java:3451)
	at org.apache.hadoop.hdfs.DFSClient.create(DFSClient.java:870)
	at org.apache.hadoop.hdfs.DistributedFileSystem.create(DistributedFileSystem.java:205)
	at org.apache.hadoop.fs.FilterFileSystem.create(FilterFileSystem.java:126)
	at org.apache.hadoop.hbase.util.FSUtils.create(FSUtils.java:341)
	at org.apache.hadoop.hbase.util.FSUtils.create(FSUtils.java:316)
	at org.apache.hadoop.hbase.io.hfile.AbstractHFileWriter.createOutputStream(AbstractHFileWriter.java:266)
	at org.apache.hadoop.hbase.io.hfile.HFile$WriterFactory.create(HFile.java:391)
	at org.apache.hadoop.hbase.regionserver.StoreFile$Writer.<init>(StoreFile.java:728)
	at org.apache.hadoop.hbase.regionserver.StoreFile$Writer.<init>(StoreFile.java:679)
	at org.apache.hadoop.hbase.regionserver.StoreFile$WriterBuilder.build(StoreFile.java:617)
	at org.apache.hadoop.hbase.regionserver.HStore.createWriterInTmp(HStore.java:889)
	at org.apache.hadoop.hbase.regionserver.DefaultStoreFlusher.flushSnapshot(DefaultStoreFlusher.java:69)
	at org.apache.hadoop.hbase.regionserver.HStore.flushCache(HStore.java:798)
	at org.apache.hadoop.hbase.regionserver.HStore$StoreFlusherImpl.flushCache(HStore.java:1949)
	at org.apache.hadoop.hbase.regionserver.HRegion.internalFlushcache(HRegion.java:1776)
	at org.apache.hadoop.hbase.regionserver.HRegion.internalFlushcache(HRegion.java:1658)
	at org.apache.hadoop.hbase.regionserver.HRegion.doClose(HRegion.java:1153)
	at org.apache.hadoop.hbase.regionserver.HRegion.close(HRegion.java:1085)
	at org.apache.hadoop.hbase.regionserver.handler.CloseRegionHandler.process(CloseRegionHandler.java:147)
	at org.apache.hadoop.hbase.executor.EventHandler.run(EventHandler.java:128)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1146)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:701)
2014-07-01 15:51:45,243 FATAL [RS_CLOSE_REGION-slave1:60020-2] regionserver.HRegionServer: ABORTING region server slave1,60020,1404253868846: Unrecoverable exception while closing region usertable,,1404228265525.9cef9e7d34f96f1d9b111be50040a208., still finishing close
org.apache.hadoop.hbase.DroppedSnapshotException: region: usertable,,1404228265525.9cef9e7d34f96f1d9b111be50040a208.
	at org.apache.hadoop.hbase.regionserver.HRegion.internalFlushcache(HRegion.java:1801)
	at org.apache.hadoop.hbase.regionserver.HRegion.internalFlushcache(HRegion.java:1658)
	at org.apache.hadoop.hbase.regionserver.HRegion.doClose(HRegion.java:1153)
	at org.apache.hadoop.hbase.regionserver.HRegion.close(HRegion.java:1085)
	at org.apache.hadoop.hbase.regionserver.handler.CloseRegionHandler.process(CloseRegionHandler.java:147)
	at org.apache.hadoop.hbase.executor.EventHandler.run(EventHandler.java:128)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1146)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:701)
Caused by: org.apache.hadoop.ipc.RemoteException: org.apache.hadoop.hdfs.server.namenode.SafeModeException: Cannot create /hbase/data/default/usertable/9cef9e7d34f96f1d9b111be50040a208/.tmp/f4049263d57a4d2db84bb85a298ef661. Name node is in safe mode.
The reported blocks 486 has reached the threshold 0.9990 of total blocks 481. Safe mode will be turned off automatically in 18 seconds.
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.startFileInternal(FSNamesystem.java:1561)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.startFile(FSNamesystem.java:1527)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.create(NameNode.java:710)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.create(NameNode.java:689)
	at sun.reflect.GeneratedMethodAccessor3.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:622)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:587)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1432)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1428)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:416)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1190)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:1426)

	at org.apache.hadoop.ipc.Client.call(Client.java:1113)
	at org.apache.hadoop.ipc.RPC$Invoker.invoke(RPC.java:229)
	at com.sun.proxy.$Proxy8.create(Unknown Source)
	at sun.reflect.GeneratedMethodAccessor26.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:622)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:85)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:62)
	at com.sun.proxy.$Proxy8.create(Unknown Source)
	at sun.reflect.GeneratedMethodAccessor26.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:622)
	at org.apache.hadoop.hbase.fs.HFileSystem$1.invoke(HFileSystem.java:294)
	at com.sun.proxy.$Proxy9.create(Unknown Source)
	at org.apache.hadoop.hdfs.DFSClient$DFSOutputStream.<init>(DFSClient.java:3451)
	at org.apache.hadoop.hdfs.DFSClient.create(DFSClient.java:870)
	at org.apache.hadoop.hdfs.DistributedFileSystem.create(DistributedFileSystem.java:205)
	at org.apache.hadoop.fs.FilterFileSystem.create(FilterFileSystem.java:126)
	at org.apache.hadoop.hbase.util.FSUtils.create(FSUtils.java:341)
	at org.apache.hadoop.hbase.util.FSUtils.create(FSUtils.java:316)
	at org.apache.hadoop.hbase.io.hfile.AbstractHFileWriter.createOutputStream(AbstractHFileWriter.java:266)
	at org.apache.hadoop.hbase.io.hfile.HFile$WriterFactory.create(HFile.java:391)
	at org.apache.hadoop.hbase.regionserver.StoreFile$Writer.<init>(StoreFile.java:728)
	at org.apache.hadoop.hbase.regionserver.StoreFile$Writer.<init>(StoreFile.java:679)
	at org.apache.hadoop.hbase.regionserver.StoreFile$WriterBuilder.build(StoreFile.java:617)
	at org.apache.hadoop.hbase.regionserver.HStore.createWriterInTmp(HStore.java:889)
	at org.apache.hadoop.hbase.regionserver.DefaultStoreFlusher.flushSnapshot(DefaultStoreFlusher.java:69)
	at org.apache.hadoop.hbase.regionserver.HStore.flushCache(HStore.java:798)
	at org.apache.hadoop.hbase.regionserver.HStore$StoreFlusherImpl.flushCache(HStore.java:1949)
	at org.apache.hadoop.hbase.regionserver.HRegion.internalFlushcache(HRegion.java:1776)
	... 8 more
2014-07-01 15:51:45,243 FATAL [RS_CLOSE_REGION-slave1:60020-2] regionserver.HRegionServer: RegionServer abort: loaded coprocessors are: []
2014-07-01 15:51:45,322 DEBUG [RS_CLOSE_REGION-slave1:60020-1] util.FSUtils: DFS Client does not support most favored nodes create; using default create
2014-07-01 15:51:45,346 WARN  [RS_CLOSE_REGION-slave1:60020-1] regionserver.HStore: Failed flushing store file, retrying num=8
org.apache.hadoop.ipc.RemoteException: org.apache.hadoop.hdfs.server.namenode.SafeModeException: Cannot create /hbase/data/default/usertable/bb79ae773f71289e0745068f68a0abd9/.tmp/704384e31e05431c9e6c30a8ed0a7540. Name node is in safe mode.
The reported blocks 486 has reached the threshold 0.9990 of total blocks 481. Safe mode will be turned off automatically in 18 seconds.
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.startFileInternal(FSNamesystem.java:1561)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.startFile(FSNamesystem.java:1527)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.create(NameNode.java:710)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.create(NameNode.java:689)
	at sun.reflect.GeneratedMethodAccessor3.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:622)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:587)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1432)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1428)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:416)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1190)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:1426)

	at org.apache.hadoop.ipc.Client.call(Client.java:1113)
	at org.apache.hadoop.ipc.RPC$Invoker.invoke(RPC.java:229)
	at com.sun.proxy.$Proxy8.create(Unknown Source)
	at sun.reflect.GeneratedMethodAccessor26.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:622)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:85)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:62)
	at com.sun.proxy.$Proxy8.create(Unknown Source)
	at sun.reflect.GeneratedMethodAccessor26.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:622)
	at org.apache.hadoop.hbase.fs.HFileSystem$1.invoke(HFileSystem.java:294)
	at com.sun.proxy.$Proxy9.create(Unknown Source)
	at org.apache.hadoop.hdfs.DFSClient$DFSOutputStream.<init>(DFSClient.java:3451)
	at org.apache.hadoop.hdfs.DFSClient.create(DFSClient.java:870)
	at org.apache.hadoop.hdfs.DistributedFileSystem.create(DistributedFileSystem.java:205)
	at org.apache.hadoop.fs.FilterFileSystem.create(FilterFileSystem.java:126)
	at org.apache.hadoop.hbase.util.FSUtils.create(FSUtils.java:341)
	at org.apache.hadoop.hbase.util.FSUtils.create(FSUtils.java:316)
	at org.apache.hadoop.hbase.io.hfile.AbstractHFileWriter.createOutputStream(AbstractHFileWriter.java:266)
	at org.apache.hadoop.hbase.io.hfile.HFile$WriterFactory.create(HFile.java:391)
	at org.apache.hadoop.hbase.regionserver.StoreFile$Writer.<init>(StoreFile.java:728)
	at org.apache.hadoop.hbase.regionserver.StoreFile$Writer.<init>(StoreFile.java:679)
	at org.apache.hadoop.hbase.regionserver.StoreFile$WriterBuilder.build(StoreFile.java:617)
	at org.apache.hadoop.hbase.regionserver.HStore.createWriterInTmp(HStore.java:889)
	at org.apache.hadoop.hbase.regionserver.DefaultStoreFlusher.flushSnapshot(DefaultStoreFlusher.java:69)
	at org.apache.hadoop.hbase.regionserver.HStore.flushCache(HStore.java:798)
	at org.apache.hadoop.hbase.regionserver.HStore$StoreFlusherImpl.flushCache(HStore.java:1949)
	at org.apache.hadoop.hbase.regionserver.HRegion.internalFlushcache(HRegion.java:1776)
	at org.apache.hadoop.hbase.regionserver.HRegion.internalFlushcache(HRegion.java:1658)
	at org.apache.hadoop.hbase.regionserver.HRegion.doClose(HRegion.java:1153)
	at org.apache.hadoop.hbase.regionserver.HRegion.close(HRegion.java:1085)
	at org.apache.hadoop.hbase.regionserver.handler.CloseRegionHandler.process(CloseRegionHandler.java:147)
	at org.apache.hadoop.hbase.executor.EventHandler.run(EventHandler.java:128)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1146)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:701)
2014-07-01 15:51:45,352 WARN  [org.apache.hadoop.hdfs.LeaseRenewer$1@4fae46d5] hdfs.LeaseRenewer: Failed to renew lease for [DFSClient_hb_rs_slave1,60020,1404253868846_-130361673_31] for 30 seconds.  Will retry shortly ...
org.apache.hadoop.ipc.RemoteException: org.apache.hadoop.hdfs.server.namenode.SafeModeException: Cannot renew lease for DFSClient_hb_rs_slave1,60020,1404253868846_-130361673_31. Name node is in safe mode.
The reported blocks 486 has reached the threshold 0.9990 of total blocks 481. Safe mode will be turned off automatically in 18 seconds.
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.renewLease(FSNamesystem.java:2771)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.renewLease(NameNode.java:919)
	at sun.reflect.GeneratedMethodAccessor4.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:622)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:587)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1432)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1428)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:416)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1190)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:1426)

	at org.apache.hadoop.ipc.Client.call(Client.java:1113)
	at org.apache.hadoop.ipc.RPC$Invoker.invoke(RPC.java:229)
	at com.sun.proxy.$Proxy8.renewLease(Unknown Source)
	at sun.reflect.GeneratedMethodAccessor29.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:622)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:85)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:62)
	at com.sun.proxy.$Proxy8.renewLease(Unknown Source)
	at sun.reflect.GeneratedMethodAccessor29.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:622)
	at org.apache.hadoop.hbase.fs.HFileSystem$1.invoke(HFileSystem.java:294)
	at com.sun.proxy.$Proxy9.renewLease(Unknown Source)
	at org.apache.hadoop.hdfs.DFSClient.renewLease(DFSClient.java:379)
	at org.apache.hadoop.hdfs.LeaseRenewer.renew(LeaseRenewer.java:378)
	at org.apache.hadoop.hdfs.LeaseRenewer.run(LeaseRenewer.java:400)
	at org.apache.hadoop.hdfs.LeaseRenewer.access$600(LeaseRenewer.java:69)
	at org.apache.hadoop.hdfs.LeaseRenewer$1.run(LeaseRenewer.java:273)
	at java.lang.Thread.run(Thread.java:701)
2014-07-01 15:51:45,352 INFO  [RS_CLOSE_REGION-slave1:60020-2] regionserver.HRegionServer: STOPPED: Unrecoverable exception while closing region usertable,,1404228265525.9cef9e7d34f96f1d9b111be50040a208., still finishing close
2014-07-01 15:51:45,353 INFO  [regionserver60020] ipc.RpcServer: Stopping server on 60020
2014-07-01 15:51:45,353 ERROR [RS_CLOSE_REGION-slave1:60020-2] executor.EventHandler: Caught throwable while processing event M_RS_CLOSE_REGION
java.lang.RuntimeException: org.apache.hadoop.hbase.DroppedSnapshotException: region: usertable,,1404228265525.9cef9e7d34f96f1d9b111be50040a208.
	at org.apache.hadoop.hbase.regionserver.handler.CloseRegionHandler.process(CloseRegionHandler.java:161)
	at org.apache.hadoop.hbase.executor.EventHandler.run(EventHandler.java:128)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1146)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:701)
Caused by: org.apache.hadoop.hbase.DroppedSnapshotException: region: usertable,,1404228265525.9cef9e7d34f96f1d9b111be50040a208.
	at org.apache.hadoop.hbase.regionserver.HRegion.internalFlushcache(HRegion.java:1801)
	at org.apache.hadoop.hbase.regionserver.HRegion.internalFlushcache(HRegion.java:1658)
	at org.apache.hadoop.hbase.regionserver.HRegion.doClose(HRegion.java:1153)
	at org.apache.hadoop.hbase.regionserver.HRegion.close(HRegion.java:1085)
	at org.apache.hadoop.hbase.regionserver.handler.CloseRegionHandler.process(CloseRegionHandler.java:147)
	... 4 more
Caused by: org.apache.hadoop.ipc.RemoteException: org.apache.hadoop.hdfs.server.namenode.SafeModeException: Cannot create /hbase/data/default/usertable/9cef9e7d34f96f1d9b111be50040a208/.tmp/f4049263d57a4d2db84bb85a298ef661. Name node is in safe mode.
The reported blocks 486 has reached the threshold 0.9990 of total blocks 481. Safe mode will be turned off automatically in 18 seconds.
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.startFileInternal(FSNamesystem.java:1561)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.startFile(FSNamesystem.java:1527)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.create(NameNode.java:710)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.create(NameNode.java:689)
	at sun.reflect.GeneratedMethodAccessor3.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:622)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:587)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1432)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1428)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:416)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1190)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:1426)

	at org.apache.hadoop.ipc.Client.call(Client.java:1113)
	at org.apache.hadoop.ipc.RPC$Invoker.invoke(RPC.java:229)
	at com.sun.proxy.$Proxy8.create(Unknown Source)
	at sun.reflect.GeneratedMethodAccessor26.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:622)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:85)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:62)
	at com.sun.proxy.$Proxy8.create(Unknown Source)
	at sun.reflect.GeneratedMethodAccessor26.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:622)
	at org.apache.hadoop.hbase.fs.HFileSystem$1.invoke(HFileSystem.java:294)
	at com.sun.proxy.$Proxy9.create(Unknown Source)
	at org.apache.hadoop.hdfs.DFSClient$DFSOutputStream.<init>(DFSClient.java:3451)
	at org.apache.hadoop.hdfs.DFSClient.create(DFSClient.java:870)
	at org.apache.hadoop.hdfs.DistributedFileSystem.create(DistributedFileSystem.java:205)
	at org.apache.hadoop.fs.FilterFileSystem.create(FilterFileSystem.java:126)
	at org.apache.hadoop.hbase.util.FSUtils.create(FSUtils.java:341)
	at org.apache.hadoop.hbase.util.FSUtils.create(FSUtils.java:316)
	at org.apache.hadoop.hbase.io.hfile.AbstractHFileWriter.createOutputStream(AbstractHFileWriter.java:266)
	at org.apache.hadoop.hbase.io.hfile.HFile$WriterFactory.create(HFile.java:391)
	at org.apache.hadoop.hbase.regionserver.StoreFile$Writer.<init>(StoreFile.java:728)
	at org.apache.hadoop.hbase.regionserver.StoreFile$Writer.<init>(StoreFile.java:679)
	at org.apache.hadoop.hbase.regionserver.StoreFile$WriterBuilder.build(StoreFile.java:617)
	at org.apache.hadoop.hbase.regionserver.HStore.createWriterInTmp(HStore.java:889)
	at org.apache.hadoop.hbase.regionserver.DefaultStoreFlusher.flushSnapshot(DefaultStoreFlusher.java:69)
	at org.apache.hadoop.hbase.regionserver.HStore.flushCache(HStore.java:798)
	at org.apache.hadoop.hbase.regionserver.HStore$StoreFlusherImpl.flushCache(HStore.java:1949)
	at org.apache.hadoop.hbase.regionserver.HRegion.internalFlushcache(HRegion.java:1776)
	... 8 more
2014-07-01 15:51:45,354 DEBUG [RS_CLOSE_REGION-slave1:60020-2] handler.CloseRegionHandler: Processing close of usertable,user3,1404228265526.93253e37c284f3174309fede3f32339f.
2014-07-01 15:51:45,355 INFO  [RpcServer.listener,port=60020] ipc.RpcServer: RpcServer.listener,port=60020: stopping
2014-07-01 15:51:45,356 DEBUG [RS_CLOSE_REGION-slave1:60020-2] regionserver.HRegion: Closing usertable,user3,1404228265526.93253e37c284f3174309fede3f32339f.: disabling compactions & flushes
2014-07-01 15:51:45,357 INFO  [regionserver60020] regionserver.SplitLogWorker: Sending interrupt to stop the worker thread
2014-07-01 15:51:45,357 INFO  [RS_CLOSE_REGION-slave1:60020-2] regionserver.HRegion: Running close preflush of usertable,user3,1404228265526.93253e37c284f3174309fede3f32339f.
2014-07-01 15:51:45,358 INFO  [regionserver60020] regionserver.HRegionServer: Stopping infoServer
2014-07-01 15:51:45,358 INFO  [SplitLogWorker-slave1,60020,1404253868846] regionserver.SplitLogWorker: SplitLogWorker interrupted while waiting for task, exiting: java.lang.InterruptedException
2014-07-01 15:51:45,359 INFO  [SplitLogWorker-slave1,60020,1404253868846] regionserver.SplitLogWorker: SplitLogWorker slave1,60020,1404253868846 exiting
2014-07-01 15:51:45,357 INFO  [RpcServer.responder] ipc.RpcServer: RpcServer.responder: stopped
2014-07-01 15:51:45,359 INFO  [RpcServer.responder] ipc.RpcServer: RpcServer.responder: stopping
2014-07-01 15:51:45,358 DEBUG [RS_CLOSE_REGION-slave1:60020-2] regionserver.HRegion: Updates disabled for region usertable,user3,1404228265526.93253e37c284f3174309fede3f32339f.
2014-07-01 15:51:45,360 FATAL [RS_CLOSE_REGION-slave1:60020-2] regionserver.HRegionServer: ABORTING region server slave1,60020,1404253868846: Unrecoverable exception while closing region usertable,user3,1404228265526.93253e37c284f3174309fede3f32339f., still finishing close
java.io.IOException: Aborting flush because server is abortted...
	at org.apache.hadoop.hbase.regionserver.HRegion.internalFlushcache(HRegion.java:1675)
	at org.apache.hadoop.hbase.regionserver.HRegion.internalFlushcache(HRegion.java:1658)
	at org.apache.hadoop.hbase.regionserver.HRegion.doClose(HRegion.java:1153)
	at org.apache.hadoop.hbase.regionserver.HRegion.close(HRegion.java:1085)
	at org.apache.hadoop.hbase.regionserver.handler.CloseRegionHandler.process(CloseRegionHandler.java:147)
	at org.apache.hadoop.hbase.executor.EventHandler.run(EventHandler.java:128)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1146)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:701)
2014-07-01 15:51:45,360 FATAL [RS_CLOSE_REGION-slave1:60020-2] regionserver.HRegionServer: RegionServer abort: loaded coprocessors are: []
2014-07-01 15:51:45,379 INFO  [regionserver60020] mortbay.log: Stopped SelectChannelConnector@0.0.0.0:60030
2014-07-01 15:51:45,381 ERROR [RS_CLOSE_REGION-slave1:60020-2] executor.EventHandler: Caught throwable while processing event M_RS_CLOSE_REGION
java.lang.RuntimeException: java.io.IOException: Aborting flush because server is abortted...
	at org.apache.hadoop.hbase.regionserver.handler.CloseRegionHandler.process(CloseRegionHandler.java:161)
	at org.apache.hadoop.hbase.executor.EventHandler.run(EventHandler.java:128)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1146)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:701)
Caused by: java.io.IOException: Aborting flush because server is abortted...
	at org.apache.hadoop.hbase.regionserver.HRegion.internalFlushcache(HRegion.java:1675)
	at org.apache.hadoop.hbase.regionserver.HRegion.internalFlushcache(HRegion.java:1658)
	at org.apache.hadoop.hbase.regionserver.HRegion.doClose(HRegion.java:1153)
	at org.apache.hadoop.hbase.regionserver.HRegion.close(HRegion.java:1085)
	at org.apache.hadoop.hbase.regionserver.handler.CloseRegionHandler.process(CloseRegionHandler.java:147)
	... 4 more
2014-07-01 15:51:45,383 DEBUG [RS_CLOSE_REGION-slave1:60020-2] handler.CloseRegionHandler: Processing close of usertable,user7,1404228265526.667c1f2c34fe37d48ba25a789d8aba98.
2014-07-01 15:51:45,384 DEBUG [RS_CLOSE_REGION-slave1:60020-2] regionserver.HRegion: Closing usertable,user7,1404228265526.667c1f2c34fe37d48ba25a789d8aba98.: disabling compactions & flushes
2014-07-01 15:51:45,384 INFO  [RS_CLOSE_REGION-slave1:60020-2] regionserver.HRegion: Running close preflush of usertable,user7,1404228265526.667c1f2c34fe37d48ba25a789d8aba98.
2014-07-01 15:51:45,384 DEBUG [RS_CLOSE_REGION-slave1:60020-2] regionserver.HRegion: Updates disabled for region usertable,user7,1404228265526.667c1f2c34fe37d48ba25a789d8aba98.
2014-07-01 15:51:45,385 FATAL [RS_CLOSE_REGION-slave1:60020-2] regionserver.HRegionServer: ABORTING region server slave1,60020,1404253868846: Unrecoverable exception while closing region usertable,user7,1404228265526.667c1f2c34fe37d48ba25a789d8aba98., still finishing close
java.io.IOException: Aborting flush because server is abortted...
	at org.apache.hadoop.hbase.regionserver.HRegion.internalFlushcache(HRegion.java:1675)
	at org.apache.hadoop.hbase.regionserver.HRegion.internalFlushcache(HRegion.java:1658)
	at org.apache.hadoop.hbase.regionserver.HRegion.doClose(HRegion.java:1153)
	at org.apache.hadoop.hbase.regionserver.HRegion.close(HRegion.java:1085)
	at org.apache.hadoop.hbase.regionserver.handler.CloseRegionHandler.process(CloseRegionHandler.java:147)
	at org.apache.hadoop.hbase.executor.EventHandler.run(EventHandler.java:128)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1146)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:701)
2014-07-01 15:51:45,387 FATAL [RS_CLOSE_REGION-slave1:60020-2] regionserver.HRegionServer: RegionServer abort: loaded coprocessors are: []
2014-07-01 15:51:45,391 ERROR [RS_CLOSE_REGION-slave1:60020-2] executor.EventHandler: Caught throwable while processing event M_RS_CLOSE_REGION
java.lang.RuntimeException: java.io.IOException: Aborting flush because server is abortted...
	at org.apache.hadoop.hbase.regionserver.handler.CloseRegionHandler.process(CloseRegionHandler.java:161)
	at org.apache.hadoop.hbase.executor.EventHandler.run(EventHandler.java:128)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1146)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:701)
Caused by: java.io.IOException: Aborting flush because server is abortted...
	at org.apache.hadoop.hbase.regionserver.HRegion.internalFlushcache(HRegion.java:1675)
	at org.apache.hadoop.hbase.regionserver.HRegion.internalFlushcache(HRegion.java:1658)
	at org.apache.hadoop.hbase.regionserver.HRegion.doClose(HRegion.java:1153)
	at org.apache.hadoop.hbase.regionserver.HRegion.close(HRegion.java:1085)
	at org.apache.hadoop.hbase.regionserver.handler.CloseRegionHandler.process(CloseRegionHandler.java:147)
	... 4 more
2014-07-01 15:51:45,451 INFO  [regionserver60020.leaseChecker] regionserver.Leases: regionserver60020.leaseChecker closing leases
2014-07-01 15:51:45,451 INFO  [regionserver60020.leaseChecker] regionserver.Leases: regionserver60020.leaseChecker closed leases
2014-07-01 15:51:45,512 INFO  [MemStoreFlusher.0] regionserver.MemStoreFlusher: MemStoreFlusher.0 exiting
2014-07-01 15:51:45,512 INFO  [MemStoreFlusher.1] regionserver.MemStoreFlusher: MemStoreFlusher.1 exiting
2014-07-01 15:51:45,512 INFO  [regionserver60020.compactionChecker] regionserver.HRegionServer$CompactionChecker: regionserver60020.compactionChecker exiting
2014-07-01 15:51:45,512 INFO  [regionserver60020] snapshot.RegionServerSnapshotManager: Stopping RegionServerSnapshotManager abruptly.
2014-07-01 15:51:45,512 INFO  [regionserver60020.logRoller] regionserver.LogRoller: LogRoller exiting.
2014-07-01 15:51:45,513 INFO  [regionserver60020.nonceCleaner] regionserver.ServerNonceManager$1: regionserver60020.nonceCleaner exiting
2014-07-01 15:51:45,513 INFO  [regionserver60020] regionserver.HRegionServer: aborting server slave1,60020,1404253868846
2014-07-01 15:51:45,513 DEBUG [regionserver60020] catalog.CatalogTracker: Stopping catalog tracker org.apache.hadoop.hbase.catalog.CatalogTracker@5589aea3
2014-07-01 15:51:45,514 INFO  [regionserver60020] client.HConnectionManager$HConnectionImplementation: Closing zookeeper sessionid=0x46f40d941a0003
2014-07-01 15:51:45,519 INFO  [regionserver60020] zookeeper.ZooKeeper: Session: 0x46f40d941a0003 closed
2014-07-01 15:51:45,520 INFO  [regionserver60020-EventThread] zookeeper.ClientCnxn: EventThread shut down
2014-07-01 15:51:45,547 INFO  [regionserver60020] regionserver.HRegionServer: Waiting on 5 regions to close
2014-07-01 15:51:45,547 DEBUG [regionserver60020] regionserver.HRegionServer: {122e92eabfab38cb15a9965b21db25c5=usertable,user5,1404228265526.122e92eabfab38cb15a9965b21db25c5., bb79ae773f71289e0745068f68a0abd9=usertable,user9,1404228265526.bb79ae773f71289e0745068f68a0abd9., 9cef9e7d34f96f1d9b111be50040a208=usertable,,1404228265525.9cef9e7d34f96f1d9b111be50040a208., 93253e37c284f3174309fede3f32339f=usertable,user3,1404228265526.93253e37c284f3174309fede3f32339f., 667c1f2c34fe37d48ba25a789d8aba98=usertable,user7,1404228265526.667c1f2c34fe37d48ba25a789d8aba98.}
2014-07-01 15:51:45,548 DEBUG [RS_CLOSE_REGION-slave1:60020-2] handler.CloseRegionHandler: Processing close of usertable,,1404228265525.9cef9e7d34f96f1d9b111be50040a208.
2014-07-01 15:51:45,548 DEBUG [RS_CLOSE_REGION-slave1:60020-2] regionserver.HRegion: Closing usertable,,1404228265525.9cef9e7d34f96f1d9b111be50040a208.: disabling compactions & flushes
2014-07-01 15:51:45,548 DEBUG [RS_CLOSE_REGION-slave1:60020-2] regionserver.HRegion: Updates disabled for region usertable,,1404228265525.9cef9e7d34f96f1d9b111be50040a208.
2014-07-01 15:51:45,580 INFO  [StoreCloserThread-usertable,,1404228265525.9cef9e7d34f96f1d9b111be50040a208.-1] regionserver.HStore: Closed family
2014-07-01 15:51:45,581 ERROR [RS_CLOSE_REGION-slave1:60020-2] regionserver.HRegion: Memstore size is 157756880
2014-07-01 15:51:45,598 INFO  [RS_CLOSE_REGION-slave1:60020-2] regionserver.HRegion: Closed usertable,,1404228265525.9cef9e7d34f96f1d9b111be50040a208.
2014-07-01 15:51:45,599 DEBUG [RS_CLOSE_REGION-slave1:60020-2] handler.CloseRegionHandler: Closed usertable,,1404228265525.9cef9e7d34f96f1d9b111be50040a208.
2014-07-01 15:51:45,599 DEBUG [RS_CLOSE_REGION-slave1:60020-2] handler.CloseRegionHandler: Processing close of usertable,user3,1404228265526.93253e37c284f3174309fede3f32339f.
2014-07-01 15:51:45,599 DEBUG [RS_CLOSE_REGION-slave1:60020-2] regionserver.HRegion: Closing usertable,user3,1404228265526.93253e37c284f3174309fede3f32339f.: disabling compactions & flushes
2014-07-01 15:51:45,599 DEBUG [RS_CLOSE_REGION-slave1:60020-2] regionserver.HRegion: Updates disabled for region usertable,user3,1404228265526.93253e37c284f3174309fede3f32339f.
2014-07-01 15:51:45,611 INFO  [StoreCloserThread-usertable,user3,1404228265526.93253e37c284f3174309fede3f32339f.-1] regionserver.HStore: Closed family
2014-07-01 15:51:45,612 ERROR [RS_CLOSE_REGION-slave1:60020-2] regionserver.HRegion: Memstore size is 167871120
2014-07-01 15:51:45,612 INFO  [RS_CLOSE_REGION-slave1:60020-2] regionserver.HRegion: Closed usertable,user3,1404228265526.93253e37c284f3174309fede3f32339f.
2014-07-01 15:51:45,613 DEBUG [RS_CLOSE_REGION-slave1:60020-2] handler.CloseRegionHandler: Closed usertable,user3,1404228265526.93253e37c284f3174309fede3f32339f.
2014-07-01 15:51:45,613 DEBUG [RS_CLOSE_REGION-slave1:60020-2] handler.CloseRegionHandler: Processing close of usertable,user7,1404228265526.667c1f2c34fe37d48ba25a789d8aba98.
2014-07-01 15:51:45,613 DEBUG [RS_CLOSE_REGION-slave1:60020-2] regionserver.HRegion: Closing usertable,user7,1404228265526.667c1f2c34fe37d48ba25a789d8aba98.: disabling compactions & flushes
2014-07-01 15:51:45,613 DEBUG [RS_CLOSE_REGION-slave1:60020-2] regionserver.HRegion: Updates disabled for region usertable,user7,1404228265526.667c1f2c34fe37d48ba25a789d8aba98.
2014-07-01 15:51:45,622 INFO  [StoreCloserThread-usertable,user7,1404228265526.667c1f2c34fe37d48ba25a789d8aba98.-1] regionserver.HStore: Closed family
2014-07-01 15:51:45,622 ERROR [RS_CLOSE_REGION-slave1:60020-2] regionserver.HRegion: Memstore size is 20441680
2014-07-01 15:51:45,623 INFO  [RS_CLOSE_REGION-slave1:60020-2] regionserver.HRegion: Closed usertable,user7,1404228265526.667c1f2c34fe37d48ba25a789d8aba98.
2014-07-01 15:51:45,623 DEBUG [RS_CLOSE_REGION-slave1:60020-2] handler.CloseRegionHandler: Closed usertable,user7,1404228265526.667c1f2c34fe37d48ba25a789d8aba98.
2014-07-01 15:51:45,786 DEBUG [RS_CLOSE_REGION-slave1:60020-0] util.FSUtils: DFS Client does not support most favored nodes create; using default create
2014-07-01 15:51:45,789 WARN  [RS_CLOSE_REGION-slave1:60020-0] regionserver.HStore: Failed flushing store file, retrying num=7
org.apache.hadoop.ipc.RemoteException: org.apache.hadoop.hdfs.server.namenode.SafeModeException: Cannot create /hbase/data/default/usertable/122e92eabfab38cb15a9965b21db25c5/.tmp/07208daa61f341308aeaba8df12e4bfa. Name node is in safe mode.
The reported blocks 486 has reached the threshold 0.9990 of total blocks 481. Safe mode will be turned off automatically in 18 seconds.
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.startFileInternal(FSNamesystem.java:1561)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.startFile(FSNamesystem.java:1527)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.create(NameNode.java:710)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.create(NameNode.java:689)
	at sun.reflect.GeneratedMethodAccessor3.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:622)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:587)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1432)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1428)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:416)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1190)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:1426)

	at org.apache.hadoop.ipc.Client.call(Client.java:1113)
	at org.apache.hadoop.ipc.RPC$Invoker.invoke(RPC.java:229)
	at com.sun.proxy.$Proxy8.create(Unknown Source)
	at sun.reflect.GeneratedMethodAccessor26.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:622)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:85)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:62)
	at com.sun.proxy.$Proxy8.create(Unknown Source)
	at sun.reflect.GeneratedMethodAccessor26.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:622)
	at org.apache.hadoop.hbase.fs.HFileSystem$1.invoke(HFileSystem.java:294)
	at com.sun.proxy.$Proxy9.create(Unknown Source)
	at org.apache.hadoop.hdfs.DFSClient$DFSOutputStream.<init>(DFSClient.java:3451)
	at org.apache.hadoop.hdfs.DFSClient.create(DFSClient.java:870)
	at org.apache.hadoop.hdfs.DistributedFileSystem.create(DistributedFileSystem.java:205)
	at org.apache.hadoop.fs.FilterFileSystem.create(FilterFileSystem.java:126)
	at org.apache.hadoop.hbase.util.FSUtils.create(FSUtils.java:341)
	at org.apache.hadoop.hbase.util.FSUtils.create(FSUtils.java:316)
	at org.apache.hadoop.hbase.io.hfile.AbstractHFileWriter.createOutputStream(AbstractHFileWriter.java:266)
	at org.apache.hadoop.hbase.io.hfile.HFile$WriterFactory.create(HFile.java:391)
	at org.apache.hadoop.hbase.regionserver.StoreFile$Writer.<init>(StoreFile.java:728)
	at org.apache.hadoop.hbase.regionserver.StoreFile$Writer.<init>(StoreFile.java:679)
	at org.apache.hadoop.hbase.regionserver.StoreFile$WriterBuilder.build(StoreFile.java:617)
	at org.apache.hadoop.hbase.regionserver.HStore.createWriterInTmp(HStore.java:889)
	at org.apache.hadoop.hbase.regionserver.DefaultStoreFlusher.flushSnapshot(DefaultStoreFlusher.java:69)
	at org.apache.hadoop.hbase.regionserver.HStore.flushCache(HStore.java:798)
	at org.apache.hadoop.hbase.regionserver.HStore$StoreFlusherImpl.flushCache(HStore.java:1949)
	at org.apache.hadoop.hbase.regionserver.HRegion.internalFlushcache(HRegion.java:1776)
	at org.apache.hadoop.hbase.regionserver.HRegion.internalFlushcache(HRegion.java:1658)
	at org.apache.hadoop.hbase.regionserver.HRegion.doClose(HRegion.java:1153)
	at org.apache.hadoop.hbase.regionserver.HRegion.close(HRegion.java:1085)
	at org.apache.hadoop.hbase.regionserver.handler.CloseRegionHandler.process(CloseRegionHandler.java:147)
	at org.apache.hadoop.hbase.executor.EventHandler.run(EventHandler.java:128)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1146)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:701)
2014-07-01 15:51:46,355 WARN  [org.apache.hadoop.hdfs.LeaseRenewer$1@4fae46d5] hdfs.LeaseRenewer: Failed to renew lease for [DFSClient_hb_rs_slave1,60020,1404253868846_-130361673_31] for 30 seconds.  Will retry shortly ...
org.apache.hadoop.ipc.RemoteException: org.apache.hadoop.hdfs.server.namenode.SafeModeException: Cannot renew lease for DFSClient_hb_rs_slave1,60020,1404253868846_-130361673_31. Name node is in safe mode.
The reported blocks 486 has reached the threshold 0.9990 of total blocks 481. Safe mode will be turned off automatically in 17 seconds.
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.renewLease(FSNamesystem.java:2771)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.renewLease(NameNode.java:919)
	at sun.reflect.GeneratedMethodAccessor4.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:622)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:587)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1432)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1428)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:416)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1190)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:1426)

	at org.apache.hadoop.ipc.Client.call(Client.java:1113)
	at org.apache.hadoop.ipc.RPC$Invoker.invoke(RPC.java:229)
	at com.sun.proxy.$Proxy8.renewLease(Unknown Source)
	at sun.reflect.GeneratedMethodAccessor29.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:622)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:85)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:62)
	at com.sun.proxy.$Proxy8.renewLease(Unknown Source)
	at sun.reflect.GeneratedMethodAccessor29.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:622)
	at org.apache.hadoop.hbase.fs.HFileSystem$1.invoke(HFileSystem.java:294)
	at com.sun.proxy.$Proxy9.renewLease(Unknown Source)
	at org.apache.hadoop.hdfs.DFSClient.renewLease(DFSClient.java:379)
	at org.apache.hadoop.hdfs.LeaseRenewer.renew(LeaseRenewer.java:378)
	at org.apache.hadoop.hdfs.LeaseRenewer.run(LeaseRenewer.java:400)
	at org.apache.hadoop.hdfs.LeaseRenewer.access$600(LeaseRenewer.java:69)
	at org.apache.hadoop.hdfs.LeaseRenewer$1.run(LeaseRenewer.java:273)
	at java.lang.Thread.run(Thread.java:701)
2014-07-01 15:51:46,400 DEBUG [RS_CLOSE_REGION-slave1:60020-1] util.FSUtils: DFS Client does not support most favored nodes create; using default create
2014-07-01 15:51:46,402 WARN  [RS_CLOSE_REGION-slave1:60020-1] regionserver.HStore: Failed flushing store file, retrying num=9
org.apache.hadoop.ipc.RemoteException: org.apache.hadoop.hdfs.server.namenode.SafeModeException: Cannot create /hbase/data/default/usertable/bb79ae773f71289e0745068f68a0abd9/.tmp/98645a810534460ab0d9be134a2538ea. Name node is in safe mode.
The reported blocks 486 has reached the threshold 0.9990 of total blocks 481. Safe mode will be turned off automatically in 17 seconds.
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.startFileInternal(FSNamesystem.java:1561)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.startFile(FSNamesystem.java:1527)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.create(NameNode.java:710)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.create(NameNode.java:689)
	at sun.reflect.GeneratedMethodAccessor3.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:622)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:587)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1432)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1428)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:416)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1190)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:1426)

	at org.apache.hadoop.ipc.Client.call(Client.java:1113)
	at org.apache.hadoop.ipc.RPC$Invoker.invoke(RPC.java:229)
	at com.sun.proxy.$Proxy8.create(Unknown Source)
	at sun.reflect.GeneratedMethodAccessor26.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:622)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:85)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:62)
	at com.sun.proxy.$Proxy8.create(Unknown Source)
	at sun.reflect.GeneratedMethodAccessor26.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:622)
	at org.apache.hadoop.hbase.fs.HFileSystem$1.invoke(HFileSystem.java:294)
	at com.sun.proxy.$Proxy9.create(Unknown Source)
	at org.apache.hadoop.hdfs.DFSClient$DFSOutputStream.<init>(DFSClient.java:3451)
	at org.apache.hadoop.hdfs.DFSClient.create(DFSClient.java:870)
	at org.apache.hadoop.hdfs.DistributedFileSystem.create(DistributedFileSystem.java:205)
	at org.apache.hadoop.fs.FilterFileSystem.create(FilterFileSystem.java:126)
	at org.apache.hadoop.hbase.util.FSUtils.create(FSUtils.java:341)
	at org.apache.hadoop.hbase.util.FSUtils.create(FSUtils.java:316)
	at org.apache.hadoop.hbase.io.hfile.AbstractHFileWriter.createOutputStream(AbstractHFileWriter.java:266)
	at org.apache.hadoop.hbase.io.hfile.HFile$WriterFactory.create(HFile.java:391)
	at org.apache.hadoop.hbase.regionserver.StoreFile$Writer.<init>(StoreFile.java:728)
	at org.apache.hadoop.hbase.regionserver.StoreFile$Writer.<init>(StoreFile.java:679)
	at org.apache.hadoop.hbase.regionserver.StoreFile$WriterBuilder.build(StoreFile.java:617)
	at org.apache.hadoop.hbase.regionserver.HStore.createWriterInTmp(HStore.java:889)
	at org.apache.hadoop.hbase.regionserver.DefaultStoreFlusher.flushSnapshot(DefaultStoreFlusher.java:69)
	at org.apache.hadoop.hbase.regionserver.HStore.flushCache(HStore.java:798)
	at org.apache.hadoop.hbase.regionserver.HStore$StoreFlusherImpl.flushCache(HStore.java:1949)
	at org.apache.hadoop.hbase.regionserver.HRegion.internalFlushcache(HRegion.java:1776)
	at org.apache.hadoop.hbase.regionserver.HRegion.internalFlushcache(HRegion.java:1658)
	at org.apache.hadoop.hbase.regionserver.HRegion.doClose(HRegion.java:1153)
	at org.apache.hadoop.hbase.regionserver.HRegion.close(HRegion.java:1085)
	at org.apache.hadoop.hbase.regionserver.handler.CloseRegionHandler.process(CloseRegionHandler.java:147)
	at org.apache.hadoop.hbase.executor.EventHandler.run(EventHandler.java:128)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1146)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:701)
2014-07-01 15:51:46,403 FATAL [RS_CLOSE_REGION-slave1:60020-1] regionserver.HRegionServer: ABORTING region server slave1,60020,1404253868846: Unrecoverable exception while closing region usertable,user9,1404228265526.bb79ae773f71289e0745068f68a0abd9., still finishing close
org.apache.hadoop.hbase.DroppedSnapshotException: region: usertable,user9,1404228265526.bb79ae773f71289e0745068f68a0abd9.
	at org.apache.hadoop.hbase.regionserver.HRegion.internalFlushcache(HRegion.java:1801)
	at org.apache.hadoop.hbase.regionserver.HRegion.internalFlushcache(HRegion.java:1658)
	at org.apache.hadoop.hbase.regionserver.HRegion.doClose(HRegion.java:1153)
	at org.apache.hadoop.hbase.regionserver.HRegion.close(HRegion.java:1085)
	at org.apache.hadoop.hbase.regionserver.handler.CloseRegionHandler.process(CloseRegionHandler.java:147)
	at org.apache.hadoop.hbase.executor.EventHandler.run(EventHandler.java:128)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1146)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:701)
Caused by: org.apache.hadoop.ipc.RemoteException: org.apache.hadoop.hdfs.server.namenode.SafeModeException: Cannot create /hbase/data/default/usertable/bb79ae773f71289e0745068f68a0abd9/.tmp/98645a810534460ab0d9be134a2538ea. Name node is in safe mode.
The reported blocks 486 has reached the threshold 0.9990 of total blocks 481. Safe mode will be turned off automatically in 17 seconds.
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.startFileInternal(FSNamesystem.java:1561)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.startFile(FSNamesystem.java:1527)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.create(NameNode.java:710)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.create(NameNode.java:689)
	at sun.reflect.GeneratedMethodAccessor3.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:622)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:587)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1432)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1428)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:416)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1190)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:1426)

	at org.apache.hadoop.ipc.Client.call(Client.java:1113)
	at org.apache.hadoop.ipc.RPC$Invoker.invoke(RPC.java:229)
	at com.sun.proxy.$Proxy8.create(Unknown Source)
	at sun.reflect.GeneratedMethodAccessor26.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:622)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:85)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:62)
	at com.sun.proxy.$Proxy8.create(Unknown Source)
	at sun.reflect.GeneratedMethodAccessor26.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:622)
	at org.apache.hadoop.hbase.fs.HFileSystem$1.invoke(HFileSystem.java:294)
	at com.sun.proxy.$Proxy9.create(Unknown Source)
	at org.apache.hadoop.hdfs.DFSClient$DFSOutputStream.<init>(DFSClient.java:3451)
	at org.apache.hadoop.hdfs.DFSClient.create(DFSClient.java:870)
	at org.apache.hadoop.hdfs.DistributedFileSystem.create(DistributedFileSystem.java:205)
	at org.apache.hadoop.fs.FilterFileSystem.create(FilterFileSystem.java:126)
	at org.apache.hadoop.hbase.util.FSUtils.create(FSUtils.java:341)
	at org.apache.hadoop.hbase.util.FSUtils.create(FSUtils.java:316)
	at org.apache.hadoop.hbase.io.hfile.AbstractHFileWriter.createOutputStream(AbstractHFileWriter.java:266)
	at org.apache.hadoop.hbase.io.hfile.HFile$WriterFactory.create(HFile.java:391)
	at org.apache.hadoop.hbase.regionserver.StoreFile$Writer.<init>(StoreFile.java:728)
	at org.apache.hadoop.hbase.regionserver.StoreFile$Writer.<init>(StoreFile.java:679)
	at org.apache.hadoop.hbase.regionserver.StoreFile$WriterBuilder.build(StoreFile.java:617)
	at org.apache.hadoop.hbase.regionserver.HStore.createWriterInTmp(HStore.java:889)
	at org.apache.hadoop.hbase.regionserver.DefaultStoreFlusher.flushSnapshot(DefaultStoreFlusher.java:69)
	at org.apache.hadoop.hbase.regionserver.HStore.flushCache(HStore.java:798)
	at org.apache.hadoop.hbase.regionserver.HStore$StoreFlusherImpl.flushCache(HStore.java:1949)
	at org.apache.hadoop.hbase.regionserver.HRegion.internalFlushcache(HRegion.java:1776)
	... 8 more
2014-07-01 15:51:46,404 FATAL [RS_CLOSE_REGION-slave1:60020-1] regionserver.HRegionServer: RegionServer abort: loaded coprocessors are: []
2014-07-01 15:51:46,407 ERROR [RS_CLOSE_REGION-slave1:60020-1] executor.EventHandler: Caught throwable while processing event M_RS_CLOSE_REGION
java.lang.RuntimeException: org.apache.hadoop.hbase.DroppedSnapshotException: region: usertable,user9,1404228265526.bb79ae773f71289e0745068f68a0abd9.
	at org.apache.hadoop.hbase.regionserver.handler.CloseRegionHandler.process(CloseRegionHandler.java:161)
	at org.apache.hadoop.hbase.executor.EventHandler.run(EventHandler.java:128)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1146)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:701)
Caused by: org.apache.hadoop.hbase.DroppedSnapshotException: region: usertable,user9,1404228265526.bb79ae773f71289e0745068f68a0abd9.
	at org.apache.hadoop.hbase.regionserver.HRegion.internalFlushcache(HRegion.java:1801)
	at org.apache.hadoop.hbase.regionserver.HRegion.internalFlushcache(HRegion.java:1658)
	at org.apache.hadoop.hbase.regionserver.HRegion.doClose(HRegion.java:1153)
	at org.apache.hadoop.hbase.regionserver.HRegion.close(HRegion.java:1085)
	at org.apache.hadoop.hbase.regionserver.handler.CloseRegionHandler.process(CloseRegionHandler.java:147)
	... 4 more
Caused by: org.apache.hadoop.ipc.RemoteException: org.apache.hadoop.hdfs.server.namenode.SafeModeException: Cannot create /hbase/data/default/usertable/bb79ae773f71289e0745068f68a0abd9/.tmp/98645a810534460ab0d9be134a2538ea. Name node is in safe mode.
The reported blocks 486 has reached the threshold 0.9990 of total blocks 481. Safe mode will be turned off automatically in 17 seconds.
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.startFileInternal(FSNamesystem.java:1561)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.startFile(FSNamesystem.java:1527)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.create(NameNode.java:710)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.create(NameNode.java:689)
	at sun.reflect.GeneratedMethodAccessor3.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:622)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:587)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1432)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1428)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:416)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1190)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:1426)

	at org.apache.hadoop.ipc.Client.call(Client.java:1113)
	at org.apache.hadoop.ipc.RPC$Invoker.invoke(RPC.java:229)
	at com.sun.proxy.$Proxy8.create(Unknown Source)
	at sun.reflect.GeneratedMethodAccessor26.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:622)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:85)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:62)
	at com.sun.proxy.$Proxy8.create(Unknown Source)
	at sun.reflect.GeneratedMethodAccessor26.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:622)
	at org.apache.hadoop.hbase.fs.HFileSystem$1.invoke(HFileSystem.java:294)
	at com.sun.proxy.$Proxy9.create(Unknown Source)
	at org.apache.hadoop.hdfs.DFSClient$DFSOutputStream.<init>(DFSClient.java:3451)
	at org.apache.hadoop.hdfs.DFSClient.create(DFSClient.java:870)
	at org.apache.hadoop.hdfs.DistributedFileSystem.create(DistributedFileSystem.java:205)
	at org.apache.hadoop.fs.FilterFileSystem.create(FilterFileSystem.java:126)
	at org.apache.hadoop.hbase.util.FSUtils.create(FSUtils.java:341)
	at org.apache.hadoop.hbase.util.FSUtils.create(FSUtils.java:316)
	at org.apache.hadoop.hbase.io.hfile.AbstractHFileWriter.createOutputStream(AbstractHFileWriter.java:266)
	at org.apache.hadoop.hbase.io.hfile.HFile$WriterFactory.create(HFile.java:391)
	at org.apache.hadoop.hbase.regionserver.StoreFile$Writer.<init>(StoreFile.java:728)
	at org.apache.hadoop.hbase.regionserver.StoreFile$Writer.<init>(StoreFile.java:679)
	at org.apache.hadoop.hbase.regionserver.StoreFile$WriterBuilder.build(StoreFile.java:617)
	at org.apache.hadoop.hbase.regionserver.HStore.createWriterInTmp(HStore.java:889)
	at org.apache.hadoop.hbase.regionserver.DefaultStoreFlusher.flushSnapshot(DefaultStoreFlusher.java:69)
	at org.apache.hadoop.hbase.regionserver.HStore.flushCache(HStore.java:798)
	at org.apache.hadoop.hbase.regionserver.HStore$StoreFlusherImpl.flushCache(HStore.java:1949)
	at org.apache.hadoop.hbase.regionserver.HRegion.internalFlushcache(HRegion.java:1776)
	... 8 more
2014-07-01 15:51:46,549 INFO  [regionserver60020] regionserver.HRegionServer: Waiting on 2 regions to close
2014-07-01 15:51:46,549 DEBUG [regionserver60020] regionserver.HRegionServer: {122e92eabfab38cb15a9965b21db25c5=usertable,user5,1404228265526.122e92eabfab38cb15a9965b21db25c5., bb79ae773f71289e0745068f68a0abd9=usertable,user9,1404228265526.bb79ae773f71289e0745068f68a0abd9.}
2014-07-01 15:51:46,549 DEBUG [RS_CLOSE_REGION-slave1:60020-2] handler.CloseRegionHandler: Processing close of usertable,user9,1404228265526.bb79ae773f71289e0745068f68a0abd9.
2014-07-01 15:51:46,549 DEBUG [RS_CLOSE_REGION-slave1:60020-2] regionserver.HRegion: Closing usertable,user9,1404228265526.bb79ae773f71289e0745068f68a0abd9.: disabling compactions & flushes
2014-07-01 15:51:46,549 DEBUG [RS_CLOSE_REGION-slave1:60020-2] regionserver.HRegion: Updates disabled for region usertable,user9,1404228265526.bb79ae773f71289e0745068f68a0abd9.
2014-07-01 15:51:46,554 INFO  [StoreCloserThread-usertable,user9,1404228265526.bb79ae773f71289e0745068f68a0abd9.-1] regionserver.HStore: Closed family
2014-07-01 15:51:46,555 ERROR [RS_CLOSE_REGION-slave1:60020-2] regionserver.HRegion: Memstore size is 141321360
2014-07-01 15:51:46,555 INFO  [RS_CLOSE_REGION-slave1:60020-2] regionserver.HRegion: Closed usertable,user9,1404228265526.bb79ae773f71289e0745068f68a0abd9.
2014-07-01 15:51:46,556 DEBUG [RS_CLOSE_REGION-slave1:60020-2] handler.CloseRegionHandler: Closed usertable,user9,1404228265526.bb79ae773f71289e0745068f68a0abd9.
2014-07-01 15:51:46,813 DEBUG [RS_CLOSE_REGION-slave1:60020-0] util.FSUtils: DFS Client does not support most favored nodes create; using default create
2014-07-01 15:51:46,816 WARN  [RS_CLOSE_REGION-slave1:60020-0] regionserver.HStore: Failed flushing store file, retrying num=8
org.apache.hadoop.ipc.RemoteException: org.apache.hadoop.hdfs.server.namenode.SafeModeException: Cannot create /hbase/data/default/usertable/122e92eabfab38cb15a9965b21db25c5/.tmp/43a623f3b91f4ec1a561ebe0cd0fb023. Name node is in safe mode.
The reported blocks 486 has reached the threshold 0.9990 of total blocks 481. Safe mode will be turned off automatically in 17 seconds.
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.startFileInternal(FSNamesystem.java:1561)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.startFile(FSNamesystem.java:1527)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.create(NameNode.java:710)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.create(NameNode.java:689)
	at sun.reflect.GeneratedMethodAccessor3.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:622)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:587)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1432)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1428)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:416)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1190)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:1426)

	at org.apache.hadoop.ipc.Client.call(Client.java:1113)
	at org.apache.hadoop.ipc.RPC$Invoker.invoke(RPC.java:229)
	at com.sun.proxy.$Proxy8.create(Unknown Source)
	at sun.reflect.GeneratedMethodAccessor26.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:622)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:85)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:62)
	at com.sun.proxy.$Proxy8.create(Unknown Source)
	at sun.reflect.GeneratedMethodAccessor26.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:622)
	at org.apache.hadoop.hbase.fs.HFileSystem$1.invoke(HFileSystem.java:294)
	at com.sun.proxy.$Proxy9.create(Unknown Source)
	at org.apache.hadoop.hdfs.DFSClient$DFSOutputStream.<init>(DFSClient.java:3451)
	at org.apache.hadoop.hdfs.DFSClient.create(DFSClient.java:870)
	at org.apache.hadoop.hdfs.DistributedFileSystem.create(DistributedFileSystem.java:205)
	at org.apache.hadoop.fs.FilterFileSystem.create(FilterFileSystem.java:126)
	at org.apache.hadoop.hbase.util.FSUtils.create(FSUtils.java:341)
	at org.apache.hadoop.hbase.util.FSUtils.create(FSUtils.java:316)
	at org.apache.hadoop.hbase.io.hfile.AbstractHFileWriter.createOutputStream(AbstractHFileWriter.java:266)
	at org.apache.hadoop.hbase.io.hfile.HFile$WriterFactory.create(HFile.java:391)
	at org.apache.hadoop.hbase.regionserver.StoreFile$Writer.<init>(StoreFile.java:728)
	at org.apache.hadoop.hbase.regionserver.StoreFile$Writer.<init>(StoreFile.java:679)
	at org.apache.hadoop.hbase.regionserver.StoreFile$WriterBuilder.build(StoreFile.java:617)
	at org.apache.hadoop.hbase.regionserver.HStore.createWriterInTmp(HStore.java:889)
	at org.apache.hadoop.hbase.regionserver.DefaultStoreFlusher.flushSnapshot(DefaultStoreFlusher.java:69)
	at org.apache.hadoop.hbase.regionserver.HStore.flushCache(HStore.java:798)
	at org.apache.hadoop.hbase.regionserver.HStore$StoreFlusherImpl.flushCache(HStore.java:1949)
	at org.apache.hadoop.hbase.regionserver.HRegion.internalFlushcache(HRegion.java:1776)
	at org.apache.hadoop.hbase.regionserver.HRegion.internalFlushcache(HRegion.java:1658)
	at org.apache.hadoop.hbase.regionserver.HRegion.doClose(HRegion.java:1153)
	at org.apache.hadoop.hbase.regionserver.HRegion.close(HRegion.java:1085)
	at org.apache.hadoop.hbase.regionserver.handler.CloseRegionHandler.process(CloseRegionHandler.java:147)
	at org.apache.hadoop.hbase.executor.EventHandler.run(EventHandler.java:128)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1146)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:701)
2014-07-01 15:51:47,358 WARN  [org.apache.hadoop.hdfs.LeaseRenewer$1@4fae46d5] hdfs.LeaseRenewer: Failed to renew lease for [DFSClient_hb_rs_slave1,60020,1404253868846_-130361673_31] for 30 seconds.  Will retry shortly ...
org.apache.hadoop.ipc.RemoteException: org.apache.hadoop.hdfs.server.namenode.SafeModeException: Cannot renew lease for DFSClient_hb_rs_slave1,60020,1404253868846_-130361673_31. Name node is in safe mode.
The reported blocks 486 has reached the threshold 0.9990 of total blocks 481. Safe mode will be turned off automatically in 16 seconds.
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.renewLease(FSNamesystem.java:2771)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.renewLease(NameNode.java:919)
	at sun.reflect.GeneratedMethodAccessor4.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:622)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:587)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1432)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1428)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:416)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1190)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:1426)

	at org.apache.hadoop.ipc.Client.call(Client.java:1113)
	at org.apache.hadoop.ipc.RPC$Invoker.invoke(RPC.java:229)
	at com.sun.proxy.$Proxy8.renewLease(Unknown Source)
	at sun.reflect.GeneratedMethodAccessor29.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:622)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:85)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:62)
	at com.sun.proxy.$Proxy8.renewLease(Unknown Source)
	at sun.reflect.GeneratedMethodAccessor29.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:622)
	at org.apache.hadoop.hbase.fs.HFileSystem$1.invoke(HFileSystem.java:294)
	at com.sun.proxy.$Proxy9.renewLease(Unknown Source)
	at org.apache.hadoop.hdfs.DFSClient.renewLease(DFSClient.java:379)
	at org.apache.hadoop.hdfs.LeaseRenewer.renew(LeaseRenewer.java:378)
	at org.apache.hadoop.hdfs.LeaseRenewer.run(LeaseRenewer.java:400)
	at org.apache.hadoop.hdfs.LeaseRenewer.access$600(LeaseRenewer.java:69)
	at org.apache.hadoop.hdfs.LeaseRenewer$1.run(LeaseRenewer.java:273)
	at java.lang.Thread.run(Thread.java:701)
2014-07-01 15:51:47,550 INFO  [regionserver60020] regionserver.HRegionServer: Waiting on 1 regions to close
2014-07-01 15:51:47,550 DEBUG [regionserver60020] regionserver.HRegionServer: {122e92eabfab38cb15a9965b21db25c5=usertable,user5,1404228265526.122e92eabfab38cb15a9965b21db25c5.}
2014-07-01 15:51:47,832 DEBUG [RS_CLOSE_REGION-slave1:60020-0] util.FSUtils: DFS Client does not support most favored nodes create; using default create
2014-07-01 15:51:47,834 WARN  [RS_CLOSE_REGION-slave1:60020-0] regionserver.HStore: Failed flushing store file, retrying num=9
org.apache.hadoop.ipc.RemoteException: org.apache.hadoop.hdfs.server.namenode.SafeModeException: Cannot create /hbase/data/default/usertable/122e92eabfab38cb15a9965b21db25c5/.tmp/a6ec0a48685a4fa0a1d6b00d45173a2a. Name node is in safe mode.
The reported blocks 486 has reached the threshold 0.9990 of total blocks 481. Safe mode will be turned off automatically in 16 seconds.
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.startFileInternal(FSNamesystem.java:1561)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.startFile(FSNamesystem.java:1527)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.create(NameNode.java:710)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.create(NameNode.java:689)
	at sun.reflect.GeneratedMethodAccessor3.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:622)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:587)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1432)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1428)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:416)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1190)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:1426)

	at org.apache.hadoop.ipc.Client.call(Client.java:1113)
	at org.apache.hadoop.ipc.RPC$Invoker.invoke(RPC.java:229)
	at com.sun.proxy.$Proxy8.create(Unknown Source)
	at sun.reflect.GeneratedMethodAccessor26.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:622)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:85)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:62)
	at com.sun.proxy.$Proxy8.create(Unknown Source)
	at sun.reflect.GeneratedMethodAccessor26.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:622)
	at org.apache.hadoop.hbase.fs.HFileSystem$1.invoke(HFileSystem.java:294)
	at com.sun.proxy.$Proxy9.create(Unknown Source)
	at org.apache.hadoop.hdfs.DFSClient$DFSOutputStream.<init>(DFSClient.java:3451)
	at org.apache.hadoop.hdfs.DFSClient.create(DFSClient.java:870)
	at org.apache.hadoop.hdfs.DistributedFileSystem.create(DistributedFileSystem.java:205)
	at org.apache.hadoop.fs.FilterFileSystem.create(FilterFileSystem.java:126)
	at org.apache.hadoop.hbase.util.FSUtils.create(FSUtils.java:341)
	at org.apache.hadoop.hbase.util.FSUtils.create(FSUtils.java:316)
	at org.apache.hadoop.hbase.io.hfile.AbstractHFileWriter.createOutputStream(AbstractHFileWriter.java:266)
	at org.apache.hadoop.hbase.io.hfile.HFile$WriterFactory.create(HFile.java:391)
	at org.apache.hadoop.hbase.regionserver.StoreFile$Writer.<init>(StoreFile.java:728)
	at org.apache.hadoop.hbase.regionserver.StoreFile$Writer.<init>(StoreFile.java:679)
	at org.apache.hadoop.hbase.regionserver.StoreFile$WriterBuilder.build(StoreFile.java:617)
	at org.apache.hadoop.hbase.regionserver.HStore.createWriterInTmp(HStore.java:889)
	at org.apache.hadoop.hbase.regionserver.DefaultStoreFlusher.flushSnapshot(DefaultStoreFlusher.java:69)
	at org.apache.hadoop.hbase.regionserver.HStore.flushCache(HStore.java:798)
	at org.apache.hadoop.hbase.regionserver.HStore$StoreFlusherImpl.flushCache(HStore.java:1949)
	at org.apache.hadoop.hbase.regionserver.HRegion.internalFlushcache(HRegion.java:1776)
	at org.apache.hadoop.hbase.regionserver.HRegion.internalFlushcache(HRegion.java:1658)
	at org.apache.hadoop.hbase.regionserver.HRegion.doClose(HRegion.java:1153)
	at org.apache.hadoop.hbase.regionserver.HRegion.close(HRegion.java:1085)
	at org.apache.hadoop.hbase.regionserver.handler.CloseRegionHandler.process(CloseRegionHandler.java:147)
	at org.apache.hadoop.hbase.executor.EventHandler.run(EventHandler.java:128)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1146)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:701)
2014-07-01 15:51:47,835 FATAL [RS_CLOSE_REGION-slave1:60020-0] regionserver.HRegionServer: ABORTING region server slave1,60020,1404253868846: Unrecoverable exception while closing region usertable,user5,1404228265526.122e92eabfab38cb15a9965b21db25c5., still finishing close
org.apache.hadoop.hbase.DroppedSnapshotException: region: usertable,user5,1404228265526.122e92eabfab38cb15a9965b21db25c5.
	at org.apache.hadoop.hbase.regionserver.HRegion.internalFlushcache(HRegion.java:1801)
	at org.apache.hadoop.hbase.regionserver.HRegion.internalFlushcache(HRegion.java:1658)
	at org.apache.hadoop.hbase.regionserver.HRegion.doClose(HRegion.java:1153)
	at org.apache.hadoop.hbase.regionserver.HRegion.close(HRegion.java:1085)
	at org.apache.hadoop.hbase.regionserver.handler.CloseRegionHandler.process(CloseRegionHandler.java:147)
	at org.apache.hadoop.hbase.executor.EventHandler.run(EventHandler.java:128)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1146)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:701)
Caused by: org.apache.hadoop.ipc.RemoteException: org.apache.hadoop.hdfs.server.namenode.SafeModeException: Cannot create /hbase/data/default/usertable/122e92eabfab38cb15a9965b21db25c5/.tmp/a6ec0a48685a4fa0a1d6b00d45173a2a. Name node is in safe mode.
The reported blocks 486 has reached the threshold 0.9990 of total blocks 481. Safe mode will be turned off automatically in 16 seconds.
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.startFileInternal(FSNamesystem.java:1561)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.startFile(FSNamesystem.java:1527)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.create(NameNode.java:710)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.create(NameNode.java:689)
	at sun.reflect.GeneratedMethodAccessor3.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:622)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:587)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1432)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1428)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:416)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1190)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:1426)

	at org.apache.hadoop.ipc.Client.call(Client.java:1113)
	at org.apache.hadoop.ipc.RPC$Invoker.invoke(RPC.java:229)
	at com.sun.proxy.$Proxy8.create(Unknown Source)
	at sun.reflect.GeneratedMethodAccessor26.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:622)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:85)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:62)
	at com.sun.proxy.$Proxy8.create(Unknown Source)
	at sun.reflect.GeneratedMethodAccessor26.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:622)
	at org.apache.hadoop.hbase.fs.HFileSystem$1.invoke(HFileSystem.java:294)
	at com.sun.proxy.$Proxy9.create(Unknown Source)
	at org.apache.hadoop.hdfs.DFSClient$DFSOutputStream.<init>(DFSClient.java:3451)
	at org.apache.hadoop.hdfs.DFSClient.create(DFSClient.java:870)
	at org.apache.hadoop.hdfs.DistributedFileSystem.create(DistributedFileSystem.java:205)
	at org.apache.hadoop.fs.FilterFileSystem.create(FilterFileSystem.java:126)
	at org.apache.hadoop.hbase.util.FSUtils.create(FSUtils.java:341)
	at org.apache.hadoop.hbase.util.FSUtils.create(FSUtils.java:316)
	at org.apache.hadoop.hbase.io.hfile.AbstractHFileWriter.createOutputStream(AbstractHFileWriter.java:266)
	at org.apache.hadoop.hbase.io.hfile.HFile$WriterFactory.create(HFile.java:391)
	at org.apache.hadoop.hbase.regionserver.StoreFile$Writer.<init>(StoreFile.java:728)
	at org.apache.hadoop.hbase.regionserver.StoreFile$Writer.<init>(StoreFile.java:679)
	at org.apache.hadoop.hbase.regionserver.StoreFile$WriterBuilder.build(StoreFile.java:617)
	at org.apache.hadoop.hbase.regionserver.HStore.createWriterInTmp(HStore.java:889)
	at org.apache.hadoop.hbase.regionserver.DefaultStoreFlusher.flushSnapshot(DefaultStoreFlusher.java:69)
	at org.apache.hadoop.hbase.regionserver.HStore.flushCache(HStore.java:798)
	at org.apache.hadoop.hbase.regionserver.HStore$StoreFlusherImpl.flushCache(HStore.java:1949)
	at org.apache.hadoop.hbase.regionserver.HRegion.internalFlushcache(HRegion.java:1776)
	... 8 more
2014-07-01 15:51:47,835 FATAL [RS_CLOSE_REGION-slave1:60020-0] regionserver.HRegionServer: RegionServer abort: loaded coprocessors are: []
2014-07-01 15:51:47,839 ERROR [RS_CLOSE_REGION-slave1:60020-0] executor.EventHandler: Caught throwable while processing event M_RS_CLOSE_REGION
java.lang.RuntimeException: org.apache.hadoop.hbase.DroppedSnapshotException: region: usertable,user5,1404228265526.122e92eabfab38cb15a9965b21db25c5.
	at org.apache.hadoop.hbase.regionserver.handler.CloseRegionHandler.process(CloseRegionHandler.java:161)
	at org.apache.hadoop.hbase.executor.EventHandler.run(EventHandler.java:128)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1146)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:701)
Caused by: org.apache.hadoop.hbase.DroppedSnapshotException: region: usertable,user5,1404228265526.122e92eabfab38cb15a9965b21db25c5.
	at org.apache.hadoop.hbase.regionserver.HRegion.internalFlushcache(HRegion.java:1801)
	at org.apache.hadoop.hbase.regionserver.HRegion.internalFlushcache(HRegion.java:1658)
	at org.apache.hadoop.hbase.regionserver.HRegion.doClose(HRegion.java:1153)
	at org.apache.hadoop.hbase.regionserver.HRegion.close(HRegion.java:1085)
	at org.apache.hadoop.hbase.regionserver.handler.CloseRegionHandler.process(CloseRegionHandler.java:147)
	... 4 more
Caused by: org.apache.hadoop.ipc.RemoteException: org.apache.hadoop.hdfs.server.namenode.SafeModeException: Cannot create /hbase/data/default/usertable/122e92eabfab38cb15a9965b21db25c5/.tmp/a6ec0a48685a4fa0a1d6b00d45173a2a. Name node is in safe mode.
The reported blocks 486 has reached the threshold 0.9990 of total blocks 481. Safe mode will be turned off automatically in 16 seconds.
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.startFileInternal(FSNamesystem.java:1561)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.startFile(FSNamesystem.java:1527)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.create(NameNode.java:710)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.create(NameNode.java:689)
	at sun.reflect.GeneratedMethodAccessor3.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:622)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:587)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1432)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1428)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:416)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1190)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:1426)

	at org.apache.hadoop.ipc.Client.call(Client.java:1113)
	at org.apache.hadoop.ipc.RPC$Invoker.invoke(RPC.java:229)
	at com.sun.proxy.$Proxy8.create(Unknown Source)
	at sun.reflect.GeneratedMethodAccessor26.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:622)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:85)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:62)
	at com.sun.proxy.$Proxy8.create(Unknown Source)
	at sun.reflect.GeneratedMethodAccessor26.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:622)
	at org.apache.hadoop.hbase.fs.HFileSystem$1.invoke(HFileSystem.java:294)
	at com.sun.proxy.$Proxy9.create(Unknown Source)
	at org.apache.hadoop.hdfs.DFSClient$DFSOutputStream.<init>(DFSClient.java:3451)
	at org.apache.hadoop.hdfs.DFSClient.create(DFSClient.java:870)
	at org.apache.hadoop.hdfs.DistributedFileSystem.create(DistributedFileSystem.java:205)
	at org.apache.hadoop.fs.FilterFileSystem.create(FilterFileSystem.java:126)
	at org.apache.hadoop.hbase.util.FSUtils.create(FSUtils.java:341)
	at org.apache.hadoop.hbase.util.FSUtils.create(FSUtils.java:316)
	at org.apache.hadoop.hbase.io.hfile.AbstractHFileWriter.createOutputStream(AbstractHFileWriter.java:266)
	at org.apache.hadoop.hbase.io.hfile.HFile$WriterFactory.create(HFile.java:391)
	at org.apache.hadoop.hbase.regionserver.StoreFile$Writer.<init>(StoreFile.java:728)
	at org.apache.hadoop.hbase.regionserver.StoreFile$Writer.<init>(StoreFile.java:679)
	at org.apache.hadoop.hbase.regionserver.StoreFile$WriterBuilder.build(StoreFile.java:617)
	at org.apache.hadoop.hbase.regionserver.HStore.createWriterInTmp(HStore.java:889)
	at org.apache.hadoop.hbase.regionserver.DefaultStoreFlusher.flushSnapshot(DefaultStoreFlusher.java:69)
	at org.apache.hadoop.hbase.regionserver.HStore.flushCache(HStore.java:798)
	at org.apache.hadoop.hbase.regionserver.HStore$StoreFlusherImpl.flushCache(HStore.java:1949)
	at org.apache.hadoop.hbase.regionserver.HRegion.internalFlushcache(HRegion.java:1776)
	... 8 more
2014-07-01 15:51:47,951 DEBUG [RS_CLOSE_REGION-slave1:60020-1] handler.CloseRegionHandler: Processing close of usertable,user5,1404228265526.122e92eabfab38cb15a9965b21db25c5.
2014-07-01 15:51:47,951 DEBUG [RS_CLOSE_REGION-slave1:60020-1] regionserver.HRegion: Closing usertable,user5,1404228265526.122e92eabfab38cb15a9965b21db25c5.: disabling compactions & flushes
2014-07-01 15:51:47,951 DEBUG [RS_CLOSE_REGION-slave1:60020-1] regionserver.HRegion: Updates disabled for region usertable,user5,1404228265526.122e92eabfab38cb15a9965b21db25c5.
2014-07-01 15:51:47,959 INFO  [StoreCloserThread-usertable,user5,1404228265526.122e92eabfab38cb15a9965b21db25c5.-1] regionserver.HStore: Closed family
2014-07-01 15:51:47,960 ERROR [RS_CLOSE_REGION-slave1:60020-1] regionserver.HRegion: Memstore size is 88120160
2014-07-01 15:51:47,961 INFO  [RS_CLOSE_REGION-slave1:60020-1] regionserver.HRegion: Closed usertable,user5,1404228265526.122e92eabfab38cb15a9965b21db25c5.
2014-07-01 15:51:47,961 DEBUG [RS_CLOSE_REGION-slave1:60020-1] handler.CloseRegionHandler: Closed usertable,user5,1404228265526.122e92eabfab38cb15a9965b21db25c5.
2014-07-01 15:51:48,151 INFO  [regionserver60020] regionserver.HRegionServer: stopping server slave1,60020,1404253868846; all regions closed.
2014-07-01 15:51:48,152 DEBUG [regionserver60020-WAL.AsyncNotifier] wal.FSHLog: regionserver60020-WAL.AsyncNotifier interrupted while waiting for  notification from AsyncSyncer thread
2014-07-01 15:51:48,152 INFO  [regionserver60020-WAL.AsyncNotifier] wal.FSHLog: regionserver60020-WAL.AsyncNotifier exiting
2014-07-01 15:51:48,153 DEBUG [regionserver60020-WAL.AsyncSyncer0] wal.FSHLog: regionserver60020-WAL.AsyncSyncer0 interrupted while waiting for notification from AsyncWriter thread
2014-07-01 15:51:48,153 INFO  [regionserver60020-WAL.AsyncSyncer0] wal.FSHLog: regionserver60020-WAL.AsyncSyncer0 exiting
2014-07-01 15:51:48,154 DEBUG [regionserver60020-WAL.AsyncSyncer1] wal.FSHLog: regionserver60020-WAL.AsyncSyncer1 interrupted while waiting for notification from AsyncWriter thread
2014-07-01 15:51:48,154 INFO  [regionserver60020-WAL.AsyncSyncer1] wal.FSHLog: regionserver60020-WAL.AsyncSyncer1 exiting
2014-07-01 15:51:48,166 DEBUG [regionserver60020-WAL.AsyncSyncer2] wal.FSHLog: regionserver60020-WAL.AsyncSyncer2 interrupted while waiting for notification from AsyncWriter thread
2014-07-01 15:51:48,166 INFO  [regionserver60020-WAL.AsyncSyncer2] wal.FSHLog: regionserver60020-WAL.AsyncSyncer2 exiting
2014-07-01 15:51:48,168 DEBUG [regionserver60020-WAL.AsyncSyncer3] wal.FSHLog: regionserver60020-WAL.AsyncSyncer3 interrupted while waiting for notification from AsyncWriter thread
2014-07-01 15:51:48,169 INFO  [regionserver60020-WAL.AsyncSyncer3] wal.FSHLog: regionserver60020-WAL.AsyncSyncer3 exiting
2014-07-01 15:51:48,169 DEBUG [regionserver60020-WAL.AsyncSyncer4] wal.FSHLog: regionserver60020-WAL.AsyncSyncer4 interrupted while waiting for notification from AsyncWriter thread
2014-07-01 15:51:48,169 INFO  [regionserver60020-WAL.AsyncSyncer4] wal.FSHLog: regionserver60020-WAL.AsyncSyncer4 exiting
2014-07-01 15:51:48,170 DEBUG [regionserver60020-WAL.AsyncWriter] wal.FSHLog: regionserver60020-WAL.AsyncWriter interrupted while waiting for newer writes added to local buffer
2014-07-01 15:51:48,170 INFO  [regionserver60020-WAL.AsyncWriter] wal.FSHLog: regionserver60020-WAL.AsyncWriter exiting
2014-07-01 15:51:48,170 DEBUG [regionserver60020] wal.FSHLog: Closing WAL writer in hdfs://master:54310/hbase/WALs/slave1,60020,1404253868846
2014-07-01 15:51:48,171 ERROR [regionserver60020] regionserver.HRegionServer: Close and delete failed
java.io.IOException: Error Recovery for block blk_-3161611795271398729_29985 failed  because recovery from primary datanode 9.1.143.59:50010 failed 6 times.  Pipeline was 9.1.143.59:50010. Aborting...
	at org.apache.hadoop.hdfs.DFSClient$DFSOutputStream.processDatanodeError(DFSClient.java:3355)
	at org.apache.hadoop.hdfs.DFSClient$DFSOutputStream.access$2200(DFSClient.java:2783)
	at org.apache.hadoop.hdfs.DFSClient$DFSOutputStream$DataStreamer.run(DFSClient.java:2987)
2014-07-01 15:51:48,272 INFO  [regionserver60020] regionserver.Leases: regionserver60020 closing leases
2014-07-01 15:51:48,272 INFO  [regionserver60020] regionserver.Leases: regionserver60020 closed leases
2014-07-01 15:51:48,361 WARN  [org.apache.hadoop.hdfs.LeaseRenewer$1@4fae46d5] hdfs.LeaseRenewer: Failed to renew lease for [DFSClient_hb_rs_slave1,60020,1404253868846_-130361673_31] for 30 seconds.  Will retry shortly ...
org.apache.hadoop.ipc.RemoteException: org.apache.hadoop.hdfs.server.namenode.SafeModeException: Cannot renew lease for DFSClient_hb_rs_slave1,60020,1404253868846_-130361673_31. Name node is in safe mode.
The reported blocks 486 has reached the threshold 0.9990 of total blocks 481. Safe mode will be turned off automatically in 15 seconds.
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.renewLease(FSNamesystem.java:2771)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.renewLease(NameNode.java:919)
	at sun.reflect.GeneratedMethodAccessor4.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:622)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:587)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1432)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1428)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:416)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1190)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:1426)

	at org.apache.hadoop.ipc.Client.call(Client.java:1113)
	at org.apache.hadoop.ipc.RPC$Invoker.invoke(RPC.java:229)
	at com.sun.proxy.$Proxy8.renewLease(Unknown Source)
	at sun.reflect.GeneratedMethodAccessor29.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:622)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:85)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:62)
	at com.sun.proxy.$Proxy8.renewLease(Unknown Source)
	at sun.reflect.GeneratedMethodAccessor29.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:622)
	at org.apache.hadoop.hbase.fs.HFileSystem$1.invoke(HFileSystem.java:294)
	at com.sun.proxy.$Proxy9.renewLease(Unknown Source)
	at org.apache.hadoop.hdfs.DFSClient.renewLease(DFSClient.java:379)
	at org.apache.hadoop.hdfs.LeaseRenewer.renew(LeaseRenewer.java:378)
	at org.apache.hadoop.hdfs.LeaseRenewer.run(LeaseRenewer.java:400)
	at org.apache.hadoop.hdfs.LeaseRenewer.access$600(LeaseRenewer.java:69)
	at org.apache.hadoop.hdfs.LeaseRenewer$1.run(LeaseRenewer.java:273)
	at java.lang.Thread.run(Thread.java:701)
2014-07-01 15:51:49,364 WARN  [org.apache.hadoop.hdfs.LeaseRenewer$1@4fae46d5] hdfs.LeaseRenewer: Failed to renew lease for [DFSClient_hb_rs_slave1,60020,1404253868846_-130361673_31] for 30 seconds.  Will retry shortly ...
org.apache.hadoop.ipc.RemoteException: org.apache.hadoop.hdfs.server.namenode.SafeModeException: Cannot renew lease for DFSClient_hb_rs_slave1,60020,1404253868846_-130361673_31. Name node is in safe mode.
The reported blocks 486 has reached the threshold 0.9990 of total blocks 481. Safe mode will be turned off automatically in 14 seconds.
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.renewLease(FSNamesystem.java:2771)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.renewLease(NameNode.java:919)
	at sun.reflect.GeneratedMethodAccessor4.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:622)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:587)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1432)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1428)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:416)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1190)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:1426)

	at org.apache.hadoop.ipc.Client.call(Client.java:1113)
	at org.apache.hadoop.ipc.RPC$Invoker.invoke(RPC.java:229)
	at com.sun.proxy.$Proxy8.renewLease(Unknown Source)
	at sun.reflect.GeneratedMethodAccessor29.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:622)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:85)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:62)
	at com.sun.proxy.$Proxy8.renewLease(Unknown Source)
	at sun.reflect.GeneratedMethodAccessor29.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:622)
	at org.apache.hadoop.hbase.fs.HFileSystem$1.invoke(HFileSystem.java:294)
	at com.sun.proxy.$Proxy9.renewLease(Unknown Source)
	at org.apache.hadoop.hdfs.DFSClient.renewLease(DFSClient.java:379)
	at org.apache.hadoop.hdfs.LeaseRenewer.renew(LeaseRenewer.java:378)
	at org.apache.hadoop.hdfs.LeaseRenewer.run(LeaseRenewer.java:400)
	at org.apache.hadoop.hdfs.LeaseRenewer.access$600(LeaseRenewer.java:69)
	at org.apache.hadoop.hdfs.LeaseRenewer$1.run(LeaseRenewer.java:273)
	at java.lang.Thread.run(Thread.java:701)
2014-07-01 15:51:49,829 INFO  [regionserver60020.periodicFlusher] regionserver.HRegionServer$PeriodicMemstoreFlusher: regionserver60020.periodicFlusher exiting
2014-07-01 15:51:49,829 INFO  [regionserver60020] regionserver.CompactSplitThread: Waiting for Split Thread to finish...
2014-07-01 15:51:49,829 INFO  [regionserver60020] regionserver.CompactSplitThread: Waiting for Merge Thread to finish...
2014-07-01 15:51:49,830 INFO  [regionserver60020] regionserver.CompactSplitThread: Waiting for Large Compaction Thread to finish...
2014-07-01 15:51:49,830 INFO  [regionserver60020] regionserver.CompactSplitThread: Waiting for Small Compaction Thread to finish...
2014-07-01 15:51:49,850 INFO  [regionserver60020] client.HConnectionManager$HConnectionImplementation: Closing zookeeper sessionid=0x146f40d8cbe0003
2014-07-01 15:51:49,853 INFO  [regionserver60020] zookeeper.ZooKeeper: Session: 0x146f40d8cbe0003 closed
2014-07-01 15:51:49,858 INFO  [regionserver60020-EventThread] zookeeper.ClientCnxn: EventThread shut down
2014-07-01 15:51:49,863 INFO  [regionserver60020] zookeeper.ZooKeeper: Session: 0x146f40d8cbe0002 closed
2014-07-01 15:51:49,863 INFO  [regionserver60020-EventThread] zookeeper.ClientCnxn: EventThread shut down
2014-07-01 15:51:49,863 INFO  [regionserver60020] regionserver.HRegionServer: stopping server slave1,60020,1404253868846; zookeeper connection closed.
2014-07-01 15:51:49,864 INFO  [regionserver60020] regionserver.HRegionServer: regionserver60020 exiting
2014-07-01 15:51:49,884 ERROR [main] regionserver.HRegionServerCommandLine: Region server exiting
java.lang.RuntimeException: HRegionServer Aborted
	at org.apache.hadoop.hbase.regionserver.HRegionServerCommandLine.start(HRegionServerCommandLine.java:66)
	at org.apache.hadoop.hbase.regionserver.HRegionServerCommandLine.run(HRegionServerCommandLine.java:85)
	at org.apache.hadoop.util.ToolRunner.run(ToolRunner.java:65)
	at org.apache.hadoop.hbase.util.ServerCommandLine.doMain(ServerCommandLine.java:126)
	at org.apache.hadoop.hbase.regionserver.HRegionServer.main(HRegionServer.java:2410)
2014-07-01 15:51:49,894 INFO  [Shutdownhook:regionserver60020] regionserver.ShutdownHook: Shutdown hook starting; hbase.shutdown.hook=true; fsShutdownHook=Thread[Thread-9,5,main]
2014-07-01 15:51:49,894 INFO  [Shutdownhook:regionserver60020] regionserver.ShutdownHook: Starting fs shutdown hook thread.
2014-07-01 15:51:49,895 ERROR [Thread-9] hdfs.DFSClient: Failed to close file /hbase/data/default/usertable/667c1f2c34fe37d48ba25a789d8aba98/.tmp/fc32759b35c444c89da742a5c2316f15
java.io.IOException: Call to master/9.1.143.58:54310 failed on local exception: java.io.EOFException
	at org.apache.hadoop.ipc.Client.wrapException(Client.java:1150)
	at org.apache.hadoop.ipc.Client.call(Client.java:1118)
	at org.apache.hadoop.ipc.RPC$Invoker.invoke(RPC.java:229)
	at com.sun.proxy.$Proxy8.addBlock(Unknown Source)
	at sun.reflect.GeneratedMethodAccessor4.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:622)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:85)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:62)
	at com.sun.proxy.$Proxy8.addBlock(Unknown Source)
	at sun.reflect.GeneratedMethodAccessor4.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:622)
	at org.apache.hadoop.hbase.fs.HFileSystem$1.invoke(HFileSystem.java:294)
	at com.sun.proxy.$Proxy9.addBlock(Unknown Source)
	at org.apache.hadoop.hdfs.DFSClient$DFSOutputStream.locateFollowingBlock(DFSClient.java:3720)
	at org.apache.hadoop.hdfs.DFSClient$DFSOutputStream.nextBlockOutputStream(DFSClient.java:3580)
	at org.apache.hadoop.hdfs.DFSClient$DFSOutputStream.access$2600(DFSClient.java:2783)
	at org.apache.hadoop.hdfs.DFSClient$DFSOutputStream$DataStreamer.run(DFSClient.java:3023)
Caused by: java.io.EOFException
	at java.io.DataInputStream.readInt(DataInputStream.java:392)
	at org.apache.hadoop.ipc.Client$Connection.receiveResponse(Client.java:845)
	at org.apache.hadoop.ipc.Client$Connection.run(Client.java:790)
2014-07-01 15:51:49,896 ERROR [Thread-9] hdfs.DFSClient: Failed to close file /hbase/WALs/slave1,60020,1404253868846/slave1%2C60020%2C1404253868846.1404254547570
java.io.IOException: Error Recovery for block blk_-3161611795271398729_29985 failed  because recovery from primary datanode 9.1.143.59:50010 failed 6 times.  Pipeline was 9.1.143.59:50010. Aborting...
	at org.apache.hadoop.hdfs.DFSClient$DFSOutputStream.processDatanodeError(DFSClient.java:3355)
	at org.apache.hadoop.hdfs.DFSClient$DFSOutputStream.access$2200(DFSClient.java:2783)
	at org.apache.hadoop.hdfs.DFSClient$DFSOutputStream$DataStreamer.run(DFSClient.java:2987)
2014-07-01 15:51:49,896 INFO  [Shutdownhook:regionserver60020] regionserver.ShutdownHook: Shutdown hook finished.
