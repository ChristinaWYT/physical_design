Mon Jun 30 19:46:41 PDT 2014 Starting regionserver on sceplus-vm48
core file size          (blocks, -c) 0
data seg size           (kbytes, -d) unlimited
scheduling priority             (-e) 0
file size               (blocks, -f) unlimited
pending signals                 (-i) 128203
max locked memory       (kbytes, -l) 64
max memory size         (kbytes, -m) unlimited
open files                      (-n) 32768
pipe size            (512 bytes, -p) 8
POSIX message queues     (bytes, -q) 819200
real-time priority              (-r) 0
stack size              (kbytes, -s) 8192
cpu time               (seconds, -t) unlimited
max user processes              (-u) 128203
virtual memory          (kbytes, -v) unlimited
file locks                      (-x) unlimited
2014-06-30 19:46:42,268 INFO  [main] util.VersionInfo: HBase 0.98.3-hadoop1
2014-06-30 19:46:42,269 INFO  [main] util.VersionInfo: Subversion git://acer/usr/src/Hadoop/hbase -r d5e65a9144e315bb0a964e7730871af32f5018d5
2014-06-30 19:46:42,269 INFO  [main] util.VersionInfo: Compiled by apurtell on Sat May 31 19:34:57 PDT 2014
2014-06-30 19:46:42,478 INFO  [main] util.ServerCommandLine: env:JAVA_HOME=/usr/lib/jvm/java-1.6.0-openjdk-amd64/
2014-06-30 19:46:42,478 INFO  [main] util.ServerCommandLine: env:SHLVL=3
2014-06-30 19:46:42,478 INFO  [main] util.ServerCommandLine: env:HBASE_LOG_DIR=/home/hadoop/hbase/bin/../logs
2014-06-30 19:46:42,478 INFO  [main] util.ServerCommandLine: env:HBASE_HOME=/home/hadoop/hbase/bin/..
2014-06-30 19:46:42,478 INFO  [main] util.ServerCommandLine: env:HBASE_OPTS=-XX:+UseConcMarkSweepGC   -Dhbase.log.dir=/home/hadoop/hbase/bin/../logs -Dhbase.log.file=hbase-hadoop-regionserver-sceplus-vm48.log -Dhbase.home.dir=/home/hadoop/hbase/bin/.. -Dhbase.id.str=hadoop -Dhbase.root.logger=INFO,RFA -Djava.library.path=/home/hadoop/hbase/bin/../lib/native/Linux-amd64-64 -Dhbase.security.logger=INFO,RFAS
2014-06-30 19:46:42,478 INFO  [main] util.ServerCommandLine: env:HBASE_ENV_INIT=true
2014-06-30 19:46:42,478 INFO  [main] util.ServerCommandLine: env:SSH_CLIENT=9.1.143.58 33177 22
2014-06-30 19:46:42,478 INFO  [main] util.ServerCommandLine: env:MAIL=/var/mail/hadoop
2014-06-30 19:46:42,479 INFO  [main] util.ServerCommandLine: env:HBASE_ZNODE_FILE=/var/hadoop/pids/hbase-hadoop-regionserver.znode
2014-06-30 19:46:42,479 INFO  [main] util.ServerCommandLine: env:PWD=/home/hadoop/hbase
2014-06-30 19:46:42,479 INFO  [main] util.ServerCommandLine: env:LOGNAME=hadoop
2014-06-30 19:46:42,479 INFO  [main] util.ServerCommandLine: env:HBASE_MANAGES_ZK=true
2014-06-30 19:46:42,479 INFO  [main] util.ServerCommandLine: env:HBASE_NICENESS=0
2014-06-30 19:46:42,479 INFO  [main] util.ServerCommandLine: env:HBASE_REST_OPTS=
2014-06-30 19:46:42,479 INFO  [main] util.ServerCommandLine: env:LD_LIBRARY_PATH=/usr/lib/jvm/java-6-openjdk-amd64/jre/lib/amd64/server:/usr/lib/jvm/java-6-openjdk-amd64/jre/lib/amd64:/usr/lib/jvm/java-6-openjdk-amd64/jre/../lib/amd64::/home/hadoop/hbase/bin/../lib/native/Linux-amd64-64
2014-06-30 19:46:42,479 INFO  [main] util.ServerCommandLine: env:MALLOC_ARENA_MAX=4
2014-06-30 19:46:42,479 INFO  [main] util.ServerCommandLine: env:SSH_CONNECTION=9.1.143.58 33177 9.1.143.58 22
2014-06-30 19:46:42,479 INFO  [main] util.ServerCommandLine: env:SHELL=/bin/bash
2014-06-30 19:46:42,479 INFO  [main] util.ServerCommandLine: env:HBASE_PID_DIR=/var/hadoop/pids
2014-06-30 19:46:42,479 INFO  [main] util.ServerCommandLine: env:HBASE_ROOT_LOGGER=INFO,RFA
2014-06-30 19:46:42,482 INFO  [main] util.ServerCommandLine: env:CLASSPATH=/home/hadoop/hbase/conf:/usr/lib/jvm/java-1.6.0-openjdk-amd64//lib/tools.jar:/home/hadoop/hbase/bin/..:/home/hadoop/hbase/bin/../lib/activation-1.1.jar:/home/hadoop/hbase/bin/../lib/asm-3.1.jar:/home/hadoop/hbase/bin/../lib/commons-beanutils-1.7.0.jar:/home/hadoop/hbase/bin/../lib/commons-beanutils-core-1.8.0.jar:/home/hadoop/hbase/bin/../lib/commons-cli-1.2.jar:/home/hadoop/hbase/bin/../lib/commons-codec-1.7.jar:/home/hadoop/hbase/bin/../lib/commons-collections-3.2.1.jar:/home/hadoop/hbase/bin/../lib/commons-configuration-1.6.jar:/home/hadoop/hbase/bin/../lib/commons-digester-1.8.jar:/home/hadoop/hbase/bin/../lib/commons-el-1.0.jar:/home/hadoop/hbase/bin/../lib/commons-httpclient-3.1.jar:/home/hadoop/hbase/bin/../lib/commons-io-2.4.jar:/home/hadoop/hbase/bin/../lib/commons-lang-2.6.jar:/home/hadoop/hbase/bin/../lib/commons-logging-1.1.1.jar:/home/hadoop/hbase/bin/../lib/commons-math-2.1.jar:/home/hadoop/hbase/bin/../lib/commons-net-1.4.1.jar:/home/hadoop/hbase/bin/../lib/findbugs-annotations-1.3.9-1.jar:/home/hadoop/hbase/bin/../lib/guava-12.0.1.jar:/home/hadoop/hbase/bin/../lib/hadoop-core-1.2.1.jar:/home/hadoop/hbase/bin/../lib/hamcrest-core-1.3.jar:/home/hadoop/hbase/bin/../lib/hbase-client-0.98.3-hadoop1.jar:/home/hadoop/hbase/bin/../lib/hbase-common-0.98.3-hadoop1.jar:/home/hadoop/hbase/bin/../lib/hbase-common-0.98.3-hadoop1-tests.jar:/home/hadoop/hbase/bin/../lib/hbase-examples-0.98.3-hadoop1.jar:/home/hadoop/hbase/bin/../lib/hbase-hadoop1-compat-0.98.3-hadoop1.jar:/home/hadoop/hbase/bin/../lib/hbase-hadoop-compat-0.98.3-hadoop1.jar:/home/hadoop/hbase/bin/../lib/hbase-it-0.98.3-hadoop1.jar:/home/hadoop/hbase/bin/../lib/hbase-it-0.98.3-hadoop1-tests.jar:/home/hadoop/hbase/bin/../lib/hbase-prefix-tree-0.98.3-hadoop1.jar:/home/hadoop/hbase/bin/../lib/hbase-protocol-0.98.3-hadoop1.jar:/home/hadoop/hbase/bin/../lib/hbase-server-0.98.3-hadoop1.jar:/home/hadoop/hbase/bin/../lib/hbase-server-0.98.3-hadoop1-tests.jar:/home/hadoop/hbase/bin/../lib/hbase-shell-0.98.3-hadoop1.jar:/home/hadoop/hbase/bin/../lib/hbase-testing-util-0.98.3-hadoop1.jar:/home/hadoop/hbase/bin/../lib/hbase-thrift-0.98.3-hadoop1.jar:/home/hadoop/hbase/bin/../lib/high-scale-lib-1.1.1.jar:/home/hadoop/hbase/bin/../lib/htrace-core-2.04.jar:/home/hadoop/hbase/bin/../lib/httpclient-4.1.3.jar:/home/hadoop/hbase/bin/../lib/httpcore-4.1.3.jar:/home/hadoop/hbase/bin/../lib/jackson-core-asl-1.8.8.jar:/home/hadoop/hbase/bin/../lib/jackson-jaxrs-1.8.8.jar:/home/hadoop/hbase/bin/../lib/jackson-mapper-asl-1.8.8.jar:/home/hadoop/hbase/bin/../lib/jackson-xc-1.8.8.jar:/home/hadoop/hbase/bin/../lib/jamon-runtime-2.3.1.jar:/home/hadoop/hbase/bin/../lib/jasper-compiler-5.5.23.jar:/home/hadoop/hbase/bin/../lib/jasper-runtime-5.5.23.jar:/home/hadoop/hbase/bin/../lib/jaxb-api-2.2.2.jar:/home/hadoop/hbase/bin/../lib/jaxb-impl-2.2.3-1.jar:/home/hadoop/hbase/bin/../lib/jersey-core-1.8.jar:/home/hadoop/hbase/bin/../lib/jersey-json-1.8.jar:/home/hadoop/hbase/bin/../lib/jersey-server-1.8.jar:/home/hadoop/hbase/bin/../lib/jettison-1.3.1.jar:/home/hadoop/hbase/bin/../lib/jetty-6.1.26.jar:/home/hadoop/hbase/bin/../lib/jetty-sslengine-6.1.26.jar:/home/hadoop/hbase/bin/../lib/jetty-util-6.1.26.jar:/home/hadoop/hbase/bin/../lib/jruby-complete-1.6.8.jar:/home/hadoop/hbase/bin/../lib/jsp-2.1-6.1.14.jar:/home/hadoop/hbase/bin/../lib/jsp-api-2.1-6.1.14.jar:/home/hadoop/hbase/bin/../lib/jsr305-1.3.9.jar:/home/hadoop/hbase/bin/../lib/junit-4.11.jar:/home/hadoop/hbase/bin/../lib/libthrift-0.9.0.jar:/home/hadoop/hbase/bin/../lib/log4j-1.2.17.jar:/home/hadoop/hbase/bin/../lib/metrics-core-2.1.2.jar:/home/hadoop/hbase/bin/../lib/netty-3.6.6.Final.jar:/home/hadoop/hbase/bin/../lib/protobuf-java-2.5.0.jar:/home/hadoop/hbase/bin/../lib/servlet-api-2.5-6.1.14.jar:/home/hadoop/hbase/bin/../lib/slf4j-api-1.6.4.jar:/home/hadoop/hbase/bin/../lib/slf4j-log4j12-1.6.4.jar:/home/hadoop/hbase/bin/../lib/xmlenc-0.52.jar:/home/hadoop/hbase/bin/../lib/zookeeper-3.4.6.jar:
2014-06-30 19:46:42,482 INFO  [main] util.ServerCommandLine: env:HBASE_THRIFT_OPTS=
2014-06-30 19:46:42,482 INFO  [main] util.ServerCommandLine: env:PATH=/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/usr/games:/usr/local/games
2014-06-30 19:46:42,482 INFO  [main] util.ServerCommandLine: env:USER=hadoop
2014-06-30 19:46:42,482 INFO  [main] util.ServerCommandLine: env:HBASE_SECURITY_LOGGER=INFO,RFAS
2014-06-30 19:46:42,482 INFO  [main] util.ServerCommandLine: env:HOME=/home/hadoop
2014-06-30 19:46:42,482 INFO  [main] util.ServerCommandLine: env:HBASE_LIBRARY_PATH=/home/hadoop/hbase/lib/native/Linux-amd64-64
2014-06-30 19:46:42,482 INFO  [main] util.ServerCommandLine: env:HBASE_START_FILE=/var/hadoop/pids/hbase-hadoop-regionserver.autorestart
2014-06-30 19:46:42,483 INFO  [main] util.ServerCommandLine: env:XDG_SESSION_ID=6
2014-06-30 19:46:42,483 INFO  [main] util.ServerCommandLine: env:HBASE_LOGFILE=hbase-hadoop-regionserver-sceplus-vm48.log
2014-06-30 19:46:42,483 INFO  [main] util.ServerCommandLine: env:XDG_RUNTIME_DIR=/run/user/1001
2014-06-30 19:46:42,483 INFO  [main] util.ServerCommandLine: env:LANG=en_US.UTF-8
2014-06-30 19:46:42,483 INFO  [main] util.ServerCommandLine: env:HBASE_LOG_PREFIX=hbase-hadoop-regionserver-sceplus-vm48
2014-06-30 19:46:42,483 INFO  [main] util.ServerCommandLine: env:HBASE_IDENT_STRING=hadoop
2014-06-30 19:46:42,485 INFO  [main] util.ServerCommandLine: vmName=OpenJDK 64-Bit Server VM, vmVendor=Sun Microsystems Inc., vmVersion=23.25-b01
2014-06-30 19:46:42,485 INFO  [main] util.ServerCommandLine: vmInputArguments=[-Dproc_regionserver, -XX:OnOutOfMemoryError=kill -9 %p, -Xmx1000m, -XX:+UseConcMarkSweepGC, -Dhbase.log.dir=/home/hadoop/hbase/bin/../logs, -Dhbase.log.file=hbase-hadoop-regionserver-sceplus-vm48.log, -Dhbase.home.dir=/home/hadoop/hbase/bin/.., -Dhbase.id.str=hadoop, -Dhbase.root.logger=INFO,RFA, -Djava.library.path=/home/hadoop/hbase/bin/../lib/native/Linux-amd64-64, -Dhbase.security.logger=INFO,RFAS]
2014-06-30 19:46:42,682 DEBUG [main] regionserver.HRegionServer: regionserver/sceplus-vm48.almaden.ibm.com/9.1.143.58:60020 HConnection server-to-server retries=350
2014-06-30 19:46:43,025 INFO  [main] ipc.RpcServer: regionserver/sceplus-vm48.almaden.ibm.com/9.1.143.58:60020: started 10 reader(s).
2014-06-30 19:46:43,098 INFO  [main] impl.MetricsConfig: loaded properties from hadoop-metrics2-hbase.properties
2014-06-30 19:46:43,109 INFO  [main] impl.MetricsSinkAdapter: Sink file-all started
2014-06-30 19:46:43,173 INFO  [main] impl.MetricsSourceAdapter: MBean for source MetricsSystem,sub=Stats registered.
2014-06-30 19:46:43,174 INFO  [main] impl.MetricsSystemImpl: Scheduled snapshot period at 10 second(s).
2014-06-30 19:46:43,174 INFO  [main] impl.MetricsSystemImpl: HBase metrics system started
2014-06-30 19:46:43,178 INFO  [main] impl.MetricsSourceAdapter: MBean for source jvm registered.
2014-06-30 19:46:43,182 INFO  [main] impl.MetricsSourceAdapter: MBean for source IPC,sub=IPC registered.
2014-06-30 19:46:43,252 INFO  [main] impl.MetricsSourceAdapter: MBean for source ugi registered.
2014-06-30 19:46:43,252 WARN  [main] impl.MetricsSystemImpl: Source name ugi already exists!
2014-06-30 19:46:43,256 DEBUG [main] util.DirectMemoryUtils: Failed to retrieve nio.BufferPool direct MemoryUsed attribute.
javax.management.InstanceNotFoundException: java.nio:type=BufferPool,name=direct
	at com.sun.jmx.interceptor.DefaultMBeanServerInterceptor.getMBean(DefaultMBeanServerInterceptor.java:1117)
	at com.sun.jmx.interceptor.DefaultMBeanServerInterceptor.getAttribute(DefaultMBeanServerInterceptor.java:678)
	at com.sun.jmx.mbeanserver.JmxMBeanServer.getAttribute(JmxMBeanServer.java:682)
	at org.apache.hadoop.hbase.util.DirectMemoryUtils.<clinit>(DirectMemoryUtils.java:72)
	at org.apache.hadoop.hbase.io.hfile.CacheConfig.instantiateBlockCache(CacheConfig.java:396)
	at org.apache.hadoop.hbase.io.hfile.CacheConfig.<init>(CacheConfig.java:179)
	at org.apache.hadoop.hbase.regionserver.HRegionServer.<init>(HRegionServer.java:621)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:57)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:534)
	at org.apache.hadoop.hbase.regionserver.HRegionServer.constructRegionServer(HRegionServer.java:2393)
	at org.apache.hadoop.hbase.regionserver.HRegionServerCommandLine.start(HRegionServerCommandLine.java:61)
	at org.apache.hadoop.hbase.regionserver.HRegionServerCommandLine.run(HRegionServerCommandLine.java:85)
	at org.apache.hadoop.util.ToolRunner.run(ToolRunner.java:65)
	at org.apache.hadoop.hbase.util.ServerCommandLine.doMain(ServerCommandLine.java:126)
	at org.apache.hadoop.hbase.regionserver.HRegionServer.main(HRegionServer.java:2410)
2014-06-30 19:46:43,258 INFO  [main] hfile.CacheConfig: Allocating LruBlockCache with maximum size 386.7m
2014-06-30 19:46:43,317 INFO  [main] mortbay.log: Logging to org.slf4j.impl.Log4jLoggerAdapter(org.mortbay.log) via org.mortbay.log.Slf4jLog
2014-06-30 19:46:43,366 INFO  [main] http.HttpServer: Added global filtersafety (class=org.apache.hadoop.http.HttpServer$QuotingInputFilter)
2014-06-30 19:46:43,374 INFO  [main] http.HttpServer: Port returned by webServer.getConnectors()[0].getLocalPort() before open() is -1. Opening the listener on 60030
2014-06-30 19:46:43,376 INFO  [main] http.HttpServer: listener.getLocalPort() returned 60030 webServer.getConnectors()[0].getLocalPort() returned 60030
2014-06-30 19:46:43,376 INFO  [main] http.HttpServer: Jetty bound to port 60030
2014-06-30 19:46:43,376 INFO  [main] mortbay.log: jetty-6.1.26
2014-06-30 19:46:43,674 INFO  [main] mortbay.log: Started SelectChannelConnector@0.0.0.0:60030
2014-06-30 19:46:43,716 INFO  [main] regionserver.ShutdownHook: Installed shutdown hook thread: Shutdownhook:regionserver60020
2014-06-30 19:46:43,718 INFO  [regionserver60020] zookeeper.ZooKeeper: Client environment:zookeeper.version=3.4.6-1569965, built on 02/20/2014 09:09 GMT
2014-06-30 19:46:43,718 INFO  [regionserver60020] zookeeper.ZooKeeper: Client environment:host.name=sceplus-vm48.almaden.ibm.com
2014-06-30 19:46:43,718 INFO  [regionserver60020] zookeeper.ZooKeeper: Client environment:java.version=1.6.0_31
2014-06-30 19:46:43,718 INFO  [regionserver60020] zookeeper.ZooKeeper: Client environment:java.vendor=Sun Microsystems Inc.
2014-06-30 19:46:43,718 INFO  [regionserver60020] zookeeper.ZooKeeper: Client environment:java.home=/usr/lib/jvm/java-6-openjdk-amd64/jre
2014-06-30 19:46:43,718 INFO  [regionserver60020] zookeeper.ZooKeeper: Client environment:java.class.path=/home/hadoop/hbase/conf:/usr/lib/jvm/java-1.6.0-openjdk-amd64//lib/tools.jar:/home/hadoop/hbase/bin/..:/home/hadoop/hbase/bin/../lib/activation-1.1.jar:/home/hadoop/hbase/bin/../lib/asm-3.1.jar:/home/hadoop/hbase/bin/../lib/commons-beanutils-1.7.0.jar:/home/hadoop/hbase/bin/../lib/commons-beanutils-core-1.8.0.jar:/home/hadoop/hbase/bin/../lib/commons-cli-1.2.jar:/home/hadoop/hbase/bin/../lib/commons-codec-1.7.jar:/home/hadoop/hbase/bin/../lib/commons-collections-3.2.1.jar:/home/hadoop/hbase/bin/../lib/commons-configuration-1.6.jar:/home/hadoop/hbase/bin/../lib/commons-digester-1.8.jar:/home/hadoop/hbase/bin/../lib/commons-el-1.0.jar:/home/hadoop/hbase/bin/../lib/commons-httpclient-3.1.jar:/home/hadoop/hbase/bin/../lib/commons-io-2.4.jar:/home/hadoop/hbase/bin/../lib/commons-lang-2.6.jar:/home/hadoop/hbase/bin/../lib/commons-logging-1.1.1.jar:/home/hadoop/hbase/bin/../lib/commons-math-2.1.jar:/home/hadoop/hbase/bin/../lib/commons-net-1.4.1.jar:/home/hadoop/hbase/bin/../lib/findbugs-annotations-1.3.9-1.jar:/home/hadoop/hbase/bin/../lib/guava-12.0.1.jar:/home/hadoop/hbase/bin/../lib/hadoop-core-1.2.1.jar:/home/hadoop/hbase/bin/../lib/hamcrest-core-1.3.jar:/home/hadoop/hbase/bin/../lib/hbase-client-0.98.3-hadoop1.jar:/home/hadoop/hbase/bin/../lib/hbase-common-0.98.3-hadoop1.jar:/home/hadoop/hbase/bin/../lib/hbase-common-0.98.3-hadoop1-tests.jar:/home/hadoop/hbase/bin/../lib/hbase-examples-0.98.3-hadoop1.jar:/home/hadoop/hbase/bin/../lib/hbase-hadoop1-compat-0.98.3-hadoop1.jar:/home/hadoop/hbase/bin/../lib/hbase-hadoop-compat-0.98.3-hadoop1.jar:/home/hadoop/hbase/bin/../lib/hbase-it-0.98.3-hadoop1.jar:/home/hadoop/hbase/bin/../lib/hbase-it-0.98.3-hadoop1-tests.jar:/home/hadoop/hbase/bin/../lib/hbase-prefix-tree-0.98.3-hadoop1.jar:/home/hadoop/hbase/bin/../lib/hbase-protocol-0.98.3-hadoop1.jar:/home/hadoop/hbase/bin/../lib/hbase-server-0.98.3-hadoop1.jar:/home/hadoop/hbase/bin/../lib/hbase-server-0.98.3-hadoop1-tests.jar:/home/hadoop/hbase/bin/../lib/hbase-shell-0.98.3-hadoop1.jar:/home/hadoop/hbase/bin/../lib/hbase-testing-util-0.98.3-hadoop1.jar:/home/hadoop/hbase/bin/../lib/hbase-thrift-0.98.3-hadoop1.jar:/home/hadoop/hbase/bin/../lib/high-scale-lib-1.1.1.jar:/home/hadoop/hbase/bin/../lib/htrace-core-2.04.jar:/home/hadoop/hbase/bin/../lib/httpclient-4.1.3.jar:/home/hadoop/hbase/bin/../lib/httpcore-4.1.3.jar:/home/hadoop/hbase/bin/../lib/jackson-core-asl-1.8.8.jar:/home/hadoop/hbase/bin/../lib/jackson-jaxrs-1.8.8.jar:/home/hadoop/hbase/bin/../lib/jackson-mapper-asl-1.8.8.jar:/home/hadoop/hbase/bin/../lib/jackson-xc-1.8.8.jar:/home/hadoop/hbase/bin/../lib/jamon-runtime-2.3.1.jar:/home/hadoop/hbase/bin/../lib/jasper-compiler-5.5.23.jar:/home/hadoop/hbase/bin/../lib/jasper-runtime-5.5.23.jar:/home/hadoop/hbase/bin/../lib/jaxb-api-2.2.2.jar:/home/hadoop/hbase/bin/../lib/jaxb-impl-2.2.3-1.jar:/home/hadoop/hbase/bin/../lib/jersey-core-1.8.jar:/home/hadoop/hbase/bin/../lib/jersey-json-1.8.jar:/home/hadoop/hbase/bin/../lib/jersey-server-1.8.jar:/home/hadoop/hbase/bin/../lib/jettison-1.3.1.jar:/home/hadoop/hbase/bin/../lib/jetty-6.1.26.jar:/home/hadoop/hbase/bin/../lib/jetty-sslengine-6.1.26.jar:/home/hadoop/hbase/bin/../lib/jetty-util-6.1.26.jar:/home/hadoop/hbase/bin/../lib/jruby-complete-1.6.8.jar:/home/hadoop/hbase/bin/../lib/jsp-2.1-6.1.14.jar:/home/hadoop/hbase/bin/../lib/jsp-api-2.1-6.1.14.jar:/home/hadoop/hbase/bin/../lib/jsr305-1.3.9.jar:/home/hadoop/hbase/bin/../lib/junit-4.11.jar:/home/hadoop/hbase/bin/../lib/libthrift-0.9.0.jar:/home/hadoop/hbase/bin/../lib/log4j-1.2.17.jar:/home/hadoop/hbase/bin/../lib/metrics-core-2.1.2.jar:/home/hadoop/hbase/bin/../lib/netty-3.6.6.Final.jar:/home/hadoop/hbase/bin/../lib/protobuf-java-2.5.0.jar:/home/hadoop/hbase/bin/../lib/servlet-api-2.5-6.1.14.jar:/home/hadoop/hbase/bin/../lib/slf4j-api-1.6.4.jar:/home/hadoop/hbase/bin/../lib/slf4j-log4j12-1.6.4.jar:/home/hadoop/hbase/bin/../lib/xmlenc-0.52.jar:/home/hadoop/hbase/bin/../lib/zookeeper-3.4.6.jar:
2014-06-30 19:46:43,718 INFO  [regionserver60020] zookeeper.ZooKeeper: Client environment:java.library.path=/home/hadoop/hbase/bin/../lib/native/Linux-amd64-64
2014-06-30 19:46:43,718 INFO  [regionserver60020] zookeeper.ZooKeeper: Client environment:java.io.tmpdir=/tmp
2014-06-30 19:46:43,718 INFO  [regionserver60020] zookeeper.ZooKeeper: Client environment:java.compiler=<NA>
2014-06-30 19:46:43,718 INFO  [regionserver60020] zookeeper.ZooKeeper: Client environment:os.name=Linux
2014-06-30 19:46:43,718 INFO  [regionserver60020] zookeeper.ZooKeeper: Client environment:os.arch=amd64
2014-06-30 19:46:43,718 INFO  [regionserver60020] zookeeper.ZooKeeper: Client environment:os.version=3.13.0-24-generic
2014-06-30 19:46:43,718 INFO  [regionserver60020] zookeeper.ZooKeeper: Client environment:user.name=hadoop
2014-06-30 19:46:43,718 INFO  [regionserver60020] zookeeper.ZooKeeper: Client environment:user.home=/home/hadoop
2014-06-30 19:46:43,718 INFO  [regionserver60020] zookeeper.ZooKeeper: Client environment:user.dir=/home/hadoop/hbase-0.98.3-hadoop1
2014-06-30 19:46:43,719 INFO  [regionserver60020] zookeeper.ZooKeeper: Initiating client connection, connectString=sceplus-vm49.almaden.ibm.com:2181,sceplus-vm48.almaden.ibm.com:2181 sessionTimeout=90000 watcher=regionserver:60020, quorum=sceplus-vm49.almaden.ibm.com:2181,sceplus-vm48.almaden.ibm.com:2181, baseZNode=/hbase
2014-06-30 19:46:43,739 INFO  [regionserver60020] zookeeper.RecoverableZooKeeper: Process identifier=regionserver:60020 connecting to ZooKeeper ensemble=sceplus-vm49.almaden.ibm.com:2181,sceplus-vm48.almaden.ibm.com:2181
2014-06-30 19:46:43,741 INFO  [regionserver60020-SendThread(slave1:2181)] zookeeper.ClientCnxn: Opening socket connection to server slave1/9.1.143.59:2181. Will not attempt to authenticate using SASL (unknown error)
2014-06-30 19:46:43,746 INFO  [regionserver60020-SendThread(slave1:2181)] zookeeper.ClientCnxn: Socket connection established to slave1/9.1.143.59:2181, initiating session
2014-06-30 19:46:43,756 INFO  [regionserver60020-SendThread(slave1:2181)] zookeeper.ClientCnxn: Session establishment complete on server slave1/9.1.143.59:2181, sessionid = 0x146efd1264e0001, negotiated timeout = 90000
2014-06-30 19:47:13,588 INFO  [regionserver60020] zookeeper.ZooKeeper: Initiating client connection, connectString=sceplus-vm49.almaden.ibm.com:2181,sceplus-vm48.almaden.ibm.com:2181 sessionTimeout=90000 watcher=hconnection-0x61960044, quorum=sceplus-vm49.almaden.ibm.com:2181,sceplus-vm48.almaden.ibm.com:2181, baseZNode=/hbase
2014-06-30 19:47:13,589 INFO  [regionserver60020] zookeeper.RecoverableZooKeeper: Process identifier=hconnection-0x61960044 connecting to ZooKeeper ensemble=sceplus-vm49.almaden.ibm.com:2181,sceplus-vm48.almaden.ibm.com:2181
2014-06-30 19:47:13,589 INFO  [regionserver60020-SendThread(slave1:2181)] zookeeper.ClientCnxn: Opening socket connection to server slave1/9.1.143.59:2181. Will not attempt to authenticate using SASL (unknown error)
2014-06-30 19:47:13,590 INFO  [regionserver60020-SendThread(slave1:2181)] zookeeper.ClientCnxn: Socket connection established to slave1/9.1.143.59:2181, initiating session
2014-06-30 19:47:13,592 INFO  [regionserver60020-SendThread(slave1:2181)] zookeeper.ClientCnxn: Session establishment complete on server slave1/9.1.143.59:2181, sessionid = 0x146efd1264e0003, negotiated timeout = 90000
2014-06-30 19:47:13,808 DEBUG [regionserver60020] catalog.CatalogTracker: Starting catalog tracker org.apache.hadoop.hbase.catalog.CatalogTracker@665570e2
2014-06-30 19:47:13,812 INFO  [regionserver60020] regionserver.HRegionServer: ClusterId : 3b61b992-e8ee-43f8-b0c6-14cd23a8afbe
2014-06-30 19:47:13,817 INFO  [regionserver60020] procedure.RegionServerProcedureManagerHost: Procedure online-snapshot is initializing
2014-06-30 19:47:13,840 INFO  [regionserver60020] zookeeper.RecoverableZooKeeper: Node /hbase/online-snapshot/acquired already exists and this is not a retry
2014-06-30 19:47:13,867 INFO  [regionserver60020] procedure.RegionServerProcedureManagerHost: Procedure online-snapshot is initialized
2014-06-30 19:47:13,871 INFO  [regionserver60020] regionserver.MemStoreFlusher: globalMemStoreLimit=386.7m, globalMemStoreLimitLowMark=367.3m, maxHeap=966.7m
2014-06-30 19:47:13,875 INFO  [regionserver60020] regionserver.HRegionServer: CompactionChecker runs every 10sec
2014-06-30 19:47:13,892 INFO  [regionserver60020] regionserver.HRegionServer: reportForDuty to master=sceplus-vm48.almaden.ibm.com,60000,1404182801807 with port=60020, startcode=1404182803191
2014-06-30 19:47:14,232 DEBUG [regionserver60020] regionserver.HRegionServer: Config from master: hbase.rootdir=hdfs://master:54310/hbase
2014-06-30 19:47:14,232 DEBUG [regionserver60020] regionserver.HRegionServer: Config from master: fs.default.name=hdfs://master:54310
2014-06-30 19:47:14,261 INFO  [regionserver60020] fs.HFileSystem: Added intercepting call to namenode#getBlockLocations so can do block reordering using class class org.apache.hadoop.hbase.fs.HFileSystem$ReorderWALBlocks
2014-06-30 19:47:14,268 DEBUG [regionserver60020] regionserver.HRegionServer: logdir=hdfs://master:54310/hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1404182803191
2014-06-30 19:47:14,300 DEBUG [regionserver60020] regionserver.Replication: ReplicationStatisticsThread 300
2014-06-30 19:47:14,310 INFO  [regionserver60020] wal.FSHLog: WAL/HLog configuration: blocksize=64 MB, rollsize=60.8 MB, enabled=true
2014-06-30 19:47:14,393 INFO  [regionserver60020] wal.FSHLog: New WAL /hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1404182803191/sceplus-vm48.almaden.ibm.com%2C60020%2C1404182803191.1404182834317
2014-06-30 19:47:14,410 INFO  [regionserver60020] impl.MetricsSourceAdapter: MBean for source RegionServer,sub=WAL registered.
2014-06-30 19:47:14,415 INFO  [regionserver60020] regionserver.MetricsRegionServerWrapperImpl: Computing regionserver metrics every 5000 milliseconds
2014-06-30 19:47:14,418 INFO  [regionserver60020] impl.MetricsSourceAdapter: MBean for source RegionServer,sub=Server registered.
2014-06-30 19:47:14,422 INFO  [regionserver60020] trace.SpanReceiverHost: SpanReceiver org.cloudera.htrace.impl.LocalFileSpanReceiver was loaded successfully.
2014-06-30 19:47:14,424 DEBUG [regionserver60020] executor.ExecutorService: Starting executor service name=RS_OPEN_REGION-sceplus-vm48:60020, corePoolSize=3, maxPoolSize=3
2014-06-30 19:47:14,425 DEBUG [regionserver60020] executor.ExecutorService: Starting executor service name=RS_OPEN_META-sceplus-vm48:60020, corePoolSize=1, maxPoolSize=1
2014-06-30 19:47:14,425 DEBUG [regionserver60020] executor.ExecutorService: Starting executor service name=RS_CLOSE_REGION-sceplus-vm48:60020, corePoolSize=3, maxPoolSize=3
2014-06-30 19:47:14,425 DEBUG [regionserver60020] executor.ExecutorService: Starting executor service name=RS_CLOSE_META-sceplus-vm48:60020, corePoolSize=1, maxPoolSize=1
2014-06-30 19:47:14,425 DEBUG [regionserver60020] executor.ExecutorService: Starting executor service name=RS_LOG_REPLAY_OPS-sceplus-vm48:60020, corePoolSize=2, maxPoolSize=2
2014-06-30 19:47:14,433 INFO  [regionserver60020] regionserver.ReplicationSourceManager: Current list of replicators: [sceplus-vm48.almaden.ibm.com,60020,1404182803191, slave1,60020,1404182801416] other RSs: [sceplus-vm48.almaden.ibm.com,60020,1404182803191, slave1,60020,1404182801416]
2014-06-30 19:47:14,452 INFO  [regionserver60020] impl.MetricsSourceAdapter: MBean for source RegionServer,sub=Replication registered.
2014-06-30 19:47:14,457 INFO  [regionserver60020] zookeeper.ZooKeeper: Initiating client connection, connectString=sceplus-vm49.almaden.ibm.com:2181,sceplus-vm48.almaden.ibm.com:2181 sessionTimeout=90000 watcher=hconnection-0x77b4ae76, quorum=sceplus-vm49.almaden.ibm.com:2181,sceplus-vm48.almaden.ibm.com:2181, baseZNode=/hbase
2014-06-30 19:47:14,458 INFO  [regionserver60020] zookeeper.RecoverableZooKeeper: Process identifier=hconnection-0x77b4ae76 connecting to ZooKeeper ensemble=sceplus-vm49.almaden.ibm.com:2181,sceplus-vm48.almaden.ibm.com:2181
2014-06-30 19:47:14,458 INFO  [regionserver60020-SendThread(sceplus-vm48.almaden.ibm.com:2181)] zookeeper.ClientCnxn: Opening socket connection to server sceplus-vm48.almaden.ibm.com/9.1.143.58:2181. Will not attempt to authenticate using SASL (unknown error)
2014-06-30 19:47:14,459 INFO  [regionserver60020-SendThread(sceplus-vm48.almaden.ibm.com:2181)] zookeeper.ClientCnxn: Socket connection established to sceplus-vm48.almaden.ibm.com/9.1.143.58:2181, initiating session
2014-06-30 19:47:14,463 INFO  [regionserver60020-SendThread(sceplus-vm48.almaden.ibm.com:2181)] zookeeper.ClientCnxn: Session establishment complete on server sceplus-vm48.almaden.ibm.com/9.1.143.58:2181, sessionid = 0x46efd12dab0002, negotiated timeout = 90000
2014-06-30 19:47:14,470 INFO  [RpcServer.responder] ipc.RpcServer: RpcServer.responder: starting
2014-06-30 19:47:14,470 INFO  [RpcServer.listener,port=60020] ipc.RpcServer: RpcServer.listener,port=60020: starting
2014-06-30 19:47:14,509 INFO  [regionserver60020] regionserver.HRegionServer: Serving as sceplus-vm48.almaden.ibm.com,60020,1404182803191, RpcServer on sceplus-vm48.almaden.ibm.com/9.1.143.58:60020, sessionid=0x146efd1264e0001
2014-06-30 19:47:14,509 INFO  [SplitLogWorker-sceplus-vm48.almaden.ibm.com,60020,1404182803191] regionserver.SplitLogWorker: SplitLogWorker sceplus-vm48.almaden.ibm.com,60020,1404182803191 starting
2014-06-30 19:47:14,509 INFO  [regionserver60020] procedure.RegionServerProcedureManagerHost: Procedure online-snapshot is starting
2014-06-30 19:47:14,510 DEBUG [regionserver60020] snapshot.RegionServerSnapshotManager: Start Snapshot Manager sceplus-vm48.almaden.ibm.com,60020,1404182803191
2014-06-30 19:47:14,510 DEBUG [regionserver60020] procedure.ZKProcedureMemberRpcs: Starting procedure member 'sceplus-vm48.almaden.ibm.com,60020,1404182803191'
2014-06-30 19:47:14,510 DEBUG [regionserver60020] procedure.ZKProcedureMemberRpcs: Checking for aborted procedures on node: '/hbase/online-snapshot/abort'
2014-06-30 19:47:14,511 DEBUG [regionserver60020] procedure.ZKProcedureMemberRpcs: Looking for new procedures under znode:'/hbase/online-snapshot/acquired'
2014-06-30 19:47:14,512 INFO  [regionserver60020] procedure.RegionServerProcedureManagerHost: Procedure online-snapshot is started
2014-06-30 19:47:18,211 DEBUG [regionserver60020-EventThread] regionserver.SplitLogWorker: tasks arrived or departed
2014-06-30 19:47:22,422 DEBUG [regionserver60020-EventThread] regionserver.SplitLogWorker: tasks arrived or departed
2014-06-30 19:47:23,398 DEBUG [regionserver60020-EventThread] regionserver.SplitLogWorker: tasks arrived or departed
2014-06-30 19:47:27,465 DEBUG [regionserver60020-EventThread] regionserver.SplitLogWorker: tasks arrived or departed
2014-06-30 19:47:27,518 INFO  [Priority.RpcServer.handler=1,port=60020] regionserver.HRegionServer: Open usertable,,1404036817712.893969db22c5ffcd245485ec3ac31be0.
2014-06-30 19:47:27,658 INFO  [Priority.RpcServer.handler=1,port=60020] regionserver.HRegionServer: Open usertable,user3,1404036817712.e9af4eb7752cc03c44ccb3ae701c3db3.
2014-06-30 19:47:27,659 DEBUG [RS_OPEN_REGION-sceplus-vm48:60020-0] zookeeper.ZKAssign: regionserver:60020-0x146efd1264e0001, quorum=sceplus-vm49.almaden.ibm.com:2181,sceplus-vm48.almaden.ibm.com:2181, baseZNode=/hbase Transitioning 893969db22c5ffcd245485ec3ac31be0 from M_ZK_REGION_OFFLINE to RS_ZK_REGION_OPENING
2014-06-30 19:47:27,660 INFO  [Priority.RpcServer.handler=1,port=60020] regionserver.HRegionServer: Open usertable,user5,1404036817712.c42f52cd4eca18fb6222332b49f16630.
2014-06-30 19:47:27,660 DEBUG [RS_OPEN_REGION-sceplus-vm48:60020-1] zookeeper.ZKAssign: regionserver:60020-0x146efd1264e0001, quorum=sceplus-vm49.almaden.ibm.com:2181,sceplus-vm48.almaden.ibm.com:2181, baseZNode=/hbase Transitioning e9af4eb7752cc03c44ccb3ae701c3db3 from M_ZK_REGION_OFFLINE to RS_ZK_REGION_OPENING
2014-06-30 19:47:27,661 INFO  [Priority.RpcServer.handler=1,port=60020] regionserver.HRegionServer: Open usertable,user7,1404036817713.aadb9cc7ce320e470191e98ed609814a.
2014-06-30 19:47:27,661 INFO  [Priority.RpcServer.handler=1,port=60020] regionserver.HRegionServer: Open usertable,user9,1404036817713.9827bb5b911bde4159a8b85889111cc0.
2014-06-30 19:47:27,661 DEBUG [RS_OPEN_REGION-sceplus-vm48:60020-2] zookeeper.ZKAssign: regionserver:60020-0x146efd1264e0001, quorum=sceplus-vm49.almaden.ibm.com:2181,sceplus-vm48.almaden.ibm.com:2181, baseZNode=/hbase Transitioning c42f52cd4eca18fb6222332b49f16630 from M_ZK_REGION_OFFLINE to RS_ZK_REGION_OPENING
2014-06-30 19:47:27,673 DEBUG [RS_OPEN_REGION-sceplus-vm48:60020-0] zookeeper.ZKAssign: regionserver:60020-0x146efd1264e0001, quorum=sceplus-vm49.almaden.ibm.com:2181,sceplus-vm48.almaden.ibm.com:2181, baseZNode=/hbase Transitioned node 893969db22c5ffcd245485ec3ac31be0 from M_ZK_REGION_OFFLINE to RS_ZK_REGION_OPENING
2014-06-30 19:47:27,674 DEBUG [RS_OPEN_REGION-sceplus-vm48:60020-1] zookeeper.ZKAssign: regionserver:60020-0x146efd1264e0001, quorum=sceplus-vm49.almaden.ibm.com:2181,sceplus-vm48.almaden.ibm.com:2181, baseZNode=/hbase Transitioned node e9af4eb7752cc03c44ccb3ae701c3db3 from M_ZK_REGION_OFFLINE to RS_ZK_REGION_OPENING
2014-06-30 19:47:27,674 DEBUG [RS_OPEN_REGION-sceplus-vm48:60020-2] zookeeper.ZKAssign: regionserver:60020-0x146efd1264e0001, quorum=sceplus-vm49.almaden.ibm.com:2181,sceplus-vm48.almaden.ibm.com:2181, baseZNode=/hbase Transitioned node c42f52cd4eca18fb6222332b49f16630 from M_ZK_REGION_OFFLINE to RS_ZK_REGION_OPENING
2014-06-30 19:47:27,689 DEBUG [RS_OPEN_REGION-sceplus-vm48:60020-1] regionserver.HRegion: Opening region: {ENCODED => e9af4eb7752cc03c44ccb3ae701c3db3, NAME => 'usertable,user3,1404036817712.e9af4eb7752cc03c44ccb3ae701c3db3.', STARTKEY => 'user3', ENDKEY => 'user4'}
2014-06-30 19:47:27,689 DEBUG [RS_OPEN_REGION-sceplus-vm48:60020-0] regionserver.HRegion: Opening region: {ENCODED => 893969db22c5ffcd245485ec3ac31be0, NAME => 'usertable,,1404036817712.893969db22c5ffcd245485ec3ac31be0.', STARTKEY => '', ENDKEY => 'user2'}
2014-06-30 19:47:27,689 DEBUG [RS_OPEN_REGION-sceplus-vm48:60020-2] regionserver.HRegion: Opening region: {ENCODED => c42f52cd4eca18fb6222332b49f16630, NAME => 'usertable,user5,1404036817712.c42f52cd4eca18fb6222332b49f16630.', STARTKEY => 'user5', ENDKEY => 'user6'}
2014-06-30 19:47:27,713 INFO  [RS_OPEN_REGION-sceplus-vm48:60020-2] impl.MetricsSourceAdapter: MBean for source RegionServer,sub=Regions registered.
2014-06-30 19:47:27,714 DEBUG [RS_OPEN_REGION-sceplus-vm48:60020-2] regionserver.MetricsRegionSourceImpl: Creating new MetricsRegionSourceImpl for table usertable c42f52cd4eca18fb6222332b49f16630
2014-06-30 19:47:27,714 DEBUG [RS_OPEN_REGION-sceplus-vm48:60020-1] regionserver.MetricsRegionSourceImpl: Creating new MetricsRegionSourceImpl for table usertable e9af4eb7752cc03c44ccb3ae701c3db3
2014-06-30 19:47:27,714 DEBUG [RS_OPEN_REGION-sceplus-vm48:60020-0] regionserver.MetricsRegionSourceImpl: Creating new MetricsRegionSourceImpl for table usertable 893969db22c5ffcd245485ec3ac31be0
2014-06-30 19:47:27,714 DEBUG [RS_OPEN_REGION-sceplus-vm48:60020-2] regionserver.HRegion: Instantiated usertable,user5,1404036817712.c42f52cd4eca18fb6222332b49f16630.
2014-06-30 19:47:27,714 DEBUG [RS_OPEN_REGION-sceplus-vm48:60020-1] regionserver.HRegion: Instantiated usertable,user3,1404036817712.e9af4eb7752cc03c44ccb3ae701c3db3.
2014-06-30 19:47:27,714 DEBUG [RS_OPEN_REGION-sceplus-vm48:60020-0] regionserver.HRegion: Instantiated usertable,,1404036817712.893969db22c5ffcd245485ec3ac31be0.
2014-06-30 19:47:27,779 INFO  [StoreOpener-893969db22c5ffcd245485ec3ac31be0-1] compactions.CompactionConfiguration: size [134217728, 9223372036854775807); files [3, 10); ratio 1.200000; off-peak ratio 5.000000; throttle point 2684354560; delete expired; major period 0, major jitter 0.500000
2014-06-30 19:47:27,779 INFO  [StoreOpener-e9af4eb7752cc03c44ccb3ae701c3db3-1] compactions.CompactionConfiguration: size [134217728, 9223372036854775807); files [3, 10); ratio 1.200000; off-peak ratio 5.000000; throttle point 2684354560; delete expired; major period 0, major jitter 0.500000
2014-06-30 19:47:27,786 INFO  [StoreOpener-c42f52cd4eca18fb6222332b49f16630-1] compactions.CompactionConfiguration: size [134217728, 9223372036854775807); files [3, 10); ratio 1.200000; off-peak ratio 5.000000; throttle point 2684354560; delete expired; major period 0, major jitter 0.500000
2014-06-30 19:47:27,809 INFO  [StoreFileOpenerThread-family-1] util.ChecksumType: Checksum using org.apache.hadoop.util.PureJavaCrc32
2014-06-30 19:47:27,872 DEBUG [StoreOpener-c42f52cd4eca18fb6222332b49f16630-1] regionserver.HStore: loaded hdfs://master:54310/hbase/data/default/usertable/c42f52cd4eca18fb6222332b49f16630/family/04310a900fc44286b9e3a9877ae7af06, isReference=false, isBulkLoadResult=false, seqid=1166, majorCompaction=false
2014-06-30 19:47:27,872 DEBUG [StoreOpener-e9af4eb7752cc03c44ccb3ae701c3db3-1] regionserver.HStore: loaded hdfs://master:54310/hbase/data/default/usertable/e9af4eb7752cc03c44ccb3ae701c3db3/family/66a675b1026a44db805e59801b672a28, isReference=false, isBulkLoadResult=false, seqid=1182, majorCompaction=false
2014-06-30 19:47:27,875 DEBUG [StoreOpener-893969db22c5ffcd245485ec3ac31be0-1] regionserver.HStore: loaded hdfs://master:54310/hbase/data/default/usertable/893969db22c5ffcd245485ec3ac31be0/family/3c1032acd8c7489a8b49c49a38d4aa0e, isReference=false, isBulkLoadResult=false, seqid=874, majorCompaction=true
2014-06-30 19:47:27,941 DEBUG [StoreOpener-c42f52cd4eca18fb6222332b49f16630-1] regionserver.HStore: loaded hdfs://master:54310/hbase/data/default/usertable/c42f52cd4eca18fb6222332b49f16630/family/9afe02818b8d4ac3b0b8128618881c6a, isReference=false, isBulkLoadResult=false, seqid=1131, majorCompaction=true
2014-06-30 19:47:27,941 DEBUG [StoreOpener-893969db22c5ffcd245485ec3ac31be0-1] regionserver.HStore: loaded hdfs://master:54310/hbase/data/default/usertable/893969db22c5ffcd245485ec3ac31be0/family/e4f6a835665b4da695bd964138da8194, isReference=false, isBulkLoadResult=false, seqid=883, majorCompaction=false
2014-06-30 19:47:27,964 DEBUG [RS_OPEN_REGION-sceplus-vm48:60020-0] regionserver.HRegion: Found 0 recovered edits file(s) under hdfs://master:54310/hbase/data/default/usertable/893969db22c5ffcd245485ec3ac31be0
2014-06-30 19:47:27,966 DEBUG [StoreOpener-e9af4eb7752cc03c44ccb3ae701c3db3-1] regionserver.HStore: loaded hdfs://master:54310/hbase/data/default/usertable/e9af4eb7752cc03c44ccb3ae701c3db3/family/83259cb4859f42108fda2af77cc0904d, isReference=false, isBulkLoadResult=false, seqid=1144, majorCompaction=true
2014-06-30 19:47:27,966 DEBUG [RS_OPEN_REGION-sceplus-vm48:60020-2] regionserver.HRegion: Found 0 recovered edits file(s) under hdfs://master:54310/hbase/data/default/usertable/c42f52cd4eca18fb6222332b49f16630
2014-06-30 19:47:27,970 INFO  [RS_OPEN_REGION-sceplus-vm48:60020-2] regionserver.HRegion: Onlined c42f52cd4eca18fb6222332b49f16630; next sequenceid=1167
2014-06-30 19:47:27,970 INFO  [RS_OPEN_REGION-sceplus-vm48:60020-0] regionserver.HRegion: Onlined 893969db22c5ffcd245485ec3ac31be0; next sequenceid=884
2014-06-30 19:47:27,970 DEBUG [RS_OPEN_REGION-sceplus-vm48:60020-0] zookeeper.ZKAssign: regionserver:60020-0x146efd1264e0001, quorum=sceplus-vm49.almaden.ibm.com:2181,sceplus-vm48.almaden.ibm.com:2181, baseZNode=/hbase Attempting to retransition opening state of node 893969db22c5ffcd245485ec3ac31be0
2014-06-30 19:47:27,970 DEBUG [RS_OPEN_REGION-sceplus-vm48:60020-2] zookeeper.ZKAssign: regionserver:60020-0x146efd1264e0001, quorum=sceplus-vm49.almaden.ibm.com:2181,sceplus-vm48.almaden.ibm.com:2181, baseZNode=/hbase Attempting to retransition opening state of node c42f52cd4eca18fb6222332b49f16630
2014-06-30 19:47:27,973 INFO  [PostOpenDeployTasks:893969db22c5ffcd245485ec3ac31be0] regionserver.HRegionServer: Post open deploy tasks for region=usertable,,1404036817712.893969db22c5ffcd245485ec3ac31be0.
2014-06-30 19:47:27,974 INFO  [PostOpenDeployTasks:c42f52cd4eca18fb6222332b49f16630] regionserver.HRegionServer: Post open deploy tasks for region=usertable,user5,1404036817712.c42f52cd4eca18fb6222332b49f16630.
2014-06-30 19:47:28,007 DEBUG [RS_OPEN_REGION-sceplus-vm48:60020-1] regionserver.HRegion: Found 0 recovered edits file(s) under hdfs://master:54310/hbase/data/default/usertable/e9af4eb7752cc03c44ccb3ae701c3db3
2014-06-30 19:47:28,009 INFO  [RS_OPEN_REGION-sceplus-vm48:60020-1] regionserver.HRegion: Onlined e9af4eb7752cc03c44ccb3ae701c3db3; next sequenceid=1183
2014-06-30 19:47:28,009 DEBUG [RS_OPEN_REGION-sceplus-vm48:60020-1] zookeeper.ZKAssign: regionserver:60020-0x146efd1264e0001, quorum=sceplus-vm49.almaden.ibm.com:2181,sceplus-vm48.almaden.ibm.com:2181, baseZNode=/hbase Attempting to retransition opening state of node e9af4eb7752cc03c44ccb3ae701c3db3
2014-06-30 19:47:28,011 INFO  [PostOpenDeployTasks:e9af4eb7752cc03c44ccb3ae701c3db3] regionserver.HRegionServer: Post open deploy tasks for region=usertable,user3,1404036817712.e9af4eb7752cc03c44ccb3ae701c3db3.
2014-06-30 19:47:28,096 INFO  [PostOpenDeployTasks:c42f52cd4eca18fb6222332b49f16630] catalog.MetaEditor: Updated row usertable,user5,1404036817712.c42f52cd4eca18fb6222332b49f16630. with server=sceplus-vm48.almaden.ibm.com,60020,1404182803191
2014-06-30 19:47:28,096 INFO  [PostOpenDeployTasks:e9af4eb7752cc03c44ccb3ae701c3db3] catalog.MetaEditor: Updated row usertable,user3,1404036817712.e9af4eb7752cc03c44ccb3ae701c3db3. with server=sceplus-vm48.almaden.ibm.com,60020,1404182803191
2014-06-30 19:47:28,096 INFO  [PostOpenDeployTasks:c42f52cd4eca18fb6222332b49f16630] regionserver.HRegionServer: Finished post open deploy task for usertable,user5,1404036817712.c42f52cd4eca18fb6222332b49f16630.
2014-06-30 19:47:28,096 INFO  [PostOpenDeployTasks:893969db22c5ffcd245485ec3ac31be0] catalog.MetaEditor: Updated row usertable,,1404036817712.893969db22c5ffcd245485ec3ac31be0. with server=sceplus-vm48.almaden.ibm.com,60020,1404182803191
2014-06-30 19:47:28,096 DEBUG [RS_OPEN_REGION-sceplus-vm48:60020-2] zookeeper.ZKAssign: regionserver:60020-0x146efd1264e0001, quorum=sceplus-vm49.almaden.ibm.com:2181,sceplus-vm48.almaden.ibm.com:2181, baseZNode=/hbase Transitioning c42f52cd4eca18fb6222332b49f16630 from RS_ZK_REGION_OPENING to RS_ZK_REGION_OPENED
2014-06-30 19:47:28,096 INFO  [PostOpenDeployTasks:e9af4eb7752cc03c44ccb3ae701c3db3] regionserver.HRegionServer: Finished post open deploy task for usertable,user3,1404036817712.e9af4eb7752cc03c44ccb3ae701c3db3.
2014-06-30 19:47:28,096 INFO  [PostOpenDeployTasks:893969db22c5ffcd245485ec3ac31be0] regionserver.HRegionServer: Finished post open deploy task for usertable,,1404036817712.893969db22c5ffcd245485ec3ac31be0.
2014-06-30 19:47:28,097 DEBUG [RS_OPEN_REGION-sceplus-vm48:60020-1] zookeeper.ZKAssign: regionserver:60020-0x146efd1264e0001, quorum=sceplus-vm49.almaden.ibm.com:2181,sceplus-vm48.almaden.ibm.com:2181, baseZNode=/hbase Transitioning e9af4eb7752cc03c44ccb3ae701c3db3 from RS_ZK_REGION_OPENING to RS_ZK_REGION_OPENED
2014-06-30 19:47:28,098 DEBUG [RS_OPEN_REGION-sceplus-vm48:60020-0] zookeeper.ZKAssign: regionserver:60020-0x146efd1264e0001, quorum=sceplus-vm49.almaden.ibm.com:2181,sceplus-vm48.almaden.ibm.com:2181, baseZNode=/hbase Transitioning 893969db22c5ffcd245485ec3ac31be0 from RS_ZK_REGION_OPENING to RS_ZK_REGION_OPENED
2014-06-30 19:47:28,106 DEBUG [RS_OPEN_REGION-sceplus-vm48:60020-2] zookeeper.ZKAssign: regionserver:60020-0x146efd1264e0001, quorum=sceplus-vm49.almaden.ibm.com:2181,sceplus-vm48.almaden.ibm.com:2181, baseZNode=/hbase Transitioned node c42f52cd4eca18fb6222332b49f16630 from RS_ZK_REGION_OPENING to RS_ZK_REGION_OPENED
2014-06-30 19:47:28,107 DEBUG [RS_OPEN_REGION-sceplus-vm48:60020-2] handler.OpenRegionHandler: Transitioned c42f52cd4eca18fb6222332b49f16630 to OPENED in zk on sceplus-vm48.almaden.ibm.com,60020,1404182803191
2014-06-30 19:47:28,107 DEBUG [RS_OPEN_REGION-sceplus-vm48:60020-2] handler.OpenRegionHandler: Opened usertable,user5,1404036817712.c42f52cd4eca18fb6222332b49f16630. on sceplus-vm48.almaden.ibm.com,60020,1404182803191
2014-06-30 19:47:28,107 DEBUG [RS_OPEN_REGION-sceplus-vm48:60020-2] zookeeper.ZKAssign: regionserver:60020-0x146efd1264e0001, quorum=sceplus-vm49.almaden.ibm.com:2181,sceplus-vm48.almaden.ibm.com:2181, baseZNode=/hbase Transitioning aadb9cc7ce320e470191e98ed609814a from M_ZK_REGION_OFFLINE to RS_ZK_REGION_OPENING
2014-06-30 19:47:28,108 DEBUG [RS_OPEN_REGION-sceplus-vm48:60020-0] zookeeper.ZKAssign: regionserver:60020-0x146efd1264e0001, quorum=sceplus-vm49.almaden.ibm.com:2181,sceplus-vm48.almaden.ibm.com:2181, baseZNode=/hbase Transitioned node 893969db22c5ffcd245485ec3ac31be0 from RS_ZK_REGION_OPENING to RS_ZK_REGION_OPENED
2014-06-30 19:47:28,108 DEBUG [RS_OPEN_REGION-sceplus-vm48:60020-0] handler.OpenRegionHandler: Transitioned 893969db22c5ffcd245485ec3ac31be0 to OPENED in zk on sceplus-vm48.almaden.ibm.com,60020,1404182803191
2014-06-30 19:47:28,108 DEBUG [RS_OPEN_REGION-sceplus-vm48:60020-0] handler.OpenRegionHandler: Opened usertable,,1404036817712.893969db22c5ffcd245485ec3ac31be0. on sceplus-vm48.almaden.ibm.com,60020,1404182803191
2014-06-30 19:47:28,108 DEBUG [RS_OPEN_REGION-sceplus-vm48:60020-1] zookeeper.ZKAssign: regionserver:60020-0x146efd1264e0001, quorum=sceplus-vm49.almaden.ibm.com:2181,sceplus-vm48.almaden.ibm.com:2181, baseZNode=/hbase Transitioned node e9af4eb7752cc03c44ccb3ae701c3db3 from RS_ZK_REGION_OPENING to RS_ZK_REGION_OPENED
2014-06-30 19:47:28,108 DEBUG [RS_OPEN_REGION-sceplus-vm48:60020-0] zookeeper.ZKAssign: regionserver:60020-0x146efd1264e0001, quorum=sceplus-vm49.almaden.ibm.com:2181,sceplus-vm48.almaden.ibm.com:2181, baseZNode=/hbase Transitioning 9827bb5b911bde4159a8b85889111cc0 from M_ZK_REGION_OFFLINE to RS_ZK_REGION_OPENING
2014-06-30 19:47:28,109 DEBUG [RS_OPEN_REGION-sceplus-vm48:60020-1] handler.OpenRegionHandler: Transitioned e9af4eb7752cc03c44ccb3ae701c3db3 to OPENED in zk on sceplus-vm48.almaden.ibm.com,60020,1404182803191
2014-06-30 19:47:28,109 DEBUG [RS_OPEN_REGION-sceplus-vm48:60020-1] handler.OpenRegionHandler: Opened usertable,user3,1404036817712.e9af4eb7752cc03c44ccb3ae701c3db3. on sceplus-vm48.almaden.ibm.com,60020,1404182803191
2014-06-30 19:47:28,113 DEBUG [RS_OPEN_REGION-sceplus-vm48:60020-2] zookeeper.ZKAssign: regionserver:60020-0x146efd1264e0001, quorum=sceplus-vm49.almaden.ibm.com:2181,sceplus-vm48.almaden.ibm.com:2181, baseZNode=/hbase Transitioned node aadb9cc7ce320e470191e98ed609814a from M_ZK_REGION_OFFLINE to RS_ZK_REGION_OPENING
2014-06-30 19:47:28,113 DEBUG [RS_OPEN_REGION-sceplus-vm48:60020-2] regionserver.HRegion: Opening region: {ENCODED => aadb9cc7ce320e470191e98ed609814a, NAME => 'usertable,user7,1404036817713.aadb9cc7ce320e470191e98ed609814a.', STARTKEY => 'user7', ENDKEY => 'user8'}
2014-06-30 19:47:28,114 DEBUG [RS_OPEN_REGION-sceplus-vm48:60020-2] regionserver.MetricsRegionSourceImpl: Creating new MetricsRegionSourceImpl for table usertable aadb9cc7ce320e470191e98ed609814a
2014-06-30 19:47:28,114 DEBUG [RS_OPEN_REGION-sceplus-vm48:60020-2] regionserver.HRegion: Instantiated usertable,user7,1404036817713.aadb9cc7ce320e470191e98ed609814a.
2014-06-30 19:47:28,115 DEBUG [RS_OPEN_REGION-sceplus-vm48:60020-0] zookeeper.ZKAssign: regionserver:60020-0x146efd1264e0001, quorum=sceplus-vm49.almaden.ibm.com:2181,sceplus-vm48.almaden.ibm.com:2181, baseZNode=/hbase Transitioned node 9827bb5b911bde4159a8b85889111cc0 from M_ZK_REGION_OFFLINE to RS_ZK_REGION_OPENING
2014-06-30 19:47:28,116 DEBUG [RS_OPEN_REGION-sceplus-vm48:60020-0] regionserver.HRegion: Opening region: {ENCODED => 9827bb5b911bde4159a8b85889111cc0, NAME => 'usertable,user9,1404036817713.9827bb5b911bde4159a8b85889111cc0.', STARTKEY => 'user9', ENDKEY => ''}
2014-06-30 19:47:28,116 DEBUG [RS_OPEN_REGION-sceplus-vm48:60020-0] regionserver.MetricsRegionSourceImpl: Creating new MetricsRegionSourceImpl for table usertable 9827bb5b911bde4159a8b85889111cc0
2014-06-30 19:47:28,116 DEBUG [RS_OPEN_REGION-sceplus-vm48:60020-0] regionserver.HRegion: Instantiated usertable,user9,1404036817713.9827bb5b911bde4159a8b85889111cc0.
2014-06-30 19:47:28,122 INFO  [StoreOpener-aadb9cc7ce320e470191e98ed609814a-1] compactions.CompactionConfiguration: size [134217728, 9223372036854775807); files [3, 10); ratio 1.200000; off-peak ratio 5.000000; throttle point 2684354560; delete expired; major period 0, major jitter 0.500000
2014-06-30 19:47:28,125 INFO  [StoreOpener-9827bb5b911bde4159a8b85889111cc0-1] compactions.CompactionConfiguration: size [134217728, 9223372036854775807); files [3, 10); ratio 1.200000; off-peak ratio 5.000000; throttle point 2684354560; delete expired; major period 0, major jitter 0.500000
2014-06-30 19:47:28,157 DEBUG [StoreOpener-aadb9cc7ce320e470191e98ed609814a-1] regionserver.HStore: loaded hdfs://master:54310/hbase/data/default/usertable/aadb9cc7ce320e470191e98ed609814a/family/3f0cba4332d24b50bb36bc69fd2da201, isReference=false, isBulkLoadResult=false, seqid=1284, majorCompaction=false
2014-06-30 19:47:28,162 DEBUG [StoreOpener-9827bb5b911bde4159a8b85889111cc0-1] regionserver.HStore: loaded hdfs://master:54310/hbase/data/default/usertable/9827bb5b911bde4159a8b85889111cc0/family/15aa0e1e4fd4403fa0f7fa9b5c506f8b, isReference=false, isBulkLoadResult=false, seqid=1254, majorCompaction=true
2014-06-30 19:47:28,182 DEBUG [StoreOpener-aadb9cc7ce320e470191e98ed609814a-1] regionserver.HStore: loaded hdfs://master:54310/hbase/data/default/usertable/aadb9cc7ce320e470191e98ed609814a/family/50bb0c4da8904e6cb875475b27113f5c, isReference=false, isBulkLoadResult=false, seqid=1271, majorCompaction=false
2014-06-30 19:47:28,184 DEBUG [StoreOpener-9827bb5b911bde4159a8b85889111cc0-1] regionserver.HStore: loaded hdfs://master:54310/hbase/data/default/usertable/9827bb5b911bde4159a8b85889111cc0/family/4e03b6db95bb4f1f9e730d471d85f9ed, isReference=false, isBulkLoadResult=false, seqid=1267, majorCompaction=false
2014-06-30 19:47:28,187 DEBUG [RS_OPEN_REGION-sceplus-vm48:60020-0] regionserver.HRegion: Found 0 recovered edits file(s) under hdfs://master:54310/hbase/data/default/usertable/9827bb5b911bde4159a8b85889111cc0
2014-06-30 19:47:28,189 INFO  [RS_OPEN_REGION-sceplus-vm48:60020-0] regionserver.HRegion: Onlined 9827bb5b911bde4159a8b85889111cc0; next sequenceid=1268
2014-06-30 19:47:28,190 DEBUG [RS_OPEN_REGION-sceplus-vm48:60020-0] zookeeper.ZKAssign: regionserver:60020-0x146efd1264e0001, quorum=sceplus-vm49.almaden.ibm.com:2181,sceplus-vm48.almaden.ibm.com:2181, baseZNode=/hbase Attempting to retransition opening state of node 9827bb5b911bde4159a8b85889111cc0
2014-06-30 19:47:28,191 INFO  [PostOpenDeployTasks:9827bb5b911bde4159a8b85889111cc0] regionserver.HRegionServer: Post open deploy tasks for region=usertable,user9,1404036817713.9827bb5b911bde4159a8b85889111cc0.
2014-06-30 19:47:28,200 INFO  [PostOpenDeployTasks:9827bb5b911bde4159a8b85889111cc0] catalog.MetaEditor: Updated row usertable,user9,1404036817713.9827bb5b911bde4159a8b85889111cc0. with server=sceplus-vm48.almaden.ibm.com,60020,1404182803191
2014-06-30 19:47:28,200 INFO  [PostOpenDeployTasks:9827bb5b911bde4159a8b85889111cc0] regionserver.HRegionServer: Finished post open deploy task for usertable,user9,1404036817713.9827bb5b911bde4159a8b85889111cc0.
2014-06-30 19:47:28,200 DEBUG [RS_OPEN_REGION-sceplus-vm48:60020-0] zookeeper.ZKAssign: regionserver:60020-0x146efd1264e0001, quorum=sceplus-vm49.almaden.ibm.com:2181,sceplus-vm48.almaden.ibm.com:2181, baseZNode=/hbase Transitioning 9827bb5b911bde4159a8b85889111cc0 from RS_ZK_REGION_OPENING to RS_ZK_REGION_OPENED
2014-06-30 19:47:28,204 DEBUG [RS_OPEN_REGION-sceplus-vm48:60020-0] zookeeper.ZKAssign: regionserver:60020-0x146efd1264e0001, quorum=sceplus-vm49.almaden.ibm.com:2181,sceplus-vm48.almaden.ibm.com:2181, baseZNode=/hbase Transitioned node 9827bb5b911bde4159a8b85889111cc0 from RS_ZK_REGION_OPENING to RS_ZK_REGION_OPENED
2014-06-30 19:47:28,204 DEBUG [RS_OPEN_REGION-sceplus-vm48:60020-0] handler.OpenRegionHandler: Transitioned 9827bb5b911bde4159a8b85889111cc0 to OPENED in zk on sceplus-vm48.almaden.ibm.com,60020,1404182803191
2014-06-30 19:47:28,204 DEBUG [RS_OPEN_REGION-sceplus-vm48:60020-0] handler.OpenRegionHandler: Opened usertable,user9,1404036817713.9827bb5b911bde4159a8b85889111cc0. on sceplus-vm48.almaden.ibm.com,60020,1404182803191
2014-06-30 19:47:28,209 DEBUG [StoreOpener-aadb9cc7ce320e470191e98ed609814a-1] regionserver.HStore: loaded hdfs://master:54310/hbase/data/default/usertable/aadb9cc7ce320e470191e98ed609814a/family/a2e6af9335bb4496a90d7616343b7648, isReference=false, isBulkLoadResult=false, seqid=933, majorCompaction=true
2014-06-30 19:47:28,211 DEBUG [RS_OPEN_REGION-sceplus-vm48:60020-2] regionserver.HRegion: Found 0 recovered edits file(s) under hdfs://master:54310/hbase/data/default/usertable/aadb9cc7ce320e470191e98ed609814a
2014-06-30 19:47:28,214 INFO  [RS_OPEN_REGION-sceplus-vm48:60020-2] regionserver.HRegion: Onlined aadb9cc7ce320e470191e98ed609814a; next sequenceid=1285
2014-06-30 19:47:28,214 DEBUG [RS_OPEN_REGION-sceplus-vm48:60020-2] zookeeper.ZKAssign: regionserver:60020-0x146efd1264e0001, quorum=sceplus-vm49.almaden.ibm.com:2181,sceplus-vm48.almaden.ibm.com:2181, baseZNode=/hbase Attempting to retransition opening state of node aadb9cc7ce320e470191e98ed609814a
2014-06-30 19:47:28,216 INFO  [PostOpenDeployTasks:aadb9cc7ce320e470191e98ed609814a] regionserver.HRegionServer: Post open deploy tasks for region=usertable,user7,1404036817713.aadb9cc7ce320e470191e98ed609814a.
2014-06-30 19:47:28,217 DEBUG [PostOpenDeployTasks:aadb9cc7ce320e470191e98ed609814a] regionserver.CompactSplitThread: Small Compaction requested: system; Because: Opening Region; compaction_queue=(0:0), split_queue=0, merge_queue=0
2014-06-30 19:47:28,218 DEBUG [regionserver60020-smallCompactions-1404182848217] compactions.RatioBasedCompactionPolicy: Selecting compaction from 3 store files, 0 compacting, 3 eligible, 10 blocking
2014-06-30 19:47:28,220 DEBUG [regionserver60020-smallCompactions-1404182848217] compactions.ExploringCompactionPolicy: Exploring compaction algorithm has selected 0 files of size 0 starting at candidate #-1 after considering 1 permutations with 0 in ratio
2014-06-30 19:47:28,220 DEBUG [regionserver60020-smallCompactions-1404182848217] compactions.RatioBasedCompactionPolicy: Not compacting files because we only have 0 files ready for compaction. Need 3 to initiate.
2014-06-30 19:47:28,222 DEBUG [regionserver60020-smallCompactions-1404182848217] regionserver.CompactSplitThread: Not compacting usertable,user7,1404036817713.aadb9cc7ce320e470191e98ed609814a. because compaction request was cancelled
2014-06-30 19:47:28,226 INFO  [PostOpenDeployTasks:aadb9cc7ce320e470191e98ed609814a] catalog.MetaEditor: Updated row usertable,user7,1404036817713.aadb9cc7ce320e470191e98ed609814a. with server=sceplus-vm48.almaden.ibm.com,60020,1404182803191
2014-06-30 19:47:28,226 INFO  [PostOpenDeployTasks:aadb9cc7ce320e470191e98ed609814a] regionserver.HRegionServer: Finished post open deploy task for usertable,user7,1404036817713.aadb9cc7ce320e470191e98ed609814a.
2014-06-30 19:47:28,227 DEBUG [RS_OPEN_REGION-sceplus-vm48:60020-2] zookeeper.ZKAssign: regionserver:60020-0x146efd1264e0001, quorum=sceplus-vm49.almaden.ibm.com:2181,sceplus-vm48.almaden.ibm.com:2181, baseZNode=/hbase Transitioning aadb9cc7ce320e470191e98ed609814a from RS_ZK_REGION_OPENING to RS_ZK_REGION_OPENED
2014-06-30 19:47:28,231 DEBUG [RS_OPEN_REGION-sceplus-vm48:60020-2] zookeeper.ZKAssign: regionserver:60020-0x146efd1264e0001, quorum=sceplus-vm49.almaden.ibm.com:2181,sceplus-vm48.almaden.ibm.com:2181, baseZNode=/hbase Transitioned node aadb9cc7ce320e470191e98ed609814a from RS_ZK_REGION_OPENING to RS_ZK_REGION_OPENED
2014-06-30 19:47:28,231 DEBUG [RS_OPEN_REGION-sceplus-vm48:60020-2] handler.OpenRegionHandler: Transitioned aadb9cc7ce320e470191e98ed609814a to OPENED in zk on sceplus-vm48.almaden.ibm.com,60020,1404182803191
2014-06-30 19:47:28,231 DEBUG [RS_OPEN_REGION-sceplus-vm48:60020-2] handler.OpenRegionHandler: Opened usertable,user7,1404036817713.aadb9cc7ce320e470191e98ed609814a. on sceplus-vm48.almaden.ibm.com,60020,1404182803191
2014-06-30 19:47:32,578 INFO  [Priority.RpcServer.handler=2,port=60020] regionserver.HRegionServer: Close 893969db22c5ffcd245485ec3ac31be0, via zk=yes, znode version=0, on null
2014-06-30 19:47:32,578 INFO  [Priority.RpcServer.handler=3,port=60020] regionserver.HRegionServer: Close c42f52cd4eca18fb6222332b49f16630, via zk=yes, znode version=0, on null
2014-06-30 19:47:32,579 INFO  [Priority.RpcServer.handler=4,port=60020] regionserver.HRegionServer: Close 9827bb5b911bde4159a8b85889111cc0, via zk=yes, znode version=0, on null
2014-06-30 19:47:32,579 INFO  [Priority.RpcServer.handler=5,port=60020] regionserver.HRegionServer: Close e9af4eb7752cc03c44ccb3ae701c3db3, via zk=yes, znode version=0, on null
2014-06-30 19:47:32,581 INFO  [Priority.RpcServer.handler=6,port=60020] regionserver.HRegionServer: Close aadb9cc7ce320e470191e98ed609814a, via zk=yes, znode version=0, on null
2014-06-30 19:47:32,583 DEBUG [RS_CLOSE_REGION-sceplus-vm48:60020-1] handler.CloseRegionHandler: Processing close of usertable,,1404036817712.893969db22c5ffcd245485ec3ac31be0.
2014-06-30 19:47:32,583 DEBUG [RS_CLOSE_REGION-sceplus-vm48:60020-2] handler.CloseRegionHandler: Processing close of usertable,user9,1404036817713.9827bb5b911bde4159a8b85889111cc0.
2014-06-30 19:47:32,583 DEBUG [RS_CLOSE_REGION-sceplus-vm48:60020-0] handler.CloseRegionHandler: Processing close of usertable,user5,1404036817712.c42f52cd4eca18fb6222332b49f16630.
2014-06-30 19:47:32,587 DEBUG [RS_CLOSE_REGION-sceplus-vm48:60020-1] regionserver.HRegion: Closing usertable,,1404036817712.893969db22c5ffcd245485ec3ac31be0.: disabling compactions & flushes
2014-06-30 19:47:32,587 DEBUG [RS_CLOSE_REGION-sceplus-vm48:60020-2] regionserver.HRegion: Closing usertable,user9,1404036817713.9827bb5b911bde4159a8b85889111cc0.: disabling compactions & flushes
2014-06-30 19:47:32,587 DEBUG [RS_CLOSE_REGION-sceplus-vm48:60020-0] regionserver.HRegion: Closing usertable,user5,1404036817712.c42f52cd4eca18fb6222332b49f16630.: disabling compactions & flushes
2014-06-30 19:47:32,588 DEBUG [RS_CLOSE_REGION-sceplus-vm48:60020-1] regionserver.HRegion: Updates disabled for region usertable,,1404036817712.893969db22c5ffcd245485ec3ac31be0.
2014-06-30 19:47:32,588 DEBUG [RS_CLOSE_REGION-sceplus-vm48:60020-0] regionserver.HRegion: Updates disabled for region usertable,user5,1404036817712.c42f52cd4eca18fb6222332b49f16630.
2014-06-30 19:47:32,588 DEBUG [RS_CLOSE_REGION-sceplus-vm48:60020-2] regionserver.HRegion: Updates disabled for region usertable,user9,1404036817713.9827bb5b911bde4159a8b85889111cc0.
2014-06-30 19:47:32,613 INFO  [StoreCloserThread-usertable,user5,1404036817712.c42f52cd4eca18fb6222332b49f16630.-1] regionserver.HStore: Closed family
2014-06-30 19:47:32,614 INFO  [StoreCloserThread-usertable,,1404036817712.893969db22c5ffcd245485ec3ac31be0.-1] regionserver.HStore: Closed family
2014-06-30 19:47:32,615 INFO  [StoreCloserThread-usertable,user9,1404036817713.9827bb5b911bde4159a8b85889111cc0.-1] regionserver.HStore: Closed family
2014-06-30 19:47:32,619 INFO  [RS_CLOSE_REGION-sceplus-vm48:60020-1] regionserver.HRegion: Closed usertable,,1404036817712.893969db22c5ffcd245485ec3ac31be0.
2014-06-30 19:47:32,619 INFO  [RS_CLOSE_REGION-sceplus-vm48:60020-2] regionserver.HRegion: Closed usertable,user9,1404036817713.9827bb5b911bde4159a8b85889111cc0.
2014-06-30 19:47:32,619 DEBUG [RS_CLOSE_REGION-sceplus-vm48:60020-1] zookeeper.ZKAssign: regionserver:60020-0x146efd1264e0001, quorum=sceplus-vm49.almaden.ibm.com:2181,sceplus-vm48.almaden.ibm.com:2181, baseZNode=/hbase Transitioning 893969db22c5ffcd245485ec3ac31be0 from M_ZK_REGION_CLOSING to RS_ZK_REGION_CLOSED
2014-06-30 19:47:32,619 INFO  [RS_CLOSE_REGION-sceplus-vm48:60020-0] regionserver.HRegion: Closed usertable,user5,1404036817712.c42f52cd4eca18fb6222332b49f16630.
2014-06-30 19:47:32,619 DEBUG [RS_CLOSE_REGION-sceplus-vm48:60020-2] zookeeper.ZKAssign: regionserver:60020-0x146efd1264e0001, quorum=sceplus-vm49.almaden.ibm.com:2181,sceplus-vm48.almaden.ibm.com:2181, baseZNode=/hbase Transitioning 9827bb5b911bde4159a8b85889111cc0 from M_ZK_REGION_CLOSING to RS_ZK_REGION_CLOSED
2014-06-30 19:47:32,620 DEBUG [RS_CLOSE_REGION-sceplus-vm48:60020-0] zookeeper.ZKAssign: regionserver:60020-0x146efd1264e0001, quorum=sceplus-vm49.almaden.ibm.com:2181,sceplus-vm48.almaden.ibm.com:2181, baseZNode=/hbase Transitioning c42f52cd4eca18fb6222332b49f16630 from M_ZK_REGION_CLOSING to RS_ZK_REGION_CLOSED
2014-06-30 19:47:32,625 DEBUG [RS_CLOSE_REGION-sceplus-vm48:60020-1] zookeeper.ZKAssign: regionserver:60020-0x146efd1264e0001, quorum=sceplus-vm49.almaden.ibm.com:2181,sceplus-vm48.almaden.ibm.com:2181, baseZNode=/hbase Transitioned node 893969db22c5ffcd245485ec3ac31be0 from M_ZK_REGION_CLOSING to RS_ZK_REGION_CLOSED
2014-06-30 19:47:32,625 DEBUG [RS_CLOSE_REGION-sceplus-vm48:60020-1] handler.CloseRegionHandler: Set closed state in zk for usertable,,1404036817712.893969db22c5ffcd245485ec3ac31be0. on sceplus-vm48.almaden.ibm.com,60020,1404182803191
2014-06-30 19:47:32,625 DEBUG [RS_CLOSE_REGION-sceplus-vm48:60020-1] handler.CloseRegionHandler: Closed usertable,,1404036817712.893969db22c5ffcd245485ec3ac31be0.
2014-06-30 19:47:32,625 DEBUG [RS_CLOSE_REGION-sceplus-vm48:60020-1] handler.CloseRegionHandler: Processing close of usertable,user7,1404036817713.aadb9cc7ce320e470191e98ed609814a.
2014-06-30 19:47:32,626 DEBUG [RS_CLOSE_REGION-sceplus-vm48:60020-2] zookeeper.ZKAssign: regionserver:60020-0x146efd1264e0001, quorum=sceplus-vm49.almaden.ibm.com:2181,sceplus-vm48.almaden.ibm.com:2181, baseZNode=/hbase Transitioned node 9827bb5b911bde4159a8b85889111cc0 from M_ZK_REGION_CLOSING to RS_ZK_REGION_CLOSED
2014-06-30 19:47:32,626 DEBUG [RS_CLOSE_REGION-sceplus-vm48:60020-2] handler.CloseRegionHandler: Set closed state in zk for usertable,user9,1404036817713.9827bb5b911bde4159a8b85889111cc0. on sceplus-vm48.almaden.ibm.com,60020,1404182803191
2014-06-30 19:47:32,626 DEBUG [RS_CLOSE_REGION-sceplus-vm48:60020-2] handler.CloseRegionHandler: Closed usertable,user9,1404036817713.9827bb5b911bde4159a8b85889111cc0.
2014-06-30 19:47:32,626 DEBUG [RS_CLOSE_REGION-sceplus-vm48:60020-0] zookeeper.ZKAssign: regionserver:60020-0x146efd1264e0001, quorum=sceplus-vm49.almaden.ibm.com:2181,sceplus-vm48.almaden.ibm.com:2181, baseZNode=/hbase Transitioned node c42f52cd4eca18fb6222332b49f16630 from M_ZK_REGION_CLOSING to RS_ZK_REGION_CLOSED
2014-06-30 19:47:32,626 DEBUG [RS_CLOSE_REGION-sceplus-vm48:60020-0] handler.CloseRegionHandler: Set closed state in zk for usertable,user5,1404036817712.c42f52cd4eca18fb6222332b49f16630. on sceplus-vm48.almaden.ibm.com,60020,1404182803191
2014-06-30 19:47:32,626 DEBUG [RS_CLOSE_REGION-sceplus-vm48:60020-0] handler.CloseRegionHandler: Closed usertable,user5,1404036817712.c42f52cd4eca18fb6222332b49f16630.
2014-06-30 19:47:32,626 DEBUG [RS_CLOSE_REGION-sceplus-vm48:60020-2] handler.CloseRegionHandler: Processing close of usertable,user3,1404036817712.e9af4eb7752cc03c44ccb3ae701c3db3.
2014-06-30 19:47:32,627 DEBUG [RS_CLOSE_REGION-sceplus-vm48:60020-1] regionserver.HRegion: Closing usertable,user7,1404036817713.aadb9cc7ce320e470191e98ed609814a.: disabling compactions & flushes
2014-06-30 19:47:32,627 DEBUG [RS_CLOSE_REGION-sceplus-vm48:60020-1] regionserver.HRegion: Updates disabled for region usertable,user7,1404036817713.aadb9cc7ce320e470191e98ed609814a.
2014-06-30 19:47:32,630 INFO  [StoreCloserThread-usertable,user7,1404036817713.aadb9cc7ce320e470191e98ed609814a.-1] regionserver.HStore: Closed family
2014-06-30 19:47:32,631 INFO  [RS_CLOSE_REGION-sceplus-vm48:60020-1] regionserver.HRegion: Closed usertable,user7,1404036817713.aadb9cc7ce320e470191e98ed609814a.
2014-06-30 19:47:32,631 DEBUG [RS_CLOSE_REGION-sceplus-vm48:60020-1] zookeeper.ZKAssign: regionserver:60020-0x146efd1264e0001, quorum=sceplus-vm49.almaden.ibm.com:2181,sceplus-vm48.almaden.ibm.com:2181, baseZNode=/hbase Transitioning aadb9cc7ce320e470191e98ed609814a from M_ZK_REGION_CLOSING to RS_ZK_REGION_CLOSED
2014-06-30 19:47:32,635 DEBUG [RS_CLOSE_REGION-sceplus-vm48:60020-2] regionserver.HRegion: Closing usertable,user3,1404036817712.e9af4eb7752cc03c44ccb3ae701c3db3.: disabling compactions & flushes
2014-06-30 19:47:32,635 DEBUG [RS_CLOSE_REGION-sceplus-vm48:60020-2] regionserver.HRegion: Updates disabled for region usertable,user3,1404036817712.e9af4eb7752cc03c44ccb3ae701c3db3.
2014-06-30 19:47:32,638 INFO  [StoreCloserThread-usertable,user3,1404036817712.e9af4eb7752cc03c44ccb3ae701c3db3.-1] regionserver.HStore: Closed family
2014-06-30 19:47:32,638 INFO  [RS_CLOSE_REGION-sceplus-vm48:60020-2] regionserver.HRegion: Closed usertable,user3,1404036817712.e9af4eb7752cc03c44ccb3ae701c3db3.
2014-06-30 19:47:32,639 DEBUG [RS_CLOSE_REGION-sceplus-vm48:60020-2] zookeeper.ZKAssign: regionserver:60020-0x146efd1264e0001, quorum=sceplus-vm49.almaden.ibm.com:2181,sceplus-vm48.almaden.ibm.com:2181, baseZNode=/hbase Transitioning e9af4eb7752cc03c44ccb3ae701c3db3 from M_ZK_REGION_CLOSING to RS_ZK_REGION_CLOSED
2014-06-30 19:47:32,646 DEBUG [RS_CLOSE_REGION-sceplus-vm48:60020-1] zookeeper.ZKAssign: regionserver:60020-0x146efd1264e0001, quorum=sceplus-vm49.almaden.ibm.com:2181,sceplus-vm48.almaden.ibm.com:2181, baseZNode=/hbase Transitioned node aadb9cc7ce320e470191e98ed609814a from M_ZK_REGION_CLOSING to RS_ZK_REGION_CLOSED
2014-06-30 19:47:32,646 DEBUG [RS_CLOSE_REGION-sceplus-vm48:60020-1] handler.CloseRegionHandler: Set closed state in zk for usertable,user7,1404036817713.aadb9cc7ce320e470191e98ed609814a. on sceplus-vm48.almaden.ibm.com,60020,1404182803191
2014-06-30 19:47:32,646 DEBUG [RS_CLOSE_REGION-sceplus-vm48:60020-1] handler.CloseRegionHandler: Closed usertable,user7,1404036817713.aadb9cc7ce320e470191e98ed609814a.
2014-06-30 19:47:32,658 DEBUG [RS_CLOSE_REGION-sceplus-vm48:60020-2] zookeeper.ZKAssign: regionserver:60020-0x146efd1264e0001, quorum=sceplus-vm49.almaden.ibm.com:2181,sceplus-vm48.almaden.ibm.com:2181, baseZNode=/hbase Transitioned node e9af4eb7752cc03c44ccb3ae701c3db3 from M_ZK_REGION_CLOSING to RS_ZK_REGION_CLOSED
2014-06-30 19:47:32,658 DEBUG [RS_CLOSE_REGION-sceplus-vm48:60020-2] handler.CloseRegionHandler: Set closed state in zk for usertable,user3,1404036817712.e9af4eb7752cc03c44ccb3ae701c3db3. on sceplus-vm48.almaden.ibm.com,60020,1404182803191
2014-06-30 19:47:32,658 DEBUG [RS_CLOSE_REGION-sceplus-vm48:60020-2] handler.CloseRegionHandler: Closed usertable,user3,1404036817712.e9af4eb7752cc03c44ccb3ae701c3db3.
2014-06-30 19:51:43,266 DEBUG [LruStats #0] hfile.LruBlockCache: Total=407.28 KB, free=386.28 MB, max=386.68 MB, blocks=0, accesses=0, hits=0, hitRatio=0, cachingAccesses=0, cachingHits=0, cachingHitsRatio=0,evictions=0, evicted=0, evictedPerRun=NaN
2014-06-30 19:52:40,732 INFO  [Priority.RpcServer.handler=7,port=60020] regionserver.HRegionServer: Open usertable,user2,1404183160444.5446f1f888398e97c0ec3cc30da5bf97.
2014-06-30 19:52:40,747 INFO  [Priority.RpcServer.handler=7,port=60020] regionserver.HRegionServer: Open usertable,user9,1404183160445.9638edaa7b212ccfbe14e9984bd5a5a3.
2014-06-30 19:52:40,748 DEBUG [RS_OPEN_REGION-sceplus-vm48:60020-1] zookeeper.ZKAssign: regionserver:60020-0x146efd1264e0001, quorum=sceplus-vm49.almaden.ibm.com:2181,sceplus-vm48.almaden.ibm.com:2181, baseZNode=/hbase Transitioning 5446f1f888398e97c0ec3cc30da5bf97 from M_ZK_REGION_OFFLINE to RS_ZK_REGION_OPENING
2014-06-30 19:52:40,748 DEBUG [RS_OPEN_REGION-sceplus-vm48:60020-0] zookeeper.ZKAssign: regionserver:60020-0x146efd1264e0001, quorum=sceplus-vm49.almaden.ibm.com:2181,sceplus-vm48.almaden.ibm.com:2181, baseZNode=/hbase Transitioning 9638edaa7b212ccfbe14e9984bd5a5a3 from M_ZK_REGION_OFFLINE to RS_ZK_REGION_OPENING
2014-06-30 19:52:40,748 INFO  [Priority.RpcServer.handler=7,port=60020] regionserver.HRegionServer: Open usertable,user7,1404183160444.994b975d2c4cd5bbe532bddfb5e4ac46.
2014-06-30 19:52:40,750 INFO  [Priority.RpcServer.handler=7,port=60020] regionserver.HRegionServer: Open usertable,user5,1404183160444.1d66ac045cc0407c0fa426db565fcbba.
2014-06-30 19:52:40,750 DEBUG [RS_OPEN_REGION-sceplus-vm48:60020-2] zookeeper.ZKAssign: regionserver:60020-0x146efd1264e0001, quorum=sceplus-vm49.almaden.ibm.com:2181,sceplus-vm48.almaden.ibm.com:2181, baseZNode=/hbase Transitioning 994b975d2c4cd5bbe532bddfb5e4ac46 from M_ZK_REGION_OFFLINE to RS_ZK_REGION_OPENING
2014-06-30 19:52:40,750 INFO  [Priority.RpcServer.handler=7,port=60020] regionserver.HRegionServer: Open usertable,user8,1404183160444.28938d267c1c61f7feb582b85e4c172f.
2014-06-30 19:52:40,755 DEBUG [RS_OPEN_REGION-sceplus-vm48:60020-1] zookeeper.ZKAssign: regionserver:60020-0x146efd1264e0001, quorum=sceplus-vm49.almaden.ibm.com:2181,sceplus-vm48.almaden.ibm.com:2181, baseZNode=/hbase Transitioned node 5446f1f888398e97c0ec3cc30da5bf97 from M_ZK_REGION_OFFLINE to RS_ZK_REGION_OPENING
2014-06-30 19:52:40,756 DEBUG [RS_OPEN_REGION-sceplus-vm48:60020-1] regionserver.HRegion: Opening region: {ENCODED => 5446f1f888398e97c0ec3cc30da5bf97, NAME => 'usertable,user2,1404183160444.5446f1f888398e97c0ec3cc30da5bf97.', STARTKEY => 'user2', ENDKEY => 'user3'}
2014-06-30 19:52:40,756 DEBUG [RS_OPEN_REGION-sceplus-vm48:60020-0] zookeeper.ZKAssign: regionserver:60020-0x146efd1264e0001, quorum=sceplus-vm49.almaden.ibm.com:2181,sceplus-vm48.almaden.ibm.com:2181, baseZNode=/hbase Transitioned node 9638edaa7b212ccfbe14e9984bd5a5a3 from M_ZK_REGION_OFFLINE to RS_ZK_REGION_OPENING
2014-06-30 19:52:40,757 DEBUG [RS_OPEN_REGION-sceplus-vm48:60020-0] regionserver.HRegion: Opening region: {ENCODED => 9638edaa7b212ccfbe14e9984bd5a5a3, NAME => 'usertable,user9,1404183160445.9638edaa7b212ccfbe14e9984bd5a5a3.', STARTKEY => 'user9', ENDKEY => ''}
2014-06-30 19:52:40,757 DEBUG [RS_OPEN_REGION-sceplus-vm48:60020-1] regionserver.MetricsRegionSourceImpl: Creating new MetricsRegionSourceImpl for table usertable 5446f1f888398e97c0ec3cc30da5bf97
2014-06-30 19:52:40,758 DEBUG [RS_OPEN_REGION-sceplus-vm48:60020-1] regionserver.HRegion: Instantiated usertable,user2,1404183160444.5446f1f888398e97c0ec3cc30da5bf97.
2014-06-30 19:52:40,758 DEBUG [RS_OPEN_REGION-sceplus-vm48:60020-0] regionserver.MetricsRegionSourceImpl: Creating new MetricsRegionSourceImpl for table usertable 9638edaa7b212ccfbe14e9984bd5a5a3
2014-06-30 19:52:40,758 DEBUG [RS_OPEN_REGION-sceplus-vm48:60020-0] regionserver.HRegion: Instantiated usertable,user9,1404183160445.9638edaa7b212ccfbe14e9984bd5a5a3.
2014-06-30 19:52:40,759 DEBUG [RS_OPEN_REGION-sceplus-vm48:60020-2] zookeeper.ZKAssign: regionserver:60020-0x146efd1264e0001, quorum=sceplus-vm49.almaden.ibm.com:2181,sceplus-vm48.almaden.ibm.com:2181, baseZNode=/hbase Transitioned node 994b975d2c4cd5bbe532bddfb5e4ac46 from M_ZK_REGION_OFFLINE to RS_ZK_REGION_OPENING
2014-06-30 19:52:40,760 DEBUG [RS_OPEN_REGION-sceplus-vm48:60020-2] regionserver.HRegion: Opening region: {ENCODED => 994b975d2c4cd5bbe532bddfb5e4ac46, NAME => 'usertable,user7,1404183160444.994b975d2c4cd5bbe532bddfb5e4ac46.', STARTKEY => 'user7', ENDKEY => 'user8'}
2014-06-30 19:52:40,761 DEBUG [RS_OPEN_REGION-sceplus-vm48:60020-2] regionserver.MetricsRegionSourceImpl: Creating new MetricsRegionSourceImpl for table usertable 994b975d2c4cd5bbe532bddfb5e4ac46
2014-06-30 19:52:40,761 DEBUG [RS_OPEN_REGION-sceplus-vm48:60020-2] regionserver.HRegion: Instantiated usertable,user7,1404183160444.994b975d2c4cd5bbe532bddfb5e4ac46.
2014-06-30 19:52:40,769 INFO  [StoreOpener-5446f1f888398e97c0ec3cc30da5bf97-1] compactions.CompactionConfiguration: size [134217728, 9223372036854775807); files [3, 10); ratio 1.200000; off-peak ratio 5.000000; throttle point 2684354560; delete expired; major period 0, major jitter 0.500000
2014-06-30 19:52:40,769 INFO  [StoreOpener-9638edaa7b212ccfbe14e9984bd5a5a3-1] compactions.CompactionConfiguration: size [134217728, 9223372036854775807); files [3, 10); ratio 1.200000; off-peak ratio 5.000000; throttle point 2684354560; delete expired; major period 0, major jitter 0.500000
2014-06-30 19:52:40,772 INFO  [StoreOpener-994b975d2c4cd5bbe532bddfb5e4ac46-1] compactions.CompactionConfiguration: size [134217728, 9223372036854775807); files [3, 10); ratio 1.200000; off-peak ratio 5.000000; throttle point 2684354560; delete expired; major period 0, major jitter 0.500000
2014-06-30 19:52:40,773 DEBUG [RS_OPEN_REGION-sceplus-vm48:60020-1] regionserver.HRegion: Found 0 recovered edits file(s) under hdfs://master:54310/hbase/data/default/usertable/5446f1f888398e97c0ec3cc30da5bf97
2014-06-30 19:52:40,773 DEBUG [RS_OPEN_REGION-sceplus-vm48:60020-0] regionserver.HRegion: Found 0 recovered edits file(s) under hdfs://master:54310/hbase/data/default/usertable/9638edaa7b212ccfbe14e9984bd5a5a3
2014-06-30 19:52:40,775 DEBUG [RS_OPEN_REGION-sceplus-vm48:60020-2] regionserver.HRegion: Found 0 recovered edits file(s) under hdfs://master:54310/hbase/data/default/usertable/994b975d2c4cd5bbe532bddfb5e4ac46
2014-06-30 19:52:40,775 INFO  [RS_OPEN_REGION-sceplus-vm48:60020-1] regionserver.HRegion: Onlined 5446f1f888398e97c0ec3cc30da5bf97; next sequenceid=1
2014-06-30 19:52:40,775 DEBUG [RS_OPEN_REGION-sceplus-vm48:60020-1] zookeeper.ZKAssign: regionserver:60020-0x146efd1264e0001, quorum=sceplus-vm49.almaden.ibm.com:2181,sceplus-vm48.almaden.ibm.com:2181, baseZNode=/hbase Attempting to retransition opening state of node 5446f1f888398e97c0ec3cc30da5bf97
2014-06-30 19:52:40,776 INFO  [RS_OPEN_REGION-sceplus-vm48:60020-0] regionserver.HRegion: Onlined 9638edaa7b212ccfbe14e9984bd5a5a3; next sequenceid=1
2014-06-30 19:52:40,776 DEBUG [RS_OPEN_REGION-sceplus-vm48:60020-0] zookeeper.ZKAssign: regionserver:60020-0x146efd1264e0001, quorum=sceplus-vm49.almaden.ibm.com:2181,sceplus-vm48.almaden.ibm.com:2181, baseZNode=/hbase Attempting to retransition opening state of node 9638edaa7b212ccfbe14e9984bd5a5a3
2014-06-30 19:52:40,777 INFO  [PostOpenDeployTasks:5446f1f888398e97c0ec3cc30da5bf97] regionserver.HRegionServer: Post open deploy tasks for region=usertable,user2,1404183160444.5446f1f888398e97c0ec3cc30da5bf97.
2014-06-30 19:52:40,778 INFO  [PostOpenDeployTasks:9638edaa7b212ccfbe14e9984bd5a5a3] regionserver.HRegionServer: Post open deploy tasks for region=usertable,user9,1404183160445.9638edaa7b212ccfbe14e9984bd5a5a3.
2014-06-30 19:52:40,788 INFO  [PostOpenDeployTasks:5446f1f888398e97c0ec3cc30da5bf97] catalog.MetaEditor: Updated row usertable,user2,1404183160444.5446f1f888398e97c0ec3cc30da5bf97. with server=sceplus-vm48.almaden.ibm.com,60020,1404182803191
2014-06-30 19:52:40,788 INFO  [PostOpenDeployTasks:5446f1f888398e97c0ec3cc30da5bf97] regionserver.HRegionServer: Finished post open deploy task for usertable,user2,1404183160444.5446f1f888398e97c0ec3cc30da5bf97.
2014-06-30 19:52:40,789 DEBUG [RS_OPEN_REGION-sceplus-vm48:60020-1] zookeeper.ZKAssign: regionserver:60020-0x146efd1264e0001, quorum=sceplus-vm49.almaden.ibm.com:2181,sceplus-vm48.almaden.ibm.com:2181, baseZNode=/hbase Transitioning 5446f1f888398e97c0ec3cc30da5bf97 from RS_ZK_REGION_OPENING to RS_ZK_REGION_OPENED
2014-06-30 19:52:40,790 INFO  [PostOpenDeployTasks:9638edaa7b212ccfbe14e9984bd5a5a3] catalog.MetaEditor: Updated row usertable,user9,1404183160445.9638edaa7b212ccfbe14e9984bd5a5a3. with server=sceplus-vm48.almaden.ibm.com,60020,1404182803191
2014-06-30 19:52:40,790 INFO  [PostOpenDeployTasks:9638edaa7b212ccfbe14e9984bd5a5a3] regionserver.HRegionServer: Finished post open deploy task for usertable,user9,1404183160445.9638edaa7b212ccfbe14e9984bd5a5a3.
2014-06-30 19:52:40,791 DEBUG [RS_OPEN_REGION-sceplus-vm48:60020-0] zookeeper.ZKAssign: regionserver:60020-0x146efd1264e0001, quorum=sceplus-vm49.almaden.ibm.com:2181,sceplus-vm48.almaden.ibm.com:2181, baseZNode=/hbase Transitioning 9638edaa7b212ccfbe14e9984bd5a5a3 from RS_ZK_REGION_OPENING to RS_ZK_REGION_OPENED
2014-06-30 19:52:40,798 DEBUG [RS_OPEN_REGION-sceplus-vm48:60020-1] zookeeper.ZKAssign: regionserver:60020-0x146efd1264e0001, quorum=sceplus-vm49.almaden.ibm.com:2181,sceplus-vm48.almaden.ibm.com:2181, baseZNode=/hbase Transitioned node 5446f1f888398e97c0ec3cc30da5bf97 from RS_ZK_REGION_OPENING to RS_ZK_REGION_OPENED
2014-06-30 19:52:40,798 DEBUG [RS_OPEN_REGION-sceplus-vm48:60020-1] handler.OpenRegionHandler: Transitioned 5446f1f888398e97c0ec3cc30da5bf97 to OPENED in zk on sceplus-vm48.almaden.ibm.com,60020,1404182803191
2014-06-30 19:52:40,798 DEBUG [RS_OPEN_REGION-sceplus-vm48:60020-1] handler.OpenRegionHandler: Opened usertable,user2,1404183160444.5446f1f888398e97c0ec3cc30da5bf97. on sceplus-vm48.almaden.ibm.com,60020,1404182803191
2014-06-30 19:52:40,799 DEBUG [RS_OPEN_REGION-sceplus-vm48:60020-1] zookeeper.ZKAssign: regionserver:60020-0x146efd1264e0001, quorum=sceplus-vm49.almaden.ibm.com:2181,sceplus-vm48.almaden.ibm.com:2181, baseZNode=/hbase Transitioning 1d66ac045cc0407c0fa426db565fcbba from M_ZK_REGION_OFFLINE to RS_ZK_REGION_OPENING
2014-06-30 19:52:40,810 DEBUG [RS_OPEN_REGION-sceplus-vm48:60020-0] zookeeper.ZKAssign: regionserver:60020-0x146efd1264e0001, quorum=sceplus-vm49.almaden.ibm.com:2181,sceplus-vm48.almaden.ibm.com:2181, baseZNode=/hbase Transitioned node 9638edaa7b212ccfbe14e9984bd5a5a3 from RS_ZK_REGION_OPENING to RS_ZK_REGION_OPENED
2014-06-30 19:52:40,810 DEBUG [RS_OPEN_REGION-sceplus-vm48:60020-0] handler.OpenRegionHandler: Transitioned 9638edaa7b212ccfbe14e9984bd5a5a3 to OPENED in zk on sceplus-vm48.almaden.ibm.com,60020,1404182803191
2014-06-30 19:52:40,810 DEBUG [RS_OPEN_REGION-sceplus-vm48:60020-0] handler.OpenRegionHandler: Opened usertable,user9,1404183160445.9638edaa7b212ccfbe14e9984bd5a5a3. on sceplus-vm48.almaden.ibm.com,60020,1404182803191
2014-06-30 19:52:40,811 DEBUG [RS_OPEN_REGION-sceplus-vm48:60020-0] zookeeper.ZKAssign: regionserver:60020-0x146efd1264e0001, quorum=sceplus-vm49.almaden.ibm.com:2181,sceplus-vm48.almaden.ibm.com:2181, baseZNode=/hbase Transitioning 28938d267c1c61f7feb582b85e4c172f from M_ZK_REGION_OFFLINE to RS_ZK_REGION_OPENING
2014-06-30 19:52:40,815 DEBUG [RS_OPEN_REGION-sceplus-vm48:60020-1] zookeeper.ZKAssign: regionserver:60020-0x146efd1264e0001, quorum=sceplus-vm49.almaden.ibm.com:2181,sceplus-vm48.almaden.ibm.com:2181, baseZNode=/hbase Transitioned node 1d66ac045cc0407c0fa426db565fcbba from M_ZK_REGION_OFFLINE to RS_ZK_REGION_OPENING
2014-06-30 19:52:40,816 DEBUG [RS_OPEN_REGION-sceplus-vm48:60020-1] regionserver.HRegion: Opening region: {ENCODED => 1d66ac045cc0407c0fa426db565fcbba, NAME => 'usertable,user5,1404183160444.1d66ac045cc0407c0fa426db565fcbba.', STARTKEY => 'user5', ENDKEY => 'user6'}
2014-06-30 19:52:40,817 INFO  [RS_OPEN_REGION-sceplus-vm48:60020-2] regionserver.HRegion: Onlined 994b975d2c4cd5bbe532bddfb5e4ac46; next sequenceid=1
2014-06-30 19:52:40,817 DEBUG [RS_OPEN_REGION-sceplus-vm48:60020-2] zookeeper.ZKAssign: regionserver:60020-0x146efd1264e0001, quorum=sceplus-vm49.almaden.ibm.com:2181,sceplus-vm48.almaden.ibm.com:2181, baseZNode=/hbase Attempting to retransition opening state of node 994b975d2c4cd5bbe532bddfb5e4ac46
2014-06-30 19:52:40,817 DEBUG [RS_OPEN_REGION-sceplus-vm48:60020-1] regionserver.MetricsRegionSourceImpl: Creating new MetricsRegionSourceImpl for table usertable 1d66ac045cc0407c0fa426db565fcbba
2014-06-30 19:52:40,817 DEBUG [RS_OPEN_REGION-sceplus-vm48:60020-1] regionserver.HRegion: Instantiated usertable,user5,1404183160444.1d66ac045cc0407c0fa426db565fcbba.
2014-06-30 19:52:40,824 DEBUG [RS_OPEN_REGION-sceplus-vm48:60020-0] zookeeper.ZKAssign: regionserver:60020-0x146efd1264e0001, quorum=sceplus-vm49.almaden.ibm.com:2181,sceplus-vm48.almaden.ibm.com:2181, baseZNode=/hbase Transitioned node 28938d267c1c61f7feb582b85e4c172f from M_ZK_REGION_OFFLINE to RS_ZK_REGION_OPENING
2014-06-30 19:52:40,824 DEBUG [RS_OPEN_REGION-sceplus-vm48:60020-0] regionserver.HRegion: Opening region: {ENCODED => 28938d267c1c61f7feb582b85e4c172f, NAME => 'usertable,user8,1404183160444.28938d267c1c61f7feb582b85e4c172f.', STARTKEY => 'user8', ENDKEY => 'user9'}
2014-06-30 19:52:40,825 INFO  [PostOpenDeployTasks:994b975d2c4cd5bbe532bddfb5e4ac46] regionserver.HRegionServer: Post open deploy tasks for region=usertable,user7,1404183160444.994b975d2c4cd5bbe532bddfb5e4ac46.
2014-06-30 19:52:40,825 DEBUG [RS_OPEN_REGION-sceplus-vm48:60020-0] regionserver.MetricsRegionSourceImpl: Creating new MetricsRegionSourceImpl for table usertable 28938d267c1c61f7feb582b85e4c172f
2014-06-30 19:52:40,826 DEBUG [RS_OPEN_REGION-sceplus-vm48:60020-0] regionserver.HRegion: Instantiated usertable,user8,1404183160444.28938d267c1c61f7feb582b85e4c172f.
2014-06-30 19:52:40,827 INFO  [StoreOpener-1d66ac045cc0407c0fa426db565fcbba-1] compactions.CompactionConfiguration: size [134217728, 9223372036854775807); files [3, 10); ratio 1.200000; off-peak ratio 5.000000; throttle point 2684354560; delete expired; major period 0, major jitter 0.500000
2014-06-30 19:52:40,832 DEBUG [RS_OPEN_REGION-sceplus-vm48:60020-1] regionserver.HRegion: Found 0 recovered edits file(s) under hdfs://master:54310/hbase/data/default/usertable/1d66ac045cc0407c0fa426db565fcbba
2014-06-30 19:52:40,838 INFO  [StoreOpener-28938d267c1c61f7feb582b85e4c172f-1] compactions.CompactionConfiguration: size [134217728, 9223372036854775807); files [3, 10); ratio 1.200000; off-peak ratio 5.000000; throttle point 2684354560; delete expired; major period 0, major jitter 0.500000
2014-06-30 19:52:40,838 INFO  [RS_OPEN_REGION-sceplus-vm48:60020-1] regionserver.HRegion: Onlined 1d66ac045cc0407c0fa426db565fcbba; next sequenceid=1
2014-06-30 19:52:40,838 DEBUG [RS_OPEN_REGION-sceplus-vm48:60020-1] zookeeper.ZKAssign: regionserver:60020-0x146efd1264e0001, quorum=sceplus-vm49.almaden.ibm.com:2181,sceplus-vm48.almaden.ibm.com:2181, baseZNode=/hbase Attempting to retransition opening state of node 1d66ac045cc0407c0fa426db565fcbba
2014-06-30 19:52:40,841 DEBUG [RS_OPEN_REGION-sceplus-vm48:60020-0] regionserver.HRegion: Found 0 recovered edits file(s) under hdfs://master:54310/hbase/data/default/usertable/28938d267c1c61f7feb582b85e4c172f
2014-06-30 19:52:40,841 INFO  [PostOpenDeployTasks:994b975d2c4cd5bbe532bddfb5e4ac46] catalog.MetaEditor: Updated row usertable,user7,1404183160444.994b975d2c4cd5bbe532bddfb5e4ac46. with server=sceplus-vm48.almaden.ibm.com,60020,1404182803191
2014-06-30 19:52:40,841 INFO  [PostOpenDeployTasks:994b975d2c4cd5bbe532bddfb5e4ac46] regionserver.HRegionServer: Finished post open deploy task for usertable,user7,1404183160444.994b975d2c4cd5bbe532bddfb5e4ac46.
2014-06-30 19:52:40,842 DEBUG [RS_OPEN_REGION-sceplus-vm48:60020-2] zookeeper.ZKAssign: regionserver:60020-0x146efd1264e0001, quorum=sceplus-vm49.almaden.ibm.com:2181,sceplus-vm48.almaden.ibm.com:2181, baseZNode=/hbase Transitioning 994b975d2c4cd5bbe532bddfb5e4ac46 from RS_ZK_REGION_OPENING to RS_ZK_REGION_OPENED
2014-06-30 19:52:40,843 INFO  [RS_OPEN_REGION-sceplus-vm48:60020-0] regionserver.HRegion: Onlined 28938d267c1c61f7feb582b85e4c172f; next sequenceid=1
2014-06-30 19:52:40,843 DEBUG [RS_OPEN_REGION-sceplus-vm48:60020-0] zookeeper.ZKAssign: regionserver:60020-0x146efd1264e0001, quorum=sceplus-vm49.almaden.ibm.com:2181,sceplus-vm48.almaden.ibm.com:2181, baseZNode=/hbase Attempting to retransition opening state of node 28938d267c1c61f7feb582b85e4c172f
2014-06-30 19:52:40,846 INFO  [PostOpenDeployTasks:1d66ac045cc0407c0fa426db565fcbba] regionserver.HRegionServer: Post open deploy tasks for region=usertable,user5,1404183160444.1d66ac045cc0407c0fa426db565fcbba.
2014-06-30 19:52:40,847 INFO  [PostOpenDeployTasks:28938d267c1c61f7feb582b85e4c172f] regionserver.HRegionServer: Post open deploy tasks for region=usertable,user8,1404183160444.28938d267c1c61f7feb582b85e4c172f.
2014-06-30 19:52:40,856 DEBUG [RS_OPEN_REGION-sceplus-vm48:60020-2] zookeeper.ZKAssign: regionserver:60020-0x146efd1264e0001, quorum=sceplus-vm49.almaden.ibm.com:2181,sceplus-vm48.almaden.ibm.com:2181, baseZNode=/hbase Transitioned node 994b975d2c4cd5bbe532bddfb5e4ac46 from RS_ZK_REGION_OPENING to RS_ZK_REGION_OPENED
2014-06-30 19:52:40,856 DEBUG [RS_OPEN_REGION-sceplus-vm48:60020-2] handler.OpenRegionHandler: Transitioned 994b975d2c4cd5bbe532bddfb5e4ac46 to OPENED in zk on sceplus-vm48.almaden.ibm.com,60020,1404182803191
2014-06-30 19:52:40,856 DEBUG [RS_OPEN_REGION-sceplus-vm48:60020-2] handler.OpenRegionHandler: Opened usertable,user7,1404183160444.994b975d2c4cd5bbe532bddfb5e4ac46. on sceplus-vm48.almaden.ibm.com,60020,1404182803191
2014-06-30 19:52:40,862 INFO  [PostOpenDeployTasks:1d66ac045cc0407c0fa426db565fcbba] catalog.MetaEditor: Updated row usertable,user5,1404183160444.1d66ac045cc0407c0fa426db565fcbba. with server=sceplus-vm48.almaden.ibm.com,60020,1404182803191
2014-06-30 19:52:40,862 INFO  [PostOpenDeployTasks:1d66ac045cc0407c0fa426db565fcbba] regionserver.HRegionServer: Finished post open deploy task for usertable,user5,1404183160444.1d66ac045cc0407c0fa426db565fcbba.
2014-06-30 19:52:40,863 DEBUG [RS_OPEN_REGION-sceplus-vm48:60020-1] zookeeper.ZKAssign: regionserver:60020-0x146efd1264e0001, quorum=sceplus-vm49.almaden.ibm.com:2181,sceplus-vm48.almaden.ibm.com:2181, baseZNode=/hbase Transitioning 1d66ac045cc0407c0fa426db565fcbba from RS_ZK_REGION_OPENING to RS_ZK_REGION_OPENED
2014-06-30 19:52:40,874 DEBUG [RS_OPEN_REGION-sceplus-vm48:60020-1] zookeeper.ZKAssign: regionserver:60020-0x146efd1264e0001, quorum=sceplus-vm49.almaden.ibm.com:2181,sceplus-vm48.almaden.ibm.com:2181, baseZNode=/hbase Transitioned node 1d66ac045cc0407c0fa426db565fcbba from RS_ZK_REGION_OPENING to RS_ZK_REGION_OPENED
2014-06-30 19:52:40,874 DEBUG [RS_OPEN_REGION-sceplus-vm48:60020-1] handler.OpenRegionHandler: Transitioned 1d66ac045cc0407c0fa426db565fcbba to OPENED in zk on sceplus-vm48.almaden.ibm.com,60020,1404182803191
2014-06-30 19:52:40,874 DEBUG [RS_OPEN_REGION-sceplus-vm48:60020-1] handler.OpenRegionHandler: Opened usertable,user5,1404183160444.1d66ac045cc0407c0fa426db565fcbba. on sceplus-vm48.almaden.ibm.com,60020,1404182803191
2014-06-30 19:52:40,899 INFO  [PostOpenDeployTasks:28938d267c1c61f7feb582b85e4c172f] catalog.MetaEditor: Updated row usertable,user8,1404183160444.28938d267c1c61f7feb582b85e4c172f. with server=sceplus-vm48.almaden.ibm.com,60020,1404182803191
2014-06-30 19:52:40,899 INFO  [PostOpenDeployTasks:28938d267c1c61f7feb582b85e4c172f] regionserver.HRegionServer: Finished post open deploy task for usertable,user8,1404183160444.28938d267c1c61f7feb582b85e4c172f.
2014-06-30 19:52:40,900 DEBUG [RS_OPEN_REGION-sceplus-vm48:60020-0] zookeeper.ZKAssign: regionserver:60020-0x146efd1264e0001, quorum=sceplus-vm49.almaden.ibm.com:2181,sceplus-vm48.almaden.ibm.com:2181, baseZNode=/hbase Transitioning 28938d267c1c61f7feb582b85e4c172f from RS_ZK_REGION_OPENING to RS_ZK_REGION_OPENED
2014-06-30 19:52:40,906 DEBUG [RS_OPEN_REGION-sceplus-vm48:60020-0] zookeeper.ZKAssign: regionserver:60020-0x146efd1264e0001, quorum=sceplus-vm49.almaden.ibm.com:2181,sceplus-vm48.almaden.ibm.com:2181, baseZNode=/hbase Transitioned node 28938d267c1c61f7feb582b85e4c172f from RS_ZK_REGION_OPENING to RS_ZK_REGION_OPENED
2014-06-30 19:52:40,906 DEBUG [RS_OPEN_REGION-sceplus-vm48:60020-0] handler.OpenRegionHandler: Transitioned 28938d267c1c61f7feb582b85e4c172f to OPENED in zk on sceplus-vm48.almaden.ibm.com,60020,1404182803191
2014-06-30 19:52:40,906 DEBUG [RS_OPEN_REGION-sceplus-vm48:60020-0] handler.OpenRegionHandler: Opened usertable,user8,1404183160444.28938d267c1c61f7feb582b85e4c172f. on sceplus-vm48.almaden.ibm.com,60020,1404182803191
2014-06-30 19:53:02,512 INFO  [JvmPauseMonitor] util.JvmPauseMonitor: Detected pause in JVM or host machine (eg GC): pause of approximately 1713ms
GC pool 'ParNew' had collection(s): count=3 time=260ms
GC pool 'ConcurrentMarkSweep' had collection(s): count=1 time=1843ms
2014-06-30 19:53:02,522 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-06-30 19:53:02,610 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 120 synced till here 85
2014-06-30 19:53:07,150 INFO  [JvmPauseMonitor] util.JvmPauseMonitor: Detected pause in JVM or host machine (eg GC): pause of approximately 4137ms
GC pool 'ParNew' had collection(s): count=1 time=1535ms
GC pool 'ConcurrentMarkSweep' had collection(s): count=1 time=2860ms
2014-06-30 19:53:07,470 WARN  [RpcServer.handler=12,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":10396,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.58:49864","starttimems":1404183176756,"queuetimems":0,"class":"HRegionServer","responsesize":16802,"method":"Multi"}
2014-06-30 19:53:07,477 WARN  [RpcServer.handler=12,port=60020] ipc.RpcServer: RpcServer.respondercallId: 22 service: ClientService methodName: Multi size: 2.9m connection: 9.1.143.58:49864: output error
2014-06-30 19:53:07,478 WARN  [RpcServer.handler=12,port=60020] ipc.RpcServer: RpcServer.handler=12,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-06-30 19:53:07,495 WARN  [RpcServer.handler=4,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":10985,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.58:49864","starttimems":1404183176505,"queuetimems":1,"class":"HRegionServer","responsesize":16900,"method":"Multi"}
2014-06-30 19:53:07,501 WARN  [RpcServer.handler=4,port=60020] ipc.RpcServer: RpcServer.respondercallId: 41 service: ClientService methodName: Multi size: 3.0m connection: 9.1.143.58:49864: output error
2014-06-30 19:53:07,501 WARN  [RpcServer.handler=4,port=60020] ipc.RpcServer: RpcServer.handler=4,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-06-30 19:53:07,503 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1404182803191/sceplus-vm48.almaden.ibm.com%2C60020%2C1404182803191.1404182834317 with entries=120, filesize=100.8m; new WAL /hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1404182803191/sceplus-vm48.almaden.ibm.com%2C60020%2C1404182803191.1404183182522
2014-06-30 19:53:09,487 INFO  [JvmPauseMonitor] util.JvmPauseMonitor: Detected pause in JVM or host machine (eg GC): pause of approximately 1336ms
GC pool 'ConcurrentMarkSweep' had collection(s): count=1 time=1772ms
2014-06-30 19:53:09,556 WARN  [RpcServer.handler=2,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":13302,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.58:49864","starttimems":1404183176249,"queuetimems":0,"class":"HRegionServer","responsesize":17196,"method":"Multi"}
2014-06-30 19:53:09,556 WARN  [RpcServer.handler=6,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":13023,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.58:49864","starttimems":1404183176528,"queuetimems":0,"class":"HRegionServer","responsesize":17167,"method":"Multi"}
2014-06-30 19:53:09,556 WARN  [RpcServer.handler=9,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":12828,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.58:49864","starttimems":1404183176723,"queuetimems":1,"class":"HRegionServer","responsesize":16853,"method":"Multi"}
2014-06-30 19:53:09,562 WARN  [RpcServer.handler=2,port=60020] ipc.RpcServer: RpcServer.respondercallId: 44 service: ClientService methodName: Multi size: 3.0m connection: 9.1.143.58:49864: output error
2014-06-30 19:53:09,562 WARN  [RpcServer.handler=2,port=60020] ipc.RpcServer: RpcServer.handler=2,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-06-30 19:53:09,562 WARN  [RpcServer.handler=9,port=60020] ipc.RpcServer: RpcServer.respondercallId: 28 service: ClientService methodName: Multi size: 2.9m connection: 9.1.143.58:49864: output error
2014-06-30 19:53:09,562 WARN  [RpcServer.handler=9,port=60020] ipc.RpcServer: RpcServer.handler=9,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-06-30 19:53:09,564 WARN  [RpcServer.handler=6,port=60020] ipc.RpcServer: RpcServer.respondercallId: 39 service: ClientService methodName: Multi size: 3.0m connection: 9.1.143.58:49864: output error
2014-06-30 19:53:09,564 WARN  [RpcServer.handler=6,port=60020] ipc.RpcServer: RpcServer.handler=6,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-06-30 19:53:12,019 INFO  [JvmPauseMonitor] util.JvmPauseMonitor: Detected pause in JVM or host machine (eg GC): pause of approximately 2024ms
GC pool 'ParNew' had collection(s): count=1 time=0ms
GC pool 'ConcurrentMarkSweep' had collection(s): count=1 time=2298ms
2014-06-30 19:53:12,173 WARN  [RpcServer.handler=19,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":15016,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.58:49864","starttimems":1404183177156,"queuetimems":0,"class":"HRegionServer","responsesize":17221,"method":"Multi"}
2014-06-30 19:53:12,174 WARN  [RpcServer.handler=19,port=60020] ipc.RpcServer: RpcServer.respondercallId: 7 service: ClientService methodName: Multi size: 3.0m connection: 9.1.143.58:49864: output error
2014-06-30 19:53:12,174 WARN  [RpcServer.handler=19,port=60020] ipc.RpcServer: RpcServer.handler=19,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-06-30 19:53:12,177 WARN  [RpcServer.handler=16,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":15232,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.58:49864","starttimems":1404183176945,"queuetimems":0,"class":"HRegionServer","responsesize":16853,"method":"Multi"}
2014-06-30 19:53:12,177 WARN  [RpcServer.handler=16,port=60020] ipc.RpcServer: RpcServer.respondercallId: 11 service: ClientService methodName: Multi size: 2.9m connection: 9.1.143.58:49864: output error
2014-06-30 19:53:12,177 WARN  [RpcServer.handler=16,port=60020] ipc.RpcServer: RpcServer.handler=16,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-06-30 19:53:14,782 INFO  [JvmPauseMonitor] util.JvmPauseMonitor: Detected pause in JVM or host machine (eg GC): pause of approximately 2262ms
GC pool 'ConcurrentMarkSweep' had collection(s): count=1 time=2341ms
2014-06-30 19:53:14,900 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-06-30 19:53:14,901 WARN  [RpcServer.handler=14,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":17981,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.58:49864","starttimems":1404183176920,"queuetimems":0,"class":"HRegionServer","responsesize":16580,"method":"Multi"}
2014-06-30 19:53:14,901 WARN  [RpcServer.handler=8,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":18190,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.58:49864","starttimems":1404183176711,"queuetimems":0,"class":"HRegionServer","responsesize":17208,"method":"Multi"}
2014-06-30 19:53:14,901 WARN  [RpcServer.handler=14,port=60020] ipc.RpcServer: RpcServer.respondercallId: 18 service: ClientService methodName: Multi size: 2.9m connection: 9.1.143.58:49864: output error
2014-06-30 19:53:14,902 WARN  [RpcServer.handler=14,port=60020] ipc.RpcServer: RpcServer.handler=14,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-06-30 19:53:14,902 WARN  [RpcServer.handler=8,port=60020] ipc.RpcServer: RpcServer.respondercallId: 29 service: ClientService methodName: Multi size: 3.0m connection: 9.1.143.58:49864: output error
2014-06-30 19:53:14,902 WARN  [RpcServer.handler=8,port=60020] ipc.RpcServer: RpcServer.handler=8,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-06-30 19:53:14,902 WARN  [RpcServer.handler=10,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":18167,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.58:49864","starttimems":1404183176734,"queuetimems":0,"class":"HRegionServer","responsesize":16882,"method":"Multi"}
2014-06-30 19:53:14,902 WARN  [RpcServer.handler=10,port=60020] ipc.RpcServer: RpcServer.respondercallId: 26 service: ClientService methodName: Multi size: 3.0m connection: 9.1.143.58:49864: output error
2014-06-30 19:53:14,903 WARN  [RpcServer.handler=10,port=60020] ipc.RpcServer: RpcServer.handler=10,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-06-30 19:53:14,903 WARN  [RpcServer.handler=5,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":18385,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.58:49864","starttimems":1404183176517,"queuetimems":0,"class":"HRegionServer","responsesize":17388,"method":"Multi"}
2014-06-30 19:53:14,903 WARN  [RpcServer.handler=13,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":17984,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.58:49864","starttimems":1404183176919,"queuetimems":11,"class":"HRegionServer","responsesize":16624,"method":"Multi"}
2014-06-30 19:53:14,903 WARN  [RpcServer.handler=7,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":18361,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.58:49864","starttimems":1404183176542,"queuetimems":2,"class":"HRegionServer","responsesize":17088,"method":"Multi"}
2014-06-30 19:53:14,903 WARN  [RpcServer.handler=5,port=60020] ipc.RpcServer: RpcServer.respondercallId: 40 service: ClientService methodName: Multi size: 3.0m connection: 9.1.143.58:49864: output error
2014-06-30 19:53:14,903 WARN  [RpcServer.handler=5,port=60020] ipc.RpcServer: RpcServer.handler=5,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-06-30 19:53:14,903 WARN  [RpcServer.handler=13,port=60020] ipc.RpcServer: RpcServer.respondercallId: 20 service: ClientService methodName: Multi size: 2.9m connection: 9.1.143.58:49864: output error
2014-06-30 19:53:14,904 WARN  [RpcServer.handler=13,port=60020] ipc.RpcServer: RpcServer.handler=13,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-06-30 19:53:14,904 WARN  [RpcServer.handler=7,port=60020] ipc.RpcServer: RpcServer.respondercallId: 37 service: ClientService methodName: Multi size: 3.0m connection: 9.1.143.58:49864: output error
2014-06-30 19:53:14,904 WARN  [RpcServer.handler=7,port=60020] ipc.RpcServer: RpcServer.handler=7,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-06-30 19:53:14,904 WARN  [RpcServer.handler=18,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":17777,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.58:49864","starttimems":1404183177127,"queuetimems":0,"class":"HRegionServer","responsesize":17012,"method":"Multi"}
2014-06-30 19:53:14,904 WARN  [RpcServer.handler=18,port=60020] ipc.RpcServer: RpcServer.respondercallId: 8 service: ClientService methodName: Multi size: 3.0m connection: 9.1.143.58:49864: output error
2014-06-30 19:53:14,904 WARN  [RpcServer.handler=18,port=60020] ipc.RpcServer: RpcServer.handler=18,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-06-30 19:53:14,904 WARN  [RpcServer.handler=11,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":18158,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.58:49864","starttimems":1404183176745,"queuetimems":0,"class":"HRegionServer","responsesize":16880,"method":"Multi"}
2014-06-30 19:53:14,905 WARN  [RpcServer.handler=11,port=60020] ipc.RpcServer: RpcServer.respondercallId: 24 service: ClientService methodName: Multi size: 2.9m connection: 9.1.143.58:49864: output error
2014-06-30 19:53:14,905 WARN  [RpcServer.handler=11,port=60020] ipc.RpcServer: RpcServer.handler=11,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-06-30 19:53:14,912 WARN  [RpcServer.handler=15,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":17972,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.58:49864","starttimems":1404183176932,"queuetimems":0,"class":"HRegionServer","responsesize":16760,"method":"Multi"}
2014-06-30 19:53:14,912 WARN  [RpcServer.handler=17,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":17945,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.58:49864","starttimems":1404183176959,"queuetimems":1,"class":"HRegionServer","responsesize":16829,"method":"Multi"}
2014-06-30 19:53:14,912 WARN  [RpcServer.handler=1,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":18672,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.58:49864","starttimems":1404183176232,"queuetimems":2,"class":"HRegionServer","responsesize":16943,"method":"Multi"}
2014-06-30 19:53:14,912 WARN  [RpcServer.handler=15,port=60020] ipc.RpcServer: RpcServer.respondercallId: 17 service: ClientService methodName: Multi size: 2.9m connection: 9.1.143.58:49864: output error
2014-06-30 19:53:14,912 WARN  [RpcServer.handler=15,port=60020] ipc.RpcServer: RpcServer.handler=15,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-06-30 19:53:14,912 WARN  [RpcServer.handler=17,port=60020] ipc.RpcServer: RpcServer.respondercallId: 10 service: ClientService methodName: Multi size: 2.9m connection: 9.1.143.58:49864: output error
2014-06-30 19:53:14,912 WARN  [RpcServer.handler=17,port=60020] ipc.RpcServer: RpcServer.handler=17,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-06-30 19:53:14,912 WARN  [RpcServer.handler=1,port=60020] ipc.RpcServer: RpcServer.respondercallId: 45 service: ClientService methodName: Multi size: 3.0m connection: 9.1.143.58:49864: output error
2014-06-30 19:53:14,913 WARN  [RpcServer.handler=3,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":18414,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.58:49864","starttimems":1404183176490,"queuetimems":0,"class":"HRegionServer","responsesize":17169,"method":"Multi"}
2014-06-30 19:53:14,913 WARN  [RpcServer.handler=1,port=60020] ipc.RpcServer: RpcServer.handler=1,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-06-30 19:53:14,913 WARN  [RpcServer.handler=3,port=60020] ipc.RpcServer: RpcServer.respondercallId: 42 service: ClientService methodName: Multi size: 3.0m connection: 9.1.143.58:49864: output error
2014-06-30 19:53:14,913 WARN  [RpcServer.handler=3,port=60020] ipc.RpcServer: RpcServer.handler=3,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-06-30 19:53:17,438 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 240 synced till here 205
2014-06-30 19:53:17,438 INFO  [JvmPauseMonitor] util.JvmPauseMonitor: Detected pause in JVM or host machine (eg GC): pause of approximately 2155ms
GC pool 'ConcurrentMarkSweep' had collection(s): count=1 time=2290ms
2014-06-30 19:53:17,443 WARN  [RpcServer.handler=33,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":18334,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.58:49864","starttimems":1404183179109,"queuetimems":1,"class":"HRegionServer","responsesize":16063,"method":"Multi"}
2014-06-30 19:53:17,444 WARN  [RpcServer.handler=33,port=60020] ipc.RpcServer: RpcServer.respondercallId: 61 service: ClientService methodName: Multi size: 2.8m connection: 9.1.143.58:49864: output error
2014-06-30 19:53:17,444 WARN  [RpcServer.handler=33,port=60020] ipc.RpcServer: RpcServer.handler=33,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-06-30 19:53:17,445 WARN  [RpcServer.handler=34,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":18326,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.58:49864","starttimems":1404183179119,"queuetimems":0,"class":"HRegionServer","responsesize":15616,"method":"Multi"}
2014-06-30 19:53:17,446 WARN  [RpcServer.handler=34,port=60020] ipc.RpcServer: RpcServer.respondercallId: 73 service: ClientService methodName: Multi size: 2.7m connection: 9.1.143.58:49864: output error
2014-06-30 19:53:17,446 WARN  [RpcServer.handler=34,port=60020] ipc.RpcServer: RpcServer.handler=34,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-06-30 19:53:17,459 WARN  [RpcServer.handler=35,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":18329,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.58:49864","starttimems":1404183179130,"queuetimems":0,"class":"HRegionServer","responsesize":15915,"method":"Multi"}
2014-06-30 19:53:17,460 WARN  [RpcServer.handler=35,port=60020] ipc.RpcServer: RpcServer.respondercallId: 71 service: ClientService methodName: Multi size: 2.8m connection: 9.1.143.58:49864: output error
2014-06-30 19:53:17,460 WARN  [RpcServer.handler=35,port=60020] ipc.RpcServer: RpcServer.handler=35,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-06-30 19:53:17,520 WARN  [RpcServer.handler=44,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":17236,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.58:49864","starttimems":1404183180284,"queuetimems":0,"class":"HRegionServer","responsesize":16074,"method":"Multi"}
2014-06-30 19:53:17,520 WARN  [RpcServer.handler=42,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":17255,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.58:49864","starttimems":1404183180265,"queuetimems":1,"class":"HRegionServer","responsesize":15208,"method":"Multi"}
2014-06-30 19:53:17,520 WARN  [RpcServer.handler=46,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":17094,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.58:49864","starttimems":1404183180426,"queuetimems":0,"class":"HRegionServer","responsesize":15489,"method":"Multi"}
2014-06-30 19:53:17,521 WARN  [RpcServer.handler=44,port=60020] ipc.RpcServer: RpcServer.respondercallId: 83 service: ClientService methodName: Multi size: 2.8m connection: 9.1.143.58:49864: output error
2014-06-30 19:53:17,521 WARN  [RpcServer.handler=44,port=60020] ipc.RpcServer: RpcServer.handler=44,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-06-30 19:53:17,521 WARN  [RpcServer.handler=46,port=60020] ipc.RpcServer: RpcServer.respondercallId: 79 service: ClientService methodName: Multi size: 2.7m connection: 9.1.143.58:49864: output error
2014-06-30 19:53:17,521 WARN  [RpcServer.handler=46,port=60020] ipc.RpcServer: RpcServer.handler=46,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-06-30 19:53:17,521 WARN  [RpcServer.handler=42,port=60020] ipc.RpcServer: RpcServer.respondercallId: 86 service: ClientService methodName: Multi size: 2.7m connection: 9.1.143.58:49864: output error
2014-06-30 19:53:17,521 WARN  [RpcServer.handler=38,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":17528,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.58:49864","starttimems":1404183179993,"queuetimems":0,"class":"HRegionServer","responsesize":15658,"method":"Multi"}
2014-06-30 19:53:17,521 WARN  [RpcServer.handler=38,port=60020] ipc.RpcServer: RpcServer.respondercallId: 65 service: ClientService methodName: Multi size: 2.7m connection: 9.1.143.58:49864: output error
2014-06-30 19:53:17,521 WARN  [RpcServer.handler=38,port=60020] ipc.RpcServer: RpcServer.handler=38,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-06-30 19:53:17,520 WARN  [RpcServer.handler=45,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":17104,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.58:49864","starttimems":1404183180416,"queuetimems":0,"class":"HRegionServer","responsesize":15624,"method":"Multi"}
2014-06-30 19:53:17,520 WARN  [RpcServer.handler=36,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":17556,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.58:49864","starttimems":1404183179964,"queuetimems":0,"class":"HRegionServer","responsesize":15737,"method":"Multi"}
2014-06-30 19:53:17,520 WARN  [RpcServer.handler=47,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":17079,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.58:49864","starttimems":1404183180441,"queuetimems":2,"class":"HRegionServer","responsesize":15793,"method":"Multi"}
2014-06-30 19:53:17,521 WARN  [RpcServer.handler=41,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":17271,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.58:49864","starttimems":1404183180250,"queuetimems":110,"class":"HRegionServer","responsesize":15114,"method":"Multi"}
2014-06-30 19:53:17,521 WARN  [RpcServer.handler=42,port=60020] ipc.RpcServer: RpcServer.handler=42,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-06-30 19:53:17,522 WARN  [RpcServer.handler=40,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":17397,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.58:49864","starttimems":1404183180125,"queuetimems":1,"class":"HRegionServer","responsesize":14982,"method":"Multi"}
2014-06-30 19:53:17,522 WARN  [RpcServer.handler=43,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":17247,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.58:49864","starttimems":1404183180275,"queuetimems":1,"class":"HRegionServer","responsesize":15614,"method":"Multi"}
2014-06-30 19:53:17,523 WARN  [RpcServer.handler=37,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":17540,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.58:49864","starttimems":1404183179983,"queuetimems":0,"class":"HRegionServer","responsesize":15737,"method":"Multi"}
2014-06-30 19:53:17,522 WARN  [RpcServer.handler=45,port=60020] ipc.RpcServer: RpcServer.respondercallId: 82 service: ClientService methodName: Multi size: 2.7m connection: 9.1.143.58:49864: output error
2014-06-30 19:53:17,522 WARN  [RpcServer.handler=39,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":17409,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.58:49864","starttimems":1404183180113,"queuetimems":1,"class":"HRegionServer","responsesize":15676,"method":"Multi"}
2014-06-30 19:53:17,523 WARN  [RpcServer.handler=37,port=60020] ipc.RpcServer: RpcServer.respondercallId: 67 service: ClientService methodName: Multi size: 2.8m connection: 9.1.143.58:49864: output error
2014-06-30 19:53:17,523 WARN  [RpcServer.handler=37,port=60020] ipc.RpcServer: RpcServer.handler=37,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-06-30 19:53:17,523 WARN  [RpcServer.handler=45,port=60020] ipc.RpcServer: RpcServer.handler=45,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-06-30 19:53:17,523 WARN  [RpcServer.handler=39,port=60020] ipc.RpcServer: RpcServer.respondercallId: 63 service: ClientService methodName: Multi size: 2.7m connection: 9.1.143.58:49864: output error
2014-06-30 19:53:17,523 WARN  [RpcServer.handler=39,port=60020] ipc.RpcServer: RpcServer.handler=39,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-06-30 19:53:17,527 WARN  [RpcServer.handler=43,port=60020] ipc.RpcServer: RpcServer.respondercallId: 84 service: ClientService methodName: Multi size: 2.7m connection: 9.1.143.58:49864: output error
2014-06-30 19:53:17,527 WARN  [RpcServer.handler=43,port=60020] ipc.RpcServer: RpcServer.handler=43,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-06-30 19:53:17,527 WARN  [RpcServer.handler=40,port=60020] ipc.RpcServer: RpcServer.respondercallId: 90 service: ClientService methodName: Multi size: 2.6m connection: 9.1.143.58:49864: output error
2014-06-30 19:53:17,527 WARN  [RpcServer.handler=40,port=60020] ipc.RpcServer: RpcServer.handler=40,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-06-30 19:53:17,527 WARN  [RpcServer.handler=41,port=60020] ipc.RpcServer: RpcServer.respondercallId: 89 service: ClientService methodName: Multi size: 2.6m connection: 9.1.143.58:49864: output error
2014-06-30 19:53:17,527 WARN  [RpcServer.handler=41,port=60020] ipc.RpcServer: RpcServer.handler=41,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-06-30 19:53:17,527 WARN  [RpcServer.handler=47,port=60020] ipc.RpcServer: RpcServer.respondercallId: 76 service: ClientService methodName: Multi size: 2.8m connection: 9.1.143.58:49864: output error
2014-06-30 19:53:17,528 WARN  [RpcServer.handler=47,port=60020] ipc.RpcServer: RpcServer.handler=47,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-06-30 19:53:17,528 WARN  [RpcServer.handler=36,port=60020] ipc.RpcServer: RpcServer.respondercallId: 69 service: ClientService methodName: Multi size: 2.8m connection: 9.1.143.58:49864: output error
2014-06-30 19:53:17,528 WARN  [RpcServer.handler=36,port=60020] ipc.RpcServer: RpcServer.handler=36,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-06-30 19:53:17,542 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1404182803191/sceplus-vm48.almaden.ibm.com%2C60020%2C1404182803191.1404183182522 with entries=120, filesize=92.4m; new WAL /hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1404182803191/sceplus-vm48.almaden.ibm.com%2C60020%2C1404182803191.1404183194900
2014-06-30 19:53:17,647 WARN  [RpcServer.handler=26,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":18943,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.58:49864","starttimems":1404183178704,"queuetimems":0,"class":"HRegionServer","responsesize":16580,"method":"Multi"}
2014-06-30 19:53:17,647 WARN  [RpcServer.handler=20,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":19387,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.58:49864","starttimems":1404183178260,"queuetimems":1,"class":"HRegionServer","responsesize":17221,"method":"Multi"}
2014-06-30 19:53:17,648 WARN  [RpcServer.handler=26,port=60020] ipc.RpcServer: RpcServer.respondercallId: 59 service: ClientService methodName: Multi size: 2.9m connection: 9.1.143.58:49864: output error
2014-06-30 19:53:17,648 WARN  [RpcServer.handler=26,port=60020] ipc.RpcServer: RpcServer.handler=26,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-06-30 19:53:17,648 WARN  [RpcServer.handler=32,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":18704,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.58:49864","starttimems":1404183178944,"queuetimems":1,"class":"HRegionServer","responsesize":16829,"method":"Multi"}
2014-06-30 19:53:17,648 WARN  [RpcServer.handler=22,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":19228,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.58:49864","starttimems":1404183178420,"queuetimems":0,"class":"HRegionServer","responsesize":16760,"method":"Multi"}
2014-06-30 19:53:17,648 WARN  [RpcServer.handler=32,port=60020] ipc.RpcServer: RpcServer.respondercallId: 53 service: ClientService methodName: Multi size: 2.9m connection: 9.1.143.58:49864: output error
2014-06-30 19:53:17,648 WARN  [RpcServer.handler=30,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":18743,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.58:49864","starttimems":1404183178905,"queuetimems":1,"class":"HRegionServer","responsesize":16853,"method":"Multi"}
2014-06-30 19:53:17,648 WARN  [RpcServer.handler=28,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":18918,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.58:49864","starttimems":1404183178730,"queuetimems":0,"class":"HRegionServer","responsesize":17012,"method":"Multi"}
2014-06-30 19:53:17,648 WARN  [RpcServer.handler=21,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":19250,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.58:49864","starttimems":1404183178398,"queuetimems":0,"class":"HRegionServer","responsesize":16880,"method":"Multi"}
2014-06-30 19:53:17,648 WARN  [RpcServer.handler=20,port=60020] ipc.RpcServer: RpcServer.respondercallId: 48 service: ClientService methodName: Multi size: 3.0m connection: 9.1.143.58:49864: output error
2014-06-30 19:53:17,649 WARN  [RpcServer.handler=20,port=60020] ipc.RpcServer: RpcServer.handler=20,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-06-30 19:53:17,648 WARN  [RpcServer.handler=32,port=60020] ipc.RpcServer: RpcServer.handler=32,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-06-30 19:53:17,649 WARN  [RpcServer.handler=28,port=60020] ipc.RpcServer: RpcServer.respondercallId: 57 service: ClientService methodName: Multi size: 3.0m connection: 9.1.143.58:49864: output error
2014-06-30 19:53:17,648 WARN  [RpcServer.handler=31,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":18721,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.58:49864","starttimems":1404183178927,"queuetimems":0,"class":"HRegionServer","responsesize":16853,"method":"Multi"}
2014-06-30 19:53:17,649 WARN  [RpcServer.handler=30,port=60020] ipc.RpcServer: RpcServer.respondercallId: 55 service: ClientService methodName: Multi size: 2.9m connection: 9.1.143.58:49864: output error
2014-06-30 19:53:17,649 WARN  [RpcServer.handler=28,port=60020] ipc.RpcServer: RpcServer.handler=28,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-06-30 19:53:17,649 WARN  [RpcServer.handler=29,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":18756,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.58:49864","starttimems":1404183178892,"queuetimems":6,"class":"HRegionServer","responsesize":16882,"method":"Multi"}
2014-06-30 19:53:17,649 WARN  [RpcServer.handler=25,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":19046,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.58:49864","starttimems":1404183178602,"queuetimems":1,"class":"HRegionServer","responsesize":17189,"method":"Multi"}
2014-06-30 19:53:17,649 WARN  [RpcServer.handler=49,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":17046,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.58:49864","starttimems":1404183180603,"queuetimems":1,"class":"HRegionServer","responsesize":17167,"method":"Multi"}
2014-06-30 19:53:17,649 WARN  [RpcServer.handler=22,port=60020] ipc.RpcServer: RpcServer.respondercallId: 51 service: ClientService methodName: Multi size: 2.9m connection: 9.1.143.58:49864: output error
2014-06-30 19:53:17,649 WARN  [RpcServer.handler=30,port=60020] ipc.RpcServer: RpcServer.handler=30,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-06-30 19:53:17,649 WARN  [RpcServer.handler=48,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":17198,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.58:49864","starttimems":1404183180451,"queuetimems":0,"class":"HRegionServer","responsesize":17388,"method":"Multi"}
2014-06-30 19:53:17,649 WARN  [RpcServer.handler=23,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":19074,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.58:49864","starttimems":1404183178574,"queuetimems":0,"class":"HRegionServer","responsesize":17208,"method":"Multi"}
2014-06-30 19:53:17,649 WARN  [RpcServer.handler=27,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":18931,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.58:49864","starttimems":1404183178717,"queuetimems":0,"class":"HRegionServer","responsesize":16802,"method":"Multi"}
2014-06-30 19:53:17,649 WARN  [RpcServer.handler=24,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":19060,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.58:49864","starttimems":1404183178588,"queuetimems":0,"class":"HRegionServer","responsesize":16624,"method":"Multi"}
2014-06-30 19:53:17,649 WARN  [RpcServer.handler=25,port=60020] ipc.RpcServer: RpcServer.respondercallId: 60 service: ClientService methodName: Multi size: 3.0m connection: 9.1.143.58:49864: output error
2014-06-30 19:53:17,649 WARN  [RpcServer.handler=22,port=60020] ipc.RpcServer: RpcServer.handler=22,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-06-30 19:53:17,650 WARN  [RpcServer.handler=27,port=60020] ipc.RpcServer: RpcServer.respondercallId: 58 service: ClientService methodName: Multi size: 2.9m connection: 9.1.143.58:49864: output error
2014-06-30 19:53:17,650 WARN  [RpcServer.handler=25,port=60020] ipc.RpcServer: RpcServer.handler=25,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-06-30 19:53:17,651 WARN  [RpcServer.handler=23,port=60020] ipc.RpcServer: RpcServer.respondercallId: 50 service: ClientService methodName: Multi size: 3.0m connection: 9.1.143.58:49864: output error
2014-06-30 19:53:17,651 WARN  [RpcServer.handler=23,port=60020] ipc.RpcServer: RpcServer.handler=23,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-06-30 19:53:17,650 WARN  [RpcServer.handler=27,port=60020] ipc.RpcServer: RpcServer.handler=27,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-06-30 19:53:17,651 WARN  [RpcServer.handler=48,port=60020] ipc.RpcServer: RpcServer.respondercallId: 93 service: ClientService methodName: Multi size: 3.0m connection: 9.1.143.58:49864: output error
2014-06-30 19:53:17,651 WARN  [RpcServer.handler=48,port=60020] ipc.RpcServer: RpcServer.handler=48,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-06-30 19:53:17,651 DEBUG [MemStoreFlusher.0] regionserver.MemStoreFlusher: Flush thread woke up because memory above low water=367.3m
2014-06-30 19:53:17,652 INFO  [MemStoreFlusher.0] regionserver.MemStoreFlusher: Flush of region usertable,user2,1404183160444.5446f1f888398e97c0ec3cc30da5bf97. due to global heap pressure
2014-06-30 19:53:17,652 DEBUG [MemStoreFlusher.0] regionserver.HRegion: Started memstore flush for usertable,user2,1404183160444.5446f1f888398e97c0ec3cc30da5bf97., current region memstore size 92.5m
2014-06-30 19:53:17,654 WARN  [RpcServer.handler=49,port=60020] ipc.RpcServer: RpcServer.respondercallId: 99 service: ClientService methodName: Multi size: 3.0m connection: 9.1.143.58:49864: output error
2014-06-30 19:53:17,655 WARN  [RpcServer.handler=49,port=60020] ipc.RpcServer: RpcServer.handler=49,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-06-30 19:53:17,655 WARN  [RpcServer.handler=29,port=60020] ipc.RpcServer: RpcServer.respondercallId: 56 service: ClientService methodName: Multi size: 3.0m connection: 9.1.143.58:49864: output error
2014-06-30 19:53:17,655 WARN  [RpcServer.handler=29,port=60020] ipc.RpcServer: RpcServer.handler=29,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-06-30 19:53:17,655 WARN  [RpcServer.handler=31,port=60020] ipc.RpcServer: RpcServer.respondercallId: 54 service: ClientService methodName: Multi size: 2.9m connection: 9.1.143.58:49864: output error
2014-06-30 19:53:17,655 WARN  [RpcServer.handler=31,port=60020] ipc.RpcServer: RpcServer.handler=31,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-06-30 19:53:17,655 WARN  [RpcServer.handler=21,port=60020] ipc.RpcServer: RpcServer.respondercallId: 52 service: ClientService methodName: Multi size: 2.9m connection: 9.1.143.58:49864: output error
2014-06-30 19:53:17,655 WARN  [RpcServer.handler=21,port=60020] ipc.RpcServer: RpcServer.handler=21,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-06-30 19:53:17,655 WARN  [RpcServer.handler=24,port=60020] ipc.RpcServer: RpcServer.respondercallId: 49 service: ClientService methodName: Multi size: 2.9m connection: 9.1.143.58:49864: output error
2014-06-30 19:53:17,655 WARN  [RpcServer.handler=24,port=60020] ipc.RpcServer: RpcServer.handler=24,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-06-30 19:53:17,681 WARN  [RpcServer.handler=12,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":10203,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.58:49870","starttimems":1404183187478,"queuetimems":303,"class":"HRegionServer","responsesize":16829,"method":"Multi"}
2014-06-30 19:53:17,681 WARN  [RpcServer.handler=4,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":10179,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.58:49870","starttimems":1404183187502,"queuetimems":280,"class":"HRegionServer","responsesize":17196,"method":"Multi"}
2014-06-30 19:53:17,681 WARN  [RpcServer.handler=0,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":15120,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.58:49864","starttimems":1404183182561,"queuetimems":1945,"class":"HRegionServer","responsesize":17169,"method":"Multi"}
2014-06-30 19:53:17,682 WARN  [RpcServer.handler=12,port=60020] ipc.RpcServer: RpcServer.respondercallId: 329 service: ClientService methodName: Multi size: 2.9m connection: 9.1.143.58:49870: output error
2014-06-30 19:53:17,683 WARN  [RpcServer.handler=0,port=60020] ipc.RpcServer: RpcServer.respondercallId: 98 service: ClientService methodName: Multi size: 3.0m connection: 9.1.143.58:49864: output error
2014-06-30 19:53:17,683 WARN  [RpcServer.handler=12,port=60020] ipc.RpcServer: RpcServer.handler=12,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-06-30 19:53:17,683 WARN  [RpcServer.handler=4,port=60020] ipc.RpcServer: RpcServer.respondercallId: 340 service: ClientService methodName: Multi size: 3.0m connection: 9.1.143.58:49870: output error
2014-06-30 19:53:17,683 WARN  [RpcServer.handler=0,port=60020] ipc.RpcServer: RpcServer.handler=0,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-06-30 19:53:17,684 WARN  [RpcServer.handler=4,port=60020] ipc.RpcServer: RpcServer.handler=4,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-06-30 19:53:17,691 WARN  [RpcServer.handler=9,port=60020] ipc.RpcServer: RpcServer.respondercallId: 338 service: ClientService methodName: Multi size: 2.9m connection: 9.1.143.58:49870: output error
2014-06-30 19:53:17,692 WARN  [RpcServer.handler=9,port=60020] ipc.RpcServer: RpcServer.handler=9,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-06-30 19:53:17,692 WARN  [RpcServer.handler=2,port=60020] ipc.RpcServer: RpcServer.respondercallId: 339 service: ClientService methodName: Multi size: 2.9m connection: 9.1.143.58:49870: output error
2014-06-30 19:53:17,692 WARN  [RpcServer.handler=2,port=60020] ipc.RpcServer: RpcServer.handler=2,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-06-30 19:53:17,712 WARN  [RpcServer.handler=6,port=60020] ipc.RpcServer: RpcServer.respondercallId: 337 service: ClientService methodName: Multi size: 3.0m connection: 9.1.143.58:49870: output error
2014-06-30 19:53:17,712 WARN  [RpcServer.handler=6,port=60020] ipc.RpcServer: RpcServer.handler=6,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-06-30 19:53:17,723 INFO  [RpcServer.handler=16,port=60020] regionserver.MemStoreFlusher: Blocking updates on sceplus-vm48.almaden.ibm.com,60020,1404182803191: the global memstore size 387.4m is >= than blocking 386.7m size
2014-06-30 19:53:17,724 WARN  [RpcServer.handler=19,port=60020] ipc.RpcServer: RpcServer.respondercallId: 336 service: ClientService methodName: Multi size: 3.0m connection: 9.1.143.58:49870: output error
2014-06-30 19:53:17,724 WARN  [RpcServer.handler=19,port=60020] ipc.RpcServer: RpcServer.handler=19,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-06-30 19:53:17,725 INFO  [RpcServer.handler=8,port=60020] regionserver.MemStoreFlusher: Blocking updates on sceplus-vm48.almaden.ibm.com,60020,1404182803191: the global memstore size 387.8m is >= than blocking 386.7m size
2014-06-30 19:53:17,743 DEBUG [MemStoreFlusher.0] util.FSUtils: DFS Client does not support most favored nodes create; using default create
2014-06-30 19:53:18,607 INFO  [RpcServer.handler=14,port=60020] regionserver.MemStoreFlusher: Blocking updates on sceplus-vm48.almaden.ibm.com,60020,1404182803191: the global memstore size 389.5m is >= than blocking 386.7m size
2014-06-30 19:53:18,771 INFO  [RpcServer.handler=10,port=60020] regionserver.MemStoreFlusher: Blocking updates on sceplus-vm48.almaden.ibm.com,60020,1404182803191: the global memstore size 389.5m is >= than blocking 386.7m size
2014-06-30 19:53:18,836 INFO  [RpcServer.handler=5,port=60020] regionserver.MemStoreFlusher: Blocking updates on sceplus-vm48.almaden.ibm.com,60020,1404182803191: the global memstore size 389.5m is >= than blocking 386.7m size
2014-06-30 19:53:18,845 INFO  [RpcServer.handler=13,port=60020] regionserver.MemStoreFlusher: Blocking updates on sceplus-vm48.almaden.ibm.com,60020,1404182803191: the global memstore size 389.5m is >= than blocking 386.7m size
2014-06-30 19:53:18,859 INFO  [RpcServer.handler=7,port=60020] regionserver.MemStoreFlusher: Blocking updates on sceplus-vm48.almaden.ibm.com,60020,1404182803191: the global memstore size 389.5m is >= than blocking 386.7m size
2014-06-30 19:53:18,876 INFO  [RpcServer.handler=18,port=60020] regionserver.MemStoreFlusher: Blocking updates on sceplus-vm48.almaden.ibm.com,60020,1404182803191: the global memstore size 389.5m is >= than blocking 386.7m size
2014-06-30 19:53:18,894 INFO  [RpcServer.handler=11,port=60020] regionserver.MemStoreFlusher: Blocking updates on sceplus-vm48.almaden.ibm.com,60020,1404182803191: the global memstore size 389.5m is >= than blocking 386.7m size
2014-06-30 19:53:20,027 INFO  [RpcServer.handler=15,port=60020] regionserver.MemStoreFlusher: Blocking updates on sceplus-vm48.almaden.ibm.com,60020,1404182803191: the global memstore size 389.5m is >= than blocking 386.7m size
2014-06-30 19:53:20,035 INFO  [RpcServer.handler=17,port=60020] regionserver.MemStoreFlusher: Blocking updates on sceplus-vm48.almaden.ibm.com,60020,1404182803191: the global memstore size 389.5m is >= than blocking 386.7m size
2014-06-30 19:53:20,046 INFO  [RpcServer.handler=1,port=60020] regionserver.MemStoreFlusher: Blocking updates on sceplus-vm48.almaden.ibm.com,60020,1404182803191: the global memstore size 389.5m is >= than blocking 386.7m size
2014-06-30 19:53:20,055 INFO  [RpcServer.handler=3,port=60020] regionserver.MemStoreFlusher: Blocking updates on sceplus-vm48.almaden.ibm.com,60020,1404182803191: the global memstore size 389.5m is >= than blocking 386.7m size
2014-06-30 19:53:20,065 INFO  [RpcServer.handler=33,port=60020] regionserver.MemStoreFlusher: Blocking updates on sceplus-vm48.almaden.ibm.com,60020,1404182803191: the global memstore size 389.5m is >= than blocking 386.7m size
2014-06-30 19:53:20,075 INFO  [RpcServer.handler=34,port=60020] regionserver.MemStoreFlusher: Blocking updates on sceplus-vm48.almaden.ibm.com,60020,1404182803191: the global memstore size 389.5m is >= than blocking 386.7m size
2014-06-30 19:53:20,084 INFO  [RpcServer.handler=35,port=60020] regionserver.MemStoreFlusher: Blocking updates on sceplus-vm48.almaden.ibm.com,60020,1404182803191: the global memstore size 389.5m is >= than blocking 386.7m size
2014-06-30 19:53:20,093 INFO  [RpcServer.handler=44,port=60020] regionserver.MemStoreFlusher: Blocking updates on sceplus-vm48.almaden.ibm.com,60020,1404182803191: the global memstore size 389.5m is >= than blocking 386.7m size
2014-06-30 19:53:20,110 INFO  [RpcServer.handler=46,port=60020] regionserver.MemStoreFlusher: Blocking updates on sceplus-vm48.almaden.ibm.com,60020,1404182803191: the global memstore size 389.5m is >= than blocking 386.7m size
2014-06-30 19:53:20,127 INFO  [RpcServer.handler=38,port=60020] regionserver.MemStoreFlusher: Blocking updates on sceplus-vm48.almaden.ibm.com,60020,1404182803191: the global memstore size 389.5m is >= than blocking 386.7m size
2014-06-30 19:53:20,142 INFO  [RpcServer.handler=42,port=60020] regionserver.MemStoreFlusher: Blocking updates on sceplus-vm48.almaden.ibm.com,60020,1404182803191: the global memstore size 389.5m is >= than blocking 386.7m size
2014-06-30 19:53:20,159 INFO  [RpcServer.handler=37,port=60020] regionserver.MemStoreFlusher: Blocking updates on sceplus-vm48.almaden.ibm.com,60020,1404182803191: the global memstore size 389.5m is >= than blocking 386.7m size
2014-06-30 19:53:20,173 INFO  [RpcServer.handler=45,port=60020] regionserver.MemStoreFlusher: Blocking updates on sceplus-vm48.almaden.ibm.com,60020,1404182803191: the global memstore size 389.5m is >= than blocking 386.7m size
2014-06-30 19:53:20,190 INFO  [RpcServer.handler=39,port=60020] regionserver.MemStoreFlusher: Blocking updates on sceplus-vm48.almaden.ibm.com,60020,1404182803191: the global memstore size 389.5m is >= than blocking 386.7m size
2014-06-30 19:53:20,208 INFO  [RpcServer.handler=43,port=60020] regionserver.MemStoreFlusher: Blocking updates on sceplus-vm48.almaden.ibm.com,60020,1404182803191: the global memstore size 389.5m is >= than blocking 386.7m size
2014-06-30 19:53:20,225 INFO  [RpcServer.handler=40,port=60020] regionserver.MemStoreFlusher: Blocking updates on sceplus-vm48.almaden.ibm.com,60020,1404182803191: the global memstore size 389.5m is >= than blocking 386.7m size
2014-06-30 19:53:20,241 INFO  [RpcServer.handler=41,port=60020] regionserver.MemStoreFlusher: Blocking updates on sceplus-vm48.almaden.ibm.com,60020,1404182803191: the global memstore size 389.5m is >= than blocking 386.7m size
2014-06-30 19:53:20,259 INFO  [RpcServer.handler=47,port=60020] regionserver.MemStoreFlusher: Blocking updates on sceplus-vm48.almaden.ibm.com,60020,1404182803191: the global memstore size 389.5m is >= than blocking 386.7m size
2014-06-30 19:53:20,277 INFO  [RpcServer.handler=36,port=60020] regionserver.MemStoreFlusher: Blocking updates on sceplus-vm48.almaden.ibm.com,60020,1404182803191: the global memstore size 389.5m is >= than blocking 386.7m size
2014-06-30 19:53:20,730 INFO  [RpcServer.handler=26,port=60020] regionserver.MemStoreFlusher: Blocking updates on sceplus-vm48.almaden.ibm.com,60020,1404182803191: the global memstore size 389.5m is >= than blocking 386.7m size
2014-06-30 19:53:20,741 INFO  [RpcServer.handler=20,port=60020] regionserver.MemStoreFlusher: Blocking updates on sceplus-vm48.almaden.ibm.com,60020,1404182803191: the global memstore size 389.5m is >= than blocking 386.7m size
2014-06-30 19:53:20,750 INFO  [RpcServer.handler=32,port=60020] regionserver.MemStoreFlusher: Blocking updates on sceplus-vm48.almaden.ibm.com,60020,1404182803191: the global memstore size 389.5m is >= than blocking 386.7m size
2014-06-30 19:53:20,758 INFO  [RpcServer.handler=28,port=60020] regionserver.MemStoreFlusher: Blocking updates on sceplus-vm48.almaden.ibm.com,60020,1404182803191: the global memstore size 389.5m is >= than blocking 386.7m size
2014-06-30 19:53:20,767 INFO  [RpcServer.handler=30,port=60020] regionserver.MemStoreFlusher: Blocking updates on sceplus-vm48.almaden.ibm.com,60020,1404182803191: the global memstore size 389.5m is >= than blocking 386.7m size
2014-06-30 19:53:20,776 INFO  [RpcServer.handler=22,port=60020] regionserver.MemStoreFlusher: Blocking updates on sceplus-vm48.almaden.ibm.com,60020,1404182803191: the global memstore size 389.5m is >= than blocking 386.7m size
2014-06-30 19:53:20,784 INFO  [RpcServer.handler=25,port=60020] regionserver.MemStoreFlusher: Blocking updates on sceplus-vm48.almaden.ibm.com,60020,1404182803191: the global memstore size 389.5m is >= than blocking 386.7m size
2014-06-30 19:53:20,806 INFO  [RpcServer.handler=23,port=60020] regionserver.MemStoreFlusher: Blocking updates on sceplus-vm48.almaden.ibm.com,60020,1404182803191: the global memstore size 389.5m is >= than blocking 386.7m size
2014-06-30 19:53:20,825 INFO  [RpcServer.handler=27,port=60020] regionserver.MemStoreFlusher: Blocking updates on sceplus-vm48.almaden.ibm.com,60020,1404182803191: the global memstore size 389.5m is >= than blocking 386.7m size
2014-06-30 19:53:20,846 INFO  [RpcServer.handler=48,port=60020] regionserver.MemStoreFlusher: Blocking updates on sceplus-vm48.almaden.ibm.com,60020,1404182803191: the global memstore size 389.5m is >= than blocking 386.7m size
2014-06-30 19:53:21,107 INFO  [MemStoreFlusher.0] regionserver.DefaultStoreFlusher: Flushed, sequenceid=62, memsize=55.3m, hasBloomFilter=true, into tmp file hdfs://master:54310/hbase/data/default/usertable/5446f1f888398e97c0ec3cc30da5bf97/.tmp/28c2e48911c740559fb5ee390001c1a2
2014-06-30 19:53:21,124 DEBUG [MemStoreFlusher.0] regionserver.HRegionFileSystem: Committing store file hdfs://master:54310/hbase/data/default/usertable/5446f1f888398e97c0ec3cc30da5bf97/.tmp/28c2e48911c740559fb5ee390001c1a2 as hdfs://master:54310/hbase/data/default/usertable/5446f1f888398e97c0ec3cc30da5bf97/family/28c2e48911c740559fb5ee390001c1a2
2014-06-30 19:53:21,136 INFO  [MemStoreFlusher.0] regionserver.HStore: Added hdfs://master:54310/hbase/data/default/usertable/5446f1f888398e97c0ec3cc30da5bf97/family/28c2e48911c740559fb5ee390001c1a2, entries=201440, sequenceid=62, filesize=30.0m
2014-06-30 19:53:21,137 INFO  [MemStoreFlusher.0] regionserver.HRegion: Finished memstore flush of ~92.5m/97028480, currentsize=0.0/0 for region usertable,user2,1404183160444.5446f1f888398e97c0ec3cc30da5bf97. in 3484ms, sequenceid=62, compaction requested=false
2014-06-30 19:53:21,139 WARN  [RpcServer.handler=48,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 293ms
2014-06-30 19:53:21,139 INFO  [RpcServer.handler=48,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server sceplus-vm48.almaden.ibm.com,60020,1404182803191
2014-06-30 19:53:21,139 WARN  [RpcServer.handler=27,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 314ms
2014-06-30 19:53:21,139 INFO  [RpcServer.handler=27,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server sceplus-vm48.almaden.ibm.com,60020,1404182803191
2014-06-30 19:53:21,139 WARN  [RpcServer.handler=23,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 333ms
2014-06-30 19:53:21,139 INFO  [RpcServer.handler=23,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server sceplus-vm48.almaden.ibm.com,60020,1404182803191
2014-06-30 19:53:21,139 WARN  [RpcServer.handler=25,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 355ms
2014-06-30 19:53:21,139 INFO  [RpcServer.handler=25,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server sceplus-vm48.almaden.ibm.com,60020,1404182803191
2014-06-30 19:53:21,141 WARN  [RpcServer.handler=22,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 366ms
2014-06-30 19:53:21,141 INFO  [RpcServer.handler=22,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server sceplus-vm48.almaden.ibm.com,60020,1404182803191
2014-06-30 19:53:21,141 WARN  [RpcServer.handler=30,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 374ms
2014-06-30 19:53:21,141 INFO  [RpcServer.handler=30,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server sceplus-vm48.almaden.ibm.com,60020,1404182803191
2014-06-30 19:53:21,142 WARN  [RpcServer.handler=28,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 384ms
2014-06-30 19:53:21,142 INFO  [RpcServer.handler=28,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server sceplus-vm48.almaden.ibm.com,60020,1404182803191
2014-06-30 19:53:21,142 WARN  [RpcServer.handler=32,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 392ms
2014-06-30 19:53:21,142 INFO  [RpcServer.handler=32,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server sceplus-vm48.almaden.ibm.com,60020,1404182803191
2014-06-30 19:53:21,142 WARN  [RpcServer.handler=20,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 401ms
2014-06-30 19:53:21,142 INFO  [RpcServer.handler=20,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server sceplus-vm48.almaden.ibm.com,60020,1404182803191
2014-06-30 19:53:21,142 WARN  [RpcServer.handler=26,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 412ms
2014-06-30 19:53:21,142 INFO  [RpcServer.handler=26,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server sceplus-vm48.almaden.ibm.com,60020,1404182803191
2014-06-30 19:53:21,142 WARN  [RpcServer.handler=36,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 865ms
2014-06-30 19:53:21,142 INFO  [RpcServer.handler=36,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server sceplus-vm48.almaden.ibm.com,60020,1404182803191
2014-06-30 19:53:21,145 WARN  [RpcServer.handler=47,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 886ms
2014-06-30 19:53:21,145 INFO  [RpcServer.handler=47,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server sceplus-vm48.almaden.ibm.com,60020,1404182803191
2014-06-30 19:53:21,145 WARN  [RpcServer.handler=41,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 904ms
2014-06-30 19:53:21,145 INFO  [RpcServer.handler=41,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server sceplus-vm48.almaden.ibm.com,60020,1404182803191
2014-06-30 19:53:21,146 WARN  [RpcServer.handler=40,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 922ms
2014-06-30 19:53:21,146 INFO  [RpcServer.handler=40,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server sceplus-vm48.almaden.ibm.com,60020,1404182803191
2014-06-30 19:53:21,146 WARN  [RpcServer.handler=43,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 938ms
2014-06-30 19:53:21,146 INFO  [RpcServer.handler=43,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server sceplus-vm48.almaden.ibm.com,60020,1404182803191
2014-06-30 19:53:21,146 WARN  [RpcServer.handler=39,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 956ms
2014-06-30 19:53:21,146 INFO  [RpcServer.handler=39,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server sceplus-vm48.almaden.ibm.com,60020,1404182803191
2014-06-30 19:53:21,146 WARN  [RpcServer.handler=45,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 973ms
2014-06-30 19:53:21,146 INFO  [RpcServer.handler=45,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server sceplus-vm48.almaden.ibm.com,60020,1404182803191
2014-06-30 19:53:21,149 WARN  [RpcServer.handler=37,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 990ms
2014-06-30 19:53:21,149 INFO  [RpcServer.handler=37,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server sceplus-vm48.almaden.ibm.com,60020,1404182803191
2014-06-30 19:53:21,149 WARN  [RpcServer.handler=42,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 1007ms
2014-06-30 19:53:21,149 INFO  [RpcServer.handler=42,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server sceplus-vm48.almaden.ibm.com,60020,1404182803191
2014-06-30 19:53:21,151 WARN  [RpcServer.handler=38,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 1024ms
2014-06-30 19:53:21,151 INFO  [RpcServer.handler=38,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server sceplus-vm48.almaden.ibm.com,60020,1404182803191
2014-06-30 19:53:21,151 WARN  [RpcServer.handler=46,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 1041ms
2014-06-30 19:53:21,151 INFO  [RpcServer.handler=46,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server sceplus-vm48.almaden.ibm.com,60020,1404182803191
2014-06-30 19:53:21,153 WARN  [RpcServer.handler=44,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 1060ms
2014-06-30 19:53:21,153 INFO  [RpcServer.handler=44,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server sceplus-vm48.almaden.ibm.com,60020,1404182803191
2014-06-30 19:53:21,153 WARN  [RpcServer.handler=35,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 1069ms
2014-06-30 19:53:21,153 INFO  [RpcServer.handler=35,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server sceplus-vm48.almaden.ibm.com,60020,1404182803191
2014-06-30 19:53:21,153 WARN  [RpcServer.handler=34,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 1078ms
2014-06-30 19:53:21,154 INFO  [RpcServer.handler=34,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server sceplus-vm48.almaden.ibm.com,60020,1404182803191
2014-06-30 19:53:21,154 WARN  [RpcServer.handler=33,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 1089ms
2014-06-30 19:53:21,154 INFO  [RpcServer.handler=33,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server sceplus-vm48.almaden.ibm.com,60020,1404182803191
2014-06-30 19:53:21,154 WARN  [RpcServer.handler=3,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 1099ms
2014-06-30 19:53:21,154 INFO  [RpcServer.handler=3,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server sceplus-vm48.almaden.ibm.com,60020,1404182803191
2014-06-30 19:53:21,154 WARN  [RpcServer.handler=1,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 1109ms
2014-06-30 19:53:21,154 INFO  [RpcServer.handler=1,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server sceplus-vm48.almaden.ibm.com,60020,1404182803191
2014-06-30 19:53:21,154 WARN  [RpcServer.handler=17,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 1119ms
2014-06-30 19:53:21,154 INFO  [RpcServer.handler=17,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server sceplus-vm48.almaden.ibm.com,60020,1404182803191
2014-06-30 19:53:21,155 WARN  [RpcServer.handler=15,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 1128ms
2014-06-30 19:53:21,155 INFO  [RpcServer.handler=15,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server sceplus-vm48.almaden.ibm.com,60020,1404182803191
2014-06-30 19:53:21,155 WARN  [RpcServer.handler=11,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 2261ms
2014-06-30 19:53:21,155 INFO  [RpcServer.handler=11,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server sceplus-vm48.almaden.ibm.com,60020,1404182803191
2014-06-30 19:53:21,155 WARN  [RpcServer.handler=18,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 2279ms
2014-06-30 19:53:21,155 INFO  [RpcServer.handler=18,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server sceplus-vm48.almaden.ibm.com,60020,1404182803191
2014-06-30 19:53:21,155 WARN  [RpcServer.handler=7,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 2296ms
2014-06-30 19:53:21,155 INFO  [RpcServer.handler=7,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server sceplus-vm48.almaden.ibm.com,60020,1404182803191
2014-06-30 19:53:21,161 WARN  [RpcServer.handler=13,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 2316ms
2014-06-30 19:53:21,161 INFO  [RpcServer.handler=13,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server sceplus-vm48.almaden.ibm.com,60020,1404182803191
2014-06-30 19:53:21,161 WARN  [RpcServer.handler=5,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 2325ms
2014-06-30 19:53:21,161 INFO  [RpcServer.handler=5,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server sceplus-vm48.almaden.ibm.com,60020,1404182803191
2014-06-30 19:53:21,161 WARN  [RpcServer.handler=10,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 2390ms
2014-06-30 19:53:21,161 INFO  [RpcServer.handler=10,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server sceplus-vm48.almaden.ibm.com,60020,1404182803191
2014-06-30 19:53:21,161 WARN  [RpcServer.handler=14,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 2554ms
2014-06-30 19:53:21,161 INFO  [RpcServer.handler=14,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server sceplus-vm48.almaden.ibm.com,60020,1404182803191
2014-06-30 19:53:21,161 WARN  [RpcServer.handler=8,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 3437ms
2014-06-30 19:53:21,161 INFO  [RpcServer.handler=8,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server sceplus-vm48.almaden.ibm.com,60020,1404182803191
2014-06-30 19:53:21,162 WARN  [RpcServer.handler=16,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 3439ms
2014-06-30 19:53:21,162 INFO  [RpcServer.handler=16,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server sceplus-vm48.almaden.ibm.com,60020,1404182803191
2014-06-30 19:53:21,185 WARN  [RpcServer.handler=14,port=60020] ipc.RpcServer: RpcServer.respondercallId: 388 service: ClientService methodName: Multi size: 3.0m connection: 9.1.143.58:49871: output error
2014-06-30 19:53:21,185 WARN  [RpcServer.handler=14,port=60020] ipc.RpcServer: RpcServer.handler=14,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-06-30 19:53:21,186 WARN  [RpcServer.handler=16,port=60020] ipc.RpcServer: RpcServer.respondercallId: 335 service: ClientService methodName: Multi size: 2.9m connection: 9.1.143.58:49870: output error
2014-06-30 19:53:21,186 WARN  [RpcServer.handler=16,port=60020] ipc.RpcServer: RpcServer.handler=16,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-06-30 19:53:23,312 INFO  [JvmPauseMonitor] util.JvmPauseMonitor: Detected pause in JVM or host machine (eg GC): pause of approximately 1589ms
GC pool 'ParNew' had collection(s): count=1 time=0ms
GC pool 'ConcurrentMarkSweep' had collection(s): count=2 time=2015ms
2014-06-30 19:53:23,628 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-06-30 19:53:23,646 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 336 synced till here 330
2014-06-30 19:53:23,687 DEBUG [MemStoreFlusher.1] regionserver.MemStoreFlusher: Flush thread woke up because memory above low water=367.3m
2014-06-30 19:53:23,687 INFO  [MemStoreFlusher.1] regionserver.MemStoreFlusher: Flush of region usertable,user5,1404183160444.1d66ac045cc0407c0fa426db565fcbba. due to global heap pressure
2014-06-30 19:53:23,687 DEBUG [MemStoreFlusher.1] regionserver.HRegion: Started memstore flush for usertable,user5,1404183160444.1d66ac045cc0407c0fa426db565fcbba., current region memstore size 100.9m
2014-06-30 19:53:23,708 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1404182803191/sceplus-vm48.almaden.ibm.com%2C60020%2C1404182803191.1404183194900 with entries=96, filesize=66.7m; new WAL /hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1404182803191/sceplus-vm48.almaden.ibm.com%2C60020%2C1404182803191.1404183203628
2014-06-30 19:53:26,396 INFO  [JvmPauseMonitor] util.JvmPauseMonitor: Detected pause in JVM or host machine (eg GC): pause of approximately 1773ms
GC pool 'ParNew' had collection(s): count=1 time=0ms
GC pool 'ConcurrentMarkSweep' had collection(s): count=2 time=2250ms
2014-06-30 19:53:26,638 INFO  [RpcServer.handler=25,port=60020] regionserver.MemStoreFlusher: Blocking updates on sceplus-vm48.almaden.ibm.com,60020,1404182803191: the global memstore size 393.9m is >= than blocking 386.7m size
2014-06-30 19:53:26,640 INFO  [RpcServer.handler=23,port=60020] regionserver.MemStoreFlusher: Blocking updates on sceplus-vm48.almaden.ibm.com,60020,1404182803191: the global memstore size 398.6m is >= than blocking 386.7m size
2014-06-30 19:53:26,640 INFO  [RpcServer.handler=35,port=60020] regionserver.MemStoreFlusher: Blocking updates on sceplus-vm48.almaden.ibm.com,60020,1404182803191: the global memstore size 398.6m is >= than blocking 386.7m size
2014-06-30 19:53:26,641 INFO  [RpcServer.handler=46,port=60020] regionserver.MemStoreFlusher: Blocking updates on sceplus-vm48.almaden.ibm.com,60020,1404182803191: the global memstore size 401.6m is >= than blocking 386.7m size
2014-06-30 19:53:26,642 INFO  [RpcServer.handler=3,port=60020] regionserver.MemStoreFlusher: Blocking updates on sceplus-vm48.almaden.ibm.com,60020,1404182803191: the global memstore size 401.6m is >= than blocking 386.7m size
2014-06-30 19:53:26,642 INFO  [RpcServer.handler=48,port=60020] regionserver.MemStoreFlusher: Blocking updates on sceplus-vm48.almaden.ibm.com,60020,1404182803191: the global memstore size 407.8m is >= than blocking 386.7m size
2014-06-30 19:53:26,643 INFO  [RpcServer.handler=10,port=60020] regionserver.MemStoreFlusher: Blocking updates on sceplus-vm48.almaden.ibm.com,60020,1404182803191: the global memstore size 407.8m is >= than blocking 386.7m size
2014-06-30 19:53:26,644 INFO  [RpcServer.handler=49,port=60020] regionserver.MemStoreFlusher: Blocking updates on sceplus-vm48.almaden.ibm.com,60020,1404182803191: the global memstore size 407.8m is >= than blocking 386.7m size
2014-06-30 19:53:26,645 INFO  [RpcServer.handler=31,port=60020] regionserver.MemStoreFlusher: Blocking updates on sceplus-vm48.almaden.ibm.com,60020,1404182803191: the global memstore size 409.2m is >= than blocking 386.7m size
2014-06-30 19:53:26,645 INFO  [RpcServer.handler=7,port=60020] regionserver.MemStoreFlusher: Blocking updates on sceplus-vm48.almaden.ibm.com,60020,1404182803191: the global memstore size 409.2m is >= than blocking 386.7m size
2014-06-30 19:53:26,647 INFO  [RpcServer.handler=5,port=60020] regionserver.MemStoreFlusher: Blocking updates on sceplus-vm48.almaden.ibm.com,60020,1404182803191: the global memstore size 415.5m is >= than blocking 386.7m size
2014-06-30 19:53:26,647 DEBUG [RpcServer.handler=13,port=60020] regionserver.HRegion: Flush requested on usertable,user5,1404183160444.1d66ac045cc0407c0fa426db565fcbba.
2014-06-30 19:53:28,652 DEBUG [MemStoreFlusher.0] regionserver.HRegion: NOT flushing memstore for region usertable,user5,1404183160444.1d66ac045cc0407c0fa426db565fcbba., flushing=true, writesEnabled=true
2014-06-30 19:53:28,653 INFO  [JvmPauseMonitor] util.JvmPauseMonitor: Detected pause in JVM or host machine (eg GC): pause of approximately 1755ms
GC pool 'ParNew' had collection(s): count=1 time=0ms
GC pool 'ConcurrentMarkSweep' had collection(s): count=1 time=2002ms
2014-06-30 19:53:28,656 INFO  [RpcServer.handler=15,port=60020] regionserver.MemStoreFlusher: Blocking updates on sceplus-vm48.almaden.ibm.com,60020,1404182803191: the global memstore size 418.5m is >= than blocking 386.7m size
2014-06-30 19:53:28,657 INFO  [RpcServer.handler=41,port=60020] regionserver.MemStoreFlusher: Blocking updates on sceplus-vm48.almaden.ibm.com,60020,1404182803191: the global memstore size 427.7m is >= than blocking 386.7m size
2014-06-30 19:53:28,657 INFO  [RpcServer.handler=21,port=60020] regionserver.MemStoreFlusher: Blocking updates on sceplus-vm48.almaden.ibm.com,60020,1404182803191: the global memstore size 427.7m is >= than blocking 386.7m size
2014-06-30 19:53:28,658 INFO  [RpcServer.handler=37,port=60020] regionserver.MemStoreFlusher: Blocking updates on sceplus-vm48.almaden.ibm.com,60020,1404182803191: the global memstore size 427.7m is >= than blocking 386.7m size
2014-06-30 19:53:28,658 INFO  [RpcServer.handler=26,port=60020] regionserver.MemStoreFlusher: Blocking updates on sceplus-vm48.almaden.ibm.com,60020,1404182803191: the global memstore size 427.7m is >= than blocking 386.7m size
2014-06-30 19:53:28,658 INFO  [RpcServer.handler=40,port=60020] regionserver.MemStoreFlusher: Blocking updates on sceplus-vm48.almaden.ibm.com,60020,1404182803191: the global memstore size 427.7m is >= than blocking 386.7m size
2014-06-30 19:53:28,659 INFO  [RpcServer.handler=38,port=60020] regionserver.MemStoreFlusher: Blocking updates on sceplus-vm48.almaden.ibm.com,60020,1404182803191: the global memstore size 427.7m is >= than blocking 386.7m size
2014-06-30 19:53:28,659 INFO  [RpcServer.handler=17,port=60020] regionserver.MemStoreFlusher: Blocking updates on sceplus-vm48.almaden.ibm.com,60020,1404182803191: the global memstore size 427.7m is >= than blocking 386.7m size
2014-06-30 19:53:28,659 INFO  [RpcServer.handler=13,port=60020] regionserver.MemStoreFlusher: Blocking updates on sceplus-vm48.almaden.ibm.com,60020,1404182803191: the global memstore size 427.7m is >= than blocking 386.7m size
2014-06-30 19:53:28,659 INFO  [RpcServer.handler=29,port=60020] regionserver.MemStoreFlusher: Blocking updates on sceplus-vm48.almaden.ibm.com,60020,1404182803191: the global memstore size 427.7m is >= than blocking 386.7m size
2014-06-30 19:53:28,660 INFO  [RpcServer.handler=1,port=60020] regionserver.MemStoreFlusher: Blocking updates on sceplus-vm48.almaden.ibm.com,60020,1404182803191: the global memstore size 427.7m is >= than blocking 386.7m size
2014-06-30 19:53:28,660 WARN  [RpcServer.handler=29,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 6ms
2014-06-30 19:53:28,660 DEBUG [MemStoreFlusher.0] regionserver.MemStoreFlusher: Flush thread woke up because memory above low water=367.3m
2014-06-30 19:53:28,660 WARN  [RpcServer.handler=13,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5ms
2014-06-30 19:53:28,660 INFO  [MemStoreFlusher.0] regionserver.MemStoreFlusher: Flush of region usertable,user8,1404183160444.28938d267c1c61f7feb582b85e4c172f. due to global heap pressure
2014-06-30 19:53:28,660 WARN  [RpcServer.handler=17,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5ms
2014-06-30 19:53:28,660 WARN  [RpcServer.handler=38,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 4ms
2014-06-30 19:53:28,660 DEBUG [MemStoreFlusher.0] regionserver.HRegion: Started memstore flush for usertable,user8,1404183160444.28938d267c1c61f7feb582b85e4c172f., current region memstore size 100.6m
2014-06-30 19:53:28,660 WARN  [RpcServer.handler=40,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 4ms
2014-06-30 19:53:28,660 WARN  [RpcServer.handler=26,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 4ms
2014-06-30 19:53:28,661 WARN  [RpcServer.handler=37,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 4ms
2014-06-30 19:53:28,661 WARN  [RpcServer.handler=21,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 2016ms
2014-06-30 19:53:28,661 WARN  [RpcServer.handler=41,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 2015ms
2014-06-30 19:53:28,661 WARN  [RpcServer.handler=15,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 2014ms
2014-06-30 19:53:28,661 WARN  [RpcServer.handler=5,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 2014ms
2014-06-30 19:53:28,661 WARN  [RpcServer.handler=7,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 2020ms
2014-06-30 19:53:28,662 WARN  [RpcServer.handler=31,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 2017ms
2014-06-30 19:53:28,662 WARN  [RpcServer.handler=49,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 2018ms
2014-06-30 19:53:28,662 WARN  [RpcServer.handler=10,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 2019ms
2014-06-30 19:53:28,663 WARN  [RpcServer.handler=48,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 2020ms
2014-06-30 19:53:28,663 WARN  [RpcServer.handler=3,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 2021ms
2014-06-30 19:53:28,663 WARN  [RpcServer.handler=46,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 2022ms
2014-06-30 19:53:28,664 WARN  [RpcServer.handler=35,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 2024ms
2014-06-30 19:53:28,664 WARN  [RpcServer.handler=23,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 2026ms
2014-06-30 19:53:28,664 WARN  [RpcServer.handler=25,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 2026ms
2014-06-30 19:53:28,664 INFO  [RpcServer.handler=33,port=60020] regionserver.MemStoreFlusher: Blocking updates on sceplus-vm48.almaden.ibm.com,60020,1404182803191: the global memstore size 427.7m is >= than blocking 386.7m size
2014-06-30 19:53:28,665 INFO  [RpcServer.handler=42,port=60020] regionserver.MemStoreFlusher: Blocking updates on sceplus-vm48.almaden.ibm.com,60020,1404182803191: the global memstore size 427.7m is >= than blocking 386.7m size
2014-06-30 19:53:28,665 INFO  [RpcServer.handler=34,port=60020] regionserver.MemStoreFlusher: Blocking updates on sceplus-vm48.almaden.ibm.com,60020,1404182803191: the global memstore size 427.7m is >= than blocking 386.7m size
2014-06-30 19:53:28,665 INFO  [RpcServer.handler=44,port=60020] regionserver.MemStoreFlusher: Blocking updates on sceplus-vm48.almaden.ibm.com,60020,1404182803191: the global memstore size 427.7m is >= than blocking 386.7m size
2014-06-30 19:53:28,665 INFO  [RpcServer.handler=11,port=60020] regionserver.MemStoreFlusher: Blocking updates on sceplus-vm48.almaden.ibm.com,60020,1404182803191: the global memstore size 427.7m is >= than blocking 386.7m size
2014-06-30 19:53:28,665 INFO  [RpcServer.handler=39,port=60020] regionserver.MemStoreFlusher: Blocking updates on sceplus-vm48.almaden.ibm.com,60020,1404182803191: the global memstore size 427.7m is >= than blocking 386.7m size
2014-06-30 19:53:28,665 INFO  [RpcServer.handler=47,port=60020] regionserver.MemStoreFlusher: Blocking updates on sceplus-vm48.almaden.ibm.com,60020,1404182803191: the global memstore size 427.7m is >= than blocking 386.7m size
2014-06-30 19:53:28,734 INFO  [RpcServer.handler=36,port=60020] regionserver.MemStoreFlusher: Blocking updates on sceplus-vm48.almaden.ibm.com,60020,1404182803191: the global memstore size 435.3m is >= than blocking 386.7m size
2014-06-30 19:53:28,734 INFO  [RpcServer.handler=4,port=60020] regionserver.MemStoreFlusher: Blocking updates on sceplus-vm48.almaden.ibm.com,60020,1404182803191: the global memstore size 440.0m is >= than blocking 386.7m size
2014-06-30 19:53:28,735 INFO  [RpcServer.handler=0,port=60020] regionserver.MemStoreFlusher: Blocking updates on sceplus-vm48.almaden.ibm.com,60020,1404182803191: the global memstore size 441.5m is >= than blocking 386.7m size
2014-06-30 19:53:28,735 INFO  [RpcServer.handler=12,port=60020] regionserver.MemStoreFlusher: Blocking updates on sceplus-vm48.almaden.ibm.com,60020,1404182803191: the global memstore size 441.5m is >= than blocking 386.7m size
2014-06-30 19:53:28,735 INFO  [RpcServer.handler=24,port=60020] regionserver.MemStoreFlusher: Blocking updates on sceplus-vm48.almaden.ibm.com,60020,1404182803191: the global memstore size 441.5m is >= than blocking 386.7m size
2014-06-30 19:53:28,737 INFO  [RpcServer.handler=32,port=60020] regionserver.MemStoreFlusher: Blocking updates on sceplus-vm48.almaden.ibm.com,60020,1404182803191: the global memstore size 441.5m is >= than blocking 386.7m size
2014-06-30 19:53:28,737 INFO  [RpcServer.handler=20,port=60020] regionserver.MemStoreFlusher: Blocking updates on sceplus-vm48.almaden.ibm.com,60020,1404182803191: the global memstore size 441.5m is >= than blocking 386.7m size
2014-06-30 19:53:28,737 INFO  [RpcServer.handler=30,port=60020] regionserver.MemStoreFlusher: Blocking updates on sceplus-vm48.almaden.ibm.com,60020,1404182803191: the global memstore size 444.5m is >= than blocking 386.7m size
2014-06-30 19:53:28,737 INFO  [RpcServer.handler=9,port=60020] regionserver.MemStoreFlusher: Blocking updates on sceplus-vm48.almaden.ibm.com,60020,1404182803191: the global memstore size 447.5m is >= than blocking 386.7m size
2014-06-30 19:53:28,739 INFO  [RpcServer.handler=45,port=60020] regionserver.MemStoreFlusher: Blocking updates on sceplus-vm48.almaden.ibm.com,60020,1404182803191: the global memstore size 447.5m is >= than blocking 386.7m size
2014-06-30 19:53:28,740 INFO  [RpcServer.handler=28,port=60020] regionserver.MemStoreFlusher: Blocking updates on sceplus-vm48.almaden.ibm.com,60020,1404182803191: the global memstore size 447.5m is >= than blocking 386.7m size
2014-06-30 19:53:28,740 INFO  [RpcServer.handler=18,port=60020] regionserver.MemStoreFlusher: Blocking updates on sceplus-vm48.almaden.ibm.com,60020,1404182803191: the global memstore size 447.5m is >= than blocking 386.7m size
2014-06-30 19:53:28,740 INFO  [RpcServer.handler=43,port=60020] regionserver.MemStoreFlusher: Blocking updates on sceplus-vm48.almaden.ibm.com,60020,1404182803191: the global memstore size 447.5m is >= than blocking 386.7m size
2014-06-30 19:53:28,780 DEBUG [MemStoreFlusher.0] util.FSUtils: DFS Client does not support most favored nodes create; using default create
2014-06-30 19:53:28,812 DEBUG [MemStoreFlusher.1] util.FSUtils: DFS Client does not support most favored nodes create; using default create
2014-06-30 19:53:28,888 INFO  [RpcServer.handler=2,port=60020] regionserver.MemStoreFlusher: Blocking updates on sceplus-vm48.almaden.ibm.com,60020,1404182803191: the global memstore size 447.5m is >= than blocking 386.7m size
2014-06-30 19:53:29,570 INFO  [MemStoreFlusher.0] regionserver.DefaultStoreFlusher: Flushed, sequenceid=67, memsize=55.5m, hasBloomFilter=true, into tmp file hdfs://master:54310/hbase/data/default/usertable/28938d267c1c61f7feb582b85e4c172f/.tmp/2f35affe8d22424bb64a9c60ce6f5911
2014-06-30 19:53:29,585 DEBUG [MemStoreFlusher.0] regionserver.HRegionFileSystem: Committing store file hdfs://master:54310/hbase/data/default/usertable/28938d267c1c61f7feb582b85e4c172f/.tmp/2f35affe8d22424bb64a9c60ce6f5911 as hdfs://master:54310/hbase/data/default/usertable/28938d267c1c61f7feb582b85e4c172f/family/2f35affe8d22424bb64a9c60ce6f5911
2014-06-30 19:53:29,597 INFO  [MemStoreFlusher.0] regionserver.HStore: Added hdfs://master:54310/hbase/data/default/usertable/28938d267c1c61f7feb582b85e4c172f/family/2f35affe8d22424bb64a9c60ce6f5911, entries=202110, sequenceid=67, filesize=30.1m
2014-06-30 19:53:29,597 INFO  [MemStoreFlusher.0] regionserver.HRegion: Finished memstore flush of ~100.6m/105518880, currentsize=0.0/0 for region usertable,user8,1404183160444.28938d267c1c61f7feb582b85e4c172f. in 937ms, sequenceid=67, compaction requested=false
2014-06-30 19:53:29,598 WARN  [RpcServer.handler=2,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 709ms
2014-06-30 19:53:29,598 INFO  [RpcServer.handler=2,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server sceplus-vm48.almaden.ibm.com,60020,1404182803191
2014-06-30 19:53:29,598 WARN  [RpcServer.handler=43,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 858ms
2014-06-30 19:53:29,598 INFO  [RpcServer.handler=43,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server sceplus-vm48.almaden.ibm.com,60020,1404182803191
2014-06-30 19:53:29,598 WARN  [RpcServer.handler=18,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 858ms
2014-06-30 19:53:29,598 INFO  [RpcServer.handler=18,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server sceplus-vm48.almaden.ibm.com,60020,1404182803191
2014-06-30 19:53:29,602 WARN  [RpcServer.handler=28,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 862ms
2014-06-30 19:53:29,603 INFO  [RpcServer.handler=28,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server sceplus-vm48.almaden.ibm.com,60020,1404182803191
2014-06-30 19:53:29,603 WARN  [RpcServer.handler=45,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 864ms
2014-06-30 19:53:29,603 INFO  [RpcServer.handler=45,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server sceplus-vm48.almaden.ibm.com,60020,1404182803191
2014-06-30 19:53:29,603 WARN  [RpcServer.handler=9,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 866ms
2014-06-30 19:53:29,603 INFO  [RpcServer.handler=9,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server sceplus-vm48.almaden.ibm.com,60020,1404182803191
2014-06-30 19:53:29,604 WARN  [RpcServer.handler=30,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 866ms
2014-06-30 19:53:29,604 INFO  [RpcServer.handler=30,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server sceplus-vm48.almaden.ibm.com,60020,1404182803191
2014-06-30 19:53:29,604 WARN  [RpcServer.handler=20,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 867ms
2014-06-30 19:53:29,604 INFO  [RpcServer.handler=20,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server sceplus-vm48.almaden.ibm.com,60020,1404182803191
2014-06-30 19:53:29,605 WARN  [RpcServer.handler=32,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 867ms
2014-06-30 19:53:29,605 INFO  [RpcServer.handler=32,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server sceplus-vm48.almaden.ibm.com,60020,1404182803191
2014-06-30 19:53:29,605 WARN  [RpcServer.handler=24,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 871ms
2014-06-30 19:53:29,605 INFO  [RpcServer.handler=24,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server sceplus-vm48.almaden.ibm.com,60020,1404182803191
2014-06-30 19:53:29,605 WARN  [RpcServer.handler=12,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 871ms
2014-06-30 19:53:29,605 INFO  [RpcServer.handler=12,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server sceplus-vm48.almaden.ibm.com,60020,1404182803191
2014-06-30 19:53:29,605 WARN  [RpcServer.handler=0,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 871ms
2014-06-30 19:53:29,605 INFO  [RpcServer.handler=0,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server sceplus-vm48.almaden.ibm.com,60020,1404182803191
2014-06-30 19:53:29,606 WARN  [RpcServer.handler=4,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 872ms
2014-06-30 19:53:29,606 INFO  [RpcServer.handler=4,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server sceplus-vm48.almaden.ibm.com,60020,1404182803191
2014-06-30 19:53:29,608 WARN  [RpcServer.handler=36,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 874ms
2014-06-30 19:53:29,608 INFO  [RpcServer.handler=36,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server sceplus-vm48.almaden.ibm.com,60020,1404182803191
2014-06-30 19:53:29,608 WARN  [RpcServer.handler=47,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 950ms
2014-06-30 19:53:29,609 INFO  [RpcServer.handler=47,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server sceplus-vm48.almaden.ibm.com,60020,1404182803191
2014-06-30 19:53:29,609 WARN  [RpcServer.handler=39,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 951ms
2014-06-30 19:53:29,609 INFO  [RpcServer.handler=39,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server sceplus-vm48.almaden.ibm.com,60020,1404182803191
2014-06-30 19:53:29,610 WARN  [RpcServer.handler=11,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 952ms
2014-06-30 19:53:29,610 INFO  [RpcServer.handler=11,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server sceplus-vm48.almaden.ibm.com,60020,1404182803191
2014-06-30 19:53:29,610 WARN  [RpcServer.handler=44,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 952ms
2014-06-30 19:53:29,611 INFO  [RpcServer.handler=44,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server sceplus-vm48.almaden.ibm.com,60020,1404182803191
2014-06-30 19:53:29,611 WARN  [RpcServer.handler=34,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 953ms
2014-06-30 19:53:29,611 INFO  [RpcServer.handler=34,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server sceplus-vm48.almaden.ibm.com,60020,1404182803191
2014-06-30 19:53:29,611 WARN  [RpcServer.handler=42,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 952ms
2014-06-30 19:53:29,611 INFO  [RpcServer.handler=42,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server sceplus-vm48.almaden.ibm.com,60020,1404182803191
2014-06-30 19:53:29,611 WARN  [RpcServer.handler=33,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 952ms
2014-06-30 19:53:29,611 INFO  [RpcServer.handler=33,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server sceplus-vm48.almaden.ibm.com,60020,1404182803191
2014-06-30 19:53:29,611 WARN  [RpcServer.handler=25,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 2973ms
2014-06-30 19:53:29,611 INFO  [RpcServer.handler=25,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server sceplus-vm48.almaden.ibm.com,60020,1404182803191
2014-06-30 19:53:29,612 WARN  [RpcServer.handler=23,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 2974ms
2014-06-30 19:53:29,612 INFO  [RpcServer.handler=23,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server sceplus-vm48.almaden.ibm.com,60020,1404182803191
2014-06-30 19:53:29,614 WARN  [RpcServer.handler=35,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 2974ms
2014-06-30 19:53:29,615 INFO  [RpcServer.handler=35,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server sceplus-vm48.almaden.ibm.com,60020,1404182803191
2014-06-30 19:53:29,615 WARN  [RpcServer.handler=46,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 2974ms
2014-06-30 19:53:29,615 INFO  [RpcServer.handler=46,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server sceplus-vm48.almaden.ibm.com,60020,1404182803191
2014-06-30 19:53:29,615 WARN  [RpcServer.handler=3,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 2973ms
2014-06-30 19:53:29,615 INFO  [RpcServer.handler=3,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server sceplus-vm48.almaden.ibm.com,60020,1404182803191
2014-06-30 19:53:29,616 WARN  [RpcServer.handler=48,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 2973ms
2014-06-30 19:53:29,616 INFO  [RpcServer.handler=48,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server sceplus-vm48.almaden.ibm.com,60020,1404182803191
2014-06-30 19:53:29,618 WARN  [RpcServer.handler=10,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 2975ms
2014-06-30 19:53:29,618 INFO  [RpcServer.handler=10,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server sceplus-vm48.almaden.ibm.com,60020,1404182803191
2014-06-30 19:53:29,622 WARN  [RpcServer.handler=49,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 2978ms
2014-06-30 19:53:29,622 INFO  [RpcServer.handler=49,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server sceplus-vm48.almaden.ibm.com,60020,1404182803191
2014-06-30 19:53:29,623 WARN  [RpcServer.handler=31,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 2977ms
2014-06-30 19:53:29,623 INFO  [RpcServer.handler=31,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server sceplus-vm48.almaden.ibm.com,60020,1404182803191
2014-06-30 19:53:29,623 WARN  [RpcServer.handler=7,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 2982ms
2014-06-30 19:53:29,623 INFO  [RpcServer.handler=7,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server sceplus-vm48.almaden.ibm.com,60020,1404182803191
2014-06-30 19:53:29,623 WARN  [RpcServer.handler=5,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 2976ms
2014-06-30 19:53:29,623 INFO  [RpcServer.handler=5,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server sceplus-vm48.almaden.ibm.com,60020,1404182803191
2014-06-30 19:53:29,624 WARN  [RpcServer.handler=15,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 2977ms
2014-06-30 19:53:29,624 INFO  [RpcServer.handler=15,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server sceplus-vm48.almaden.ibm.com,60020,1404182803191
2014-06-30 19:53:29,624 WARN  [RpcServer.handler=41,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 2978ms
2014-06-30 19:53:29,624 INFO  [RpcServer.handler=41,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server sceplus-vm48.almaden.ibm.com,60020,1404182803191
2014-06-30 19:53:29,626 WARN  [RpcServer.handler=21,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 2980ms
2014-06-30 19:53:29,626 INFO  [RpcServer.handler=21,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server sceplus-vm48.almaden.ibm.com,60020,1404182803191
2014-06-30 19:53:29,627 WARN  [RpcServer.handler=37,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 969ms
2014-06-30 19:53:29,627 INFO  [RpcServer.handler=37,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server sceplus-vm48.almaden.ibm.com,60020,1404182803191
2014-06-30 19:53:29,627 WARN  [RpcServer.handler=26,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 971ms
2014-06-30 19:53:29,627 INFO  [RpcServer.handler=26,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server sceplus-vm48.almaden.ibm.com,60020,1404182803191
2014-06-30 19:53:29,627 WARN  [RpcServer.handler=40,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 971ms
2014-06-30 19:53:29,627 INFO  [RpcServer.handler=40,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server sceplus-vm48.almaden.ibm.com,60020,1404182803191
2014-06-30 19:53:29,627 WARN  [RpcServer.handler=38,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 971ms
2014-06-30 19:53:29,627 INFO  [RpcServer.handler=38,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server sceplus-vm48.almaden.ibm.com,60020,1404182803191
2014-06-30 19:53:29,628 WARN  [RpcServer.handler=17,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 973ms
2014-06-30 19:53:29,628 INFO  [RpcServer.handler=17,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server sceplus-vm48.almaden.ibm.com,60020,1404182803191
2014-06-30 19:53:29,628 WARN  [RpcServer.handler=13,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 973ms
2014-06-30 19:53:29,628 INFO  [RpcServer.handler=13,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server sceplus-vm48.almaden.ibm.com,60020,1404182803191
2014-06-30 19:53:29,629 WARN  [RpcServer.handler=29,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 974ms
2014-06-30 19:53:29,629 INFO  [RpcServer.handler=29,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server sceplus-vm48.almaden.ibm.com,60020,1404182803191
2014-06-30 19:53:29,629 WARN  [RpcServer.handler=1,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 2981ms
2014-06-30 19:53:29,629 INFO  [RpcServer.handler=1,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server sceplus-vm48.almaden.ibm.com,60020,1404182803191
2014-06-30 19:53:31,789 INFO  [JvmPauseMonitor] util.JvmPauseMonitor: Detected pause in JVM or host machine (eg GC): pause of approximately 1635ms
GC pool 'ConcurrentMarkSweep' had collection(s): count=1 time=1973ms
2014-06-30 19:53:31,824 WARN  [RpcServer.handler=23,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":11019,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.58:49877","starttimems":1404183200804,"queuetimems":0,"class":"HRegionServer","responsesize":16900,"method":"Multi"}
2014-06-30 19:53:32,038 WARN  [RpcServer.handler=48,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":11193,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.58:49877","starttimems":1404183200844,"queuetimems":0,"class":"HRegionServer","responsesize":17167,"method":"Multi"}
2014-06-30 19:53:34,323 INFO  [JvmPauseMonitor] util.JvmPauseMonitor: Detected pause in JVM or host machine (eg GC): pause of approximately 2034ms
GC pool 'ParNew' had collection(s): count=1 time=0ms
GC pool 'ConcurrentMarkSweep' had collection(s): count=1 time=2342ms
2014-06-30 19:53:34,601 INFO  [MemStoreFlusher.1] regionserver.DefaultStoreFlusher: Flushed, sequenceid=97, memsize=68.7m, hasBloomFilter=true, into tmp file hdfs://master:54310/hbase/data/default/usertable/1d66ac045cc0407c0fa426db565fcbba/.tmp/0bb24800daf14d04bf8ce1fe364391ad
2014-06-30 19:53:34,617 DEBUG [MemStoreFlusher.1] regionserver.HRegionFileSystem: Committing store file hdfs://master:54310/hbase/data/default/usertable/1d66ac045cc0407c0fa426db565fcbba/.tmp/0bb24800daf14d04bf8ce1fe364391ad as hdfs://master:54310/hbase/data/default/usertable/1d66ac045cc0407c0fa426db565fcbba/family/0bb24800daf14d04bf8ce1fe364391ad
2014-06-30 19:53:34,620 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-06-30 19:53:34,625 WARN  [RpcServer.handler=18,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":15750,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.58:49877","starttimems":1404183198874,"queuetimems":1,"class":"HRegionServer","responsesize":15083,"method":"Multi"}
2014-06-30 19:53:34,625 WARN  [RpcServer.handler=18,port=60020] ipc.RpcServer: RpcServer.respondercallId: 446 service: ClientService methodName: Multi size: 2.6m connection: 9.1.143.58:49877: output error
2014-06-30 19:53:34,626 WARN  [RpcServer.handler=36,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":14351,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.58:49877","starttimems":1404183200275,"queuetimems":0,"class":"HRegionServer","responsesize":15614,"method":"Multi"}
2014-06-30 19:53:34,627 DEBUG [MemStoreFlusher.0] regionserver.MemStoreFlusher: Flush thread woke up because memory above low water=367.3m
2014-06-30 19:53:34,627 INFO  [MemStoreFlusher.0] regionserver.MemStoreFlusher: Flush of region usertable,user7,1404183160444.994b975d2c4cd5bbe532bddfb5e4ac46. due to global heap pressure
2014-06-30 19:53:34,628 DEBUG [MemStoreFlusher.0] regionserver.HRegion: Started memstore flush for usertable,user7,1404183160444.994b975d2c4cd5bbe532bddfb5e4ac46., current region memstore size 120.2m
2014-06-30 19:53:34,630 WARN  [RpcServer.handler=20,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":13892,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.58:49877","starttimems":1404183200738,"queuetimems":0,"class":"HRegionServer","responsesize":16074,"method":"Multi"}
2014-06-30 19:53:34,630 WARN  [RpcServer.handler=45,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":14458,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.58:49877","starttimems":1404183200172,"queuetimems":1,"class":"HRegionServer","responsesize":14017,"method":"Multi"}
2014-06-30 19:53:34,630 WARN  [RpcServer.handler=28,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":13873,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.58:49877","starttimems":1404183200757,"queuetimems":1,"class":"HRegionServer","responsesize":15624,"method":"Multi"}
2014-06-30 19:53:34,631 WARN  [RpcServer.handler=32,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":13882,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.58:49877","starttimems":1404183200748,"queuetimems":1,"class":"HRegionServer","responsesize":15676,"method":"Multi"}
2014-06-30 19:53:34,632 WARN  [RpcServer.handler=18,port=60020] ipc.RpcServer: RpcServer.handler=18,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-06-30 19:53:34,633 WARN  [RpcServer.handler=32,port=60020] ipc.RpcServer: RpcServer.respondercallId: 468 service: ClientService methodName: Multi size: 2.7m connection: 9.1.143.58:49877: output error
2014-06-30 19:53:34,633 WARN  [RpcServer.handler=32,port=60020] ipc.RpcServer: RpcServer.handler=32,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-06-30 19:53:34,633 WARN  [RpcServer.handler=28,port=60020] ipc.RpcServer: RpcServer.respondercallId: 467 service: ClientService methodName: Multi size: 2.7m connection: 9.1.143.58:49877: output error
2014-06-30 19:53:34,633 WARN  [RpcServer.handler=28,port=60020] ipc.RpcServer: RpcServer.handler=28,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-06-30 19:53:34,633 WARN  [RpcServer.handler=45,port=60020] ipc.RpcServer: RpcServer.respondercallId: 477 service: ClientService methodName: Multi size: 2.5m connection: 9.1.143.58:49877: output error
2014-06-30 19:53:34,633 WARN  [RpcServer.handler=45,port=60020] ipc.RpcServer: RpcServer.handler=45,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-06-30 19:53:34,633 WARN  [RpcServer.handler=20,port=60020] ipc.RpcServer: RpcServer.respondercallId: 469 service: ClientService methodName: Multi size: 2.8m connection: 9.1.143.58:49877: output error
2014-06-30 19:53:34,633 WARN  [RpcServer.handler=20,port=60020] ipc.RpcServer: RpcServer.handler=20,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-06-30 19:53:34,633 WARN  [RpcServer.handler=36,port=60020] ipc.RpcServer: RpcServer.respondercallId: 471 service: ClientService methodName: Multi size: 2.7m connection: 9.1.143.58:49877: output error
2014-06-30 19:53:34,633 WARN  [RpcServer.handler=36,port=60020] ipc.RpcServer: RpcServer.handler=36,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-06-30 19:53:34,647 INFO  [MemStoreFlusher.1] regionserver.HStore: Added hdfs://master:54310/hbase/data/default/usertable/1d66ac045cc0407c0fa426db565fcbba/family/0bb24800daf14d04bf8ce1fe364391ad, entries=250300, sequenceid=97, filesize=37.3m
2014-06-30 19:53:34,647 INFO  [MemStoreFlusher.1] regionserver.HRegion: Finished memstore flush of ~146.7m/153870800, currentsize=13.8m/14494400 for region usertable,user5,1404183160444.1d66ac045cc0407c0fa426db565fcbba. in 10960ms, sequenceid=97, compaction requested=false
2014-06-30 19:53:34,655 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 438 synced till here 420
2014-06-30 19:53:34,746 DEBUG [RpcServer.handler=1,port=60020] regionserver.HRegion: Flush requested on usertable,user7,1404183160444.994b975d2c4cd5bbe532bddfb5e4ac46.
2014-06-30 19:53:34,747 DEBUG [MemStoreFlusher.1] regionserver.HRegion: NOT flushing memstore for region usertable,user7,1404183160444.994b975d2c4cd5bbe532bddfb5e4ac46., flushing=true, writesEnabled=true
2014-06-30 19:53:34,772 WARN  [RpcServer.handler=30,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":14006,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.58:49877","starttimems":1404183200765,"queuetimems":0,"class":"HRegionServer","responsesize":15616,"method":"Multi"}
2014-06-30 19:53:34,772 WARN  [RpcServer.handler=30,port=60020] ipc.RpcServer: RpcServer.respondercallId: 466 service: ClientService methodName: Multi size: 2.7m connection: 9.1.143.58:49877: output error
2014-06-30 19:53:34,772 WARN  [RpcServer.handler=30,port=60020] ipc.RpcServer: RpcServer.handler=30,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-06-30 19:53:34,774 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1404182803191/sceplus-vm48.almaden.ibm.com%2C60020%2C1404182803191.1404183203628 with entries=102, filesize=83.8m; new WAL /hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1404182803191/sceplus-vm48.almaden.ibm.com%2C60020%2C1404182803191.1404183214620
2014-06-30 19:53:34,797 WARN  [RpcServer.handler=43,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":14590,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.58:49877","starttimems":1404183200206,"queuetimems":0,"class":"HRegionServer","responsesize":15737,"method":"Multi"}
2014-06-30 19:53:34,797 WARN  [RpcServer.handler=43,port=60020] ipc.RpcServer: RpcServer.respondercallId: 475 service: ClientService methodName: Multi size: 2.8m connection: 9.1.143.58:49877: output error
2014-06-30 19:53:34,797 WARN  [RpcServer.handler=43,port=60020] ipc.RpcServer: RpcServer.handler=43,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-06-30 19:53:36,533 INFO  [JvmPauseMonitor] util.JvmPauseMonitor: Detected pause in JVM or host machine (eg GC): pause of approximately 1208ms
GC pool 'ConcurrentMarkSweep' had collection(s): count=1 time=1619ms
2014-06-30 19:53:36,725 WARN  [RpcServer.handler=46,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":16616,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.58:49877","starttimems":1404183200108,"queuetimems":0,"class":"HRegionServer","responsesize":14654,"method":"Multi"}
2014-06-30 19:53:36,725 WARN  [RpcServer.handler=35,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":16642,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.58:49877","starttimems":1404183200082,"queuetimems":0,"class":"HRegionServer","responsesize":15624,"method":"Multi"}
2014-06-30 19:53:36,725 WARN  [RpcServer.handler=38,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":16599,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.58:49877","starttimems":1404183200125,"queuetimems":0,"class":"HRegionServer","responsesize":15737,"method":"Multi"}
2014-06-30 19:53:36,725 WARN  [RpcServer.handler=46,port=60020] ipc.RpcServer: RpcServer.respondercallId: 452 service: ClientService methodName: Multi size: 2.6m connection: 9.1.143.58:49877: output error
2014-06-30 19:53:36,725 WARN  [RpcServer.handler=46,port=60020] ipc.RpcServer: RpcServer.handler=46,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-06-30 19:53:36,726 WARN  [RpcServer.handler=40,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":16502,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.58:49877","starttimems":1404183200223,"queuetimems":0,"class":"HRegionServer","responsesize":15384,"method":"Multi"}
2014-06-30 19:53:36,726 WARN  [RpcServer.handler=38,port=60020] ipc.RpcServer: RpcServer.respondercallId: 451 service: ClientService methodName: Multi size: 2.8m connection: 9.1.143.58:49877: output error
2014-06-30 19:53:36,726 WARN  [RpcServer.handler=38,port=60020] ipc.RpcServer: RpcServer.handler=38,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-06-30 19:53:36,726 WARN  [RpcServer.handler=40,port=60020] ipc.RpcServer: RpcServer.respondercallId: 474 service: ClientService methodName: Multi size: 2.7m connection: 9.1.143.58:49877: output error
2014-06-30 19:53:36,726 WARN  [RpcServer.handler=40,port=60020] ipc.RpcServer: RpcServer.handler=40,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-06-30 19:53:36,726 WARN  [RpcServer.handler=35,port=60020] ipc.RpcServer: RpcServer.respondercallId: 454 service: ClientService methodName: Multi size: 2.7m connection: 9.1.143.58:49877: output error
2014-06-30 19:53:36,726 WARN  [RpcServer.handler=35,port=60020] ipc.RpcServer: RpcServer.handler=35,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-06-30 19:53:36,727 WARN  [RpcServer.handler=47,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":16470,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.58:49877","starttimems":1404183200257,"queuetimems":0,"class":"HRegionServer","responsesize":16063,"method":"Multi"}
2014-06-30 19:53:36,727 WARN  [RpcServer.handler=47,port=60020] ipc.RpcServer: RpcServer.respondercallId: 472 service: ClientService methodName: Multi size: 2.8m connection: 9.1.143.58:49877: output error
2014-06-30 19:53:36,727 WARN  [RpcServer.handler=47,port=60020] ipc.RpcServer: RpcServer.handler=47,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-06-30 19:53:36,728 WARN  [RpcServer.handler=42,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":16586,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.58:49877","starttimems":1404183200141,"queuetimems":0,"class":"HRegionServer","responsesize":14982,"method":"Multi"}
2014-06-30 19:53:36,728 WARN  [RpcServer.handler=42,port=60020] ipc.RpcServer: RpcServer.respondercallId: 450 service: ClientService methodName: Multi size: 2.6m connection: 9.1.143.58:49877: output error
2014-06-30 19:53:36,728 WARN  [RpcServer.handler=42,port=60020] ipc.RpcServer: RpcServer.handler=42,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-06-30 19:53:36,853 DEBUG [MemStoreFlusher.0] util.FSUtils: DFS Client does not support most favored nodes create; using default create
2014-06-30 19:53:36,895 WARN  [RpcServer.handler=39,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":16707,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.58:49877","starttimems":1404183200188,"queuetimems":0,"class":"HRegionServer","responsesize":15208,"method":"Multi"}
2014-06-30 19:53:36,895 WARN  [RpcServer.handler=39,port=60020] ipc.RpcServer: RpcServer.respondercallId: 476 service: ClientService methodName: Multi size: 2.7m connection: 9.1.143.58:49877: output error
2014-06-30 19:53:36,896 WARN  [RpcServer.handler=39,port=60020] ipc.RpcServer: RpcServer.handler=39,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-06-30 19:53:36,905 WARN  [RpcServer.handler=13,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":18066,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.58:49877","starttimems":1404183198838,"queuetimems":1,"class":"HRegionServer","responsesize":13985,"method":"Multi"}
2014-06-30 19:53:36,905 WARN  [RpcServer.handler=44,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":16813,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.58:49877","starttimems":1404183200091,"queuetimems":0,"class":"HRegionServer","responsesize":15078,"method":"Multi"}
2014-06-30 19:53:36,905 WARN  [RpcServer.handler=13,port=60020] ipc.RpcServer: RpcServer.respondercallId: 448 service: ClientService methodName: Multi size: 2.4m connection: 9.1.143.58:49877: output error
2014-06-30 19:53:36,905 WARN  [RpcServer.handler=13,port=60020] ipc.RpcServer: RpcServer.handler=13,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-06-30 19:53:36,905 WARN  [RpcServer.handler=44,port=60020] ipc.RpcServer: RpcServer.respondercallId: 453 service: ClientService methodName: Multi size: 2.6m connection: 9.1.143.58:49877: output error
2014-06-30 19:53:36,905 WARN  [RpcServer.handler=44,port=60020] ipc.RpcServer: RpcServer.handler=44,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-06-30 19:53:36,905 WARN  [RpcServer.handler=41,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":16665,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.58:49877","starttimems":1404183200240,"queuetimems":1,"class":"HRegionServer","responsesize":15915,"method":"Multi"}
2014-06-30 19:53:36,905 WARN  [RpcServer.handler=41,port=60020] ipc.RpcServer: RpcServer.respondercallId: 473 service: ClientService methodName: Multi size: 2.8m connection: 9.1.143.58:49877: output error
2014-06-30 19:53:36,905 WARN  [RpcServer.handler=41,port=60020] ipc.RpcServer: RpcServer.handler=41,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-06-30 19:53:36,906 WARN  [RpcServer.handler=7,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":18052,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.58:49877","starttimems":1404183198853,"queuetimems":0,"class":"HRegionServer","responsesize":13576,"method":"Multi"}
2014-06-30 19:53:36,906 WARN  [RpcServer.handler=11,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":18014,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.58:49877","starttimems":1404183198891,"queuetimems":0,"class":"HRegionServer","responsesize":15056,"method":"Multi"}
2014-06-30 19:53:36,906 WARN  [RpcServer.handler=7,port=60020] ipc.RpcServer: RpcServer.respondercallId: 447 service: ClientService methodName: Multi size: 2.4m connection: 9.1.143.58:49877: output error
2014-06-30 19:53:36,906 WARN  [RpcServer.handler=7,port=60020] ipc.RpcServer: RpcServer.handler=7,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-06-30 19:53:36,906 WARN  [RpcServer.handler=11,port=60020] ipc.RpcServer: RpcServer.respondercallId: 445 service: ClientService methodName: Multi size: 2.6m connection: 9.1.143.58:49877: output error
2014-06-30 19:53:36,906 WARN  [RpcServer.handler=11,port=60020] ipc.RpcServer: RpcServer.handler=11,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-06-30 19:53:37,245 WARN  [RpcServer.handler=26,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":16516,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.58:49877","starttimems":1404183200728,"queuetimems":0,"class":"HRegionServer","responsesize":15658,"method":"Multi"}
2014-06-30 19:53:37,245 WARN  [RpcServer.handler=10,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":18479,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.58:49877","starttimems":1404183198765,"queuetimems":1,"class":"HRegionServer","responsesize":17221,"method":"Multi"}
2014-06-30 19:53:37,245 WARN  [RpcServer.handler=26,port=60020] ipc.RpcServer: RpcServer.respondercallId: 470 service: ClientService methodName: Multi size: 2.7m connection: 9.1.143.58:49877: output error
2014-06-30 19:53:37,245 WARN  [RpcServer.handler=26,port=60020] ipc.RpcServer: RpcServer.handler=26,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-06-30 19:53:37,245 WARN  [RpcServer.handler=10,port=60020] ipc.RpcServer: RpcServer.respondercallId: 441 service: ClientService methodName: Multi size: 3.0m connection: 9.1.143.58:49877: output error
2014-06-30 19:53:37,245 WARN  [RpcServer.handler=10,port=60020] ipc.RpcServer: RpcServer.handler=10,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-06-30 19:53:39,065 INFO  [JvmPauseMonitor] util.JvmPauseMonitor: Detected pause in JVM or host machine (eg GC): pause of approximately 1324ms
GC pool 'ParNew' had collection(s): count=1 time=0ms
GC pool 'ConcurrentMarkSweep' had collection(s): count=2 time=1817ms
2014-06-30 19:53:39,071 WARN  [RpcServer.handler=33,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":17181,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.58:49877","starttimems":1404183200063,"queuetimems":0,"class":"HRegionServer","responsesize":17012,"method":"Multi"}
2014-06-30 19:53:39,074 WARN  [RpcServer.handler=33,port=60020] ipc.RpcServer: RpcServer.respondercallId: 456 service: ClientService methodName: Multi size: 3.0m connection: 9.1.143.58:49877: output error
2014-06-30 19:53:39,074 WARN  [RpcServer.handler=33,port=60020] ipc.RpcServer: RpcServer.handler=33,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-06-30 19:53:39,078 WARN  [RpcServer.handler=5,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":20249,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.58:49877","starttimems":1404183198829,"queuetimems":1,"class":"HRegionServer","responsesize":15793,"method":"Multi"}
2014-06-30 19:53:39,078 WARN  [RpcServer.handler=5,port=60020] ipc.RpcServer: RpcServer.respondercallId: 444 service: ClientService methodName: Multi size: 2.8m connection: 9.1.143.58:49877: output error
2014-06-30 19:53:39,078 WARN  [RpcServer.handler=5,port=60020] ipc.RpcServer: RpcServer.handler=5,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-06-30 19:53:39,090 WARN  [RpcServer.handler=15,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":19066,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.58:49877","starttimems":1404183200023,"queuetimems":0,"class":"HRegionServer","responsesize":16943,"method":"Multi"}
2014-06-30 19:53:39,090 WARN  [RpcServer.handler=15,port=60020] ipc.RpcServer: RpcServer.respondercallId: 460 service: ClientService methodName: Multi size: 3.0m connection: 9.1.143.58:49877: output error
2014-06-30 19:53:39,090 WARN  [RpcServer.handler=15,port=60020] ipc.RpcServer: RpcServer.handler=15,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-06-30 19:53:39,295 WARN  [RpcServer.handler=49,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":12887,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.58:49877","starttimems":1404183206407,"queuetimems":0,"class":"HRegionServer","responsesize":16882,"method":"Multi"}
2014-06-30 19:53:39,296 DEBUG [MemStoreFlusher.1] regionserver.MemStoreFlusher: Flush thread woke up because memory above low water=367.3m
2014-06-30 19:53:39,296 INFO  [MemStoreFlusher.1] regionserver.MemStoreFlusher: Flush of region usertable,user2,1404183160444.5446f1f888398e97c0ec3cc30da5bf97. due to global heap pressure
2014-06-30 19:53:39,296 WARN  [RpcServer.handler=49,port=60020] ipc.RpcServer: RpcServer.respondercallId: 480 service: ClientService methodName: Multi size: 3.0m connection: 9.1.143.58:49877: output error
2014-06-30 19:53:39,296 WARN  [RpcServer.handler=49,port=60020] ipc.RpcServer: RpcServer.handler=49,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-06-30 19:53:39,296 DEBUG [MemStoreFlusher.1] regionserver.HRegion: Started memstore flush for usertable,user2,1404183160444.5446f1f888398e97c0ec3cc30da5bf97., current region memstore size 83.4m
2014-06-30 19:53:39,297 WARN  [RpcServer.handler=19,port=60020] ipc.RpcServer: RpcServer.respondercallId: 493 service: ClientService methodName: Multi size: 2.4m connection: 9.1.143.58:49877: output error
2014-06-30 19:53:39,297 WARN  [RpcServer.handler=19,port=60020] ipc.RpcServer: RpcServer.handler=19,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-06-30 19:53:39,298 WARN  [RpcServer.handler=34,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":19225,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.58:49877","starttimems":1404183200073,"queuetimems":0,"class":"HRegionServer","responsesize":17388,"method":"Multi"}
2014-06-30 19:53:39,298 WARN  [RpcServer.handler=0,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":12808,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.58:49877","starttimems":1404183206490,"queuetimems":0,"class":"HRegionServer","responsesize":16829,"method":"Multi"}
2014-06-30 19:53:39,298 WARN  [RpcServer.handler=21,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":12851,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.58:49877","starttimems":1404183206447,"queuetimems":0,"class":"HRegionServer","responsesize":17208,"method":"Multi"}
2014-06-30 19:53:39,298 WARN  [RpcServer.handler=0,port=60020] ipc.RpcServer: RpcServer.respondercallId: 486 service: ClientService methodName: Multi size: 2.9m connection: 9.1.143.58:49877: output error
2014-06-30 19:53:39,299 WARN  [RpcServer.handler=0,port=60020] ipc.RpcServer: RpcServer.handler=0,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-06-30 19:53:39,299 WARN  [RpcServer.handler=16,port=60020] ipc.RpcServer: RpcServer.respondercallId: 495 service: ClientService methodName: Multi size: 2.7m connection: 9.1.143.58:49877: output error
2014-06-30 19:53:39,299 WARN  [RpcServer.handler=16,port=60020] ipc.RpcServer: RpcServer.handler=16,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-06-30 19:53:39,299 WARN  [RpcServer.handler=21,port=60020] ipc.RpcServer: RpcServer.respondercallId: 482 service: ClientService methodName: Multi size: 3.0m connection: 9.1.143.58:49877: output error
2014-06-30 19:53:39,299 WARN  [RpcServer.handler=21,port=60020] ipc.RpcServer: RpcServer.handler=21,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-06-30 19:53:39,299 WARN  [RpcServer.handler=8,port=60020] ipc.RpcServer: RpcServer.respondercallId: 494 service: ClientService methodName: Multi size: 2.6m connection: 9.1.143.58:49877: output error
2014-06-30 19:53:39,299 WARN  [RpcServer.handler=8,port=60020] ipc.RpcServer: RpcServer.handler=8,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-06-30 19:53:39,299 WARN  [RpcServer.handler=3,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":19246,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.58:49877","starttimems":1404183200053,"queuetimems":0,"class":"HRegionServer","responsesize":17196,"method":"Multi"}
2014-06-30 19:53:39,300 WARN  [RpcServer.handler=3,port=60020] ipc.RpcServer: RpcServer.respondercallId: 457 service: ClientService methodName: Multi size: 3.0m connection: 9.1.143.58:49877: output error
2014-06-30 19:53:39,300 WARN  [RpcServer.handler=3,port=60020] ipc.RpcServer: RpcServer.handler=3,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-06-30 19:53:39,300 WARN  [RpcServer.handler=37,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":19142,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.58:49877","starttimems":1404183200158,"queuetimems":0,"class":"HRegionServer","responsesize":16760,"method":"Multi"}
2014-06-30 19:53:39,301 WARN  [RpcServer.handler=37,port=60020] ipc.RpcServer: RpcServer.respondercallId: 449 service: ClientService methodName: Multi size: 2.9m connection: 9.1.143.58:49877: output error
2014-06-30 19:53:39,301 WARN  [RpcServer.handler=37,port=60020] ipc.RpcServer: RpcServer.handler=37,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-06-30 19:53:39,301 WARN  [RpcServer.handler=17,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":19268,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.58:49877","starttimems":1404183200033,"queuetimems":0,"class":"HRegionServer","responsesize":17169,"method":"Multi"}
2014-06-30 19:53:39,301 WARN  [RpcServer.handler=17,port=60020] ipc.RpcServer: RpcServer.respondercallId: 459 service: ClientService methodName: Multi size: 3.0m connection: 9.1.143.58:49877: output error
2014-06-30 19:53:39,302 WARN  [RpcServer.handler=17,port=60020] ipc.RpcServer: RpcServer.handler=17,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-06-30 19:53:39,302 WARN  [RpcServer.handler=34,port=60020] ipc.RpcServer: RpcServer.respondercallId: 455 service: ClientService methodName: Multi size: 3.0m connection: 9.1.143.58:49877: output error
2014-06-30 19:53:39,302 WARN  [RpcServer.handler=4,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":12787,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.58:49877","starttimems":1404183206514,"queuetimems":0,"class":"HRegionServer","responsesize":16624,"method":"Multi"}
2014-06-30 19:53:39,302 WARN  [RpcServer.handler=4,port=60020] ipc.RpcServer: RpcServer.respondercallId: 485 service: ClientService methodName: Multi size: 2.9m connection: 9.1.143.58:49877: output error
2014-06-30 19:53:39,302 WARN  [RpcServer.handler=4,port=60020] ipc.RpcServer: RpcServer.handler=4,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-06-30 19:53:39,301 WARN  [RpcServer.handler=1,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":19257,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.58:49877","starttimems":1404183200044,"queuetimems":1,"class":"HRegionServer","responsesize":16853,"method":"Multi"}
2014-06-30 19:53:39,302 WARN  [RpcServer.handler=34,port=60020] ipc.RpcServer: RpcServer.handler=34,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-06-30 19:53:39,302 WARN  [RpcServer.handler=1,port=60020] ipc.RpcServer: RpcServer.respondercallId: 458 service: ClientService methodName: Multi size: 2.9m connection: 9.1.143.58:49877: output error
2014-06-30 19:53:39,302 WARN  [RpcServer.handler=1,port=60020] ipc.RpcServer: RpcServer.handler=1,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-06-30 19:53:39,304 WARN  [RpcServer.handler=2,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":10418,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.58:49877","starttimems":1404183208886,"queuetimems":0,"class":"HRegionServer","responsesize":14462,"method":"Multi"}
2014-06-30 19:53:39,304 WARN  [RpcServer.handler=2,port=60020] ipc.RpcServer: RpcServer.respondercallId: 491 service: ClientService methodName: Multi size: 2.5m connection: 9.1.143.58:49877: output error
2014-06-30 19:53:39,305 WARN  [RpcServer.handler=2,port=60020] ipc.RpcServer: RpcServer.handler=2,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-06-30 19:53:41,319 INFO  [JvmPauseMonitor] util.JvmPauseMonitor: Detected pause in JVM or host machine (eg GC): pause of approximately 1753ms
GC pool 'ParNew' had collection(s): count=1 time=570ms
GC pool 'ConcurrentMarkSweep' had collection(s): count=1 time=1468ms
2014-06-30 19:53:41,343 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-06-30 19:53:41,346 WARN  [RpcServer.handler=23,port=60020] ipc.RpcServer: RpcServer.respondercallId: 523 service: ClientService methodName: Multi size: 2.7m connection: 9.1.143.58:49877: output error
2014-06-30 19:53:41,346 WARN  [RpcServer.handler=23,port=60020] ipc.RpcServer: RpcServer.handler=23,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-06-30 19:53:41,347 INFO  [RpcServer.handler=22,port=60020] regionserver.MemStoreFlusher: Blocking updates on sceplus-vm48.almaden.ibm.com,60020,1404182803191: the global memstore size 397.0m is >= than blocking 386.7m size
2014-06-30 19:53:41,349 INFO  [RpcServer.handler=20,port=60020] regionserver.MemStoreFlusher: Blocking updates on sceplus-vm48.almaden.ibm.com,60020,1404182803191: the global memstore size 397.0m is >= than blocking 386.7m size
2014-06-30 19:53:41,349 INFO  [RpcServer.handler=45,port=60020] regionserver.MemStoreFlusher: Blocking updates on sceplus-vm48.almaden.ibm.com,60020,1404182803191: the global memstore size 397.0m is >= than blocking 386.7m size
2014-06-30 19:53:41,349 INFO  [RpcServer.handler=28,port=60020] regionserver.MemStoreFlusher: Blocking updates on sceplus-vm48.almaden.ibm.com,60020,1404182803191: the global memstore size 397.0m is >= than blocking 386.7m size
2014-06-30 19:53:41,349 INFO  [RpcServer.handler=30,port=60020] regionserver.MemStoreFlusher: Blocking updates on sceplus-vm48.almaden.ibm.com,60020,1404182803191: the global memstore size 397.0m is >= than blocking 386.7m size
2014-06-30 19:53:41,349 INFO  [RpcServer.handler=36,port=60020] regionserver.MemStoreFlusher: Blocking updates on sceplus-vm48.almaden.ibm.com,60020,1404182803191: the global memstore size 397.0m is >= than blocking 386.7m size
2014-06-30 19:53:41,357 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 535 synced till here 521
2014-06-30 19:53:41,451 WARN  [RpcServer.handler=12,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":14955,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.58:49877","starttimems":1404183206471,"queuetimems":1,"class":"HRegionServer","responsesize":16802,"method":"Multi"}
2014-06-30 19:53:41,451 WARN  [RpcServer.handler=29,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":15008,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.58:49877","starttimems":1404183206418,"queuetimems":0,"class":"HRegionServer","responsesize":17189,"method":"Multi"}
2014-06-30 19:53:41,452 WARN  [RpcServer.handler=12,port=60020] ipc.RpcServer: RpcServer.respondercallId: 487 service: ClientService methodName: Multi size: 2.9m connection: 9.1.143.58:49877: output error
2014-06-30 19:53:41,451 WARN  [RpcServer.handler=31,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":15017,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.58:49877","starttimems":1404183206434,"queuetimems":0,"class":"HRegionServer","responsesize":16853,"method":"Multi"}
2014-06-30 19:53:41,451 WARN  [RpcServer.handler=24,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":14968,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.58:49877","starttimems":1404183206458,"queuetimems":1,"class":"HRegionServer","responsesize":16580,"method":"Multi"}
2014-06-30 19:53:41,452 WARN  [RpcServer.handler=31,port=60020] ipc.RpcServer: RpcServer.respondercallId: 483 service: ClientService methodName: Multi size: 2.9m connection: 9.1.143.58:49877: output error
2014-06-30 19:53:41,452 WARN  [RpcServer.handler=31,port=60020] ipc.RpcServer: RpcServer.handler=31,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-06-30 19:53:41,452 WARN  [RpcServer.handler=24,port=60020] ipc.RpcServer: RpcServer.respondercallId: 488 service: ClientService methodName: Multi size: 2.9m connection: 9.1.143.58:49877: output error
2014-06-30 19:53:41,452 WARN  [RpcServer.handler=24,port=60020] ipc.RpcServer: RpcServer.handler=24,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-06-30 19:53:41,451 WARN  [RpcServer.handler=9,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":14914,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.58:49877","starttimems":1404183206537,"queuetimems":0,"class":"HRegionServer","responsesize":16880,"method":"Multi"}
2014-06-30 19:53:41,453 WARN  [RpcServer.handler=29,port=60020] ipc.RpcServer: RpcServer.respondercallId: 481 service: ClientService methodName: Multi size: 3.0m connection: 9.1.143.58:49877: output error
2014-06-30 19:53:41,452 WARN  [RpcServer.handler=12,port=60020] ipc.RpcServer: RpcServer.handler=12,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-06-30 19:53:41,453 WARN  [RpcServer.handler=29,port=60020] ipc.RpcServer: RpcServer.handler=29,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-06-30 19:53:41,453 WARN  [RpcServer.handler=9,port=60020] ipc.RpcServer: RpcServer.respondercallId: 484 service: ClientService methodName: Multi size: 2.9m connection: 9.1.143.58:49877: output error
2014-06-30 19:53:41,453 INFO  [RpcServer.handler=43,port=60020] regionserver.MemStoreFlusher: Blocking updates on sceplus-vm48.almaden.ibm.com,60020,1404182803191: the global memstore size 402.4m is >= than blocking 386.7m size
2014-06-30 19:53:41,453 WARN  [RpcServer.handler=9,port=60020] ipc.RpcServer: RpcServer.handler=9,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-06-30 19:53:41,453 INFO  [RpcServer.handler=46,port=60020] regionserver.MemStoreFlusher: Blocking updates on sceplus-vm48.almaden.ibm.com,60020,1404182803191: the global memstore size 402.4m is >= than blocking 386.7m size
2014-06-30 19:53:41,455 WARN  [RpcServer.handler=14,port=60020] ipc.RpcServer: RpcServer.respondercallId: 496 service: ClientService methodName: Multi size: 2.6m connection: 9.1.143.58:49877: output error
2014-06-30 19:53:41,455 WARN  [RpcServer.handler=14,port=60020] ipc.RpcServer: RpcServer.handler=14,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-06-30 19:53:41,455 WARN  [RpcServer.handler=48,port=60020] ipc.RpcServer: RpcServer.respondercallId: 522 service: ClientService methodName: Multi size: 2.6m connection: 9.1.143.58:49877: output error
2014-06-30 19:53:41,456 WARN  [RpcServer.handler=48,port=60020] ipc.RpcServer: RpcServer.handler=48,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-06-30 19:53:41,456 INFO  [RpcServer.handler=6,port=60020] regionserver.MemStoreFlusher: Blocking updates on sceplus-vm48.almaden.ibm.com,60020,1404182803191: the global memstore size 413.3m is >= than blocking 386.7m size
2014-06-30 19:53:41,457 INFO  [RpcServer.handler=27,port=60020] regionserver.MemStoreFlusher: Blocking updates on sceplus-vm48.almaden.ibm.com,60020,1404182803191: the global memstore size 413.3m is >= than blocking 386.7m size
2014-06-30 19:53:41,457 INFO  [RpcServer.handler=25,port=60020] regionserver.MemStoreFlusher: Blocking updates on sceplus-vm48.almaden.ibm.com,60020,1404182803191: the global memstore size 413.3m is >= than blocking 386.7m size
2014-06-30 19:53:41,458 INFO  [RpcServer.handler=18,port=60020] regionserver.MemStoreFlusher: Blocking updates on sceplus-vm48.almaden.ibm.com,60020,1404182803191: the global memstore size 413.3m is >= than blocking 386.7m size
2014-06-30 19:53:41,459 INFO  [RpcServer.handler=32,port=60020] regionserver.MemStoreFlusher: Blocking updates on sceplus-vm48.almaden.ibm.com,60020,1404182803191: the global memstore size 413.3m is >= than blocking 386.7m size
2014-06-30 19:53:41,464 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1404182803191/sceplus-vm48.almaden.ibm.com%2C60020%2C1404182803191.1404183214620 with entries=97, filesize=71.8m; new WAL /hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1404182803191/sceplus-vm48.almaden.ibm.com%2C60020%2C1404182803191.1404183221344
2014-06-30 19:53:41,532 DEBUG [MemStoreFlusher.1] util.FSUtils: DFS Client does not support most favored nodes create; using default create
2014-06-30 19:53:41,936 INFO  [RpcServer.handler=38,port=60020] regionserver.MemStoreFlusher: Blocking updates on sceplus-vm48.almaden.ibm.com,60020,1404182803191: the global memstore size 413.3m is >= than blocking 386.7m size
2014-06-30 19:53:41,996 INFO  [MemStoreFlusher.0] regionserver.DefaultStoreFlusher: Flushed, sequenceid=100, memsize=68.7m, hasBloomFilter=true, into tmp file hdfs://master:54310/hbase/data/default/usertable/994b975d2c4cd5bbe532bddfb5e4ac46/.tmp/abc6c91e26f946d7973d591194563e37
2014-06-30 19:53:42,010 DEBUG [MemStoreFlusher.0] regionserver.HRegionFileSystem: Committing store file hdfs://master:54310/hbase/data/default/usertable/994b975d2c4cd5bbe532bddfb5e4ac46/.tmp/abc6c91e26f946d7973d591194563e37 as hdfs://master:54310/hbase/data/default/usertable/994b975d2c4cd5bbe532bddfb5e4ac46/family/abc6c91e26f946d7973d591194563e37
2014-06-30 19:53:42,021 INFO  [MemStoreFlusher.0] regionserver.HStore: Added hdfs://master:54310/hbase/data/default/usertable/994b975d2c4cd5bbe532bddfb5e4ac46/family/abc6c91e26f946d7973d591194563e37, entries=250080, sequenceid=100, filesize=37.3m
2014-06-30 19:53:42,022 INFO  [MemStoreFlusher.0] regionserver.HRegion: Finished memstore flush of ~150.8m/158091520, currentsize=25.9m/27111600 for region usertable,user7,1404183160444.994b975d2c4cd5bbe532bddfb5e4ac46. in 7394ms, sequenceid=100, compaction requested=false
2014-06-30 19:53:42,022 WARN  [RpcServer.handler=38,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 86ms
2014-06-30 19:53:42,022 INFO  [RpcServer.handler=38,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server sceplus-vm48.almaden.ibm.com,60020,1404182803191
2014-06-30 19:53:42,022 WARN  [RpcServer.handler=32,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 563ms
2014-06-30 19:53:42,023 INFO  [RpcServer.handler=32,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server sceplus-vm48.almaden.ibm.com,60020,1404182803191
2014-06-30 19:53:42,023 WARN  [RpcServer.handler=18,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 565ms
2014-06-30 19:53:42,023 INFO  [RpcServer.handler=18,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server sceplus-vm48.almaden.ibm.com,60020,1404182803191
2014-06-30 19:53:42,023 WARN  [RpcServer.handler=25,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 566ms
2014-06-30 19:53:42,023 INFO  [RpcServer.handler=25,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server sceplus-vm48.almaden.ibm.com,60020,1404182803191
2014-06-30 19:53:42,023 WARN  [RpcServer.handler=27,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 566ms
2014-06-30 19:53:42,023 INFO  [RpcServer.handler=27,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server sceplus-vm48.almaden.ibm.com,60020,1404182803191
2014-06-30 19:53:42,023 WARN  [RpcServer.handler=6,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 568ms
2014-06-30 19:53:42,023 INFO  [RpcServer.handler=6,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server sceplus-vm48.almaden.ibm.com,60020,1404182803191
2014-06-30 19:53:42,023 WARN  [RpcServer.handler=46,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 570ms
2014-06-30 19:53:42,024 INFO  [RpcServer.handler=46,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server sceplus-vm48.almaden.ibm.com,60020,1404182803191
2014-06-30 19:53:42,024 WARN  [RpcServer.handler=43,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 571ms
2014-06-30 19:53:42,024 INFO  [RpcServer.handler=43,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server sceplus-vm48.almaden.ibm.com,60020,1404182803191
2014-06-30 19:53:42,024 WARN  [RpcServer.handler=36,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 675ms
2014-06-30 19:53:42,024 INFO  [RpcServer.handler=36,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server sceplus-vm48.almaden.ibm.com,60020,1404182803191
2014-06-30 19:53:42,024 WARN  [RpcServer.handler=30,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 675ms
2014-06-30 19:53:42,024 INFO  [RpcServer.handler=30,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server sceplus-vm48.almaden.ibm.com,60020,1404182803191
2014-06-30 19:53:42,024 WARN  [RpcServer.handler=28,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 675ms
2014-06-30 19:53:42,024 INFO  [RpcServer.handler=28,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server sceplus-vm48.almaden.ibm.com,60020,1404182803191
2014-06-30 19:53:42,025 WARN  [RpcServer.handler=45,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 676ms
2014-06-30 19:53:42,025 INFO  [RpcServer.handler=45,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server sceplus-vm48.almaden.ibm.com,60020,1404182803191
2014-06-30 19:53:42,025 WARN  [RpcServer.handler=20,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 677ms
2014-06-30 19:53:42,025 INFO  [RpcServer.handler=20,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server sceplus-vm48.almaden.ibm.com,60020,1404182803191
2014-06-30 19:53:42,030 WARN  [RpcServer.handler=22,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 684ms
2014-06-30 19:53:42,030 INFO  [RpcServer.handler=22,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server sceplus-vm48.almaden.ibm.com,60020,1404182803191
2014-06-30 19:53:42,051 WARN  [RpcServer.handler=25,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":10036,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.58:49877","starttimems":1404183212015,"queuetimems":0,"class":"HRegionServer","responsesize":16943,"method":"Multi"}
2014-06-30 19:53:42,052 WARN  [RpcServer.handler=25,port=60020] ipc.RpcServer: RpcServer.respondercallId: 524 service: ClientService methodName: Multi size: 3.0m connection: 9.1.143.58:49877: output error
2014-06-30 19:53:42,052 WARN  [RpcServer.handler=25,port=60020] ipc.RpcServer: RpcServer.handler=25,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-06-30 19:53:42,065 WARN  [RpcServer.handler=27,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":10107,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.58:49877","starttimems":1404183211958,"queuetimems":1,"class":"HRegionServer","responsesize":17169,"method":"Multi"}
2014-06-30 19:53:42,066 WARN  [RpcServer.handler=27,port=60020] ipc.RpcServer: RpcServer.respondercallId: 526 service: ClientService methodName: Multi size: 3.0m connection: 9.1.143.58:49877: output error
2014-06-30 19:53:42,066 WARN  [RpcServer.handler=27,port=60020] ipc.RpcServer: RpcServer.handler=27,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-06-30 19:53:42,080 WARN  [RpcServer.handler=22,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":10082,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.58:49877","starttimems":1404183211997,"queuetimems":1,"class":"HRegionServer","responsesize":17388,"method":"Multi"}
2014-06-30 19:53:42,080 WARN  [RpcServer.handler=6,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":10279,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.58:49877","starttimems":1404183211800,"queuetimems":0,"class":"HRegionServer","responsesize":17221,"method":"Multi"}
2014-06-30 19:53:42,080 WARN  [RpcServer.handler=22,port=60020] ipc.RpcServer: RpcServer.respondercallId: 525 service: ClientService methodName: Multi size: 3.0m connection: 9.1.143.58:49877: output error
2014-06-30 19:53:42,080 WARN  [RpcServer.handler=22,port=60020] ipc.RpcServer: RpcServer.handler=22,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-06-30 19:53:42,080 WARN  [RpcServer.handler=6,port=60020] ipc.RpcServer: RpcServer.respondercallId: 492 service: ClientService methodName: Multi size: 3.0m connection: 9.1.143.58:49877: output error
2014-06-30 19:53:42,081 WARN  [RpcServer.handler=6,port=60020] ipc.RpcServer: RpcServer.handler=6,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-06-30 19:53:43,172 INFO  [MemStoreFlusher.1] regionserver.DefaultStoreFlusher: Flushed, sequenceid=126, memsize=68.4m, hasBloomFilter=true, into tmp file hdfs://master:54310/hbase/data/default/usertable/5446f1f888398e97c0ec3cc30da5bf97/.tmp/bafd4f0e46a04e2e8046881dd8521fb4
2014-06-30 19:53:43,187 DEBUG [MemStoreFlusher.1] regionserver.HRegionFileSystem: Committing store file hdfs://master:54310/hbase/data/default/usertable/5446f1f888398e97c0ec3cc30da5bf97/.tmp/bafd4f0e46a04e2e8046881dd8521fb4 as hdfs://master:54310/hbase/data/default/usertable/5446f1f888398e97c0ec3cc30da5bf97/family/bafd4f0e46a04e2e8046881dd8521fb4
2014-06-30 19:53:43,197 INFO  [MemStoreFlusher.1] regionserver.HStore: Added hdfs://master:54310/hbase/data/default/usertable/5446f1f888398e97c0ec3cc30da5bf97/family/bafd4f0e46a04e2e8046881dd8521fb4, entries=249040, sequenceid=126, filesize=37.1m
2014-06-30 19:53:43,198 INFO  [MemStoreFlusher.1] regionserver.HRegion: Finished memstore flush of ~95.8m/100432080, currentsize=2.9m/3092880 for region usertable,user2,1404183160444.5446f1f888398e97c0ec3cc30da5bf97. in 3902ms, sequenceid=126, compaction requested=false
2014-06-30 19:53:45,091 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-06-30 19:53:45,131 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 633 synced till here 617
2014-06-30 19:53:46,645 INFO  [JvmPauseMonitor] util.JvmPauseMonitor: Detected pause in JVM or host machine (eg GC): pause of approximately 1084ms
GC pool 'ParNew' had collection(s): count=1 time=1ms
GC pool 'ConcurrentMarkSweep' had collection(s): count=1 time=1450ms
2014-06-30 19:53:46,761 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1404182803191/sceplus-vm48.almaden.ibm.com%2C60020%2C1404182803191.1404183221344 with entries=98, filesize=73.7m; new WAL /hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1404182803191/sceplus-vm48.almaden.ibm.com%2C60020%2C1404182803191.1404183225093
2014-06-30 19:53:47,454 DEBUG [MemStoreFlusher.1] regionserver.MemStoreFlusher: Flush thread woke up because memory above low water=367.3m
2014-06-30 19:53:47,455 INFO  [MemStoreFlusher.1] regionserver.MemStoreFlusher: Flush of region usertable,user8,1404183160444.28938d267c1c61f7feb582b85e4c172f. due to global heap pressure
2014-06-30 19:53:47,455 DEBUG [MemStoreFlusher.1] regionserver.HRegion: Started memstore flush for usertable,user8,1404183160444.28938d267c1c61f7feb582b85e4c172f., current region memstore size 114.0m
2014-06-30 19:53:47,519 INFO  [RpcServer.handler=29,port=60020] regionserver.MemStoreFlusher: Blocking updates on sceplus-vm48.almaden.ibm.com,60020,1404182803191: the global memstore size 390.2m is >= than blocking 386.7m size
2014-06-30 19:53:47,520 INFO  [RpcServer.handler=24,port=60020] regionserver.MemStoreFlusher: Blocking updates on sceplus-vm48.almaden.ibm.com,60020,1404182803191: the global memstore size 390.2m is >= than blocking 386.7m size
2014-06-30 19:53:47,520 INFO  [RpcServer.handler=31,port=60020] regionserver.MemStoreFlusher: Blocking updates on sceplus-vm48.almaden.ibm.com,60020,1404182803191: the global memstore size 390.2m is >= than blocking 386.7m size
2014-06-30 19:53:47,520 INFO  [RpcServer.handler=9,port=60020] regionserver.MemStoreFlusher: Blocking updates on sceplus-vm48.almaden.ibm.com,60020,1404182803191: the global memstore size 390.2m is >= than blocking 386.7m size
2014-06-30 19:53:47,520 INFO  [RpcServer.handler=12,port=60020] regionserver.MemStoreFlusher: Blocking updates on sceplus-vm48.almaden.ibm.com,60020,1404182803191: the global memstore size 390.2m is >= than blocking 386.7m size
2014-06-30 19:53:47,520 INFO  [RpcServer.handler=2,port=60020] regionserver.MemStoreFlusher: Blocking updates on sceplus-vm48.almaden.ibm.com,60020,1404182803191: the global memstore size 390.2m is >= than blocking 386.7m size
2014-06-30 19:53:47,521 INFO  [RpcServer.handler=17,port=60020] regionserver.MemStoreFlusher: Blocking updates on sceplus-vm48.almaden.ibm.com,60020,1404182803191: the global memstore size 390.2m is >= than blocking 386.7m size
2014-06-30 19:53:47,521 INFO  [RpcServer.handler=1,port=60020] regionserver.MemStoreFlusher: Blocking updates on sceplus-vm48.almaden.ibm.com,60020,1404182803191: the global memstore size 390.2m is >= than blocking 386.7m size
2014-06-30 19:53:47,522 INFO  [RpcServer.handler=14,port=60020] regionserver.MemStoreFlusher: Blocking updates on sceplus-vm48.almaden.ibm.com,60020,1404182803191: the global memstore size 390.2m is >= than blocking 386.7m size
2014-06-30 19:53:47,567 INFO  [RpcServer.handler=21,port=60020] regionserver.MemStoreFlusher: Blocking updates on sceplus-vm48.almaden.ibm.com,60020,1404182803191: the global memstore size 404.3m is >= than blocking 386.7m size
2014-06-30 19:53:47,567 INFO  [RpcServer.handler=8,port=60020] regionserver.MemStoreFlusher: Blocking updates on sceplus-vm48.almaden.ibm.com,60020,1404182803191: the global memstore size 404.3m is >= than blocking 386.7m size
2014-06-30 19:53:47,568 INFO  [RpcServer.handler=4,port=60020] regionserver.MemStoreFlusher: Blocking updates on sceplus-vm48.almaden.ibm.com,60020,1404182803191: the global memstore size 404.3m is >= than blocking 386.7m size
2014-06-30 19:53:47,568 INFO  [RpcServer.handler=34,port=60020] regionserver.MemStoreFlusher: Blocking updates on sceplus-vm48.almaden.ibm.com,60020,1404182803191: the global memstore size 404.3m is >= than blocking 386.7m size
2014-06-30 19:53:47,568 INFO  [RpcServer.handler=23,port=60020] regionserver.MemStoreFlusher: Blocking updates on sceplus-vm48.almaden.ibm.com,60020,1404182803191: the global memstore size 404.3m is >= than blocking 386.7m size
2014-06-30 19:53:47,637 DEBUG [MemStoreFlusher.1] util.FSUtils: DFS Client does not support most favored nodes create; using default create
2014-06-30 19:53:49,590 INFO  [JvmPauseMonitor] util.JvmPauseMonitor: Detected pause in JVM or host machine (eg GC): pause of approximately 1308ms
GC pool 'ParNew' had collection(s): count=1 time=0ms
GC pool 'ConcurrentMarkSweep' had collection(s): count=1 time=1439ms
2014-06-30 19:53:49,736 INFO  [MemStoreFlusher.1] regionserver.DefaultStoreFlusher: Flushed, sequenceid=145, memsize=62.4m, hasBloomFilter=true, into tmp file hdfs://master:54310/hbase/data/default/usertable/28938d267c1c61f7feb582b85e4c172f/.tmp/b266bb0b8a3d4b0d818c5fb405aec18f
2014-06-30 19:53:49,747 DEBUG [MemStoreFlusher.1] regionserver.HRegionFileSystem: Committing store file hdfs://master:54310/hbase/data/default/usertable/28938d267c1c61f7feb582b85e4c172f/.tmp/b266bb0b8a3d4b0d818c5fb405aec18f as hdfs://master:54310/hbase/data/default/usertable/28938d267c1c61f7feb582b85e4c172f/family/b266bb0b8a3d4b0d818c5fb405aec18f
2014-06-30 19:53:49,757 INFO  [MemStoreFlusher.1] regionserver.HStore: Added hdfs://master:54310/hbase/data/default/usertable/28938d267c1c61f7feb582b85e4c172f/family/b266bb0b8a3d4b0d818c5fb405aec18f, entries=227280, sequenceid=145, filesize=33.9m
2014-06-30 19:53:49,757 INFO  [MemStoreFlusher.1] regionserver.HRegion: Finished memstore flush of ~117.2m/122942160, currentsize=9.0m/9420320 for region usertable,user8,1404183160444.28938d267c1c61f7feb582b85e4c172f. in 2302ms, sequenceid=145, compaction requested=false
2014-06-30 19:53:49,757 WARN  [RpcServer.handler=23,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 2189ms
2014-06-30 19:53:49,757 INFO  [RpcServer.handler=23,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server sceplus-vm48.almaden.ibm.com,60020,1404182803191
2014-06-30 19:53:49,757 WARN  [RpcServer.handler=34,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 2189ms
2014-06-30 19:53:49,757 INFO  [RpcServer.handler=34,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server sceplus-vm48.almaden.ibm.com,60020,1404182803191
2014-06-30 19:53:49,757 WARN  [RpcServer.handler=4,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 2189ms
2014-06-30 19:53:49,758 INFO  [RpcServer.handler=4,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server sceplus-vm48.almaden.ibm.com,60020,1404182803191
2014-06-30 19:53:49,758 WARN  [RpcServer.handler=8,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 2191ms
2014-06-30 19:53:49,758 INFO  [RpcServer.handler=8,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server sceplus-vm48.almaden.ibm.com,60020,1404182803191
2014-06-30 19:53:49,758 WARN  [RpcServer.handler=21,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 2192ms
2014-06-30 19:53:49,759 INFO  [RpcServer.handler=21,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server sceplus-vm48.almaden.ibm.com,60020,1404182803191
2014-06-30 19:53:49,760 WARN  [RpcServer.handler=14,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 2239ms
2014-06-30 19:53:49,760 INFO  [RpcServer.handler=14,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server sceplus-vm48.almaden.ibm.com,60020,1404182803191
2014-06-30 19:53:49,760 WARN  [RpcServer.handler=1,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 2241ms
2014-06-30 19:53:49,760 INFO  [RpcServer.handler=1,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server sceplus-vm48.almaden.ibm.com,60020,1404182803191
2014-06-30 19:53:49,774 WARN  [RpcServer.handler=17,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 2254ms
2014-06-30 19:53:49,774 INFO  [RpcServer.handler=17,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server sceplus-vm48.almaden.ibm.com,60020,1404182803191
2014-06-30 19:53:49,774 WARN  [RpcServer.handler=2,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 2254ms
2014-06-30 19:53:49,775 INFO  [RpcServer.handler=2,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server sceplus-vm48.almaden.ibm.com,60020,1404182803191
2014-06-30 19:53:49,778 WARN  [RpcServer.handler=12,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 2258ms
2014-06-30 19:53:49,778 INFO  [RpcServer.handler=12,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server sceplus-vm48.almaden.ibm.com,60020,1404182803191
2014-06-30 19:53:49,778 WARN  [RpcServer.handler=9,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 2259ms
2014-06-30 19:53:49,779 INFO  [RpcServer.handler=9,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server sceplus-vm48.almaden.ibm.com,60020,1404182803191
2014-06-30 19:53:49,779 WARN  [RpcServer.handler=31,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 2260ms
2014-06-30 19:53:49,779 INFO  [RpcServer.handler=31,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server sceplus-vm48.almaden.ibm.com,60020,1404182803191
2014-06-30 19:53:49,779 WARN  [RpcServer.handler=24,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 2260ms
2014-06-30 19:53:49,779 INFO  [RpcServer.handler=24,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server sceplus-vm48.almaden.ibm.com,60020,1404182803191
2014-06-30 19:53:49,779 WARN  [RpcServer.handler=29,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 2260ms
2014-06-30 19:53:49,779 INFO  [RpcServer.handler=29,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server sceplus-vm48.almaden.ibm.com,60020,1404182803191
2014-06-30 19:53:49,800 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-06-30 19:53:49,814 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 714 synced till here 709
2014-06-30 19:53:49,836 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1404182803191/sceplus-vm48.almaden.ibm.com%2C60020%2C1404182803191.1404183225093 with entries=81, filesize=63.2m; new WAL /hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1404182803191/sceplus-vm48.almaden.ibm.com%2C60020%2C1404182803191.1404183229800
2014-06-30 19:53:53,234 DEBUG [MemStoreFlusher.1] regionserver.MemStoreFlusher: Flush thread woke up because memory above low water=367.3m
2014-06-30 19:53:53,234 INFO  [MemStoreFlusher.1] regionserver.MemStoreFlusher: Flush of region usertable,user5,1404183160444.1d66ac045cc0407c0fa426db565fcbba. due to global heap pressure
2014-06-30 19:53:53,235 DEBUG [MemStoreFlusher.1] regionserver.HRegion: Started memstore flush for usertable,user5,1404183160444.1d66ac045cc0407c0fa426db565fcbba., current region memstore size 113.0m
2014-06-30 19:53:53,353 DEBUG [MemStoreFlusher.1] util.FSUtils: DFS Client does not support most favored nodes create; using default create
2014-06-30 19:53:53,422 INFO  [RpcServer.handler=7,port=60020] regionserver.MemStoreFlusher: Blocking updates on sceplus-vm48.almaden.ibm.com,60020,1404182803191: the global memstore size 386.9m is >= than blocking 386.7m size
2014-06-30 19:53:54,637 INFO  [JvmPauseMonitor] util.JvmPauseMonitor: Detected pause in JVM or host machine (eg GC): pause of approximately 1045ms
GC pool 'ParNew' had collection(s): count=1 time=0ms
GC pool 'ConcurrentMarkSweep' had collection(s): count=2 time=1274ms
2014-06-30 19:53:54,641 INFO  [RpcServer.handler=11,port=60020] regionserver.MemStoreFlusher: Blocking updates on sceplus-vm48.almaden.ibm.com,60020,1404182803191: the global memstore size 386.9m is >= than blocking 386.7m size
2014-06-30 19:53:54,646 INFO  [RpcServer.handler=26,port=60020] regionserver.MemStoreFlusher: Blocking updates on sceplus-vm48.almaden.ibm.com,60020,1404182803191: the global memstore size 386.9m is >= than blocking 386.7m size
2014-06-30 19:53:54,653 INFO  [RpcServer.handler=25,port=60020] regionserver.MemStoreFlusher: Blocking updates on sceplus-vm48.almaden.ibm.com,60020,1404182803191: the global memstore size 386.9m is >= than blocking 386.7m size
2014-06-30 19:53:54,653 INFO  [RpcServer.handler=10,port=60020] regionserver.MemStoreFlusher: Blocking updates on sceplus-vm48.almaden.ibm.com,60020,1404182803191: the global memstore size 386.9m is >= than blocking 386.7m size
2014-06-30 19:53:54,664 INFO  [RpcServer.handler=39,port=60020] regionserver.MemStoreFlusher: Blocking updates on sceplus-vm48.almaden.ibm.com,60020,1404182803191: the global memstore size 386.9m is >= than blocking 386.7m size
2014-06-30 19:53:54,668 INFO  [RpcServer.handler=48,port=60020] regionserver.MemStoreFlusher: Blocking updates on sceplus-vm48.almaden.ibm.com,60020,1404182803191: the global memstore size 388.5m is >= than blocking 386.7m size
2014-06-30 19:53:54,671 INFO  [RpcServer.handler=15,port=60020] regionserver.MemStoreFlusher: Blocking updates on sceplus-vm48.almaden.ibm.com,60020,1404182803191: the global memstore size 388.5m is >= than blocking 386.7m size
2014-06-30 19:53:55,207 INFO  [MemStoreFlusher.1] regionserver.DefaultStoreFlusher: Flushed, sequenceid=172, memsize=68.7m, hasBloomFilter=true, into tmp file hdfs://master:54310/hbase/data/default/usertable/1d66ac045cc0407c0fa426db565fcbba/.tmp/8626d549e56643d58192ebde01f19c1a
2014-06-30 19:53:55,222 DEBUG [MemStoreFlusher.1] regionserver.HRegionFileSystem: Committing store file hdfs://master:54310/hbase/data/default/usertable/1d66ac045cc0407c0fa426db565fcbba/.tmp/8626d549e56643d58192ebde01f19c1a as hdfs://master:54310/hbase/data/default/usertable/1d66ac045cc0407c0fa426db565fcbba/family/8626d549e56643d58192ebde01f19c1a
2014-06-30 19:53:55,232 INFO  [MemStoreFlusher.1] regionserver.HStore: Added hdfs://master:54310/hbase/data/default/usertable/1d66ac045cc0407c0fa426db565fcbba/family/8626d549e56643d58192ebde01f19c1a, entries=250300, sequenceid=172, filesize=37.3m
2014-06-30 19:53:55,232 INFO  [MemStoreFlusher.1] regionserver.HRegion: Finished memstore flush of ~113.0m/118489280, currentsize=1.5m/1575360 for region usertable,user5,1404183160444.1d66ac045cc0407c0fa426db565fcbba. in 1997ms, sequenceid=172, compaction requested=false
2014-06-30 19:53:55,232 WARN  [RpcServer.handler=15,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 561ms
2014-06-30 19:53:55,233 INFO  [RpcServer.handler=15,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server sceplus-vm48.almaden.ibm.com,60020,1404182803191
2014-06-30 19:53:55,233 WARN  [RpcServer.handler=48,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 565ms
2014-06-30 19:53:55,233 INFO  [RpcServer.handler=48,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server sceplus-vm48.almaden.ibm.com,60020,1404182803191
2014-06-30 19:53:55,233 WARN  [RpcServer.handler=39,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 569ms
2014-06-30 19:53:55,233 INFO  [RpcServer.handler=39,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server sceplus-vm48.almaden.ibm.com,60020,1404182803191
2014-06-30 19:53:55,233 WARN  [RpcServer.handler=10,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 580ms
2014-06-30 19:53:55,233 INFO  [RpcServer.handler=10,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server sceplus-vm48.almaden.ibm.com,60020,1404182803191
2014-06-30 19:53:55,233 WARN  [RpcServer.handler=25,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 580ms
2014-06-30 19:53:55,233 INFO  [RpcServer.handler=25,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server sceplus-vm48.almaden.ibm.com,60020,1404182803191
2014-06-30 19:53:55,233 WARN  [RpcServer.handler=26,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 588ms
2014-06-30 19:53:55,234 INFO  [RpcServer.handler=26,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server sceplus-vm48.almaden.ibm.com,60020,1404182803191
2014-06-30 19:53:55,234 WARN  [RpcServer.handler=11,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 594ms
2014-06-30 19:53:55,234 INFO  [RpcServer.handler=11,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server sceplus-vm48.almaden.ibm.com,60020,1404182803191
2014-06-30 19:53:55,243 WARN  [RpcServer.handler=7,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 1890ms
2014-06-30 19:53:55,243 INFO  [RpcServer.handler=7,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server sceplus-vm48.almaden.ibm.com,60020,1404182803191
2014-06-30 19:53:55,339 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-06-30 19:53:55,351 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 808 synced till here 805
2014-06-30 19:53:55,654 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1404182803191/sceplus-vm48.almaden.ibm.com%2C60020%2C1404182803191.1404183229800 with entries=94, filesize=64.1m; new WAL /hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1404182803191/sceplus-vm48.almaden.ibm.com%2C60020%2C1404182803191.1404183235340
2014-06-30 19:53:56,426 DEBUG [RpcServer.handler=4,port=60020] regionserver.HRegion: Flush requested on usertable,user7,1404183160444.994b975d2c4cd5bbe532bddfb5e4ac46.
2014-06-30 19:53:56,426 DEBUG [MemStoreFlusher.0] regionserver.HRegion: Started memstore flush for usertable,user7,1404183160444.994b975d2c4cd5bbe532bddfb5e4ac46., current region memstore size 128.8m
2014-06-30 19:53:56,456 DEBUG [MemStoreFlusher.1] regionserver.MemStoreFlusher: Flush thread woke up because memory above low water=367.3m
2014-06-30 19:53:56,457 INFO  [MemStoreFlusher.1] regionserver.MemStoreFlusher: Flush of region usertable,user2,1404183160444.5446f1f888398e97c0ec3cc30da5bf97. due to global heap pressure
2014-06-30 19:53:56,457 DEBUG [MemStoreFlusher.1] regionserver.HRegion: Started memstore flush for usertable,user2,1404183160444.5446f1f888398e97c0ec3cc30da5bf97., current region memstore size 93.2m
2014-06-30 19:53:56,519 DEBUG [MemStoreFlusher.1] util.FSUtils: DFS Client does not support most favored nodes create; using default create
2014-06-30 19:53:56,558 DEBUG [MemStoreFlusher.0] util.FSUtils: DFS Client does not support most favored nodes create; using default create
2014-06-30 19:53:56,669 INFO  [RpcServer.handler=18,port=60020] regionserver.MemStoreFlusher: Blocking updates on sceplus-vm48.almaden.ibm.com,60020,1404182803191: the global memstore size 387.8m is >= than blocking 386.7m size
2014-06-30 19:53:56,669 INFO  [RpcServer.handler=38,port=60020] regionserver.MemStoreFlusher: Blocking updates on sceplus-vm48.almaden.ibm.com,60020,1404182803191: the global memstore size 387.8m is >= than blocking 386.7m size
2014-06-30 19:53:56,692 INFO  [RpcServer.handler=47,port=60020] regionserver.MemStoreFlusher: Blocking updates on sceplus-vm48.almaden.ibm.com,60020,1404182803191: the global memstore size 387.8m is >= than blocking 386.7m size
2014-06-30 19:53:56,721 INFO  [RpcServer.handler=28,port=60020] regionserver.MemStoreFlusher: Blocking updates on sceplus-vm48.almaden.ibm.com,60020,1404182803191: the global memstore size 387.8m is >= than blocking 386.7m size
2014-06-30 19:53:56,975 INFO  [RpcServer.handler=35,port=60020] regionserver.MemStoreFlusher: Blocking updates on sceplus-vm48.almaden.ibm.com,60020,1404182803191: the global memstore size 387.8m is >= than blocking 386.7m size
2014-06-30 19:53:56,984 INFO  [RpcServer.handler=13,port=60020] regionserver.MemStoreFlusher: Blocking updates on sceplus-vm48.almaden.ibm.com,60020,1404182803191: the global memstore size 387.8m is >= than blocking 386.7m size
2014-06-30 19:53:56,995 INFO  [RpcServer.handler=20,port=60020] regionserver.MemStoreFlusher: Blocking updates on sceplus-vm48.almaden.ibm.com,60020,1404182803191: the global memstore size 387.8m is >= than blocking 386.7m size
2014-06-30 19:53:57,003 INFO  [RpcServer.handler=43,port=60020] regionserver.MemStoreFlusher: Blocking updates on sceplus-vm48.almaden.ibm.com,60020,1404182803191: the global memstore size 387.8m is >= than blocking 386.7m size
2014-06-30 19:53:57,011 INFO  [RpcServer.handler=41,port=60020] regionserver.MemStoreFlusher: Blocking updates on sceplus-vm48.almaden.ibm.com,60020,1404182803191: the global memstore size 387.8m is >= than blocking 386.7m size
2014-06-30 19:53:57,012 INFO  [RpcServer.handler=30,port=60020] regionserver.MemStoreFlusher: Blocking updates on sceplus-vm48.almaden.ibm.com,60020,1404182803191: the global memstore size 387.8m is >= than blocking 386.7m size
2014-06-30 19:53:57,013 INFO  [RpcServer.handler=45,port=60020] regionserver.MemStoreFlusher: Blocking updates on sceplus-vm48.almaden.ibm.com,60020,1404182803191: the global memstore size 387.8m is >= than blocking 386.7m size
2014-06-30 19:53:57,031 INFO  [RpcServer.handler=40,port=60020] regionserver.MemStoreFlusher: Blocking updates on sceplus-vm48.almaden.ibm.com,60020,1404182803191: the global memstore size 387.8m is >= than blocking 386.7m size
2014-06-30 19:53:57,051 INFO  [RpcServer.handler=10,port=60020] regionserver.MemStoreFlusher: Blocking updates on sceplus-vm48.almaden.ibm.com,60020,1404182803191: the global memstore size 387.8m is >= than blocking 386.7m size
2014-06-30 19:53:57,072 INFO  [RpcServer.handler=25,port=60020] regionserver.MemStoreFlusher: Blocking updates on sceplus-vm48.almaden.ibm.com,60020,1404182803191: the global memstore size 387.8m is >= than blocking 386.7m size
2014-06-30 19:53:57,087 INFO  [RpcServer.handler=26,port=60020] regionserver.MemStoreFlusher: Blocking updates on sceplus-vm48.almaden.ibm.com,60020,1404182803191: the global memstore size 387.8m is >= than blocking 386.7m size
2014-06-30 19:53:57,117 INFO  [RpcServer.handler=42,port=60020] regionserver.MemStoreFlusher: Blocking updates on sceplus-vm48.almaden.ibm.com,60020,1404182803191: the global memstore size 387.8m is >= than blocking 386.7m size
2014-06-30 19:53:57,129 INFO  [RpcServer.handler=5,port=60020] regionserver.MemStoreFlusher: Blocking updates on sceplus-vm48.almaden.ibm.com,60020,1404182803191: the global memstore size 387.8m is >= than blocking 386.7m size
2014-06-30 19:53:57,140 INFO  [RpcServer.handler=44,port=60020] regionserver.MemStoreFlusher: Blocking updates on sceplus-vm48.almaden.ibm.com,60020,1404182803191: the global memstore size 387.8m is >= than blocking 386.7m size
2014-06-30 19:53:57,162 INFO  [RpcServer.handler=27,port=60020] regionserver.MemStoreFlusher: Blocking updates on sceplus-vm48.almaden.ibm.com,60020,1404182803191: the global memstore size 387.8m is >= than blocking 386.7m size
2014-06-30 19:53:57,176 INFO  [RpcServer.handler=7,port=60020] regionserver.MemStoreFlusher: Blocking updates on sceplus-vm48.almaden.ibm.com,60020,1404182803191: the global memstore size 387.8m is >= than blocking 386.7m size
2014-06-30 19:53:57,197 INFO  [RpcServer.handler=15,port=60020] regionserver.MemStoreFlusher: Blocking updates on sceplus-vm48.almaden.ibm.com,60020,1404182803191: the global memstore size 387.8m is >= than blocking 386.7m size
2014-06-30 19:53:57,632 INFO  [RpcServer.handler=49,port=60020] regionserver.MemStoreFlusher: Blocking updates on sceplus-vm48.almaden.ibm.com,60020,1404182803191: the global memstore size 387.8m is >= than blocking 386.7m size
2014-06-30 19:53:57,642 INFO  [RpcServer.handler=48,port=60020] regionserver.MemStoreFlusher: Blocking updates on sceplus-vm48.almaden.ibm.com,60020,1404182803191: the global memstore size 387.8m is >= than blocking 386.7m size
2014-06-30 19:53:57,652 INFO  [RpcServer.handler=39,port=60020] regionserver.MemStoreFlusher: Blocking updates on sceplus-vm48.almaden.ibm.com,60020,1404182803191: the global memstore size 387.8m is >= than blocking 386.7m size
2014-06-30 19:53:57,659 INFO  [RpcServer.handler=33,port=60020] regionserver.MemStoreFlusher: Blocking updates on sceplus-vm48.almaden.ibm.com,60020,1404182803191: the global memstore size 387.8m is >= than blocking 386.7m size
2014-06-30 19:53:57,660 INFO  [RpcServer.handler=11,port=60020] regionserver.MemStoreFlusher: Blocking updates on sceplus-vm48.almaden.ibm.com,60020,1404182803191: the global memstore size 387.8m is >= than blocking 386.7m size
2014-06-30 19:53:57,669 INFO  [RpcServer.handler=19,port=60020] regionserver.MemStoreFlusher: Blocking updates on sceplus-vm48.almaden.ibm.com,60020,1404182803191: the global memstore size 387.8m is >= than blocking 386.7m size
2014-06-30 19:53:57,678 INFO  [RpcServer.handler=3,port=60020] regionserver.MemStoreFlusher: Blocking updates on sceplus-vm48.almaden.ibm.com,60020,1404182803191: the global memstore size 387.8m is >= than blocking 386.7m size
2014-06-30 19:53:57,690 INFO  [RpcServer.handler=0,port=60020] regionserver.MemStoreFlusher: Blocking updates on sceplus-vm48.almaden.ibm.com,60020,1404182803191: the global memstore size 387.8m is >= than blocking 386.7m size
2014-06-30 19:53:57,705 INFO  [RpcServer.handler=23,port=60020] regionserver.MemStoreFlusher: Blocking updates on sceplus-vm48.almaden.ibm.com,60020,1404182803191: the global memstore size 387.8m is >= than blocking 386.7m size
2014-06-30 19:53:57,720 INFO  [RpcServer.handler=37,port=60020] regionserver.MemStoreFlusher: Blocking updates on sceplus-vm48.almaden.ibm.com,60020,1404182803191: the global memstore size 387.8m is >= than blocking 386.7m size
2014-06-30 19:53:57,738 INFO  [RpcServer.handler=8,port=60020] regionserver.MemStoreFlusher: Blocking updates on sceplus-vm48.almaden.ibm.com,60020,1404182803191: the global memstore size 387.8m is >= than blocking 386.7m size
2014-06-30 19:53:57,755 INFO  [RpcServer.handler=16,port=60020] regionserver.MemStoreFlusher: Blocking updates on sceplus-vm48.almaden.ibm.com,60020,1404182803191: the global memstore size 387.8m is >= than blocking 386.7m size
2014-06-30 19:53:57,775 INFO  [RpcServer.handler=17,port=60020] regionserver.MemStoreFlusher: Blocking updates on sceplus-vm48.almaden.ibm.com,60020,1404182803191: the global memstore size 387.8m is >= than blocking 386.7m size
2014-06-30 19:53:57,793 INFO  [RpcServer.handler=21,port=60020] regionserver.MemStoreFlusher: Blocking updates on sceplus-vm48.almaden.ibm.com,60020,1404182803191: the global memstore size 387.8m is >= than blocking 386.7m size
2014-06-30 19:53:57,812 INFO  [RpcServer.handler=1,port=60020] regionserver.MemStoreFlusher: Blocking updates on sceplus-vm48.almaden.ibm.com,60020,1404182803191: the global memstore size 387.8m is >= than blocking 386.7m size
2014-06-30 19:53:57,828 INFO  [RpcServer.handler=14,port=60020] regionserver.MemStoreFlusher: Blocking updates on sceplus-vm48.almaden.ibm.com,60020,1404182803191: the global memstore size 387.8m is >= than blocking 386.7m size
2014-06-30 19:53:57,876 INFO  [RpcServer.handler=34,port=60020] regionserver.MemStoreFlusher: Blocking updates on sceplus-vm48.almaden.ibm.com,60020,1404182803191: the global memstore size 387.8m is >= than blocking 386.7m size
2014-06-30 19:53:59,843 INFO  [JvmPauseMonitor] util.JvmPauseMonitor: Detected pause in JVM or host machine (eg GC): pause of approximately 1697ms
GC pool 'ParNew' had collection(s): count=1 time=0ms
GC pool 'ConcurrentMarkSweep' had collection(s): count=2 time=1981ms
2014-06-30 19:53:59,844 INFO  [RpcServer.handler=24,port=60020] regionserver.MemStoreFlusher: Blocking updates on sceplus-vm48.almaden.ibm.com,60020,1404182803191: the global memstore size 387.8m is >= than blocking 386.7m size
2014-06-30 19:53:59,847 INFO  [MemStoreFlusher.1] regionserver.DefaultStoreFlusher: Flushed, sequenceid=188, memsize=85.5m, hasBloomFilter=true, into tmp file hdfs://master:54310/hbase/data/default/usertable/5446f1f888398e97c0ec3cc30da5bf97/.tmp/55017c5e21444bfc9ce99262f4af9d95
2014-06-30 19:53:59,858 INFO  [RpcServer.handler=9,port=60020] regionserver.MemStoreFlusher: Blocking updates on sceplus-vm48.almaden.ibm.com,60020,1404182803191: the global memstore size 387.8m is >= than blocking 386.7m size
2014-06-30 19:53:59,859 DEBUG [MemStoreFlusher.1] regionserver.HRegionFileSystem: Committing store file hdfs://master:54310/hbase/data/default/usertable/5446f1f888398e97c0ec3cc30da5bf97/.tmp/55017c5e21444bfc9ce99262f4af9d95 as hdfs://master:54310/hbase/data/default/usertable/5446f1f888398e97c0ec3cc30da5bf97/family/55017c5e21444bfc9ce99262f4af9d95
2014-06-30 19:53:59,868 INFO  [RpcServer.handler=31,port=60020] regionserver.MemStoreFlusher: Blocking updates on sceplus-vm48.almaden.ibm.com,60020,1404182803191: the global memstore size 387.8m is >= than blocking 386.7m size
2014-06-30 19:53:59,869 INFO  [MemStoreFlusher.1] regionserver.HStore: Added hdfs://master:54310/hbase/data/default/usertable/5446f1f888398e97c0ec3cc30da5bf97/family/55017c5e21444bfc9ce99262f4af9d95, entries=311390, sequenceid=188, filesize=46.4m
2014-06-30 19:53:59,869 INFO  [MemStoreFlusher.1] regionserver.HRegion: Finished memstore flush of ~93.2m/97727360, currentsize=3.2m/3360880 for region usertable,user2,1404183160444.5446f1f888398e97c0ec3cc30da5bf97. in 3412ms, sequenceid=188, compaction requested=true
2014-06-30 19:53:59,870 DEBUG [MemStoreFlusher.1] regionserver.CompactSplitThread: Small Compaction requested: system; Because: MemStoreFlusher.1; compaction_queue=(0:0), split_queue=0, merge_queue=0
2014-06-30 19:53:59,870 DEBUG [regionserver60020-smallCompactions-1404182848217] compactions.RatioBasedCompactionPolicy: Selecting compaction from 3 store files, 0 compacting, 3 eligible, 10 blocking
2014-06-30 19:53:59,870 WARN  [RpcServer.handler=31,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 2ms
2014-06-30 19:53:59,870 INFO  [RpcServer.handler=31,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server sceplus-vm48.almaden.ibm.com,60020,1404182803191
2014-06-30 19:53:59,870 DEBUG [regionserver60020-smallCompactions-1404182848217] compactions.ExploringCompactionPolicy: Exploring compaction algorithm has selected 3 files of size 119101535 starting at candidate #0 after considering 1 permutations with 1 in ratio
2014-06-30 19:53:59,870 WARN  [RpcServer.handler=9,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 12ms
2014-06-30 19:53:59,870 INFO  [RpcServer.handler=9,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server sceplus-vm48.almaden.ibm.com,60020,1404182803191
2014-06-30 19:53:59,870 WARN  [RpcServer.handler=24,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 26ms
2014-06-30 19:53:59,870 DEBUG [regionserver60020-smallCompactions-1404182848217] regionserver.HStore: 5446f1f888398e97c0ec3cc30da5bf97 - family: Initiating major compaction
2014-06-30 19:53:59,870 INFO  [RpcServer.handler=24,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server sceplus-vm48.almaden.ibm.com,60020,1404182803191
2014-06-30 19:53:59,871 WARN  [RpcServer.handler=34,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 1995ms
2014-06-30 19:53:59,871 INFO  [regionserver60020-smallCompactions-1404182848217] regionserver.HRegion: Starting compaction on family in region usertable,user2,1404183160444.5446f1f888398e97c0ec3cc30da5bf97.
2014-06-30 19:53:59,871 INFO  [RpcServer.handler=34,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server sceplus-vm48.almaden.ibm.com,60020,1404182803191
2014-06-30 19:53:59,871 INFO  [regionserver60020-smallCompactions-1404182848217] regionserver.HStore: Starting compaction of 3 file(s) in family of usertable,user2,1404183160444.5446f1f888398e97c0ec3cc30da5bf97. into tmpdir=hdfs://master:54310/hbase/data/default/usertable/5446f1f888398e97c0ec3cc30da5bf97/.tmp, totalSize=113.6m
2014-06-30 19:53:59,872 WARN  [RpcServer.handler=14,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 2044ms
2014-06-30 19:53:59,872 INFO  [RpcServer.handler=14,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server sceplus-vm48.almaden.ibm.com,60020,1404182803191
2014-06-30 19:53:59,873 DEBUG [regionserver60020-smallCompactions-1404182848217] compactions.Compactor: Compacting hdfs://master:54310/hbase/data/default/usertable/5446f1f888398e97c0ec3cc30da5bf97/family/28c2e48911c740559fb5ee390001c1a2, keycount=20144, bloomtype=ROW, size=30.0m, encoding=NONE, seqNum=62, earliestPutTs=1404183176708
2014-06-30 19:53:59,873 DEBUG [regionserver60020-smallCompactions-1404182848217] compactions.Compactor: Compacting hdfs://master:54310/hbase/data/default/usertable/5446f1f888398e97c0ec3cc30da5bf97/family/bafd4f0e46a04e2e8046881dd8521fb4, keycount=24904, bloomtype=ROW, size=37.1m, encoding=NONE, seqNum=126, earliestPutTs=1404183201140
2014-06-30 19:53:59,873 DEBUG [regionserver60020-smallCompactions-1404182848217] compactions.Compactor: Compacting hdfs://master:54310/hbase/data/default/usertable/5446f1f888398e97c0ec3cc30da5bf97/family/55017c5e21444bfc9ce99262f4af9d95, keycount=31139, bloomtype=ROW, size=46.4m, encoding=NONE, seqNum=188, earliestPutTs=1404183222023
2014-06-30 19:53:59,873 WARN  [RpcServer.handler=1,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 2060ms
2014-06-30 19:53:59,873 INFO  [RpcServer.handler=1,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server sceplus-vm48.almaden.ibm.com,60020,1404182803191
2014-06-30 19:53:59,873 WARN  [RpcServer.handler=21,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 2080ms
2014-06-30 19:53:59,873 INFO  [RpcServer.handler=21,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server sceplus-vm48.almaden.ibm.com,60020,1404182803191
2014-06-30 19:53:59,874 WARN  [RpcServer.handler=17,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 2098ms
2014-06-30 19:53:59,874 INFO  [RpcServer.handler=17,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server sceplus-vm48.almaden.ibm.com,60020,1404182803191
2014-06-30 19:53:59,874 WARN  [RpcServer.handler=16,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 2119ms
2014-06-30 19:53:59,874 INFO  [RpcServer.handler=16,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server sceplus-vm48.almaden.ibm.com,60020,1404182803191
2014-06-30 19:53:59,875 WARN  [RpcServer.handler=8,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 2137ms
2014-06-30 19:53:59,875 INFO  [RpcServer.handler=8,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server sceplus-vm48.almaden.ibm.com,60020,1404182803191
2014-06-30 19:53:59,875 WARN  [RpcServer.handler=37,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 2155ms
2014-06-30 19:53:59,875 INFO  [RpcServer.handler=37,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server sceplus-vm48.almaden.ibm.com,60020,1404182803191
2014-06-30 19:53:59,875 WARN  [RpcServer.handler=23,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 2170ms
2014-06-30 19:53:59,876 INFO  [RpcServer.handler=23,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server sceplus-vm48.almaden.ibm.com,60020,1404182803191
2014-06-30 19:53:59,877 WARN  [RpcServer.handler=0,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 2186ms
2014-06-30 19:53:59,877 INFO  [RpcServer.handler=0,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server sceplus-vm48.almaden.ibm.com,60020,1404182803191
2014-06-30 19:53:59,878 WARN  [RpcServer.handler=3,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 2200ms
2014-06-30 19:53:59,879 INFO  [RpcServer.handler=3,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server sceplus-vm48.almaden.ibm.com,60020,1404182803191
2014-06-30 19:53:59,879 WARN  [RpcServer.handler=19,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 2210ms
2014-06-30 19:53:59,879 INFO  [RpcServer.handler=19,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server sceplus-vm48.almaden.ibm.com,60020,1404182803191
2014-06-30 19:53:59,881 WARN  [RpcServer.handler=11,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 2221ms
2014-06-30 19:53:59,881 INFO  [RpcServer.handler=11,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server sceplus-vm48.almaden.ibm.com,60020,1404182803191
2014-06-30 19:53:59,881 WARN  [RpcServer.handler=33,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 2222ms
2014-06-30 19:53:59,917 INFO  [RpcServer.handler=33,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server sceplus-vm48.almaden.ibm.com,60020,1404182803191
2014-06-30 19:53:59,917 WARN  [RpcServer.handler=39,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 2265ms
2014-06-30 19:53:59,917 INFO  [RpcServer.handler=39,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server sceplus-vm48.almaden.ibm.com,60020,1404182803191
2014-06-30 19:53:59,918 WARN  [RpcServer.handler=48,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 2275ms
2014-06-30 19:53:59,918 INFO  [RpcServer.handler=48,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server sceplus-vm48.almaden.ibm.com,60020,1404182803191
2014-06-30 19:53:59,918 WARN  [RpcServer.handler=49,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 2286ms
2014-06-30 19:53:59,918 INFO  [RpcServer.handler=49,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server sceplus-vm48.almaden.ibm.com,60020,1404182803191
2014-06-30 19:53:59,919 WARN  [RpcServer.handler=15,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 2721ms
2014-06-30 19:53:59,919 INFO  [RpcServer.handler=15,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server sceplus-vm48.almaden.ibm.com,60020,1404182803191
2014-06-30 19:53:59,923 WARN  [RpcServer.handler=7,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 2747ms
2014-06-30 19:53:59,923 INFO  [RpcServer.handler=7,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server sceplus-vm48.almaden.ibm.com,60020,1404182803191
2014-06-30 19:53:59,923 WARN  [RpcServer.handler=27,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 2761ms
2014-06-30 19:53:59,923 INFO  [RpcServer.handler=27,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server sceplus-vm48.almaden.ibm.com,60020,1404182803191
2014-06-30 19:53:59,923 WARN  [RpcServer.handler=44,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 2783ms
2014-06-30 19:53:59,923 INFO  [RpcServer.handler=44,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server sceplus-vm48.almaden.ibm.com,60020,1404182803191
2014-06-30 19:53:59,925 WARN  [RpcServer.handler=5,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 2796ms
2014-06-30 19:53:59,925 INFO  [RpcServer.handler=5,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server sceplus-vm48.almaden.ibm.com,60020,1404182803191
2014-06-30 19:53:59,926 WARN  [RpcServer.handler=42,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 2809ms
2014-06-30 19:53:59,926 INFO  [RpcServer.handler=42,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server sceplus-vm48.almaden.ibm.com,60020,1404182803191
2014-06-30 19:53:59,928 WARN  [RpcServer.handler=26,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 2839ms
2014-06-30 19:53:59,928 INFO  [RpcServer.handler=26,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server sceplus-vm48.almaden.ibm.com,60020,1404182803191
2014-06-30 19:53:59,930 WARN  [RpcServer.handler=25,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 2858ms
2014-06-30 19:53:59,930 INFO  [RpcServer.handler=25,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server sceplus-vm48.almaden.ibm.com,60020,1404182803191
2014-06-30 19:53:59,931 WARN  [RpcServer.handler=10,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 2880ms
2014-06-30 19:53:59,931 INFO  [RpcServer.handler=10,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server sceplus-vm48.almaden.ibm.com,60020,1404182803191
2014-06-30 19:53:59,932 WARN  [RpcServer.handler=40,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 2902ms
2014-06-30 19:53:59,933 INFO  [RpcServer.handler=40,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server sceplus-vm48.almaden.ibm.com,60020,1404182803191
2014-06-30 19:53:59,936 WARN  [RpcServer.handler=45,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 2923ms
2014-06-30 19:53:59,936 INFO  [RpcServer.handler=45,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server sceplus-vm48.almaden.ibm.com,60020,1404182803191
2014-06-30 19:53:59,936 WARN  [RpcServer.handler=30,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 2924ms
2014-06-30 19:53:59,936 INFO  [RpcServer.handler=30,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server sceplus-vm48.almaden.ibm.com,60020,1404182803191
2014-06-30 19:53:59,937 WARN  [RpcServer.handler=41,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 2925ms
2014-06-30 19:53:59,937 INFO  [RpcServer.handler=41,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server sceplus-vm48.almaden.ibm.com,60020,1404182803191
2014-06-30 19:53:59,941 WARN  [RpcServer.handler=43,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 2938ms
2014-06-30 19:53:59,941 INFO  [RpcServer.handler=43,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server sceplus-vm48.almaden.ibm.com,60020,1404182803191
2014-06-30 19:53:59,944 WARN  [RpcServer.handler=20,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 2947ms
2014-06-30 19:53:59,944 INFO  [RpcServer.handler=20,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server sceplus-vm48.almaden.ibm.com,60020,1404182803191
2014-06-30 19:53:59,945 WARN  [RpcServer.handler=13,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 2961ms
2014-06-30 19:53:59,945 INFO  [RpcServer.handler=13,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server sceplus-vm48.almaden.ibm.com,60020,1404182803191
2014-06-30 19:53:59,946 WARN  [RpcServer.handler=35,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 2971ms
2014-06-30 19:53:59,946 INFO  [RpcServer.handler=35,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server sceplus-vm48.almaden.ibm.com,60020,1404182803191
2014-06-30 19:53:59,947 WARN  [RpcServer.handler=28,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 3226ms
2014-06-30 19:53:59,947 INFO  [RpcServer.handler=28,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server sceplus-vm48.almaden.ibm.com,60020,1404182803191
2014-06-30 19:53:59,950 WARN  [RpcServer.handler=47,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 3258ms
2014-06-30 19:53:59,950 INFO  [RpcServer.handler=47,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server sceplus-vm48.almaden.ibm.com,60020,1404182803191
2014-06-30 19:53:59,951 WARN  [RpcServer.handler=38,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 3282ms
2014-06-30 19:53:59,951 INFO  [RpcServer.handler=38,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server sceplus-vm48.almaden.ibm.com,60020,1404182803191
2014-06-30 19:53:59,956 WARN  [RpcServer.handler=18,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 3288ms
2014-06-30 19:53:59,956 INFO  [RpcServer.handler=18,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server sceplus-vm48.almaden.ibm.com,60020,1404182803191
2014-06-30 19:53:59,958 DEBUG [regionserver60020-smallCompactions-1404182848217] util.FSUtils: DFS Client does not support most favored nodes create; using default create
2014-06-30 19:54:02,122 INFO  [JvmPauseMonitor] util.JvmPauseMonitor: Detected pause in JVM or host machine (eg GC): pause of approximately 1778ms
GC pool 'ParNew' had collection(s): count=1 time=0ms
GC pool 'ConcurrentMarkSweep' had collection(s): count=1 time=1997ms
2014-06-30 19:54:02,194 INFO  [MemStoreFlusher.0] regionserver.DefaultStoreFlusher: Flushed, sequenceid=186, memsize=84.3m, hasBloomFilter=true, into tmp file hdfs://master:54310/hbase/data/default/usertable/994b975d2c4cd5bbe532bddfb5e4ac46/.tmp/e832b9bfa4d44567b7d5ce78013c9e6d
2014-06-30 19:54:02,204 DEBUG [MemStoreFlusher.0] regionserver.HRegionFileSystem: Committing store file hdfs://master:54310/hbase/data/default/usertable/994b975d2c4cd5bbe532bddfb5e4ac46/.tmp/e832b9bfa4d44567b7d5ce78013c9e6d as hdfs://master:54310/hbase/data/default/usertable/994b975d2c4cd5bbe532bddfb5e4ac46/family/e832b9bfa4d44567b7d5ce78013c9e6d
2014-06-30 19:54:02,214 INFO  [MemStoreFlusher.0] regionserver.HStore: Added hdfs://master:54310/hbase/data/default/usertable/994b975d2c4cd5bbe532bddfb5e4ac46/family/e832b9bfa4d44567b7d5ce78013c9e6d, entries=306830, sequenceid=186, filesize=45.8m
2014-06-30 19:54:02,214 INFO  [MemStoreFlusher.0] regionserver.HRegion: Finished memstore flush of ~130.4m/136704880, currentsize=4.8m/5080160 for region usertable,user7,1404183160444.994b975d2c4cd5bbe532bddfb5e4ac46. in 5788ms, sequenceid=186, compaction requested=false
2014-06-30 19:54:02,233 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-06-30 19:54:02,242 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 936 synced till here 914
2014-06-30 19:54:02,417 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1404182803191/sceplus-vm48.almaden.ibm.com%2C60020%2C1404182803191.1404183235340 with entries=128, filesize=89.9m; new WAL /hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1404182803191/sceplus-vm48.almaden.ibm.com%2C60020%2C1404182803191.1404183242233
2014-06-30 19:54:04,362 INFO  [JvmPauseMonitor] util.JvmPauseMonitor: Detected pause in JVM or host machine (eg GC): pause of approximately 1739ms
GC pool 'ParNew' had collection(s): count=1 time=0ms
GC pool 'ConcurrentMarkSweep' had collection(s): count=1 time=1930ms
2014-06-30 19:54:06,395 INFO  [JvmPauseMonitor] util.JvmPauseMonitor: Detected pause in JVM or host machine (eg GC): pause of approximately 1494ms
GC pool 'ParNew' had collection(s): count=1 time=0ms
GC pool 'ConcurrentMarkSweep' had collection(s): count=1 time=1769ms
2014-06-30 19:54:09,698 INFO  [JvmPauseMonitor] util.JvmPauseMonitor: Detected pause in JVM or host machine (eg GC): pause of approximately 2803ms
GC pool 'ConcurrentMarkSweep' had collection(s): count=1 time=2959ms
2014-06-30 19:54:09,867 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-06-30 19:54:09,883 WARN  [RpcServer.handler=31,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":10006,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.58:49906","starttimems":1404183239866,"queuetimems":1,"class":"HRegionServer","responsesize":16308,"method":"Multi"}
2014-06-30 19:54:09,904 WARN  [RpcServer.handler=7,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":12729,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.58:49906","starttimems":1404183237174,"queuetimems":1,"class":"HRegionServer","responsesize":15078,"method":"Multi"}
2014-06-30 19:54:09,919 WARN  [RpcServer.handler=16,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":12166,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.58:49906","starttimems":1404183237753,"queuetimems":1,"class":"HRegionServer","responsesize":15614,"method":"Multi"}
2014-06-30 19:54:12,405 INFO  [JvmPauseMonitor] util.JvmPauseMonitor: Detected pause in JVM or host machine (eg GC): pause of approximately 2205ms
GC pool 'ConcurrentMarkSweep' had collection(s): count=1 time=2387ms
2014-06-30 19:54:12,420 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 1019 synced till here 1014
2014-06-30 19:54:12,648 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1404182803191/sceplus-vm48.almaden.ibm.com%2C60020%2C1404182803191.1404183242233 with entries=83, filesize=69.9m; new WAL /hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1404182803191/sceplus-vm48.almaden.ibm.com%2C60020%2C1404182803191.1404183249883
2014-06-30 19:54:15,524 INFO  [JvmPauseMonitor] util.JvmPauseMonitor: Detected pause in JVM or host machine (eg GC): pause of approximately 2618ms
GC pool 'ConcurrentMarkSweep' had collection(s): count=1 time=2863ms
2014-06-30 19:54:15,633 WARN  [RpcServer.handler=14,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":17808,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.58:49906","starttimems":1404183237825,"queuetimems":0,"class":"HRegionServer","responsesize":14654,"method":"Multi"}
2014-06-30 19:54:15,634 WARN  [RpcServer.handler=34,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":17762,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.58:49906","starttimems":1404183237871,"queuetimems":3,"class":"HRegionServer","responsesize":15737,"method":"Multi"}
2014-06-30 19:54:15,634 WARN  [RpcServer.handler=14,port=60020] ipc.RpcServer: RpcServer.respondercallId: 672 service: ClientService methodName: Multi size: 2.6m connection: 9.1.143.58:49906: output error
2014-06-30 19:54:15,635 WARN  [RpcServer.handler=14,port=60020] ipc.RpcServer: RpcServer.handler=14,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-06-30 19:54:15,822 WARN  [RpcServer.handler=34,port=60020] ipc.RpcServer: RpcServer.respondercallId: 671 service: ClientService methodName: Multi size: 2.8m connection: 9.1.143.58:49906: output error
2014-06-30 19:54:15,822 WARN  [RpcServer.handler=34,port=60020] ipc.RpcServer: RpcServer.handler=34,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-06-30 19:54:18,178 INFO  [JvmPauseMonitor] util.JvmPauseMonitor: Detected pause in JVM or host machine (eg GC): pause of approximately 2153ms
GC pool 'ConcurrentMarkSweep' had collection(s): count=1 time=2408ms
2014-06-30 19:54:18,202 WARN  [RpcServer.handler=8,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":20465,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.58:49906","starttimems":1404183237736,"queuetimems":0,"class":"HRegionServer","responsesize":15616,"method":"Multi"}
2014-06-30 19:54:18,202 WARN  [RpcServer.handler=8,port=60020] ipc.RpcServer: RpcServer.respondercallId: 677 service: ClientService methodName: Multi size: 2.7m connection: 9.1.143.58:49906: output error
2014-06-30 19:54:18,202 WARN  [RpcServer.handler=8,port=60020] ipc.RpcServer: RpcServer.handler=8,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-06-30 19:54:18,202 WARN  [RpcServer.handler=26,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":21115,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.58:49906","starttimems":1404183237086,"queuetimems":0,"class":"HRegionServer","responsesize":15056,"method":"Multi"}
2014-06-30 19:54:18,203 WARN  [RpcServer.handler=26,port=60020] ipc.RpcServer: RpcServer.respondercallId: 695 service: ClientService methodName: Multi size: 2.6m connection: 9.1.143.58:49906: output error
2014-06-30 19:54:18,203 WARN  [RpcServer.handler=26,port=60020] ipc.RpcServer: RpcServer.handler=26,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-06-30 19:54:18,206 WARN  [RpcServer.handler=0,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":20519,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.58:49906","starttimems":1404183237686,"queuetimems":1,"class":"HRegionServer","responsesize":15915,"method":"Multi"}
2014-06-30 19:54:18,206 WARN  [RpcServer.handler=0,port=60020] ipc.RpcServer: RpcServer.respondercallId: 680 service: ClientService methodName: Multi size: 2.8m connection: 9.1.143.58:49906: output error
2014-06-30 19:54:18,206 WARN  [RpcServer.handler=0,port=60020] ipc.RpcServer: RpcServer.handler=0,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-06-30 19:54:18,206 WARN  [RpcServer.handler=20,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":21215,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.58:49906","starttimems":1404183236991,"queuetimems":0,"class":"HRegionServer","responsesize":15114,"method":"Multi"}
2014-06-30 19:54:18,207 WARN  [RpcServer.handler=20,port=60020] ipc.RpcServer: RpcServer.respondercallId: 665 service: ClientService methodName: Multi size: 2.6m connection: 9.1.143.58:49906: output error
2014-06-30 19:54:18,207 WARN  [RpcServer.handler=20,port=60020] ipc.RpcServer: RpcServer.handler=20,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-06-30 19:54:18,202 WARN  [RpcServer.handler=29,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":18283,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.58:49906","starttimems":1404183239918,"queuetimems":1,"class":"HRegionServer","responsesize":16418,"method":"Multi"}
2014-06-30 19:54:18,207 DEBUG [MemStoreFlusher.1] regionserver.MemStoreFlusher: Flush thread woke up because memory above low water=367.3m
2014-06-30 19:54:18,206 WARN  [RpcServer.handler=3,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":20529,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.58:49906","starttimems":1404183237676,"queuetimems":0,"class":"HRegionServer","responsesize":15737,"method":"Multi"}
2014-06-30 19:54:18,209 INFO  [MemStoreFlusher.1] regionserver.MemStoreFlusher: Flush of region usertable,user5,1404183160444.1d66ac045cc0407c0fa426db565fcbba. due to global heap pressure
2014-06-30 19:54:18,209 WARN  [RpcServer.handler=29,port=60020] ipc.RpcServer: RpcServer.respondercallId: 703 service: ClientService methodName: Multi size: 2.9m connection: 9.1.143.58:49906: output error
2014-06-30 19:54:18,209 WARN  [RpcServer.handler=29,port=60020] ipc.RpcServer: RpcServer.handler=29,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-06-30 19:54:18,209 WARN  [RpcServer.handler=3,port=60020] ipc.RpcServer: RpcServer.respondercallId: 681 service: ClientService methodName: Multi size: 2.8m connection: 9.1.143.58:49906: output error
2014-06-30 19:54:18,209 DEBUG [MemStoreFlusher.1] regionserver.HRegion: Started memstore flush for usertable,user5,1404183160444.1d66ac045cc0407c0fa426db565fcbba., current region memstore size 106.1m
2014-06-30 19:54:18,209 WARN  [RpcServer.handler=3,port=60020] ipc.RpcServer: RpcServer.handler=3,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-06-30 19:54:18,371 WARN  [RpcServer.handler=16,port=60020] ipc.RpcServer: RpcServer.respondercallId: 743 service: ClientService methodName: Multi size: 204.7k connection: 9.1.143.58:49906: output error
2014-06-30 19:54:18,371 WARN  [RpcServer.handler=16,port=60020] ipc.RpcServer: RpcServer.handler=16,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-06-30 19:54:18,371 WARN  [RpcServer.handler=7,port=60020] ipc.RpcServer: RpcServer.respondercallId: 740 service: ClientService methodName: Multi size: 204.7k connection: 9.1.143.58:49906: output error
2014-06-30 19:54:18,371 WARN  [RpcServer.handler=7,port=60020] ipc.RpcServer: RpcServer.handler=7,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-06-30 19:54:18,376 WARN  [RpcServer.handler=13,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":21394,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.58:49906","starttimems":1404183236982,"queuetimems":0,"class":"HRegionServer","responsesize":17196,"method":"Multi"}
2014-06-30 19:54:18,377 WARN  [RpcServer.handler=13,port=60020] ipc.RpcServer: RpcServer.respondercallId: 694 service: ClientService methodName: Multi size: 3.0m connection: 9.1.143.58:49906: output error
2014-06-30 19:54:18,377 WARN  [RpcServer.handler=13,port=60020] ipc.RpcServer: RpcServer.handler=13,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-06-30 19:54:18,387 WARN  [RpcServer.handler=48,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":20730,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.58:49906","starttimems":1404183237640,"queuetimems":1,"class":"HRegionServer","responsesize":17088,"method":"Multi"}
2014-06-30 19:54:18,393 WARN  [RpcServer.handler=48,port=60020] ipc.RpcServer: RpcServer.respondercallId: 686 service: ClientService methodName: Multi size: 3.0m connection: 9.1.143.58:49906: output error
2014-06-30 19:54:18,393 WARN  [RpcServer.handler=48,port=60020] ipc.RpcServer: RpcServer.handler=48,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-06-30 19:54:18,462 DEBUG [MemStoreFlusher.1] util.FSUtils: DFS Client does not support most favored nodes create; using default create
2014-06-30 19:54:18,689 INFO  [RpcServer.handler=27,port=60020] regionserver.MemStoreFlusher: Blocking updates on sceplus-vm48.almaden.ibm.com,60020,1404182803191: the global memstore size 436.0m is >= than blocking 386.7m size
2014-06-30 19:54:20,932 INFO  [JvmPauseMonitor] util.JvmPauseMonitor: Detected pause in JVM or host machine (eg GC): pause of approximately 1753ms
GC pool 'ConcurrentMarkSweep' had collection(s): count=1 time=2218ms
2014-06-30 19:54:20,933 INFO  [RpcServer.handler=43,port=60020] regionserver.MemStoreFlusher: Blocking updates on sceplus-vm48.almaden.ibm.com,60020,1404182803191: the global memstore size 436.0m is >= than blocking 386.7m size
2014-06-30 19:54:20,943 INFO  [RpcServer.handler=35,port=60020] regionserver.MemStoreFlusher: Blocking updates on sceplus-vm48.almaden.ibm.com,60020,1404182803191: the global memstore size 436.0m is >= than blocking 386.7m size
2014-06-30 19:54:20,944 INFO  [RpcServer.handler=32,port=60020] regionserver.MemStoreFlusher: Blocking updates on sceplus-vm48.almaden.ibm.com,60020,1404182803191: the global memstore size 436.0m is >= than blocking 386.7m size
2014-06-30 19:54:20,944 INFO  [RpcServer.handler=40,port=60020] regionserver.MemStoreFlusher: Blocking updates on sceplus-vm48.almaden.ibm.com,60020,1404182803191: the global memstore size 436.0m is >= than blocking 386.7m size
2014-06-30 19:54:20,944 INFO  [RpcServer.handler=1,port=60020] regionserver.MemStoreFlusher: Blocking updates on sceplus-vm48.almaden.ibm.com,60020,1404182803191: the global memstore size 436.0m is >= than blocking 386.7m size
2014-06-30 19:54:20,945 INFO  [RpcServer.handler=25,port=60020] regionserver.MemStoreFlusher: Blocking updates on sceplus-vm48.almaden.ibm.com,60020,1404182803191: the global memstore size 436.0m is >= than blocking 386.7m size
2014-06-30 19:54:20,945 INFO  [RpcServer.handler=36,port=60020] regionserver.MemStoreFlusher: Blocking updates on sceplus-vm48.almaden.ibm.com,60020,1404182803191: the global memstore size 436.0m is >= than blocking 386.7m size
2014-06-30 19:54:20,945 INFO  [RpcServer.handler=38,port=60020] regionserver.MemStoreFlusher: Blocking updates on sceplus-vm48.almaden.ibm.com,60020,1404182803191: the global memstore size 436.0m is >= than blocking 386.7m size
2014-06-30 19:54:20,945 INFO  [RpcServer.handler=28,port=60020] regionserver.MemStoreFlusher: Blocking updates on sceplus-vm48.almaden.ibm.com,60020,1404182803191: the global memstore size 436.0m is >= than blocking 386.7m size
2014-06-30 19:54:20,945 INFO  [RpcServer.handler=23,port=60020] regionserver.MemStoreFlusher: Blocking updates on sceplus-vm48.almaden.ibm.com,60020,1404182803191: the global memstore size 436.0m is >= than blocking 386.7m size
2014-06-30 19:54:20,946 INFO  [RpcServer.handler=21,port=60020] regionserver.MemStoreFlusher: Blocking updates on sceplus-vm48.almaden.ibm.com,60020,1404182803191: the global memstore size 436.0m is >= than blocking 386.7m size
2014-06-30 19:54:20,946 INFO  [RpcServer.handler=14,port=60020] regionserver.MemStoreFlusher: Blocking updates on sceplus-vm48.almaden.ibm.com,60020,1404182803191: the global memstore size 436.0m is >= than blocking 386.7m size
2014-06-30 19:54:20,946 INFO  [RpcServer.handler=19,port=60020] regionserver.MemStoreFlusher: Blocking updates on sceplus-vm48.almaden.ibm.com,60020,1404182803191: the global memstore size 436.0m is >= than blocking 386.7m size
2014-06-30 19:54:20,946 INFO  [RpcServer.handler=11,port=60020] regionserver.MemStoreFlusher: Blocking updates on sceplus-vm48.almaden.ibm.com,60020,1404182803191: the global memstore size 436.0m is >= than blocking 386.7m size
2014-06-30 19:54:20,947 INFO  [RpcServer.handler=22,port=60020] regionserver.MemStoreFlusher: Blocking updates on sceplus-vm48.almaden.ibm.com,60020,1404182803191: the global memstore size 436.0m is >= than blocking 386.7m size
2014-06-30 19:54:20,947 INFO  [RpcServer.handler=34,port=60020] regionserver.MemStoreFlusher: Blocking updates on sceplus-vm48.almaden.ibm.com,60020,1404182803191: the global memstore size 436.0m is >= than blocking 386.7m size
2014-06-30 19:54:20,947 INFO  [RpcServer.handler=15,port=60020] regionserver.MemStoreFlusher: Blocking updates on sceplus-vm48.almaden.ibm.com,60020,1404182803191: the global memstore size 436.0m is >= than blocking 386.7m size
2014-06-30 19:54:20,947 INFO  [RpcServer.handler=42,port=60020] regionserver.MemStoreFlusher: Blocking updates on sceplus-vm48.almaden.ibm.com,60020,1404182803191: the global memstore size 436.0m is >= than blocking 386.7m size
2014-06-30 19:54:20,947 INFO  [RpcServer.handler=9,port=60020] regionserver.MemStoreFlusher: Blocking updates on sceplus-vm48.almaden.ibm.com,60020,1404182803191: the global memstore size 436.0m is >= than blocking 386.7m size
2014-06-30 19:54:20,948 INFO  [RpcServer.handler=17,port=60020] regionserver.MemStoreFlusher: Blocking updates on sceplus-vm48.almaden.ibm.com,60020,1404182803191: the global memstore size 436.0m is >= than blocking 386.7m size
2014-06-30 19:54:20,948 INFO  [RpcServer.handler=46,port=60020] regionserver.MemStoreFlusher: Blocking updates on sceplus-vm48.almaden.ibm.com,60020,1404182803191: the global memstore size 436.0m is >= than blocking 386.7m size
2014-06-30 19:54:20,948 INFO  [RpcServer.handler=45,port=60020] regionserver.MemStoreFlusher: Blocking updates on sceplus-vm48.almaden.ibm.com,60020,1404182803191: the global memstore size 436.0m is >= than blocking 386.7m size
2014-06-30 19:54:20,948 INFO  [RpcServer.handler=39,port=60020] regionserver.MemStoreFlusher: Blocking updates on sceplus-vm48.almaden.ibm.com,60020,1404182803191: the global memstore size 436.0m is >= than blocking 386.7m size
2014-06-30 19:54:20,948 INFO  [RpcServer.handler=4,port=60020] regionserver.MemStoreFlusher: Blocking updates on sceplus-vm48.almaden.ibm.com,60020,1404182803191: the global memstore size 436.0m is >= than blocking 386.7m size
2014-06-30 19:54:20,949 INFO  [RpcServer.handler=18,port=60020] regionserver.MemStoreFlusher: Blocking updates on sceplus-vm48.almaden.ibm.com,60020,1404182803191: the global memstore size 436.0m is >= than blocking 386.7m size
2014-06-30 19:54:20,949 INFO  [RpcServer.handler=6,port=60020] regionserver.MemStoreFlusher: Blocking updates on sceplus-vm48.almaden.ibm.com,60020,1404182803191: the global memstore size 436.0m is >= than blocking 386.7m size
2014-06-30 19:54:20,949 INFO  [RpcServer.handler=10,port=60020] regionserver.MemStoreFlusher: Blocking updates on sceplus-vm48.almaden.ibm.com,60020,1404182803191: the global memstore size 436.0m is >= than blocking 386.7m size
2014-06-30 19:54:20,949 INFO  [RpcServer.handler=12,port=60020] regionserver.MemStoreFlusher: Blocking updates on sceplus-vm48.almaden.ibm.com,60020,1404182803191: the global memstore size 436.0m is >= than blocking 386.7m size
2014-06-30 19:54:20,949 INFO  [RpcServer.handler=47,port=60020] regionserver.MemStoreFlusher: Blocking updates on sceplus-vm48.almaden.ibm.com,60020,1404182803191: the global memstore size 436.0m is >= than blocking 386.7m size
2014-06-30 19:54:20,950 INFO  [RpcServer.handler=49,port=60020] regionserver.MemStoreFlusher: Blocking updates on sceplus-vm48.almaden.ibm.com,60020,1404182803191: the global memstore size 436.0m is >= than blocking 386.7m size
2014-06-30 19:54:20,950 INFO  [RpcServer.handler=5,port=60020] regionserver.MemStoreFlusher: Blocking updates on sceplus-vm48.almaden.ibm.com,60020,1404182803191: the global memstore size 436.0m is >= than blocking 386.7m size
2014-06-30 19:54:20,950 INFO  [RpcServer.handler=37,port=60020] regionserver.MemStoreFlusher: Blocking updates on sceplus-vm48.almaden.ibm.com,60020,1404182803191: the global memstore size 436.0m is >= than blocking 386.7m size
2014-06-30 19:54:20,950 INFO  [RpcServer.handler=44,port=60020] regionserver.MemStoreFlusher: Blocking updates on sceplus-vm48.almaden.ibm.com,60020,1404182803191: the global memstore size 436.0m is >= than blocking 386.7m size
2014-06-30 19:54:20,950 INFO  [RpcServer.handler=2,port=60020] regionserver.MemStoreFlusher: Blocking updates on sceplus-vm48.almaden.ibm.com,60020,1404182803191: the global memstore size 436.0m is >= than blocking 386.7m size
2014-06-30 19:54:20,950 INFO  [RpcServer.handler=41,port=60020] regionserver.MemStoreFlusher: Blocking updates on sceplus-vm48.almaden.ibm.com,60020,1404182803191: the global memstore size 436.0m is >= than blocking 386.7m size
2014-06-30 19:54:20,951 INFO  [RpcServer.handler=24,port=60020] regionserver.MemStoreFlusher: Blocking updates on sceplus-vm48.almaden.ibm.com,60020,1404182803191: the global memstore size 436.0m is >= than blocking 386.7m size
2014-06-30 19:54:20,951 INFO  [RpcServer.handler=31,port=60020] regionserver.MemStoreFlusher: Blocking updates on sceplus-vm48.almaden.ibm.com,60020,1404182803191: the global memstore size 436.0m is >= than blocking 386.7m size
2014-06-30 19:54:20,989 INFO  [RpcServer.handler=30,port=60020] regionserver.MemStoreFlusher: Blocking updates on sceplus-vm48.almaden.ibm.com,60020,1404182803191: the global memstore size 445.6m is >= than blocking 386.7m size
2014-06-30 19:54:20,989 INFO  [RpcServer.handler=8,port=60020] regionserver.MemStoreFlusher: Blocking updates on sceplus-vm48.almaden.ibm.com,60020,1404182803191: the global memstore size 445.6m is >= than blocking 386.7m size
2014-06-30 19:54:20,989 INFO  [RpcServer.handler=26,port=60020] regionserver.MemStoreFlusher: Blocking updates on sceplus-vm48.almaden.ibm.com,60020,1404182803191: the global memstore size 445.6m is >= than blocking 386.7m size
2014-06-30 19:54:20,989 INFO  [RpcServer.handler=20,port=60020] regionserver.MemStoreFlusher: Blocking updates on sceplus-vm48.almaden.ibm.com,60020,1404182803191: the global memstore size 445.6m is >= than blocking 386.7m size
2014-06-30 19:54:20,989 INFO  [RpcServer.handler=33,port=60020] regionserver.MemStoreFlusher: Blocking updates on sceplus-vm48.almaden.ibm.com,60020,1404182803191: the global memstore size 445.6m is >= than blocking 386.7m size
2014-06-30 19:54:20,990 INFO  [RpcServer.handler=0,port=60020] regionserver.MemStoreFlusher: Blocking updates on sceplus-vm48.almaden.ibm.com,60020,1404182803191: the global memstore size 445.6m is >= than blocking 386.7m size
2014-06-30 19:54:21,009 INFO  [RpcServer.handler=29,port=60020] regionserver.MemStoreFlusher: Blocking updates on sceplus-vm48.almaden.ibm.com,60020,1404182803191: the global memstore size 445.6m is >= than blocking 386.7m size
2014-06-30 19:54:21,114 INFO  [RpcServer.handler=3,port=60020] regionserver.MemStoreFlusher: Blocking updates on sceplus-vm48.almaden.ibm.com,60020,1404182803191: the global memstore size 445.6m is >= than blocking 386.7m size
2014-06-30 19:54:24,023 WARN  [RpcServer.handler=27,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5335ms
2014-06-30 19:54:24,023 DEBUG [MemStoreFlusher.0] regionserver.MemStoreFlusher: Flush thread woke up because memory above low water=367.3m
2014-06-30 19:54:24,023 INFO  [MemStoreFlusher.0] regionserver.MemStoreFlusher: Flush of region usertable,user2,1404183160444.5446f1f888398e97c0ec3cc30da5bf97. due to global heap pressure
2014-06-30 19:54:24,024 DEBUG [MemStoreFlusher.0] regionserver.HRegion: Started memstore flush for usertable,user2,1404183160444.5446f1f888398e97c0ec3cc30da5bf97., current region memstore size 93.5m
2014-06-30 19:54:24,027 INFO  [JvmPauseMonitor] util.JvmPauseMonitor: Detected pause in JVM or host machine (eg GC): pause of approximately 2094ms
GC pool 'ConcurrentMarkSweep' had collection(s): count=1 time=2399ms
2014-06-30 19:54:24,039 INFO  [RpcServer.handler=7,port=60020] regionserver.MemStoreFlusher: Blocking updates on sceplus-vm48.almaden.ibm.com,60020,1404182803191: the global memstore size 445.6m is >= than blocking 386.7m size
2014-06-30 19:54:24,040 INFO  [RpcServer.handler=16,port=60020] regionserver.MemStoreFlusher: Blocking updates on sceplus-vm48.almaden.ibm.com,60020,1404182803191: the global memstore size 445.6m is >= than blocking 386.7m size
2014-06-30 19:54:24,050 INFO  [RpcServer.handler=13,port=60020] regionserver.MemStoreFlusher: Blocking updates on sceplus-vm48.almaden.ibm.com,60020,1404182803191: the global memstore size 445.6m is >= than blocking 386.7m size
2014-06-30 19:54:24,060 INFO  [RpcServer.handler=48,port=60020] regionserver.MemStoreFlusher: Blocking updates on sceplus-vm48.almaden.ibm.com,60020,1404182803191: the global memstore size 445.6m is >= than blocking 386.7m size
2014-06-30 19:54:24,109 DEBUG [MemStoreFlusher.0] util.FSUtils: DFS Client does not support most favored nodes create; using default create
2014-06-30 19:54:24,273 INFO  [MemStoreFlusher.1] regionserver.DefaultStoreFlusher: Flushed, sequenceid=242, memsize=79.1m, hasBloomFilter=true, into tmp file hdfs://master:54310/hbase/data/default/usertable/1d66ac045cc0407c0fa426db565fcbba/.tmp/667959630abb45d49ea7a88945e7adb9
2014-06-30 19:54:24,284 DEBUG [MemStoreFlusher.1] regionserver.HRegionFileSystem: Committing store file hdfs://master:54310/hbase/data/default/usertable/1d66ac045cc0407c0fa426db565fcbba/.tmp/667959630abb45d49ea7a88945e7adb9 as hdfs://master:54310/hbase/data/default/usertable/1d66ac045cc0407c0fa426db565fcbba/family/667959630abb45d49ea7a88945e7adb9
2014-06-30 19:54:24,292 INFO  [MemStoreFlusher.1] regionserver.HStore: Added hdfs://master:54310/hbase/data/default/usertable/1d66ac045cc0407c0fa426db565fcbba/family/667959630abb45d49ea7a88945e7adb9, entries=287900, sequenceid=242, filesize=42.9m
2014-06-30 19:54:24,292 INFO  [MemStoreFlusher.1] regionserver.HRegion: Finished memstore flush of ~107.8m/112995440, currentsize=0.0/0 for region usertable,user5,1404183160444.1d66ac045cc0407c0fa426db565fcbba. in 6083ms, sequenceid=242, compaction requested=true
2014-06-30 19:54:24,292 DEBUG [MemStoreFlusher.1] regionserver.CompactSplitThread: Small Compaction requested: system; Because: MemStoreFlusher.1; compaction_queue=(0:1), split_queue=0, merge_queue=0
2014-06-30 19:54:24,293 WARN  [RpcServer.handler=48,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 234ms
2014-06-30 19:54:24,293 INFO  [RpcServer.handler=48,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server sceplus-vm48.almaden.ibm.com,60020,1404182803191
2014-06-30 19:54:24,293 WARN  [RpcServer.handler=13,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 243ms
2014-06-30 19:54:24,293 INFO  [RpcServer.handler=13,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server sceplus-vm48.almaden.ibm.com,60020,1404182803191
2014-06-30 19:54:24,293 WARN  [RpcServer.handler=16,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 253ms
2014-06-30 19:54:24,293 INFO  [RpcServer.handler=16,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server sceplus-vm48.almaden.ibm.com,60020,1404182803191
2014-06-30 19:54:24,293 WARN  [RpcServer.handler=7,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 254ms
2014-06-30 19:54:24,293 INFO  [RpcServer.handler=7,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server sceplus-vm48.almaden.ibm.com,60020,1404182803191
2014-06-30 19:54:24,293 WARN  [RpcServer.handler=27,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5605ms
2014-06-30 19:54:24,293 INFO  [RpcServer.handler=27,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server sceplus-vm48.almaden.ibm.com,60020,1404182803191
2014-06-30 19:54:24,294 WARN  [RpcServer.handler=3,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 3179ms
2014-06-30 19:54:24,294 INFO  [RpcServer.handler=3,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server sceplus-vm48.almaden.ibm.com,60020,1404182803191
2014-06-30 19:54:24,294 WARN  [RpcServer.handler=29,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 3285ms
2014-06-30 19:54:24,294 INFO  [RpcServer.handler=29,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server sceplus-vm48.almaden.ibm.com,60020,1404182803191
2014-06-30 19:54:24,296 WARN  [RpcServer.handler=0,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 3307ms
2014-06-30 19:54:24,296 INFO  [RpcServer.handler=0,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server sceplus-vm48.almaden.ibm.com,60020,1404182803191
2014-06-30 19:54:24,296 WARN  [RpcServer.handler=33,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 3307ms
2014-06-30 19:54:24,296 INFO  [RpcServer.handler=33,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server sceplus-vm48.almaden.ibm.com,60020,1404182803191
2014-06-30 19:54:24,297 WARN  [RpcServer.handler=20,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 3307ms
2014-06-30 19:54:24,297 INFO  [RpcServer.handler=20,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server sceplus-vm48.almaden.ibm.com,60020,1404182803191
2014-06-30 19:54:24,297 WARN  [RpcServer.handler=26,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 3308ms
2014-06-30 19:54:24,297 INFO  [RpcServer.handler=26,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server sceplus-vm48.almaden.ibm.com,60020,1404182803191
2014-06-30 19:54:24,297 WARN  [RpcServer.handler=8,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 3308ms
2014-06-30 19:54:24,297 INFO  [RpcServer.handler=8,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server sceplus-vm48.almaden.ibm.com,60020,1404182803191
2014-06-30 19:54:24,297 WARN  [RpcServer.handler=30,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 3309ms
2014-06-30 19:54:24,301 INFO  [RpcServer.handler=30,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server sceplus-vm48.almaden.ibm.com,60020,1404182803191
2014-06-30 19:54:24,302 WARN  [RpcServer.handler=31,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 3359ms
2014-06-30 19:54:24,302 INFO  [RpcServer.handler=31,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server sceplus-vm48.almaden.ibm.com,60020,1404182803191
2014-06-30 19:54:24,306 WARN  [RpcServer.handler=24,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 3362ms
2014-06-30 19:54:24,306 INFO  [RpcServer.handler=24,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server sceplus-vm48.almaden.ibm.com,60020,1404182803191
2014-06-30 19:54:24,307 WARN  [RpcServer.handler=41,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 3363ms
2014-06-30 19:54:24,307 INFO  [RpcServer.handler=41,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server sceplus-vm48.almaden.ibm.com,60020,1404182803191
2014-06-30 19:54:24,307 WARN  [RpcServer.handler=2,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 3363ms
2014-06-30 19:54:24,308 INFO  [RpcServer.handler=2,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server sceplus-vm48.almaden.ibm.com,60020,1404182803191
2014-06-30 19:54:24,309 WARN  [RpcServer.handler=44,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 3364ms
2014-06-30 19:54:24,310 INFO  [RpcServer.handler=44,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server sceplus-vm48.almaden.ibm.com,60020,1404182803191
2014-06-30 19:54:24,310 WARN  [RpcServer.handler=37,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 3365ms
2014-06-30 19:54:24,310 INFO  [RpcServer.handler=37,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server sceplus-vm48.almaden.ibm.com,60020,1404182803191
2014-06-30 19:54:24,310 WARN  [RpcServer.handler=5,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 3365ms
2014-06-30 19:54:24,310 INFO  [RpcServer.handler=5,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server sceplus-vm48.almaden.ibm.com,60020,1404182803191
2014-06-30 19:54:24,310 WARN  [RpcServer.handler=49,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 3364ms
2014-06-30 19:54:24,310 INFO  [RpcServer.handler=49,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server sceplus-vm48.almaden.ibm.com,60020,1404182803191
2014-06-30 19:54:24,310 WARN  [RpcServer.handler=47,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 3364ms
2014-06-30 19:54:24,310 INFO  [RpcServer.handler=47,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server sceplus-vm48.almaden.ibm.com,60020,1404182803191
2014-06-30 19:54:24,311 WARN  [RpcServer.handler=12,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 3364ms
2014-06-30 19:54:24,311 INFO  [RpcServer.handler=12,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server sceplus-vm48.almaden.ibm.com,60020,1404182803191
2014-06-30 19:54:24,320 WARN  [RpcServer.handler=10,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 3373ms
2014-06-30 19:54:24,320 INFO  [RpcServer.handler=10,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server sceplus-vm48.almaden.ibm.com,60020,1404182803191
2014-06-30 19:54:24,321 WARN  [RpcServer.handler=6,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 3372ms
2014-06-30 19:54:24,321 INFO  [RpcServer.handler=6,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server sceplus-vm48.almaden.ibm.com,60020,1404182803191
2014-06-30 19:54:24,323 WARN  [RpcServer.handler=18,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 3389ms
2014-06-30 19:54:24,323 INFO  [RpcServer.handler=18,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server sceplus-vm48.almaden.ibm.com,60020,1404182803191
2014-06-30 19:54:24,324 WARN  [RpcServer.handler=4,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 3390ms
2014-06-30 19:54:24,324 INFO  [RpcServer.handler=4,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server sceplus-vm48.almaden.ibm.com,60020,1404182803191
2014-06-30 19:54:24,324 WARN  [RpcServer.handler=39,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 3376ms
2014-06-30 19:54:24,324 INFO  [RpcServer.handler=39,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server sceplus-vm48.almaden.ibm.com,60020,1404182803191
2014-06-30 19:54:24,324 WARN  [RpcServer.handler=45,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 3390ms
2014-06-30 19:54:24,324 INFO  [RpcServer.handler=45,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server sceplus-vm48.almaden.ibm.com,60020,1404182803191
2014-06-30 19:54:24,325 WARN  [RpcServer.handler=46,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 3390ms
2014-06-30 19:54:24,325 INFO  [RpcServer.handler=46,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server sceplus-vm48.almaden.ibm.com,60020,1404182803191
2014-06-30 19:54:24,326 WARN  [RpcServer.handler=17,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 3390ms
2014-06-30 19:54:24,326 INFO  [RpcServer.handler=17,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server sceplus-vm48.almaden.ibm.com,60020,1404182803191
2014-06-30 19:54:24,326 WARN  [RpcServer.handler=9,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 3390ms
2014-06-30 19:54:24,326 INFO  [RpcServer.handler=9,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server sceplus-vm48.almaden.ibm.com,60020,1404182803191
2014-06-30 19:54:24,326 WARN  [RpcServer.handler=42,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 3390ms
2014-06-30 19:54:24,326 INFO  [RpcServer.handler=42,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server sceplus-vm48.almaden.ibm.com,60020,1404182803191
2014-06-30 19:54:24,326 WARN  [RpcServer.handler=15,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 3390ms
2014-06-30 19:54:24,327 INFO  [RpcServer.handler=15,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server sceplus-vm48.almaden.ibm.com,60020,1404182803191
2014-06-30 19:54:24,327 WARN  [RpcServer.handler=34,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 3391ms
2014-06-30 19:54:24,327 INFO  [RpcServer.handler=34,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server sceplus-vm48.almaden.ibm.com,60020,1404182803191
2014-06-30 19:54:24,327 WARN  [RpcServer.handler=22,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 3391ms
2014-06-30 19:54:24,327 INFO  [RpcServer.handler=22,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server sceplus-vm48.almaden.ibm.com,60020,1404182803191
2014-06-30 19:54:24,327 WARN  [RpcServer.handler=11,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 3390ms
2014-06-30 19:54:24,327 INFO  [RpcServer.handler=11,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server sceplus-vm48.almaden.ibm.com,60020,1404182803191
2014-06-30 19:54:24,327 WARN  [RpcServer.handler=19,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 3390ms
2014-06-30 19:54:24,327 INFO  [RpcServer.handler=19,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server sceplus-vm48.almaden.ibm.com,60020,1404182803191
2014-06-30 19:54:24,328 WARN  [RpcServer.handler=14,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 3390ms
2014-06-30 19:54:24,328 INFO  [RpcServer.handler=14,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server sceplus-vm48.almaden.ibm.com,60020,1404182803191
2014-06-30 19:54:24,330 WARN  [RpcServer.handler=21,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 3391ms
2014-06-30 19:54:24,330 INFO  [RpcServer.handler=21,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server sceplus-vm48.almaden.ibm.com,60020,1404182803191
2014-06-30 19:54:24,330 WARN  [RpcServer.handler=23,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 3391ms
2014-06-30 19:54:24,330 INFO  [RpcServer.handler=23,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server sceplus-vm48.almaden.ibm.com,60020,1404182803191
2014-06-30 19:54:24,331 WARN  [RpcServer.handler=28,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 3389ms
2014-06-30 19:54:24,331 INFO  [RpcServer.handler=28,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server sceplus-vm48.almaden.ibm.com,60020,1404182803191
2014-06-30 19:54:24,332 WARN  [RpcServer.handler=38,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 3390ms
2014-06-30 19:54:24,332 INFO  [RpcServer.handler=38,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server sceplus-vm48.almaden.ibm.com,60020,1404182803191
2014-06-30 19:54:24,332 WARN  [RpcServer.handler=36,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 3390ms
2014-06-30 19:54:24,332 INFO  [RpcServer.handler=36,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server sceplus-vm48.almaden.ibm.com,60020,1404182803191
2014-06-30 19:54:24,334 WARN  [RpcServer.handler=25,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 3390ms
2014-06-30 19:54:24,334 INFO  [RpcServer.handler=25,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server sceplus-vm48.almaden.ibm.com,60020,1404182803191
2014-06-30 19:54:24,336 WARN  [RpcServer.handler=1,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 3392ms
2014-06-30 19:54:24,336 INFO  [RpcServer.handler=1,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server sceplus-vm48.almaden.ibm.com,60020,1404182803191
2014-06-30 19:54:24,337 WARN  [RpcServer.handler=40,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 3393ms
2014-06-30 19:54:24,337 INFO  [RpcServer.handler=40,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server sceplus-vm48.almaden.ibm.com,60020,1404182803191
2014-06-30 19:54:24,337 WARN  [RpcServer.handler=32,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 3394ms
2014-06-30 19:54:24,337 INFO  [RpcServer.handler=32,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server sceplus-vm48.almaden.ibm.com,60020,1404182803191
2014-06-30 19:54:24,338 WARN  [RpcServer.handler=35,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 3394ms
2014-06-30 19:54:24,338 INFO  [RpcServer.handler=35,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server sceplus-vm48.almaden.ibm.com,60020,1404182803191
2014-06-30 19:54:24,338 WARN  [RpcServer.handler=43,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 3405ms
2014-06-30 19:54:24,338 INFO  [RpcServer.handler=43,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server sceplus-vm48.almaden.ibm.com,60020,1404182803191
2014-06-30 19:54:26,585 INFO  [JvmPauseMonitor] util.JvmPauseMonitor: Detected pause in JVM or host machine (eg GC): pause of approximately 1557ms
GC pool 'ConcurrentMarkSweep' had collection(s): count=1 time=2049ms
2014-06-30 19:54:26,891 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-06-30 19:54:26,896 WARN  [RpcServer.handler=27,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":29738,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.58:49906","starttimems":1404183237157,"queuetimems":1,"class":"HRegionServer","responsesize":15624,"method":"Multi"}
2014-06-30 19:54:26,896 WARN  [RpcServer.handler=27,port=60020] ipc.RpcServer: RpcServer.respondercallId: 690 service: ClientService methodName: Multi size: 2.7m connection: 9.1.143.58:49906: output error
2014-06-30 19:54:26,896 WARN  [RpcServer.handler=27,port=60020] ipc.RpcServer: RpcServer.handler=27,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-06-30 19:54:29,133 INFO  [JvmPauseMonitor] util.JvmPauseMonitor: Detected pause in JVM or host machine (eg GC): pause of approximately 1547ms
GC pool 'ConcurrentMarkSweep' had collection(s): count=1 time=2040ms
2014-06-30 19:54:29,228 DEBUG [regionserver60020-smallCompactions-1404182848217] regionserver.HRegionFileSystem: Committing store file hdfs://master:54310/hbase/data/default/usertable/5446f1f888398e97c0ec3cc30da5bf97/.tmp/4fba5c6740284ed39067eaf228bf3a3e as hdfs://master:54310/hbase/data/default/usertable/5446f1f888398e97c0ec3cc30da5bf97/family/4fba5c6740284ed39067eaf228bf3a3e
2014-06-30 19:54:29,231 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 1136 synced till here 1093
2014-06-30 19:54:29,376 WARN  [RpcServer.handler=33,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":27231,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.58:49906","starttimems":1404183242144,"queuetimems":0,"class":"HRegionServer","responsesize":15808,"method":"Multi"}
2014-06-30 19:54:29,377 WARN  [RpcServer.handler=33,port=60020] ipc.RpcServer: RpcServer.respondercallId: 723 service: ClientService methodName: Multi size: 2.8m connection: 9.1.143.58:49906: output error
2014-06-30 19:54:29,377 WARN  [RpcServer.handler=33,port=60020] ipc.RpcServer: RpcServer.handler=33,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-06-30 19:54:29,387 WARN  [RpcServer.handler=30,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":27218,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.58:49906","starttimems":1404183242168,"queuetimems":0,"class":"HRegionServer","responsesize":16217,"method":"Multi"}
2014-06-30 19:54:29,387 DEBUG [MemStoreFlusher.1] regionserver.MemStoreFlusher: Flush thread woke up because memory above low water=367.3m
2014-06-30 19:54:29,387 WARN  [RpcServer.handler=30,port=60020] ipc.RpcServer: RpcServer.respondercallId: 721 service: ClientService methodName: Multi size: 2.8m connection: 9.1.143.58:49906: output error
2014-06-30 19:54:29,387 WARN  [RpcServer.handler=30,port=60020] ipc.RpcServer: RpcServer.handler=30,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-06-30 19:54:29,387 INFO  [MemStoreFlusher.1] regionserver.MemStoreFlusher: Flush of region usertable,user8,1404183160444.28938d267c1c61f7feb582b85e4c172f. due to global heap pressure
2014-06-30 19:54:29,387 DEBUG [MemStoreFlusher.1] regionserver.HRegion: Started memstore flush for usertable,user8,1404183160444.28938d267c1c61f7feb582b85e4c172f., current region memstore size 95.7m
2014-06-30 19:54:29,467 WARN  [RpcServer.handler=2,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":25087,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.58:49906","starttimems":1404183244380,"queuetimems":0,"class":"HRegionServer","responsesize":16308,"method":"Multi"}
2014-06-30 19:54:29,467 WARN  [RpcServer.handler=18,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":25063,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.58:49906","starttimems":1404183244404,"queuetimems":0,"class":"HRegionServer","responsesize":15808,"method":"Multi"}
2014-06-30 19:54:29,467 WARN  [RpcServer.handler=2,port=60020] ipc.RpcServer: RpcServer.respondercallId: 738 service: ClientService methodName: Multi size: 2.9m connection: 9.1.143.58:49906: output error
2014-06-30 19:54:29,467 WARN  [RpcServer.handler=2,port=60020] ipc.RpcServer: RpcServer.handler=2,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-06-30 19:54:29,468 WARN  [RpcServer.handler=18,port=60020] ipc.RpcServer: RpcServer.respondercallId: 736 service: ClientService methodName: Multi size: 2.8m connection: 9.1.143.58:49906: output error
2014-06-30 19:54:29,468 WARN  [RpcServer.handler=18,port=60020] ipc.RpcServer: RpcServer.handler=18,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-06-30 19:54:29,469 WARN  [RpcServer.handler=32,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":27039,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.58:49906","starttimems":1404183242429,"queuetimems":0,"class":"HRegionServer","responsesize":16476,"method":"Multi"}
2014-06-30 19:54:29,469 WARN  [RpcServer.handler=32,port=60020] ipc.RpcServer: RpcServer.respondercallId: 733 service: ClientService methodName: Multi size: 2.9m connection: 9.1.143.58:49906: output error
2014-06-30 19:54:29,469 WARN  [RpcServer.handler=32,port=60020] ipc.RpcServer: RpcServer.handler=32,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-06-30 19:54:29,470 WARN  [RpcServer.handler=12,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":25101,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.58:49906","starttimems":1404183244369,"queuetimems":1,"class":"HRegionServer","responsesize":16217,"method":"Multi"}
2014-06-30 19:54:29,470 WARN  [RpcServer.handler=46,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":29327,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.58:49906","starttimems":1404183240143,"queuetimems":1,"class":"HRegionServer","responsesize":16418,"method":"Multi"}
2014-06-30 19:54:29,470 WARN  [RpcServer.handler=6,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":25075,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.58:49906","starttimems":1404183244395,"queuetimems":0,"class":"HRegionServer","responsesize":16418,"method":"Multi"}
2014-06-30 19:54:29,476 DEBUG [RpcServer.handler=47,port=60020] regionserver.HRegion: Flush requested on usertable,user8,1404183160444.28938d267c1c61f7feb582b85e4c172f.
2014-06-30 19:54:29,476 WARN  [RpcServer.handler=11,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":31819,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.58:49906","starttimems":1404183237657,"queuetimems":0,"class":"HRegionServer","responsesize":13985,"method":"Multi"}
2014-06-30 19:54:29,476 WARN  [RpcServer.handler=5,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":32349,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.58:49906","starttimems":1404183237127,"queuetimems":1,"class":"HRegionServer","responsesize":15676,"method":"Multi"}
2014-06-30 19:54:29,477 WARN  [RpcServer.handler=44,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":32339,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.58:49906","starttimems":1404183237137,"queuetimems":0,"class":"HRegionServer","responsesize":16074,"method":"Multi"}
2014-06-30 19:54:29,475 WARN  [RpcServer.handler=37,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":31757,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.58:49906","starttimems":1404183237718,"queuetimems":0,"class":"HRegionServer","responsesize":15793,"method":"Multi"}
2014-06-30 19:54:29,478 WARN  [RpcServer.handler=38,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":32810,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.58:49906","starttimems":1404183236667,"queuetimems":1,"class":"HRegionServer","responsesize":15208,"method":"Multi"}
2014-06-30 19:54:29,478 WARN  [RpcServer.handler=21,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":31687,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.58:49906","starttimems":1404183237791,"queuetimems":1,"class":"HRegionServer","responsesize":15083,"method":"Multi"}
2014-06-30 19:54:29,478 WARN  [RpcServer.handler=41,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":27321,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.58:49906","starttimems":1404183242157,"queuetimems":0,"class":"HRegionServer","responsesize":16476,"method":"Multi"}
2014-06-30 19:54:29,478 WARN  [RpcServer.handler=19,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":31811,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.58:49906","starttimems":1404183237667,"queuetimems":0,"class":"HRegionServer","responsesize":15658,"method":"Multi"}
2014-06-30 19:54:29,475 WARN  [RpcServer.handler=39,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":31825,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.58:49906","starttimems":1404183237650,"queuetimems":1,"class":"HRegionServer","responsesize":15384,"method":"Multi"}
2014-06-30 19:54:29,474 WARN  [RpcServer.handler=4,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":29546,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.58:49906","starttimems":1404183239928,"queuetimems":0,"class":"HRegionServer","responsesize":16395,"method":"Multi"}
2014-06-30 19:54:29,479 WARN  [RpcServer.handler=36,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":29362,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.58:49906","starttimems":1404183240116,"queuetimems":1,"class":"HRegionServer","responsesize":16308,"method":"Multi"}
2014-06-30 19:54:29,474 WARN  [RpcServer.handler=9,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":24990,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.58:49906","starttimems":1404183244483,"queuetimems":69,"class":"HRegionServer","responsesize":16395,"method":"Multi"}
2014-06-30 19:54:29,473 WARN  [RpcServer.handler=15,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":32277,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.58:49906","starttimems":1404183237195,"queuetimems":0,"class":"HRegionServer","responsesize":15489,"method":"Multi"}
2014-06-30 19:54:29,473 INFO  [RpcServer.handler=42,port=60020] regionserver.MemStoreFlusher: Blocking updates on sceplus-vm48.almaden.ibm.com,60020,1404182803191: the global memstore size 400.2m is >= than blocking 386.7m size
2014-06-30 19:54:29,472 WARN  [RpcServer.handler=1,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":31662,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.58:49906","starttimems":1404183237810,"queuetimems":1,"class":"HRegionServer","responsesize":14017,"method":"Multi"}
2014-06-30 19:54:29,479 INFO  [RpcServer.handler=25,port=60020] regionserver.MemStoreFlusher: Blocking updates on sceplus-vm48.almaden.ibm.com,60020,1404182803191: the global memstore size 435.3m is >= than blocking 386.7m size
2014-06-30 19:54:29,472 WARN  [RpcServer.handler=46,port=60020] ipc.RpcServer: RpcServer.respondercallId: 724 service: ClientService methodName: Multi size: 2.9m connection: 9.1.143.58:49906: output error
2014-06-30 19:54:29,479 WARN  [RpcServer.handler=43,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":32478,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.58:49906","starttimems":1404183237001,"queuetimems":1,"class":"HRegionServer","responsesize":16372,"method":"Multi"}
2014-06-30 19:54:29,479 WARN  [RpcServer.handler=46,port=60020] ipc.RpcServer: RpcServer.handler=46,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-06-30 19:54:29,478 WARN  [RpcServer.handler=22,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":29393,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.58:49906","starttimems":1404183240085,"queuetimems":0,"class":"HRegionServer","responsesize":16372,"method":"Multi"}
2014-06-30 19:54:29,478 WARN  [RpcServer.handler=23,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":31775,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.58:49906","starttimems":1404183237703,"queuetimems":1,"class":"HRegionServer","responsesize":15624,"method":"Multi"}
2014-06-30 19:54:29,477 WARN  [RpcServer.handler=47,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":32787,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.58:49906","starttimems":1404183236690,"queuetimems":1,"class":"HRegionServer","responsesize":16063,"method":"Multi"}
2014-06-30 19:54:29,480 WARN  [RpcServer.handler=1,port=60020] ipc.RpcServer: RpcServer.respondercallId: 673 service: ClientService methodName: Multi size: 2.5m connection: 9.1.143.58:49906: output error
2014-06-30 19:54:29,479 INFO  [RpcServer.handler=26,port=60020] regionserver.MemStoreFlusher: Blocking updates on sceplus-vm48.almaden.ibm.com,60020,1404182803191: the global memstore size 435.3m is >= than blocking 386.7m size
2014-06-30 19:54:29,480 WARN  [RpcServer.handler=47,port=60020] ipc.RpcServer: RpcServer.respondercallId: 669 service: ClientService methodName: Multi size: 2.8m connection: 9.1.143.58:49906: output error
2014-06-30 19:54:29,480 WARN  [RpcServer.handler=47,port=60020] ipc.RpcServer: RpcServer.handler=47,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-06-30 19:54:29,480 INFO  [RpcServer.handler=27,port=60020] regionserver.MemStoreFlusher: Blocking updates on sceplus-vm48.almaden.ibm.com,60020,1404182803191: the global memstore size 435.3m is >= than blocking 386.7m size
2014-06-30 19:54:29,480 WARN  [RpcServer.handler=1,port=60020] ipc.RpcServer: RpcServer.handler=1,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-06-30 19:54:29,481 WARN  [RpcServer.handler=15,port=60020] ipc.RpcServer: RpcServer.respondercallId: 688 service: ClientService methodName: Multi size: 2.7m connection: 9.1.143.58:49906: output error
2014-06-30 19:54:29,481 INFO  [RpcServer.handler=17,port=60020] regionserver.MemStoreFlusher: Blocking updates on sceplus-vm48.almaden.ibm.com,60020,1404182803191: the global memstore size 435.3m is >= than blocking 386.7m size
2014-06-30 19:54:29,481 WARN  [RpcServer.handler=9,port=60020] ipc.RpcServer: RpcServer.respondercallId: 735 service: ClientService methodName: Multi size: 2.9m connection: 9.1.143.58:49906: output error
2014-06-30 19:54:29,481 WARN  [RpcServer.handler=15,port=60020] ipc.RpcServer: RpcServer.handler=15,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-06-30 19:54:29,481 INFO  [RpcServer.handler=34,port=60020] regionserver.MemStoreFlusher: Blocking updates on sceplus-vm48.almaden.ibm.com,60020,1404182803191: the global memstore size 435.3m is >= than blocking 386.7m size
2014-06-30 19:54:29,481 WARN  [RpcServer.handler=9,port=60020] ipc.RpcServer: RpcServer.handler=9,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-06-30 19:54:29,481 INFO  [RpcServer.handler=28,port=60020] regionserver.MemStoreFlusher: Blocking updates on sceplus-vm48.almaden.ibm.com,60020,1404182803191: the global memstore size 435.3m is >= than blocking 386.7m size
2014-06-30 19:54:29,481 WARN  [RpcServer.handler=36,port=60020] ipc.RpcServer: RpcServer.respondercallId: 725 service: ClientService methodName: Multi size: 2.9m connection: 9.1.143.58:49906: output error
2014-06-30 19:54:29,481 INFO  [RpcServer.handler=29,port=60020] regionserver.MemStoreFlusher: Blocking updates on sceplus-vm48.almaden.ibm.com,60020,1404182803191: the global memstore size 435.3m is >= than blocking 386.7m size
2014-06-30 19:54:29,482 WARN  [RpcServer.handler=4,port=60020] ipc.RpcServer: RpcServer.respondercallId: 711 service: ClientService methodName: Multi size: 2.9m connection: 9.1.143.58:49906: output error
2014-06-30 19:54:29,481 WARN  [RpcServer.handler=36,port=60020] ipc.RpcServer: RpcServer.handler=36,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-06-30 19:54:29,482 INFO  [RpcServer.handler=35,port=60020] regionserver.MemStoreFlusher: Blocking updates on sceplus-vm48.almaden.ibm.com,60020,1404182803191: the global memstore size 435.3m is >= than blocking 386.7m size
2014-06-30 19:54:29,482 WARN  [RpcServer.handler=4,port=60020] ipc.RpcServer: RpcServer.handler=4,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-06-30 19:54:29,482 INFO  [RpcServer.handler=45,port=60020] regionserver.MemStoreFlusher: Blocking updates on sceplus-vm48.almaden.ibm.com,60020,1404182803191: the global memstore size 435.3m is >= than blocking 386.7m size
2014-06-30 19:54:29,482 WARN  [RpcServer.handler=39,port=60020] ipc.RpcServer: RpcServer.respondercallId: 685 service: ClientService methodName: Multi size: 2.7m connection: 9.1.143.58:49906: output error
2014-06-30 19:54:29,482 INFO  [RpcServer.handler=49,port=60020] regionserver.MemStoreFlusher: Blocking updates on sceplus-vm48.almaden.ibm.com,60020,1404182803191: the global memstore size 435.3m is >= than blocking 386.7m size
2014-06-30 19:54:29,482 WARN  [RpcServer.handler=19,port=60020] ipc.RpcServer: RpcServer.respondercallId: 682 service: ClientService methodName: Multi size: 2.7m connection: 9.1.143.58:49906: output error
2014-06-30 19:54:29,482 WARN  [RpcServer.handler=39,port=60020] ipc.RpcServer: RpcServer.handler=39,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-06-30 19:54:29,482 INFO  [RpcServer.handler=7,port=60020] regionserver.MemStoreFlusher: Blocking updates on sceplus-vm48.almaden.ibm.com,60020,1404182803191: the global memstore size 435.3m is >= than blocking 386.7m size
2014-06-30 19:54:29,482 WARN  [RpcServer.handler=19,port=60020] ipc.RpcServer: RpcServer.handler=19,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-06-30 19:54:29,482 INFO  [RpcServer.handler=8,port=60020] regionserver.MemStoreFlusher: Blocking updates on sceplus-vm48.almaden.ibm.com,60020,1404182803191: the global memstore size 435.3m is >= than blocking 386.7m size
2014-06-30 19:54:29,482 INFO  [MemStoreFlusher.0] regionserver.DefaultStoreFlusher: Flushed, sequenceid=249, memsize=61.5m, hasBloomFilter=true, into tmp file hdfs://master:54310/hbase/data/default/usertable/5446f1f888398e97c0ec3cc30da5bf97/.tmp/f623702cd798419f9c8c1fc2f90abb5b
2014-06-30 19:54:29,482 WARN  [RpcServer.handler=41,port=60020] ipc.RpcServer: RpcServer.respondercallId: 722 service: ClientService methodName: Multi size: 2.9m connection: 9.1.143.58:49906: output error
2014-06-30 19:54:29,482 INFO  [RpcServer.handler=16,port=60020] regionserver.MemStoreFlusher: Blocking updates on sceplus-vm48.almaden.ibm.com,60020,1404182803191: the global memstore size 435.3m is >= than blocking 386.7m size
2014-06-30 19:54:29,483 WARN  [RpcServer.handler=21,port=60020] ipc.RpcServer: RpcServer.respondercallId: 674 service: ClientService methodName: Multi size: 2.6m connection: 9.1.143.58:49906: output error
2014-06-30 19:54:29,482 WARN  [RpcServer.handler=41,port=60020] ipc.RpcServer: RpcServer.handler=41,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-06-30 19:54:29,483 INFO  [RpcServer.handler=20,port=60020] regionserver.MemStoreFlusher: Blocking updates on sceplus-vm48.almaden.ibm.com,60020,1404182803191: the global memstore size 435.3m is >= than blocking 386.7m size
2014-06-30 19:54:29,483 WARN  [RpcServer.handler=21,port=60020] ipc.RpcServer: RpcServer.handler=21,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-06-30 19:54:29,483 INFO  [RpcServer.handler=14,port=60020] regionserver.MemStoreFlusher: Blocking updates on sceplus-vm48.almaden.ibm.com,60020,1404182803191: the global memstore size 435.3m is >= than blocking 386.7m size
2014-06-30 19:54:29,483 WARN  [RpcServer.handler=38,port=60020] ipc.RpcServer: RpcServer.respondercallId: 662 service: ClientService methodName: Multi size: 2.7m connection: 9.1.143.58:49906: output error
2014-06-30 19:54:29,483 WARN  [RpcServer.handler=38,port=60020] ipc.RpcServer: RpcServer.handler=38,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-06-30 19:54:29,483 WARN  [RpcServer.handler=37,port=60020] ipc.RpcServer: RpcServer.respondercallId: 678 service: ClientService methodName: Multi size: 2.8m connection: 9.1.143.58:49906: output error
2014-06-30 19:54:29,483 WARN  [RpcServer.handler=37,port=60020] ipc.RpcServer: RpcServer.handler=37,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-06-30 19:54:29,483 WARN  [RpcServer.handler=44,port=60020] ipc.RpcServer: RpcServer.respondercallId: 691 service: ClientService methodName: Multi size: 2.8m connection: 9.1.143.58:49906: output error
2014-06-30 19:54:29,483 WARN  [RpcServer.handler=44,port=60020] ipc.RpcServer: RpcServer.handler=44,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-06-30 19:54:29,483 WARN  [RpcServer.handler=5,port=60020] ipc.RpcServer: RpcServer.respondercallId: 692 service: ClientService methodName: Multi size: 2.7m connection: 9.1.143.58:49906: output error
2014-06-30 19:54:29,483 WARN  [RpcServer.handler=5,port=60020] ipc.RpcServer: RpcServer.handler=5,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-06-30 19:54:29,484 WARN  [RpcServer.handler=11,port=60020] ipc.RpcServer: RpcServer.respondercallId: 684 service: ClientService methodName: Multi size: 2.4m connection: 9.1.143.58:49906: output error
2014-06-30 19:54:29,484 WARN  [RpcServer.handler=11,port=60020] ipc.RpcServer: RpcServer.handler=11,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-06-30 19:54:29,484 WARN  [RpcServer.handler=6,port=60020] ipc.RpcServer: RpcServer.respondercallId: 737 service: ClientService methodName: Multi size: 2.9m connection: 9.1.143.58:49906: output error
2014-06-30 19:54:29,484 WARN  [RpcServer.handler=6,port=60020] ipc.RpcServer: RpcServer.handler=6,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-06-30 19:54:29,484 WARN  [RpcServer.handler=12,port=60020] ipc.RpcServer: RpcServer.respondercallId: 739 service: ClientService methodName: Multi size: 2.8m connection: 9.1.143.58:49906: output error
2014-06-30 19:54:29,484 WARN  [RpcServer.handler=12,port=60020] ipc.RpcServer: RpcServer.handler=12,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-06-30 19:54:29,484 WARN  [RpcServer.handler=22,port=60020] ipc.RpcServer: RpcServer.respondercallId: 720 service: ClientService methodName: Multi size: 2.9m connection: 9.1.143.58:49906: output error
2014-06-30 19:54:29,484 WARN  [RpcServer.handler=22,port=60020] ipc.RpcServer: RpcServer.handler=22,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-06-30 19:54:29,484 WARN  [RpcServer.handler=23,port=60020] ipc.RpcServer: RpcServer.respondercallId: 679 service: ClientService methodName: Multi size: 2.7m connection: 9.1.143.58:49906: output error
2014-06-30 19:54:29,484 WARN  [RpcServer.handler=23,port=60020] ipc.RpcServer: RpcServer.handler=23,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-06-30 19:54:29,484 WARN  [RpcServer.handler=43,port=60020] ipc.RpcServer: RpcServer.respondercallId: 664 service: ClientService methodName: Multi size: 2.9m connection: 9.1.143.58:49906: output error
2014-06-30 19:54:29,484 WARN  [RpcServer.handler=43,port=60020] ipc.RpcServer: RpcServer.handler=43,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-06-30 19:54:29,488 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1404182803191/sceplus-vm48.almaden.ibm.com%2C60020%2C1404182803191.1404183249883 with entries=117, filesize=97.1m; new WAL /hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1404182803191/sceplus-vm48.almaden.ibm.com%2C60020%2C1404182803191.1404183266891
2014-06-30 19:54:29,493 DEBUG [MemStoreFlusher.0] regionserver.HRegionFileSystem: Committing store file hdfs://master:54310/hbase/data/default/usertable/5446f1f888398e97c0ec3cc30da5bf97/.tmp/f623702cd798419f9c8c1fc2f90abb5b as hdfs://master:54310/hbase/data/default/usertable/5446f1f888398e97c0ec3cc30da5bf97/family/f623702cd798419f9c8c1fc2f90abb5b
2014-06-30 19:54:29,502 INFO  [MemStoreFlusher.0] regionserver.HStore: Added hdfs://master:54310/hbase/data/default/usertable/5446f1f888398e97c0ec3cc30da5bf97/family/f623702cd798419f9c8c1fc2f90abb5b, entries=224040, sequenceid=249, filesize=33.4m
2014-06-30 19:54:29,502 INFO  [MemStoreFlusher.0] regionserver.HRegion: Finished memstore flush of ~93.5m/98078640, currentsize=11.2m/11747200 for region usertable,user2,1404183160444.5446f1f888398e97c0ec3cc30da5bf97. in 5478ms, sequenceid=249, compaction requested=false
2014-06-30 19:54:29,502 WARN  [RpcServer.handler=14,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 29ms
2014-06-30 19:54:29,502 INFO  [RpcServer.handler=14,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server sceplus-vm48.almaden.ibm.com,60020,1404182803191
2014-06-30 19:54:29,503 WARN  [RpcServer.handler=20,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 30ms
2014-06-30 19:54:29,503 INFO  [RpcServer.handler=20,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server sceplus-vm48.almaden.ibm.com,60020,1404182803191
2014-06-30 19:54:29,503 WARN  [RpcServer.handler=16,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 30ms
2014-06-30 19:54:29,503 INFO  [RpcServer.handler=16,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server sceplus-vm48.almaden.ibm.com,60020,1404182803191
2014-06-30 19:54:29,503 WARN  [RpcServer.handler=8,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 29ms
2014-06-30 19:54:29,503 INFO  [RpcServer.handler=8,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server sceplus-vm48.almaden.ibm.com,60020,1404182803191
2014-06-30 19:54:29,503 WARN  [RpcServer.handler=7,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 29ms
2014-06-30 19:54:29,503 INFO  [RpcServer.handler=7,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server sceplus-vm48.almaden.ibm.com,60020,1404182803191
2014-06-30 19:54:29,503 WARN  [RpcServer.handler=49,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 27ms
2014-06-30 19:54:29,503 INFO  [RpcServer.handler=49,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server sceplus-vm48.almaden.ibm.com,60020,1404182803191
2014-06-30 19:54:29,503 WARN  [RpcServer.handler=45,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 27ms
2014-06-30 19:54:29,503 INFO  [RpcServer.handler=45,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server sceplus-vm48.almaden.ibm.com,60020,1404182803191
2014-06-30 19:54:29,504 WARN  [RpcServer.handler=35,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 27ms
2014-06-30 19:54:29,504 INFO  [RpcServer.handler=35,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server sceplus-vm48.almaden.ibm.com,60020,1404182803191
2014-06-30 19:54:29,504 WARN  [RpcServer.handler=29,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 28ms
2014-06-30 19:54:29,504 INFO  [RpcServer.handler=29,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server sceplus-vm48.almaden.ibm.com,60020,1404182803191
2014-06-30 19:54:29,504 WARN  [RpcServer.handler=28,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 27ms
2014-06-30 19:54:29,504 INFO  [RpcServer.handler=28,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server sceplus-vm48.almaden.ibm.com,60020,1404182803191
2014-06-30 19:54:29,504 WARN  [RpcServer.handler=34,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 27ms
2014-06-30 19:54:29,504 INFO  [RpcServer.handler=34,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server sceplus-vm48.almaden.ibm.com,60020,1404182803191
2014-06-30 19:54:29,504 WARN  [RpcServer.handler=17,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 27ms
2014-06-30 19:54:29,504 DEBUG [MemStoreFlusher.0] regionserver.HRegion: NOT flushing memstore for region usertable,user8,1404183160444.28938d267c1c61f7feb582b85e4c172f., flushing=true, writesEnabled=true
2014-06-30 19:54:29,504 INFO  [RpcServer.handler=17,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server sceplus-vm48.almaden.ibm.com,60020,1404182803191
2014-06-30 19:54:29,504 WARN  [RpcServer.handler=27,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 26ms
2014-06-30 19:54:29,504 INFO  [RpcServer.handler=27,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server sceplus-vm48.almaden.ibm.com,60020,1404182803191
2014-06-30 19:54:29,505 WARN  [RpcServer.handler=26,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 27ms
2014-06-30 19:54:29,505 INFO  [RpcServer.handler=26,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server sceplus-vm48.almaden.ibm.com,60020,1404182803191
2014-06-30 19:54:29,505 WARN  [RpcServer.handler=25,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 26ms
2014-06-30 19:54:29,505 INFO  [RpcServer.handler=25,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server sceplus-vm48.almaden.ibm.com,60020,1404182803191
2014-06-30 19:54:29,505 WARN  [RpcServer.handler=42,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 33ms
2014-06-30 19:54:29,505 INFO  [RpcServer.handler=42,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server sceplus-vm48.almaden.ibm.com,60020,1404182803191
2014-06-30 19:54:29,575 WARN  [RpcServer.handler=31,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":19691,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.58:49906","starttimems":1404183249884,"queuetimems":5457,"class":"HRegionServer","responsesize":16372,"method":"Multi"}
2014-06-30 19:54:29,576 WARN  [RpcServer.handler=31,port=60020] ipc.RpcServer: RpcServer.respondercallId: 734 service: ClientService methodName: Multi size: 2.9m connection: 9.1.143.58:49906: output error
2014-06-30 19:54:29,576 WARN  [RpcServer.handler=31,port=60020] ipc.RpcServer: RpcServer.handler=31,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-06-30 19:54:29,623 DEBUG [regionserver60020-smallCompactions-1404182848217] regionserver.HStore: Removing store files after compaction...
2014-06-30 19:54:29,624 WARN  [RpcServer.handler=24,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":31739,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.58:49906","starttimems":1404183237884,"queuetimems":1,"class":"HRegionServer","responsesize":16943,"method":"Multi"}
2014-06-30 19:54:29,624 WARN  [RpcServer.handler=49,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":31994,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.58:49906","starttimems":1404183237629,"queuetimems":410,"class":"HRegionServer","responsesize":17012,"method":"Multi"}
2014-06-30 19:54:29,626 WARN  [RpcServer.handler=24,port=60020] ipc.RpcServer: RpcServer.respondercallId: 670 service: ClientService methodName: Multi size: 3.0m connection: 9.1.143.58:49906: output error
2014-06-30 19:54:29,626 WARN  [RpcServer.handler=24,port=60020] ipc.RpcServer: RpcServer.handler=24,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-06-30 19:54:29,626 WARN  [RpcServer.handler=49,port=60020] ipc.RpcServer: RpcServer.respondercallId: 687 service: ClientService methodName: Multi size: 3.0m connection: 9.1.143.58:49906: output error
2014-06-30 19:54:29,626 WARN  [RpcServer.handler=49,port=60020] ipc.RpcServer: RpcServer.handler=49,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-06-30 19:54:29,647 DEBUG [regionserver60020-smallCompactions-1404182848217] backup.HFileArchiver: Finished archiving from class org.apache.hadoop.hbase.backup.HFileArchiver$FileableStoreFile, file:hdfs://master:54310/hbase/data/default/usertable/5446f1f888398e97c0ec3cc30da5bf97/family/28c2e48911c740559fb5ee390001c1a2, to hdfs://master:54310/hbase/archive/data/default/usertable/5446f1f888398e97c0ec3cc30da5bf97/family/28c2e48911c740559fb5ee390001c1a2
2014-06-30 19:54:29,651 DEBUG [regionserver60020-smallCompactions-1404182848217] backup.HFileArchiver: Finished archiving from class org.apache.hadoop.hbase.backup.HFileArchiver$FileableStoreFile, file:hdfs://master:54310/hbase/data/default/usertable/5446f1f888398e97c0ec3cc30da5bf97/family/bafd4f0e46a04e2e8046881dd8521fb4, to hdfs://master:54310/hbase/archive/data/default/usertable/5446f1f888398e97c0ec3cc30da5bf97/family/bafd4f0e46a04e2e8046881dd8521fb4
2014-06-30 19:54:29,656 DEBUG [regionserver60020-smallCompactions-1404182848217] backup.HFileArchiver: Finished archiving from class org.apache.hadoop.hbase.backup.HFileArchiver$FileableStoreFile, file:hdfs://master:54310/hbase/data/default/usertable/5446f1f888398e97c0ec3cc30da5bf97/family/55017c5e21444bfc9ce99262f4af9d95, to hdfs://master:54310/hbase/archive/data/default/usertable/5446f1f888398e97c0ec3cc30da5bf97/family/55017c5e21444bfc9ce99262f4af9d95
2014-06-30 19:54:29,656 INFO  [regionserver60020-smallCompactions-1404182848217] regionserver.HStore: Completed major compaction of 3 file(s) in family of usertable,user2,1404183160444.5446f1f888398e97c0ec3cc30da5bf97. into 4fba5c6740284ed39067eaf228bf3a3e(size=46.1m), total size for store is 79.5m. This selection was in queue for 0sec, and took 29sec to execute.
2014-06-30 19:54:29,668 INFO  [regionserver60020-smallCompactions-1404182848217] regionserver.CompactSplitThread: Completed compaction: Request = regionName=usertable,user2,1404183160444.5446f1f888398e97c0ec3cc30da5bf97., storeName=family, fileCount=3, fileSize=113.6m, priority=7, time=3382228109773; duration=29sec
2014-06-30 19:54:29,671 DEBUG [regionserver60020-smallCompactions-1404182848217] regionserver.CompactSplitThread: CompactSplitThread Status: compaction_queue=(0:1), split_queue=0, merge_queue=0
2014-06-30 19:54:29,673 DEBUG [regionserver60020-smallCompactions-1404182848217] compactions.RatioBasedCompactionPolicy: Selecting compaction from 3 store files, 0 compacting, 3 eligible, 10 blocking
2014-06-30 19:54:29,674 DEBUG [regionserver60020-smallCompactions-1404182848217] compactions.ExploringCompactionPolicy: Exploring compaction algorithm has selected 3 files of size 123259628 starting at candidate #0 after considering 1 permutations with 1 in ratio
2014-06-30 19:54:29,676 DEBUG [regionserver60020-smallCompactions-1404182848217] regionserver.HStore: 1d66ac045cc0407c0fa426db565fcbba - family: Initiating major compaction
2014-06-30 19:54:29,676 INFO  [regionserver60020-smallCompactions-1404182848217] regionserver.HRegion: Starting compaction on family in region usertable,user5,1404183160444.1d66ac045cc0407c0fa426db565fcbba.
2014-06-30 19:54:29,680 INFO  [regionserver60020-smallCompactions-1404182848217] regionserver.HStore: Starting compaction of 3 file(s) in family of usertable,user5,1404183160444.1d66ac045cc0407c0fa426db565fcbba. into tmpdir=hdfs://master:54310/hbase/data/default/usertable/1d66ac045cc0407c0fa426db565fcbba/.tmp, totalSize=117.5m
2014-06-30 19:54:29,680 DEBUG [regionserver60020-smallCompactions-1404182848217] compactions.Compactor: Compacting hdfs://master:54310/hbase/data/default/usertable/1d66ac045cc0407c0fa426db565fcbba/family/0bb24800daf14d04bf8ce1fe364391ad, keycount=25030, bloomtype=ROW, size=37.3m, encoding=NONE, seqNum=97, earliestPutTs=1404183179490
2014-06-30 19:54:29,681 DEBUG [regionserver60020-smallCompactions-1404182848217] compactions.Compactor: Compacting hdfs://master:54310/hbase/data/default/usertable/1d66ac045cc0407c0fa426db565fcbba/family/8626d549e56643d58192ebde01f19c1a, keycount=25030, bloomtype=ROW, size=37.3m, encoding=NONE, seqNum=172, earliestPutTs=1404183222083
2014-06-30 19:54:29,682 DEBUG [regionserver60020-smallCompactions-1404182848217] compactions.Compactor: Compacting hdfs://master:54310/hbase/data/default/usertable/1d66ac045cc0407c0fa426db565fcbba/family/667959630abb45d49ea7a88945e7adb9, keycount=28790, bloomtype=ROW, size=42.9m, encoding=NONE, seqNum=242, earliestPutTs=1404183235288
2014-06-30 19:54:29,701 DEBUG [regionserver60020-smallCompactions-1404182848217] util.FSUtils: DFS Client does not support most favored nodes create; using default create
2014-06-30 19:54:29,722 DEBUG [MemStoreFlusher.1] util.FSUtils: DFS Client does not support most favored nodes create; using default create
2014-06-30 19:54:29,723 WARN  [RpcServer.handler=0,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":11517,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.58:49969","starttimems":1404183258206,"queuetimems":0,"class":"HRegionServer","responsesize":16217,"method":"Multi"}
2014-06-30 19:54:29,724 WARN  [RpcServer.handler=0,port=60020] ipc.RpcServer: RpcServer.respondercallId: 820 service: ClientService methodName: Multi size: 2.8m connection: 9.1.143.58:49969: output error
2014-06-30 19:54:29,724 WARN  [RpcServer.handler=0,port=60020] ipc.RpcServer: RpcServer.handler=0,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-06-30 19:54:31,713 INFO  [JvmPauseMonitor] util.JvmPauseMonitor: Detected pause in JVM or host machine (eg GC): pause of approximately 1579ms
GC pool 'ConcurrentMarkSweep' had collection(s): count=1 time=1969ms
2014-06-30 19:54:31,732 WARN  [RpcServer.handler=35,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":34759,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.58:49906","starttimems":1404183236972,"queuetimems":0,"class":"HRegionServer","responsesize":17388,"method":"Multi"}
2014-06-30 19:54:31,732 WARN  [RpcServer.handler=35,port=60020] ipc.RpcServer: RpcServer.respondercallId: 667 service: ClientService methodName: Multi size: 3.0m connection: 9.1.143.58:49906: output error
2014-06-30 19:54:31,732 WARN  [RpcServer.handler=35,port=60020] ipc.RpcServer: RpcServer.handler=35,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-06-30 19:54:31,733 WARN  [RpcServer.handler=45,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":34721,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.58:49906","starttimems":1404183237010,"queuetimems":0,"class":"HRegionServer","responsesize":16900,"method":"Multi"}
2014-06-30 19:54:31,733 WARN  [RpcServer.handler=45,port=60020] ipc.RpcServer: RpcServer.respondercallId: 663 service: ClientService methodName: Multi size: 3.0m connection: 9.1.143.58:49906: output error
2014-06-30 19:54:31,733 WARN  [RpcServer.handler=45,port=60020] ipc.RpcServer: RpcServer.handler=45,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-06-30 19:54:31,733 WARN  [RpcServer.handler=17,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":33960,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.58:49906","starttimems":1404183237773,"queuetimems":1,"class":"HRegionServer","responsesize":16760,"method":"Multi"}
2014-06-30 19:54:31,733 WARN  [RpcServer.handler=17,port=60020] ipc.RpcServer: RpcServer.respondercallId: 675 service: ClientService methodName: Multi size: 2.9m connection: 9.1.143.58:49906: output error
2014-06-30 19:54:31,734 WARN  [RpcServer.handler=17,port=60020] ipc.RpcServer: RpcServer.handler=17,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-06-30 19:54:31,734 WARN  [RpcServer.handler=28,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":35014,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.58:49906","starttimems":1404183236719,"queuetimems":1,"class":"HRegionServer","responsesize":16853,"method":"Multi"}
2014-06-30 19:54:31,734 WARN  [RpcServer.handler=28,port=60020] ipc.RpcServer: RpcServer.respondercallId: 668 service: ClientService methodName: Multi size: 2.9m connection: 9.1.143.58:49906: output error
2014-06-30 19:54:31,734 WARN  [RpcServer.handler=28,port=60020] ipc.RpcServer: RpcServer.handler=28,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-06-30 19:54:31,775 WARN  [RpcServer.handler=25,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":34704,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.58:49906","starttimems":1404183237070,"queuetimems":1,"class":"HRegionServer","responsesize":17169,"method":"Multi"}
2014-06-30 19:54:31,775 WARN  [RpcServer.handler=42,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":34660,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.58:49906","starttimems":1404183237114,"queuetimems":1,"class":"HRegionServer","responsesize":17167,"method":"Multi"}
2014-06-30 19:54:31,775 WARN  [RpcServer.handler=8,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":13572,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.58:49969","starttimems":1404183258202,"queuetimems":19,"class":"HRegionServer","responsesize":16372,"method":"Multi"}
2014-06-30 19:54:31,775 WARN  [RpcServer.handler=10,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":34725,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.58:49906","starttimems":1404183237049,"queuetimems":1,"class":"HRegionServer","responsesize":16476,"method":"Multi"}
2014-06-30 19:54:31,775 WARN  [RpcServer.handler=25,port=60020] ipc.RpcServer: RpcServer.respondercallId: 697 service: ClientService methodName: Multi size: 3.0m connection: 9.1.143.58:49906: output error
2014-06-30 19:54:31,775 WARN  [RpcServer.handler=8,port=60020] ipc.RpcServer: RpcServer.respondercallId: 822 service: ClientService methodName: Multi size: 2.9m connection: 9.1.143.58:49969: output error
2014-06-30 19:54:31,775 WARN  [RpcServer.handler=8,port=60020] ipc.RpcServer: RpcServer.handler=8,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-06-30 19:54:31,776 WARN  [RpcServer.handler=26,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":13572,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.58:49969","starttimems":1404183258203,"queuetimems":9,"class":"HRegionServer","responsesize":16395,"method":"Multi"}
2014-06-30 19:54:31,775 WARN  [RpcServer.handler=20,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":13556,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.58:49969","starttimems":1404183258219,"queuetimems":1,"class":"HRegionServer","responsesize":16418,"method":"Multi"}
2014-06-30 19:54:31,777 WARN  [RpcServer.handler=34,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":15953,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.58:49969","starttimems":1404183255823,"queuetimems":198,"class":"HRegionServer","responsesize":15808,"method":"Multi"}
2014-06-30 19:54:31,777 WARN  [RpcServer.handler=26,port=60020] ipc.RpcServer: RpcServer.respondercallId: 821 service: ClientService methodName: Multi size: 2.9m connection: 9.1.143.58:49969: output error
2014-06-30 19:54:31,775 WARN  [RpcServer.handler=10,port=60020] ipc.RpcServer: RpcServer.respondercallId: 698 service: ClientService methodName: Multi size: 2.9m connection: 9.1.143.58:49906: output error
2014-06-30 19:54:31,775 WARN  [RpcServer.handler=25,port=60020] ipc.RpcServer: RpcServer.handler=25,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-06-30 19:54:31,778 WARN  [RpcServer.handler=10,port=60020] ipc.RpcServer: RpcServer.handler=10,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-06-30 19:54:31,778 WARN  [RpcServer.handler=34,port=60020] ipc.RpcServer: RpcServer.respondercallId: 823 service: ClientService methodName: Multi size: 2.8m connection: 9.1.143.58:49969: output error
2014-06-30 19:54:31,778 WARN  [RpcServer.handler=26,port=60020] ipc.RpcServer: RpcServer.handler=26,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-06-30 19:54:31,778 WARN  [RpcServer.handler=34,port=60020] ipc.RpcServer: RpcServer.handler=34,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-06-30 19:54:31,778 WARN  [RpcServer.handler=42,port=60020] ipc.RpcServer: RpcServer.respondercallId: 693 service: ClientService methodName: Multi size: 3.0m connection: 9.1.143.58:49906: output error
2014-06-30 19:54:31,778 WARN  [RpcServer.handler=20,port=60020] ipc.RpcServer: RpcServer.respondercallId: 819 service: ClientService methodName: Multi size: 2.9m connection: 9.1.143.58:49969: output error
2014-06-30 19:54:31,778 WARN  [RpcServer.handler=42,port=60020] ipc.RpcServer: RpcServer.handler=42,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-06-30 19:54:31,778 WARN  [RpcServer.handler=20,port=60020] ipc.RpcServer: RpcServer.handler=20,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-06-30 19:54:31,823 WARN  [RpcServer.handler=40,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":34794,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.58:49906","starttimems":1404183237028,"queuetimems":0,"class":"HRegionServer","responsesize":16217,"method":"Multi"}
2014-06-30 19:54:31,823 WARN  [RpcServer.handler=3,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":10796,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.58:49972","starttimems":1404183261026,"queuetimems":1,"class":"HRegionServer","responsesize":16418,"method":"Multi"}
2014-06-30 19:54:31,823 WARN  [RpcServer.handler=40,port=60020] ipc.RpcServer: RpcServer.respondercallId: 700 service: ClientService methodName: Multi size: 2.8m connection: 9.1.143.58:49906: output error
2014-06-30 19:54:31,823 WARN  [RpcServer.handler=40,port=60020] ipc.RpcServer: RpcServer.handler=40,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-06-30 19:54:31,825 DEBUG [MemStoreFlusher.0] regionserver.MemStoreFlusher: Flush thread woke up because memory above low water=367.3m
2014-06-30 19:54:31,826 INFO  [MemStoreFlusher.0] regionserver.MemStoreFlusher: Flush of region usertable,user7,1404183160444.994b975d2c4cd5bbe532bddfb5e4ac46. due to global heap pressure
2014-06-30 19:54:31,826 DEBUG [MemStoreFlusher.0] regionserver.HRegion: Started memstore flush for usertable,user7,1404183160444.994b975d2c4cd5bbe532bddfb5e4ac46., current region memstore size 104.7m
2014-06-30 19:54:31,829 WARN  [RpcServer.handler=14,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":16008,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.58:49969","starttimems":1404183255821,"queuetimems":282,"class":"HRegionServer","responsesize":16476,"method":"Multi"}
2014-06-30 19:54:31,829 WARN  [RpcServer.handler=14,port=60020] ipc.RpcServer: RpcServer.respondercallId: 817 service: ClientService methodName: Multi size: 2.9m connection: 9.1.143.58:49969: output error
2014-06-30 19:54:31,829 WARN  [RpcServer.handler=14,port=60020] ipc.RpcServer: RpcServer.handler=14,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-06-30 19:54:31,880 WARN  [RpcServer.handler=29,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":10873,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.58:49972","starttimems":1404183261007,"queuetimems":0,"class":"HRegionServer","responsesize":16476,"method":"Multi"}
2014-06-30 19:54:31,920 DEBUG [MemStoreFlusher.0] util.FSUtils: DFS Client does not support most favored nodes create; using default create
2014-06-30 19:54:32,137 INFO  [RpcServer.handler=2,port=60020] regionserver.MemStoreFlusher: Blocking updates on sceplus-vm48.almaden.ibm.com,60020,1404182803191: the global memstore size 387.6m is >= than blocking 386.7m size
2014-06-30 19:54:32,373 INFO  [RpcServer.handler=18,port=60020] regionserver.MemStoreFlusher: Blocking updates on sceplus-vm48.almaden.ibm.com,60020,1404182803191: the global memstore size 387.6m is >= than blocking 386.7m size
2014-06-30 19:54:32,388 INFO  [RpcServer.handler=32,port=60020] regionserver.MemStoreFlusher: Blocking updates on sceplus-vm48.almaden.ibm.com,60020,1404182803191: the global memstore size 387.6m is >= than blocking 386.7m size
2014-06-30 19:54:32,406 INFO  [RpcServer.handler=46,port=60020] regionserver.MemStoreFlusher: Blocking updates on sceplus-vm48.almaden.ibm.com,60020,1404182803191: the global memstore size 387.6m is >= than blocking 386.7m size
2014-06-30 19:54:32,428 INFO  [RpcServer.handler=47,port=60020] regionserver.MemStoreFlusher: Blocking updates on sceplus-vm48.almaden.ibm.com,60020,1404182803191: the global memstore size 387.6m is >= than blocking 386.7m size
2014-06-30 19:54:32,447 INFO  [RpcServer.handler=1,port=60020] regionserver.MemStoreFlusher: Blocking updates on sceplus-vm48.almaden.ibm.com,60020,1404182803191: the global memstore size 387.6m is >= than blocking 386.7m size
2014-06-30 19:54:32,470 INFO  [RpcServer.handler=15,port=60020] regionserver.MemStoreFlusher: Blocking updates on sceplus-vm48.almaden.ibm.com,60020,1404182803191: the global memstore size 387.6m is >= than blocking 386.7m size
2014-06-30 19:54:32,485 INFO  [RpcServer.handler=9,port=60020] regionserver.MemStoreFlusher: Blocking updates on sceplus-vm48.almaden.ibm.com,60020,1404182803191: the global memstore size 387.6m is >= than blocking 386.7m size
2014-06-30 19:54:32,498 INFO  [MemStoreFlusher.0] regionserver.DefaultStoreFlusher: Flushed, sequenceid=254, memsize=63.2m, hasBloomFilter=true, into tmp file hdfs://master:54310/hbase/data/default/usertable/994b975d2c4cd5bbe532bddfb5e4ac46/.tmp/3ee9e75386be4ef5b774fbd319a79f4e
2014-06-30 19:54:32,505 INFO  [RpcServer.handler=36,port=60020] regionserver.MemStoreFlusher: Blocking updates on sceplus-vm48.almaden.ibm.com,60020,1404182803191: the global memstore size 387.6m is >= than blocking 386.7m size
2014-06-30 19:54:32,507 DEBUG [MemStoreFlusher.0] regionserver.HRegionFileSystem: Committing store file hdfs://master:54310/hbase/data/default/usertable/994b975d2c4cd5bbe532bddfb5e4ac46/.tmp/3ee9e75386be4ef5b774fbd319a79f4e as hdfs://master:54310/hbase/data/default/usertable/994b975d2c4cd5bbe532bddfb5e4ac46/family/3ee9e75386be4ef5b774fbd319a79f4e
2014-06-30 19:54:32,515 INFO  [MemStoreFlusher.0] regionserver.HStore: Added hdfs://master:54310/hbase/data/default/usertable/994b975d2c4cd5bbe532bddfb5e4ac46/family/3ee9e75386be4ef5b774fbd319a79f4e, entries=230230, sequenceid=254, filesize=34.3m
2014-06-30 19:54:32,515 INFO  [MemStoreFlusher.0] regionserver.HRegion: Finished memstore flush of ~104.7m/109781360, currentsize=3.2m/3332160 for region usertable,user7,1404183160444.994b975d2c4cd5bbe532bddfb5e4ac46. in 689ms, sequenceid=254, compaction requested=true
2014-06-30 19:54:32,516 DEBUG [MemStoreFlusher.0] regionserver.CompactSplitThread: Small Compaction requested: system; Because: MemStoreFlusher.0; compaction_queue=(0:1), split_queue=0, merge_queue=0
2014-06-30 19:54:32,516 WARN  [RpcServer.handler=36,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 11ms
2014-06-30 19:54:32,516 INFO  [RpcServer.handler=36,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server sceplus-vm48.almaden.ibm.com,60020,1404182803191
2014-06-30 19:54:32,516 WARN  [RpcServer.handler=9,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 31ms
2014-06-30 19:54:32,516 INFO  [RpcServer.handler=9,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server sceplus-vm48.almaden.ibm.com,60020,1404182803191
2014-06-30 19:54:32,516 WARN  [RpcServer.handler=15,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 46ms
2014-06-30 19:54:32,516 INFO  [RpcServer.handler=15,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server sceplus-vm48.almaden.ibm.com,60020,1404182803191
2014-06-30 19:54:32,516 WARN  [RpcServer.handler=1,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 69ms
2014-06-30 19:54:32,516 INFO  [RpcServer.handler=1,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server sceplus-vm48.almaden.ibm.com,60020,1404182803191
2014-06-30 19:54:32,516 WARN  [RpcServer.handler=47,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 88ms
2014-06-30 19:54:32,517 INFO  [RpcServer.handler=47,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server sceplus-vm48.almaden.ibm.com,60020,1404182803191
2014-06-30 19:54:32,518 WARN  [RpcServer.handler=46,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 112ms
2014-06-30 19:54:32,518 INFO  [RpcServer.handler=46,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server sceplus-vm48.almaden.ibm.com,60020,1404182803191
2014-06-30 19:54:32,518 WARN  [RpcServer.handler=32,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 131ms
2014-06-30 19:54:32,518 INFO  [RpcServer.handler=32,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server sceplus-vm48.almaden.ibm.com,60020,1404182803191
2014-06-30 19:54:32,524 WARN  [RpcServer.handler=18,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 151ms
2014-06-30 19:54:32,524 INFO  [RpcServer.handler=18,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server sceplus-vm48.almaden.ibm.com,60020,1404182803191
2014-06-30 19:54:32,524 WARN  [RpcServer.handler=2,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 387ms
2014-06-30 19:54:32,524 INFO  [RpcServer.handler=2,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server sceplus-vm48.almaden.ibm.com,60020,1404182803191
2014-06-30 19:54:32,850 INFO  [MemStoreFlusher.1] regionserver.DefaultStoreFlusher: Flushed, sequenceid=242, memsize=92.7m, hasBloomFilter=true, into tmp file hdfs://master:54310/hbase/data/default/usertable/28938d267c1c61f7feb582b85e4c172f/.tmp/c2f00aa29bc64d308d3d591cba97ca3e
2014-06-30 19:54:32,878 DEBUG [MemStoreFlusher.1] regionserver.HRegionFileSystem: Committing store file hdfs://master:54310/hbase/data/default/usertable/28938d267c1c61f7feb582b85e4c172f/.tmp/c2f00aa29bc64d308d3d591cba97ca3e as hdfs://master:54310/hbase/data/default/usertable/28938d267c1c61f7feb582b85e4c172f/family/c2f00aa29bc64d308d3d591cba97ca3e
2014-06-30 19:54:32,887 INFO  [MemStoreFlusher.1] regionserver.HStore: Added hdfs://master:54310/hbase/data/default/usertable/28938d267c1c61f7feb582b85e4c172f/family/c2f00aa29bc64d308d3d591cba97ca3e, entries=337600, sequenceid=242, filesize=50.3m
2014-06-30 19:54:32,887 INFO  [MemStoreFlusher.1] regionserver.HRegion: Finished memstore flush of ~146.5m/153584960, currentsize=24.7m/25905040 for region usertable,user8,1404183160444.28938d267c1c61f7feb582b85e4c172f. in 3500ms, sequenceid=242, compaction requested=true
2014-06-30 19:54:33,899 DEBUG [MemStoreFlusher.1] regionserver.CompactSplitThread: Small Compaction requested: system; Because: MemStoreFlusher.1; compaction_queue=(0:2), split_queue=0, merge_queue=0
2014-06-30 19:54:34,371 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-06-30 19:54:34,429 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 1233 synced till here 1220
2014-06-30 19:54:34,539 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1404182803191/sceplus-vm48.almaden.ibm.com%2C60020%2C1404182803191.1404183266891 with entries=97, filesize=74.2m; new WAL /hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1404182803191/sceplus-vm48.almaden.ibm.com%2C60020%2C1404182803191.1404183274371
2014-06-30 19:54:36,877 INFO  [JvmPauseMonitor] util.JvmPauseMonitor: Detected pause in JVM or host machine (eg GC): pause of approximately 1433ms
GC pool 'ParNew' had collection(s): count=1 time=0ms
GC pool 'ConcurrentMarkSweep' had collection(s): count=1 time=1642ms
2014-06-30 19:54:37,094 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-06-30 19:54:38,256 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1404182803191/sceplus-vm48.almaden.ibm.com%2C60020%2C1404182803191.1404183274371 with entries=97, filesize=77.5m; new WAL /hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1404182803191/sceplus-vm48.almaden.ibm.com%2C60020%2C1404182803191.1404183277094
2014-06-30 19:54:38,283 DEBUG [MemStoreFlusher.1] regionserver.MemStoreFlusher: Flush thread woke up because memory above low water=367.3m
2014-06-30 19:54:38,283 INFO  [MemStoreFlusher.1] regionserver.MemStoreFlusher: Flush of region usertable,user5,1404183160444.1d66ac045cc0407c0fa426db565fcbba. due to global heap pressure
2014-06-30 19:54:38,283 DEBUG [MemStoreFlusher.1] regionserver.HRegion: Started memstore flush for usertable,user5,1404183160444.1d66ac045cc0407c0fa426db565fcbba., current region memstore size 81.4m
2014-06-30 19:54:38,324 DEBUG [MemStoreFlusher.1] util.FSUtils: DFS Client does not support most favored nodes create; using default create
2014-06-30 19:54:38,459 DEBUG [regionserver60020-smallCompactions-1404182848217] regionserver.HRegionFileSystem: Committing store file hdfs://master:54310/hbase/data/default/usertable/1d66ac045cc0407c0fa426db565fcbba/.tmp/243baa5247984fdcbd140990e4091ee9 as hdfs://master:54310/hbase/data/default/usertable/1d66ac045cc0407c0fa426db565fcbba/family/243baa5247984fdcbd140990e4091ee9
2014-06-30 19:54:38,472 DEBUG [regionserver60020-smallCompactions-1404182848217] regionserver.HStore: Removing store files after compaction...
2014-06-30 19:54:38,479 DEBUG [regionserver60020-smallCompactions-1404182848217] backup.HFileArchiver: Finished archiving from class org.apache.hadoop.hbase.backup.HFileArchiver$FileableStoreFile, file:hdfs://master:54310/hbase/data/default/usertable/1d66ac045cc0407c0fa426db565fcbba/family/0bb24800daf14d04bf8ce1fe364391ad, to hdfs://master:54310/hbase/archive/data/default/usertable/1d66ac045cc0407c0fa426db565fcbba/family/0bb24800daf14d04bf8ce1fe364391ad
2014-06-30 19:54:38,481 DEBUG [regionserver60020-smallCompactions-1404182848217] backup.HFileArchiver: Finished archiving from class org.apache.hadoop.hbase.backup.HFileArchiver$FileableStoreFile, file:hdfs://master:54310/hbase/data/default/usertable/1d66ac045cc0407c0fa426db565fcbba/family/8626d549e56643d58192ebde01f19c1a, to hdfs://master:54310/hbase/archive/data/default/usertable/1d66ac045cc0407c0fa426db565fcbba/family/8626d549e56643d58192ebde01f19c1a
2014-06-30 19:54:38,483 DEBUG [regionserver60020-smallCompactions-1404182848217] backup.HFileArchiver: Finished archiving from class org.apache.hadoop.hbase.backup.HFileArchiver$FileableStoreFile, file:hdfs://master:54310/hbase/data/default/usertable/1d66ac045cc0407c0fa426db565fcbba/family/667959630abb45d49ea7a88945e7adb9, to hdfs://master:54310/hbase/archive/data/default/usertable/1d66ac045cc0407c0fa426db565fcbba/family/667959630abb45d49ea7a88945e7adb9
2014-06-30 19:54:38,483 INFO  [regionserver60020-smallCompactions-1404182848217] regionserver.HStore: Completed major compaction of 3 file(s) in family of usertable,user5,1404183160444.1d66ac045cc0407c0fa426db565fcbba. into 243baa5247984fdcbd140990e4091ee9(size=53.2m), total size for store is 53.2m. This selection was in queue for 0sec, and took 8sec to execute.
2014-06-30 19:54:38,483 INFO  [regionserver60020-smallCompactions-1404182848217] regionserver.CompactSplitThread: Completed compaction: Request = regionName=usertable,user5,1404183160444.1d66ac045cc0407c0fa426db565fcbba., storeName=family, fileCount=3, fileSize=117.5m, priority=7, time=3412031889477; duration=8sec
2014-06-30 19:54:38,483 DEBUG [regionserver60020-smallCompactions-1404182848217] regionserver.CompactSplitThread: CompactSplitThread Status: compaction_queue=(0:2), split_queue=0, merge_queue=0
2014-06-30 19:54:38,484 DEBUG [regionserver60020-smallCompactions-1404182848217] compactions.RatioBasedCompactionPolicy: Selecting compaction from 3 store files, 0 compacting, 3 eligible, 10 blocking
2014-06-30 19:54:38,484 DEBUG [regionserver60020-smallCompactions-1404182848217] compactions.ExploringCompactionPolicy: Exploring compaction algorithm has selected 3 files of size 123046547 starting at candidate #0 after considering 1 permutations with 1 in ratio
2014-06-30 19:54:38,484 DEBUG [regionserver60020-smallCompactions-1404182848217] regionserver.HStore: 994b975d2c4cd5bbe532bddfb5e4ac46 - family: Initiating major compaction
2014-06-30 19:54:38,484 INFO  [regionserver60020-smallCompactions-1404182848217] regionserver.HRegion: Starting compaction on family in region usertable,user7,1404183160444.994b975d2c4cd5bbe532bddfb5e4ac46.
2014-06-30 19:54:38,484 INFO  [regionserver60020-smallCompactions-1404182848217] regionserver.HStore: Starting compaction of 3 file(s) in family of usertable,user7,1404183160444.994b975d2c4cd5bbe532bddfb5e4ac46. into tmpdir=hdfs://master:54310/hbase/data/default/usertable/994b975d2c4cd5bbe532bddfb5e4ac46/.tmp, totalSize=117.3m
2014-06-30 19:54:38,484 DEBUG [regionserver60020-smallCompactions-1404182848217] compactions.Compactor: Compacting hdfs://master:54310/hbase/data/default/usertable/994b975d2c4cd5bbe532bddfb5e4ac46/family/abc6c91e26f946d7973d591194563e37, keycount=25008, bloomtype=ROW, size=37.3m, encoding=NONE, seqNum=100, earliestPutTs=1404183182547
2014-06-30 19:54:38,484 DEBUG [regionserver60020-smallCompactions-1404182848217] compactions.Compactor: Compacting hdfs://master:54310/hbase/data/default/usertable/994b975d2c4cd5bbe532bddfb5e4ac46/family/e832b9bfa4d44567b7d5ce78013c9e6d, keycount=30683, bloomtype=ROW, size=45.8m, encoding=NONE, seqNum=186, earliestPutTs=1404183223115
2014-06-30 19:54:38,484 DEBUG [regionserver60020-smallCompactions-1404182848217] compactions.Compactor: Compacting hdfs://master:54310/hbase/data/default/usertable/994b975d2c4cd5bbe532bddfb5e4ac46/family/3ee9e75386be4ef5b774fbd319a79f4e, keycount=23023, bloomtype=ROW, size=34.3m, encoding=NONE, seqNum=254, earliestPutTs=1404183236467
2014-06-30 19:54:38,491 DEBUG [regionserver60020-smallCompactions-1404182848217] util.FSUtils: DFS Client does not support most favored nodes create; using default create
2014-06-30 19:54:38,639 INFO  [RpcServer.handler=40,port=60020] regionserver.MemStoreFlusher: Blocking updates on sceplus-vm48.almaden.ibm.com,60020,1404182803191: the global memstore size 387.8m is >= than blocking 386.7m size
2014-06-30 19:54:38,856 INFO  [MemStoreFlusher.1] regionserver.DefaultStoreFlusher: Flushed, sequenceid=295, memsize=60.4m, hasBloomFilter=true, into tmp file hdfs://master:54310/hbase/data/default/usertable/1d66ac045cc0407c0fa426db565fcbba/.tmp/0cbbf83c0b3e483c9e50f7fc7fc4676b
2014-06-30 19:54:38,865 DEBUG [MemStoreFlusher.1] regionserver.HRegionFileSystem: Committing store file hdfs://master:54310/hbase/data/default/usertable/1d66ac045cc0407c0fa426db565fcbba/.tmp/0cbbf83c0b3e483c9e50f7fc7fc4676b as hdfs://master:54310/hbase/data/default/usertable/1d66ac045cc0407c0fa426db565fcbba/family/0cbbf83c0b3e483c9e50f7fc7fc4676b
2014-06-30 19:54:38,875 INFO  [MemStoreFlusher.1] regionserver.HStore: Added hdfs://master:54310/hbase/data/default/usertable/1d66ac045cc0407c0fa426db565fcbba/family/0cbbf83c0b3e483c9e50f7fc7fc4676b, entries=220080, sequenceid=295, filesize=32.8m
2014-06-30 19:54:38,875 INFO  [MemStoreFlusher.1] regionserver.HRegion: Finished memstore flush of ~81.4m/85339520, currentsize=3.2m/3384000 for region usertable,user5,1404183160444.1d66ac045cc0407c0fa426db565fcbba. in 592ms, sequenceid=295, compaction requested=false
2014-06-30 19:54:38,876 WARN  [RpcServer.handler=40,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 237ms
2014-06-30 19:54:38,876 INFO  [RpcServer.handler=40,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server sceplus-vm48.almaden.ibm.com,60020,1404182803191
2014-06-30 19:54:40,625 DEBUG [regionserver60020-smallCompactions-1404182848217] regionserver.HRegionFileSystem: Committing store file hdfs://master:54310/hbase/data/default/usertable/994b975d2c4cd5bbe532bddfb5e4ac46/.tmp/0c1ed825a8e94b1daf3c71a2cd6714ae as hdfs://master:54310/hbase/data/default/usertable/994b975d2c4cd5bbe532bddfb5e4ac46/family/0c1ed825a8e94b1daf3c71a2cd6714ae
2014-06-30 19:54:40,638 DEBUG [regionserver60020-smallCompactions-1404182848217] regionserver.HStore: Removing store files after compaction...
2014-06-30 19:54:40,650 DEBUG [regionserver60020-smallCompactions-1404182848217] backup.HFileArchiver: Finished archiving from class org.apache.hadoop.hbase.backup.HFileArchiver$FileableStoreFile, file:hdfs://master:54310/hbase/data/default/usertable/994b975d2c4cd5bbe532bddfb5e4ac46/family/abc6c91e26f946d7973d591194563e37, to hdfs://master:54310/hbase/archive/data/default/usertable/994b975d2c4cd5bbe532bddfb5e4ac46/family/abc6c91e26f946d7973d591194563e37
2014-06-30 19:54:40,652 DEBUG [regionserver60020-smallCompactions-1404182848217] backup.HFileArchiver: Finished archiving from class org.apache.hadoop.hbase.backup.HFileArchiver$FileableStoreFile, file:hdfs://master:54310/hbase/data/default/usertable/994b975d2c4cd5bbe532bddfb5e4ac46/family/e832b9bfa4d44567b7d5ce78013c9e6d, to hdfs://master:54310/hbase/archive/data/default/usertable/994b975d2c4cd5bbe532bddfb5e4ac46/family/e832b9bfa4d44567b7d5ce78013c9e6d
2014-06-30 19:54:40,654 DEBUG [regionserver60020-smallCompactions-1404182848217] backup.HFileArchiver: Finished archiving from class org.apache.hadoop.hbase.backup.HFileArchiver$FileableStoreFile, file:hdfs://master:54310/hbase/data/default/usertable/994b975d2c4cd5bbe532bddfb5e4ac46/family/3ee9e75386be4ef5b774fbd319a79f4e, to hdfs://master:54310/hbase/archive/data/default/usertable/994b975d2c4cd5bbe532bddfb5e4ac46/family/3ee9e75386be4ef5b774fbd319a79f4e
2014-06-30 19:54:40,654 INFO  [regionserver60020-smallCompactions-1404182848217] regionserver.HStore: Completed major compaction of 3 file(s) in family of usertable,user7,1404183160444.994b975d2c4cd5bbe532bddfb5e4ac46. into 0c1ed825a8e94b1daf3c71a2cd6714ae(size=53.2m), total size for store is 53.2m. This selection was in queue for 0sec, and took 2sec to execute.
2014-06-30 19:54:40,655 INFO  [regionserver60020-smallCompactions-1404182848217] regionserver.CompactSplitThread: Completed compaction: Request = regionName=usertable,user7,1404183160444.994b975d2c4cd5bbe532bddfb5e4ac46., storeName=family, fileCount=3, fileSize=117.3m, priority=7, time=3420841593028; duration=2sec
2014-06-30 19:54:40,655 DEBUG [regionserver60020-smallCompactions-1404182848217] regionserver.CompactSplitThread: CompactSplitThread Status: compaction_queue=(0:1), split_queue=0, merge_queue=0
2014-06-30 19:54:40,655 DEBUG [regionserver60020-smallCompactions-1404182848217] compactions.RatioBasedCompactionPolicy: Selecting compaction from 3 store files, 0 compacting, 3 eligible, 10 blocking
2014-06-30 19:54:40,655 DEBUG [regionserver60020-smallCompactions-1404182848217] compactions.ExploringCompactionPolicy: Exploring compaction algorithm has selected 3 files of size 119902245 starting at candidate #0 after considering 1 permutations with 1 in ratio
2014-06-30 19:54:40,655 DEBUG [regionserver60020-smallCompactions-1404182848217] regionserver.HStore: 28938d267c1c61f7feb582b85e4c172f - family: Initiating major compaction
2014-06-30 19:54:40,655 INFO  [regionserver60020-smallCompactions-1404182848217] regionserver.HRegion: Starting compaction on family in region usertable,user8,1404183160444.28938d267c1c61f7feb582b85e4c172f.
2014-06-30 19:54:40,656 INFO  [regionserver60020-smallCompactions-1404182848217] regionserver.HStore: Starting compaction of 3 file(s) in family of usertable,user8,1404183160444.28938d267c1c61f7feb582b85e4c172f. into tmpdir=hdfs://master:54310/hbase/data/default/usertable/28938d267c1c61f7feb582b85e4c172f/.tmp, totalSize=114.3m
2014-06-30 19:54:40,656 DEBUG [regionserver60020-smallCompactions-1404182848217] compactions.Compactor: Compacting hdfs://master:54310/hbase/data/default/usertable/28938d267c1c61f7feb582b85e4c172f/family/2f35affe8d22424bb64a9c60ce6f5911, keycount=20211, bloomtype=ROW, size=30.1m, encoding=NONE, seqNum=67, earliestPutTs=1404183187174
2014-06-30 19:54:40,656 DEBUG [regionserver60020-smallCompactions-1404182848217] compactions.Compactor: Compacting hdfs://master:54310/hbase/data/default/usertable/28938d267c1c61f7feb582b85e4c172f/family/b266bb0b8a3d4b0d818c5fb405aec18f, keycount=22728, bloomtype=ROW, size=33.9m, encoding=NONE, seqNum=145, earliestPutTs=1404183209648
2014-06-30 19:54:40,656 DEBUG [regionserver60020-smallCompactions-1404182848217] compactions.Compactor: Compacting hdfs://master:54310/hbase/data/default/usertable/28938d267c1c61f7feb582b85e4c172f/family/c2f00aa29bc64d308d3d591cba97ca3e, keycount=33760, bloomtype=ROW, size=50.3m, encoding=NONE, seqNum=242, earliestPutTs=1404183230882
2014-06-30 19:54:40,664 DEBUG [regionserver60020-smallCompactions-1404182848217] util.FSUtils: DFS Client does not support most favored nodes create; using default create
2014-06-30 19:54:40,907 DEBUG [MemStoreFlusher.0] regionserver.MemStoreFlusher: Flush thread woke up because memory above low water=367.3m
2014-06-30 19:54:40,907 INFO  [MemStoreFlusher.0] regionserver.MemStoreFlusher: Flush of region usertable,user8,1404183160444.28938d267c1c61f7feb582b85e4c172f. due to global heap pressure
2014-06-30 19:54:40,907 DEBUG [MemStoreFlusher.0] regionserver.HRegion: Started memstore flush for usertable,user8,1404183160444.28938d267c1c61f7feb582b85e4c172f., current region memstore size 99.5m
2014-06-30 19:54:40,994 DEBUG [MemStoreFlusher.0] util.FSUtils: DFS Client does not support most favored nodes create; using default create
2014-06-30 19:54:41,550 INFO  [RpcServer.handler=15,port=60020] regionserver.MemStoreFlusher: Blocking updates on sceplus-vm48.almaden.ibm.com,60020,1404182803191: the global memstore size 388.0m is >= than blocking 386.7m size
2014-06-30 19:54:41,688 INFO  [MemStoreFlusher.0] regionserver.DefaultStoreFlusher: Flushed, sequenceid=308, memsize=76.3m, hasBloomFilter=true, into tmp file hdfs://master:54310/hbase/data/default/usertable/28938d267c1c61f7feb582b85e4c172f/.tmp/cbe61450a0bc4936abb02a53218a6e89
2014-06-30 19:54:41,697 DEBUG [MemStoreFlusher.0] regionserver.HRegionFileSystem: Committing store file hdfs://master:54310/hbase/data/default/usertable/28938d267c1c61f7feb582b85e4c172f/.tmp/cbe61450a0bc4936abb02a53218a6e89 as hdfs://master:54310/hbase/data/default/usertable/28938d267c1c61f7feb582b85e4c172f/family/cbe61450a0bc4936abb02a53218a6e89
2014-06-30 19:54:41,705 INFO  [MemStoreFlusher.0] regionserver.HStore: Added hdfs://master:54310/hbase/data/default/usertable/28938d267c1c61f7feb582b85e4c172f/family/cbe61450a0bc4936abb02a53218a6e89, entries=277780, sequenceid=308, filesize=41.4m
2014-06-30 19:54:41,705 INFO  [MemStoreFlusher.0] regionserver.HRegion: Finished memstore flush of ~99.5m/104343040, currentsize=4.8m/5060000 for region usertable,user8,1404183160444.28938d267c1c61f7feb582b85e4c172f. in 798ms, sequenceid=308, compaction requested=false
2014-06-30 19:54:41,705 WARN  [RpcServer.handler=15,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 155ms
2014-06-30 19:54:41,705 INFO  [RpcServer.handler=15,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server sceplus-vm48.almaden.ibm.com,60020,1404182803191
2014-06-30 19:54:42,342 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-06-30 19:54:42,368 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1404182803191/sceplus-vm48.almaden.ibm.com%2C60020%2C1404182803191.1404183277094 with entries=98, filesize=61.1m; new WAL /hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1404182803191/sceplus-vm48.almaden.ibm.com%2C60020%2C1404182803191.1404183282342
2014-06-30 19:54:42,722 DEBUG [regionserver60020-smallCompactions-1404182848217] regionserver.HRegionFileSystem: Committing store file hdfs://master:54310/hbase/data/default/usertable/28938d267c1c61f7feb582b85e4c172f/.tmp/3a42b3247d78465eb0d13f74ec7379cf as hdfs://master:54310/hbase/data/default/usertable/28938d267c1c61f7feb582b85e4c172f/family/3a42b3247d78465eb0d13f74ec7379cf
2014-06-30 19:54:42,731 DEBUG [regionserver60020-smallCompactions-1404182848217] regionserver.HStore: Removing store files after compaction...
2014-06-30 19:54:42,735 DEBUG [regionserver60020-smallCompactions-1404182848217] backup.HFileArchiver: Finished archiving from class org.apache.hadoop.hbase.backup.HFileArchiver$FileableStoreFile, file:hdfs://master:54310/hbase/data/default/usertable/28938d267c1c61f7feb582b85e4c172f/family/2f35affe8d22424bb64a9c60ce6f5911, to hdfs://master:54310/hbase/archive/data/default/usertable/28938d267c1c61f7feb582b85e4c172f/family/2f35affe8d22424bb64a9c60ce6f5911
2014-06-30 19:54:42,737 DEBUG [regionserver60020-smallCompactions-1404182848217] backup.HFileArchiver: Finished archiving from class org.apache.hadoop.hbase.backup.HFileArchiver$FileableStoreFile, file:hdfs://master:54310/hbase/data/default/usertable/28938d267c1c61f7feb582b85e4c172f/family/b266bb0b8a3d4b0d818c5fb405aec18f, to hdfs://master:54310/hbase/archive/data/default/usertable/28938d267c1c61f7feb582b85e4c172f/family/b266bb0b8a3d4b0d818c5fb405aec18f
2014-06-30 19:54:42,739 DEBUG [regionserver60020-smallCompactions-1404182848217] backup.HFileArchiver: Finished archiving from class org.apache.hadoop.hbase.backup.HFileArchiver$FileableStoreFile, file:hdfs://master:54310/hbase/data/default/usertable/28938d267c1c61f7feb582b85e4c172f/family/c2f00aa29bc64d308d3d591cba97ca3e, to hdfs://master:54310/hbase/archive/data/default/usertable/28938d267c1c61f7feb582b85e4c172f/family/c2f00aa29bc64d308d3d591cba97ca3e
2014-06-30 19:54:42,739 INFO  [regionserver60020-smallCompactions-1404182848217] regionserver.HStore: Completed major compaction of 3 file(s) in family of usertable,user8,1404183160444.28938d267c1c61f7feb582b85e4c172f. into 3a42b3247d78465eb0d13f74ec7379cf(size=53.2m), total size for store is 94.6m. This selection was in queue for 0sec, and took 2sec to execute.
2014-06-30 19:54:42,739 INFO  [regionserver60020-smallCompactions-1404182848217] regionserver.CompactSplitThread: Completed compaction: Request = regionName=usertable,user8,1404183160444.28938d267c1c61f7feb582b85e4c172f., storeName=family, fileCount=3, fileSize=114.3m, priority=7, time=3423012859475; duration=2sec
2014-06-30 19:54:42,740 DEBUG [regionserver60020-smallCompactions-1404182848217] regionserver.CompactSplitThread: CompactSplitThread Status: compaction_queue=(0:0), split_queue=0, merge_queue=0
2014-06-30 19:54:44,760 DEBUG [MemStoreFlusher.1] regionserver.MemStoreFlusher: Flush thread woke up because memory above low water=367.3m
2014-06-30 19:54:44,760 INFO  [MemStoreFlusher.1] regionserver.MemStoreFlusher: Flush of region usertable,user2,1404183160444.5446f1f888398e97c0ec3cc30da5bf97. due to global heap pressure
2014-06-30 19:54:44,760 DEBUG [MemStoreFlusher.1] regionserver.HRegion: Started memstore flush for usertable,user2,1404183160444.5446f1f888398e97c0ec3cc30da5bf97., current region memstore size 111.6m
2014-06-30 19:54:44,824 DEBUG [MemStoreFlusher.1] util.FSUtils: DFS Client does not support most favored nodes create; using default create
2014-06-30 19:54:45,299 INFO  [RpcServer.handler=37,port=60020] regionserver.MemStoreFlusher: Blocking updates on sceplus-vm48.almaden.ibm.com,60020,1404182803191: the global memstore size 388.2m is >= than blocking 386.7m size
2014-06-30 19:54:45,307 INFO  [RpcServer.handler=18,port=60020] regionserver.MemStoreFlusher: Blocking updates on sceplus-vm48.almaden.ibm.com,60020,1404182803191: the global memstore size 389.8m is >= than blocking 386.7m size
2014-06-30 19:54:45,346 INFO  [RpcServer.handler=47,port=60020] regionserver.MemStoreFlusher: Blocking updates on sceplus-vm48.almaden.ibm.com,60020,1404182803191: the global memstore size 389.8m is >= than blocking 386.7m size
2014-06-30 19:54:45,362 INFO  [RpcServer.handler=33,port=60020] regionserver.MemStoreFlusher: Blocking updates on sceplus-vm48.almaden.ibm.com,60020,1404182803191: the global memstore size 389.8m is >= than blocking 386.7m size
2014-06-30 19:54:45,529 INFO  [RpcServer.handler=43,port=60020] regionserver.MemStoreFlusher: Blocking updates on sceplus-vm48.almaden.ibm.com,60020,1404182803191: the global memstore size 389.8m is >= than blocking 386.7m size
2014-06-30 19:54:45,750 INFO  [MemStoreFlusher.1] regionserver.DefaultStoreFlusher: Flushed, sequenceid=322, memsize=100.4m, hasBloomFilter=true, into tmp file hdfs://master:54310/hbase/data/default/usertable/5446f1f888398e97c0ec3cc30da5bf97/.tmp/8ed292bcbde14405bb6976c1e59bf71d
2014-06-30 19:54:45,759 DEBUG [MemStoreFlusher.1] regionserver.HRegionFileSystem: Committing store file hdfs://master:54310/hbase/data/default/usertable/5446f1f888398e97c0ec3cc30da5bf97/.tmp/8ed292bcbde14405bb6976c1e59bf71d as hdfs://master:54310/hbase/data/default/usertable/5446f1f888398e97c0ec3cc30da5bf97/family/8ed292bcbde14405bb6976c1e59bf71d
2014-06-30 19:54:45,767 INFO  [MemStoreFlusher.1] regionserver.HStore: Added hdfs://master:54310/hbase/data/default/usertable/5446f1f888398e97c0ec3cc30da5bf97/family/8ed292bcbde14405bb6976c1e59bf71d, entries=365440, sequenceid=322, filesize=54.5m
2014-06-30 19:54:45,767 INFO  [MemStoreFlusher.1] regionserver.HRegion: Finished memstore flush of ~111.6m/116990720, currentsize=6.5m/6833920 for region usertable,user2,1404183160444.5446f1f888398e97c0ec3cc30da5bf97. in 1007ms, sequenceid=322, compaction requested=true
2014-06-30 19:54:45,768 DEBUG [MemStoreFlusher.1] regionserver.CompactSplitThread: Small Compaction requested: system; Because: MemStoreFlusher.1; compaction_queue=(0:0), split_queue=0, merge_queue=0
2014-06-30 19:54:45,768 DEBUG [regionserver60020-smallCompactions-1404182848217] compactions.RatioBasedCompactionPolicy: Selecting compaction from 3 store files, 0 compacting, 3 eligible, 10 blocking
2014-06-30 19:54:45,768 WARN  [RpcServer.handler=43,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 239ms
2014-06-30 19:54:45,768 DEBUG [regionserver60020-smallCompactions-1404182848217] compactions.ExploringCompactionPolicy: Exploring compaction algorithm has selected 3 files of size 140527407 starting at candidate #0 after considering 1 permutations with 1 in ratio
2014-06-30 19:54:45,768 INFO  [RpcServer.handler=43,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server sceplus-vm48.almaden.ibm.com,60020,1404182803191
2014-06-30 19:54:45,768 DEBUG [regionserver60020-smallCompactions-1404182848217] regionserver.HStore: 5446f1f888398e97c0ec3cc30da5bf97 - family: Initiating major compaction
2014-06-30 19:54:45,768 WARN  [RpcServer.handler=33,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 406ms
2014-06-30 19:54:45,768 INFO  [RpcServer.handler=33,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server sceplus-vm48.almaden.ibm.com,60020,1404182803191
2014-06-30 19:54:45,768 INFO  [regionserver60020-smallCompactions-1404182848217] regionserver.HRegion: Starting compaction on family in region usertable,user2,1404183160444.5446f1f888398e97c0ec3cc30da5bf97.
2014-06-30 19:54:45,768 WARN  [RpcServer.handler=47,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 422ms
2014-06-30 19:54:45,768 INFO  [RpcServer.handler=47,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server sceplus-vm48.almaden.ibm.com,60020,1404182803191
2014-06-30 19:54:45,768 WARN  [RpcServer.handler=18,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 461ms
2014-06-30 19:54:45,768 INFO  [regionserver60020-smallCompactions-1404182848217] regionserver.HStore: Starting compaction of 3 file(s) in family of usertable,user2,1404183160444.5446f1f888398e97c0ec3cc30da5bf97. into tmpdir=hdfs://master:54310/hbase/data/default/usertable/5446f1f888398e97c0ec3cc30da5bf97/.tmp, totalSize=134.0m
2014-06-30 19:54:45,769 INFO  [RpcServer.handler=18,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server sceplus-vm48.almaden.ibm.com,60020,1404182803191
2014-06-30 19:54:45,769 DEBUG [regionserver60020-smallCompactions-1404182848217] compactions.Compactor: Compacting hdfs://master:54310/hbase/data/default/usertable/5446f1f888398e97c0ec3cc30da5bf97/family/4fba5c6740284ed39067eaf228bf3a3e, keycount=31139, bloomtype=ROW, size=46.1m, encoding=NONE, seqNum=188, earliestPutTs=1404183222023
2014-06-30 19:54:45,769 WARN  [RpcServer.handler=37,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 470ms
2014-06-30 19:54:45,769 INFO  [RpcServer.handler=37,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server sceplus-vm48.almaden.ibm.com,60020,1404182803191
2014-06-30 19:54:45,769 DEBUG [regionserver60020-smallCompactions-1404182848217] compactions.Compactor: Compacting hdfs://master:54310/hbase/data/default/usertable/5446f1f888398e97c0ec3cc30da5bf97/family/f623702cd798419f9c8c1fc2f90abb5b, keycount=22404, bloomtype=ROW, size=33.4m, encoding=NONE, seqNum=249, earliestPutTs=1404183236472
2014-06-30 19:54:45,769 DEBUG [regionserver60020-smallCompactions-1404182848217] compactions.Compactor: Compacting hdfs://master:54310/hbase/data/default/usertable/5446f1f888398e97c0ec3cc30da5bf97/family/8ed292bcbde14405bb6976c1e59bf71d, keycount=36544, bloomtype=ROW, size=54.5m, encoding=NONE, seqNum=322, earliestPutTs=1404183272049
2014-06-30 19:54:45,780 DEBUG [regionserver60020-smallCompactions-1404182848217] util.FSUtils: DFS Client does not support most favored nodes create; using default create
2014-06-30 19:54:46,011 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-06-30 19:54:46,040 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 1526 synced till here 1524
2014-06-30 19:54:46,052 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1404182803191/sceplus-vm48.almaden.ibm.com%2C60020%2C1404182803191.1404183282342 with entries=98, filesize=62.6m; new WAL /hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1404182803191/sceplus-vm48.almaden.ibm.com%2C60020%2C1404182803191.1404183286011
2014-06-30 19:54:47,272 DEBUG [MemStoreFlusher.0] regionserver.MemStoreFlusher: Flush thread woke up because memory above low water=367.3m
2014-06-30 19:54:47,272 INFO  [MemStoreFlusher.0] regionserver.MemStoreFlusher: Flush of region usertable,user7,1404183160444.994b975d2c4cd5bbe532bddfb5e4ac46. due to global heap pressure
2014-06-30 19:54:47,272 DEBUG [MemStoreFlusher.0] regionserver.HRegion: Started memstore flush for usertable,user7,1404183160444.994b975d2c4cd5bbe532bddfb5e4ac46., current region memstore size 126.1m
2014-06-30 19:54:47,346 DEBUG [RpcServer.handler=24,port=60020] regionserver.HRegion: Flush requested on usertable,user7,1404183160444.994b975d2c4cd5bbe532bddfb5e4ac46.
2014-06-30 19:54:47,347 DEBUG [MemStoreFlusher.1] regionserver.HRegion: NOT flushing memstore for region usertable,user7,1404183160444.994b975d2c4cd5bbe532bddfb5e4ac46., flushing=true, writesEnabled=true
2014-06-30 19:54:47,350 DEBUG [MemStoreFlusher.1] regionserver.MemStoreFlusher: Flush thread woke up because memory above low water=367.3m
2014-06-30 19:54:47,350 INFO  [MemStoreFlusher.1] regionserver.MemStoreFlusher: Flush of region usertable,user9,1404183160445.9638edaa7b212ccfbe14e9984bd5a5a3. due to global heap pressure
2014-06-30 19:54:47,350 DEBUG [MemStoreFlusher.1] regionserver.HRegion: Started memstore flush for usertable,user9,1404183160445.9638edaa7b212ccfbe14e9984bd5a5a3., current region memstore size 99.0m
2014-06-30 19:54:47,385 DEBUG [MemStoreFlusher.0] util.FSUtils: DFS Client does not support most favored nodes create; using default create
2014-06-30 19:54:47,474 DEBUG [MemStoreFlusher.1] util.FSUtils: DFS Client does not support most favored nodes create; using default create
2014-06-30 19:54:47,495 INFO  [RpcServer.handler=30,port=60020] regionserver.MemStoreFlusher: Blocking updates on sceplus-vm48.almaden.ibm.com,60020,1404182803191: the global memstore size 387.2m is >= than blocking 386.7m size
2014-06-30 19:54:47,500 INFO  [RpcServer.handler=22,port=60020] regionserver.MemStoreFlusher: Blocking updates on sceplus-vm48.almaden.ibm.com,60020,1404182803191: the global memstore size 387.2m is >= than blocking 386.7m size
2014-06-30 19:54:47,500 INFO  [RpcServer.handler=37,port=60020] regionserver.MemStoreFlusher: Blocking updates on sceplus-vm48.almaden.ibm.com,60020,1404182803191: the global memstore size 387.2m is >= than blocking 386.7m size
2014-06-30 19:54:47,520 INFO  [RpcServer.handler=32,port=60020] regionserver.MemStoreFlusher: Blocking updates on sceplus-vm48.almaden.ibm.com,60020,1404182803191: the global memstore size 390.5m is >= than blocking 386.7m size
2014-06-30 19:54:47,520 INFO  [RpcServer.handler=49,port=60020] regionserver.MemStoreFlusher: Blocking updates on sceplus-vm48.almaden.ibm.com,60020,1404182803191: the global memstore size 390.5m is >= than blocking 386.7m size
2014-06-30 19:54:47,765 INFO  [RpcServer.handler=18,port=60020] regionserver.MemStoreFlusher: Blocking updates on sceplus-vm48.almaden.ibm.com,60020,1404182803191: the global memstore size 390.5m is >= than blocking 386.7m size
2014-06-30 19:54:47,793 INFO  [RpcServer.handler=15,port=60020] regionserver.MemStoreFlusher: Blocking updates on sceplus-vm48.almaden.ibm.com,60020,1404182803191: the global memstore size 390.5m is >= than blocking 386.7m size
2014-06-30 19:54:47,814 INFO  [RpcServer.handler=45,port=60020] regionserver.MemStoreFlusher: Blocking updates on sceplus-vm48.almaden.ibm.com,60020,1404182803191: the global memstore size 390.5m is >= than blocking 386.7m size
2014-06-30 19:54:47,815 INFO  [RpcServer.handler=28,port=60020] regionserver.MemStoreFlusher: Blocking updates on sceplus-vm48.almaden.ibm.com,60020,1404182803191: the global memstore size 390.5m is >= than blocking 386.7m size
2014-06-30 19:54:48,135 INFO  [RpcServer.handler=43,port=60020] regionserver.MemStoreFlusher: Blocking updates on sceplus-vm48.almaden.ibm.com,60020,1404182803191: the global memstore size 390.5m is >= than blocking 386.7m size
2014-06-30 19:54:48,668 INFO  [MemStoreFlusher.1] regionserver.DefaultStoreFlusher: Flushed, sequenceid=261, memsize=49.0m, hasBloomFilter=true, into tmp file hdfs://master:54310/hbase/data/default/usertable/9638edaa7b212ccfbe14e9984bd5a5a3/.tmp/efa312ba9eeb461ab5ba43ef8a4093b0
2014-06-30 19:54:48,689 DEBUG [MemStoreFlusher.1] regionserver.HRegionFileSystem: Committing store file hdfs://master:54310/hbase/data/default/usertable/9638edaa7b212ccfbe14e9984bd5a5a3/.tmp/efa312ba9eeb461ab5ba43ef8a4093b0 as hdfs://master:54310/hbase/data/default/usertable/9638edaa7b212ccfbe14e9984bd5a5a3/family/efa312ba9eeb461ab5ba43ef8a4093b0
2014-06-30 19:54:48,985 INFO  [MemStoreFlusher.0] regionserver.DefaultStoreFlusher: Flushed, sequenceid=337, memsize=122.9m, hasBloomFilter=true, into tmp file hdfs://master:54310/hbase/data/default/usertable/994b975d2c4cd5bbe532bddfb5e4ac46/.tmp/b49a3c92980d4a0684a89071ca69d4a7
2014-06-30 19:54:48,995 DEBUG [regionserver60020-smallCompactions-1404182848217] regionserver.HRegionFileSystem: Committing store file hdfs://master:54310/hbase/data/default/usertable/5446f1f888398e97c0ec3cc30da5bf97/.tmp/abd1f6805378412a8e3d65c12d64c335 as hdfs://master:54310/hbase/data/default/usertable/5446f1f888398e97c0ec3cc30da5bf97/family/abd1f6805378412a8e3d65c12d64c335
2014-06-30 19:54:48,996 DEBUG [MemStoreFlusher.0] regionserver.HRegionFileSystem: Committing store file hdfs://master:54310/hbase/data/default/usertable/994b975d2c4cd5bbe532bddfb5e4ac46/.tmp/b49a3c92980d4a0684a89071ca69d4a7 as hdfs://master:54310/hbase/data/default/usertable/994b975d2c4cd5bbe532bddfb5e4ac46/family/b49a3c92980d4a0684a89071ca69d4a7
2014-06-30 19:54:49,315 INFO  [MemStoreFlusher.1] regionserver.HStore: Added hdfs://master:54310/hbase/data/default/usertable/9638edaa7b212ccfbe14e9984bd5a5a3/family/efa312ba9eeb461ab5ba43ef8a4093b0, entries=178380, sequenceid=261, filesize=26.6m
2014-06-30 19:54:49,316 INFO  [MemStoreFlusher.1] regionserver.HRegion: Finished memstore flush of ~99.0m/103795920, currentsize=1.4m/1425520 for region usertable,user9,1404183160445.9638edaa7b212ccfbe14e9984bd5a5a3. in 1965ms, sequenceid=261, compaction requested=false
2014-06-30 19:54:49,316 WARN  [RpcServer.handler=43,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 1181ms
2014-06-30 19:54:49,316 INFO  [RpcServer.handler=43,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server sceplus-vm48.almaden.ibm.com,60020,1404182803191
2014-06-30 19:54:49,316 WARN  [RpcServer.handler=28,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 1501ms
2014-06-30 19:54:49,316 INFO  [RpcServer.handler=28,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server sceplus-vm48.almaden.ibm.com,60020,1404182803191
2014-06-30 19:54:49,316 WARN  [RpcServer.handler=45,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 1502ms
2014-06-30 19:54:49,316 INFO  [RpcServer.handler=45,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server sceplus-vm48.almaden.ibm.com,60020,1404182803191
2014-06-30 19:54:49,316 WARN  [RpcServer.handler=15,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 1523ms
2014-06-30 19:54:49,317 INFO  [RpcServer.handler=15,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server sceplus-vm48.almaden.ibm.com,60020,1404182803191
2014-06-30 19:54:49,318 WARN  [RpcServer.handler=18,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 1553ms
2014-06-30 19:54:49,318 INFO  [RpcServer.handler=18,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server sceplus-vm48.almaden.ibm.com,60020,1404182803191
2014-06-30 19:54:49,319 WARN  [RpcServer.handler=49,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 1800ms
2014-06-30 19:54:49,319 INFO  [RpcServer.handler=49,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server sceplus-vm48.almaden.ibm.com,60020,1404182803191
2014-06-30 19:54:49,319 WARN  [RpcServer.handler=32,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 1800ms
2014-06-30 19:54:49,319 INFO  [RpcServer.handler=32,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server sceplus-vm48.almaden.ibm.com,60020,1404182803191
2014-06-30 19:54:49,319 WARN  [RpcServer.handler=37,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 1819ms
2014-06-30 19:54:49,320 INFO  [RpcServer.handler=37,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server sceplus-vm48.almaden.ibm.com,60020,1404182803191
2014-06-30 19:54:49,322 WARN  [RpcServer.handler=22,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 1822ms
2014-06-30 19:54:49,322 INFO  [RpcServer.handler=22,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server sceplus-vm48.almaden.ibm.com,60020,1404182803191
2014-06-30 19:54:49,322 WARN  [RpcServer.handler=30,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 1828ms
2014-06-30 19:54:49,322 INFO  [RpcServer.handler=30,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server sceplus-vm48.almaden.ibm.com,60020,1404182803191
2014-06-30 19:54:49,431 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-06-30 19:54:49,689 INFO  [MemStoreFlusher.0] regionserver.HStore: Added hdfs://master:54310/hbase/data/default/usertable/994b975d2c4cd5bbe532bddfb5e4ac46/family/b49a3c92980d4a0684a89071ca69d4a7, entries=447370, sequenceid=337, filesize=66.7m
2014-06-30 19:54:49,690 INFO  [MemStoreFlusher.0] regionserver.HRegion: Finished memstore flush of ~127.7m/133897520, currentsize=14.5m/15180320 for region usertable,user7,1404183160444.994b975d2c4cd5bbe532bddfb5e4ac46. in 2418ms, sequenceid=337, compaction requested=false
2014-06-30 19:54:49,694 DEBUG [regionserver60020-smallCompactions-1404182848217] regionserver.HStore: Removing store files after compaction...
2014-06-30 19:54:50,412 DEBUG [regionserver60020-smallCompactions-1404182848217] backup.HFileArchiver: Finished archiving from class org.apache.hadoop.hbase.backup.HFileArchiver$FileableStoreFile, file:hdfs://master:54310/hbase/data/default/usertable/5446f1f888398e97c0ec3cc30da5bf97/family/4fba5c6740284ed39067eaf228bf3a3e, to hdfs://master:54310/hbase/archive/data/default/usertable/5446f1f888398e97c0ec3cc30da5bf97/family/4fba5c6740284ed39067eaf228bf3a3e
2014-06-30 19:54:50,504 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 1690 synced till here 1689
2014-06-30 19:54:50,530 DEBUG [regionserver60020-smallCompactions-1404182848217] backup.HFileArchiver: Finished archiving from class org.apache.hadoop.hbase.backup.HFileArchiver$FileableStoreFile, file:hdfs://master:54310/hbase/data/default/usertable/5446f1f888398e97c0ec3cc30da5bf97/family/f623702cd798419f9c8c1fc2f90abb5b, to hdfs://master:54310/hbase/archive/data/default/usertable/5446f1f888398e97c0ec3cc30da5bf97/family/f623702cd798419f9c8c1fc2f90abb5b
2014-06-30 19:54:50,533 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1404182803191/sceplus-vm48.almaden.ibm.com%2C60020%2C1404182803191.1404183286011 with entries=164, filesize=106.6m; new WAL /hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1404182803191/sceplus-vm48.almaden.ibm.com%2C60020%2C1404182803191.1404183289432
2014-06-30 19:54:50,534 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1404182803191/sceplus-vm48.almaden.ibm.com%2C60020%2C1404182803191.1404182834317
2014-06-30 19:54:50,534 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1404182803191/sceplus-vm48.almaden.ibm.com%2C60020%2C1404182803191.1404183182522
2014-06-30 19:54:50,534 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1404182803191/sceplus-vm48.almaden.ibm.com%2C60020%2C1404182803191.1404183194900
2014-06-30 19:54:50,534 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1404182803191/sceplus-vm48.almaden.ibm.com%2C60020%2C1404182803191.1404183203628
2014-06-30 19:54:50,534 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1404182803191/sceplus-vm48.almaden.ibm.com%2C60020%2C1404182803191.1404183214620
2014-06-30 19:54:50,534 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1404182803191/sceplus-vm48.almaden.ibm.com%2C60020%2C1404182803191.1404183221344
2014-06-30 19:54:50,534 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1404182803191/sceplus-vm48.almaden.ibm.com%2C60020%2C1404182803191.1404183225093
2014-06-30 19:54:50,534 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1404182803191/sceplus-vm48.almaden.ibm.com%2C60020%2C1404182803191.1404183229800
2014-06-30 19:54:50,534 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1404182803191/sceplus-vm48.almaden.ibm.com%2C60020%2C1404182803191.1404183235340
2014-06-30 19:54:50,534 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1404182803191/sceplus-vm48.almaden.ibm.com%2C60020%2C1404182803191.1404183242233
2014-06-30 19:54:50,534 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1404182803191/sceplus-vm48.almaden.ibm.com%2C60020%2C1404182803191.1404183249883
2014-06-30 19:54:50,534 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1404182803191/sceplus-vm48.almaden.ibm.com%2C60020%2C1404182803191.1404183266891
2014-06-30 19:54:50,534 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1404182803191/sceplus-vm48.almaden.ibm.com%2C60020%2C1404182803191.1404183274371
2014-06-30 19:54:50,535 DEBUG [regionserver60020-smallCompactions-1404182848217] backup.HFileArchiver: Finished archiving from class org.apache.hadoop.hbase.backup.HFileArchiver$FileableStoreFile, file:hdfs://master:54310/hbase/data/default/usertable/5446f1f888398e97c0ec3cc30da5bf97/family/8ed292bcbde14405bb6976c1e59bf71d, to hdfs://master:54310/hbase/archive/data/default/usertable/5446f1f888398e97c0ec3cc30da5bf97/family/8ed292bcbde14405bb6976c1e59bf71d
2014-06-30 19:54:50,535 INFO  [regionserver60020-smallCompactions-1404182848217] regionserver.HStore: Completed major compaction of 3 file(s) in family of usertable,user2,1404183160444.5446f1f888398e97c0ec3cc30da5bf97. into abd1f6805378412a8e3d65c12d64c335(size=74.8m), total size for store is 74.8m. This selection was in queue for 0sec, and took 4sec to execute.
2014-06-30 19:54:50,535 INFO  [regionserver60020-smallCompactions-1404182848217] regionserver.CompactSplitThread: Completed compaction: Request = regionName=usertable,user2,1404183160444.5446f1f888398e97c0ec3cc30da5bf97., storeName=family, fileCount=3, fileSize=134.0m, priority=7, time=3428125897882; duration=4sec
2014-06-30 19:54:50,537 DEBUG [regionserver60020-smallCompactions-1404182848217] regionserver.CompactSplitThread: CompactSplitThread Status: compaction_queue=(0:0), split_queue=0, merge_queue=0
2014-06-30 19:54:51,944 DEBUG [MemStoreFlusher.0] regionserver.MemStoreFlusher: Flush thread woke up because memory above low water=367.3m
2014-06-30 19:54:51,944 INFO  [MemStoreFlusher.0] regionserver.MemStoreFlusher: Flush of region usertable,user5,1404183160444.1d66ac045cc0407c0fa426db565fcbba. due to global heap pressure
2014-06-30 19:54:51,944 DEBUG [MemStoreFlusher.0] regionserver.HRegion: Started memstore flush for usertable,user5,1404183160444.1d66ac045cc0407c0fa426db565fcbba., current region memstore size 121.0m
2014-06-30 19:54:52,026 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-06-30 19:54:52,046 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 1788 synced till here 1786
2014-06-30 19:54:52,064 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1404182803191/sceplus-vm48.almaden.ibm.com%2C60020%2C1404182803191.1404183289432 with entries=98, filesize=62.0m; new WAL /hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1404182803191/sceplus-vm48.almaden.ibm.com%2C60020%2C1404182803191.1404183292027
2014-06-30 19:54:52,072 DEBUG [MemStoreFlusher.0] util.FSUtils: DFS Client does not support most favored nodes create; using default create
2014-06-30 19:54:52,244 INFO  [RpcServer.handler=23,port=60020] regionserver.MemStoreFlusher: Blocking updates on sceplus-vm48.almaden.ibm.com,60020,1404182803191: the global memstore size 387.8m is >= than blocking 386.7m size
2014-06-30 19:54:52,248 INFO  [RpcServer.handler=33,port=60020] regionserver.MemStoreFlusher: Blocking updates on sceplus-vm48.almaden.ibm.com,60020,1404182803191: the global memstore size 388.3m is >= than blocking 386.7m size
2014-06-30 19:54:52,359 INFO  [RpcServer.handler=19,port=60020] regionserver.MemStoreFlusher: Blocking updates on sceplus-vm48.almaden.ibm.com,60020,1404182803191: the global memstore size 389.9m is >= than blocking 386.7m size
2014-06-30 19:54:52,360 INFO  [RpcServer.handler=0,port=60020] regionserver.MemStoreFlusher: Blocking updates on sceplus-vm48.almaden.ibm.com,60020,1404182803191: the global memstore size 389.9m is >= than blocking 386.7m size
2014-06-30 19:54:52,519 INFO  [RpcServer.handler=37,port=60020] regionserver.MemStoreFlusher: Blocking updates on sceplus-vm48.almaden.ibm.com,60020,1404182803191: the global memstore size 389.9m is >= than blocking 386.7m size
2014-06-30 19:54:52,554 INFO  [RpcServer.handler=24,port=60020] regionserver.MemStoreFlusher: Blocking updates on sceplus-vm48.almaden.ibm.com,60020,1404182803191: the global memstore size 389.9m is >= than blocking 386.7m size
2014-06-30 19:54:52,555 INFO  [RpcServer.handler=41,port=60020] regionserver.MemStoreFlusher: Blocking updates on sceplus-vm48.almaden.ibm.com,60020,1404182803191: the global memstore size 389.9m is >= than blocking 386.7m size
2014-06-30 19:54:53,030 INFO  [MemStoreFlusher.0] regionserver.DefaultStoreFlusher: Flushed, sequenceid=372, memsize=114.5m, hasBloomFilter=true, into tmp file hdfs://master:54310/hbase/data/default/usertable/1d66ac045cc0407c0fa426db565fcbba/.tmp/59a9ee279a4e4b8ba5064a989e7a1042
2014-06-30 19:54:53,039 DEBUG [MemStoreFlusher.0] regionserver.HRegionFileSystem: Committing store file hdfs://master:54310/hbase/data/default/usertable/1d66ac045cc0407c0fa426db565fcbba/.tmp/59a9ee279a4e4b8ba5064a989e7a1042 as hdfs://master:54310/hbase/data/default/usertable/1d66ac045cc0407c0fa426db565fcbba/family/59a9ee279a4e4b8ba5064a989e7a1042
2014-06-30 19:54:53,052 INFO  [MemStoreFlusher.0] regionserver.HStore: Added hdfs://master:54310/hbase/data/default/usertable/1d66ac045cc0407c0fa426db565fcbba/family/59a9ee279a4e4b8ba5064a989e7a1042, entries=416910, sequenceid=372, filesize=62.1m
2014-06-30 19:54:53,052 INFO  [MemStoreFlusher.0] regionserver.HRegion: Finished memstore flush of ~121.0m/126849440, currentsize=3.2m/3346560 for region usertable,user5,1404183160444.1d66ac045cc0407c0fa426db565fcbba. in 1108ms, sequenceid=372, compaction requested=true
2014-06-30 19:54:53,052 DEBUG [MemStoreFlusher.0] regionserver.CompactSplitThread: Small Compaction requested: system; Because: MemStoreFlusher.0; compaction_queue=(0:0), split_queue=0, merge_queue=0
2014-06-30 19:54:53,053 DEBUG [regionserver60020-smallCompactions-1404182848217] compactions.RatioBasedCompactionPolicy: Selecting compaction from 3 store files, 0 compacting, 3 eligible, 10 blocking
2014-06-30 19:54:53,053 WARN  [RpcServer.handler=41,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 498ms
2014-06-30 19:54:53,053 DEBUG [regionserver60020-smallCompactions-1404182848217] compactions.ExploringCompactionPolicy: Exploring compaction algorithm has selected 3 files of size 155360594 starting at candidate #0 after considering 1 permutations with 1 in ratio
2014-06-30 19:54:53,053 INFO  [RpcServer.handler=41,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server sceplus-vm48.almaden.ibm.com,60020,1404182803191
2014-06-30 19:54:53,053 DEBUG [regionserver60020-smallCompactions-1404182848217] regionserver.HStore: 1d66ac045cc0407c0fa426db565fcbba - family: Initiating major compaction
2014-06-30 19:54:53,053 WARN  [RpcServer.handler=24,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 499ms
2014-06-30 19:54:53,053 INFO  [RpcServer.handler=24,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server sceplus-vm48.almaden.ibm.com,60020,1404182803191
2014-06-30 19:54:53,053 INFO  [regionserver60020-smallCompactions-1404182848217] regionserver.HRegion: Starting compaction on family in region usertable,user5,1404183160444.1d66ac045cc0407c0fa426db565fcbba.
2014-06-30 19:54:53,053 WARN  [RpcServer.handler=37,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 534ms
2014-06-30 19:54:53,053 INFO  [RpcServer.handler=37,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server sceplus-vm48.almaden.ibm.com,60020,1404182803191
2014-06-30 19:54:53,054 WARN  [RpcServer.handler=0,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 694ms
2014-06-30 19:54:53,054 INFO  [regionserver60020-smallCompactions-1404182848217] regionserver.HStore: Starting compaction of 3 file(s) in family of usertable,user5,1404183160444.1d66ac045cc0407c0fa426db565fcbba. into tmpdir=hdfs://master:54310/hbase/data/default/usertable/1d66ac045cc0407c0fa426db565fcbba/.tmp, totalSize=148.2m
2014-06-30 19:54:53,054 INFO  [RpcServer.handler=0,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server sceplus-vm48.almaden.ibm.com,60020,1404182803191
2014-06-30 19:54:53,055 DEBUG [regionserver60020-smallCompactions-1404182848217] compactions.Compactor: Compacting hdfs://master:54310/hbase/data/default/usertable/1d66ac045cc0407c0fa426db565fcbba/family/243baa5247984fdcbd140990e4091ee9, keycount=35918, bloomtype=ROW, size=53.2m, encoding=NONE, seqNum=242, earliestPutTs=1404183222083
2014-06-30 19:54:53,055 DEBUG [regionserver60020-smallCompactions-1404182848217] compactions.Compactor: Compacting hdfs://master:54310/hbase/data/default/usertable/1d66ac045cc0407c0fa426db565fcbba/family/0cbbf83c0b3e483c9e50f7fc7fc4676b, keycount=22008, bloomtype=ROW, size=32.8m, encoding=NONE, seqNum=295, earliestPutTs=1404183272079
2014-06-30 19:54:53,055 WARN  [RpcServer.handler=19,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 696ms
2014-06-30 19:54:53,055 DEBUG [regionserver60020-smallCompactions-1404182848217] compactions.Compactor: Compacting hdfs://master:54310/hbase/data/default/usertable/1d66ac045cc0407c0fa426db565fcbba/family/59a9ee279a4e4b8ba5064a989e7a1042, keycount=41691, bloomtype=ROW, size=62.1m, encoding=NONE, seqNum=372, earliestPutTs=1404183278440
2014-06-30 19:54:53,055 INFO  [RpcServer.handler=19,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server sceplus-vm48.almaden.ibm.com,60020,1404182803191
2014-06-30 19:54:53,055 WARN  [RpcServer.handler=33,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 807ms
2014-06-30 19:54:53,055 INFO  [RpcServer.handler=33,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server sceplus-vm48.almaden.ibm.com,60020,1404182803191
2014-06-30 19:54:53,055 WARN  [RpcServer.handler=23,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 812ms
2014-06-30 19:54:53,055 INFO  [RpcServer.handler=23,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server sceplus-vm48.almaden.ibm.com,60020,1404182803191
2014-06-30 19:54:53,069 DEBUG [regionserver60020-smallCompactions-1404182848217] util.FSUtils: DFS Client does not support most favored nodes create; using default create
2014-06-30 19:54:54,556 DEBUG [MemStoreFlusher.1] regionserver.HRegion: Started memstore flush for usertable,user8,1404183160444.28938d267c1c61f7feb582b85e4c172f., current region memstore size 128.2m
2014-06-30 19:54:54,557 DEBUG [RpcServer.handler=31,port=60020] regionserver.HRegion: Flush requested on usertable,user8,1404183160444.28938d267c1c61f7feb582b85e4c172f.
2014-06-30 19:54:54,632 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-06-30 19:54:54,675 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 1891 synced till here 1888
2014-06-30 19:54:54,689 DEBUG [MemStoreFlusher.0] regionserver.MemStoreFlusher: Flush thread woke up because memory above low water=367.3m
2014-06-30 19:54:54,689 INFO  [MemStoreFlusher.0] regionserver.MemStoreFlusher: Flush of region usertable,user2,1404183160444.5446f1f888398e97c0ec3cc30da5bf97. due to global heap pressure
2014-06-30 19:54:54,689 DEBUG [MemStoreFlusher.0] regionserver.HRegion: Started memstore flush for usertable,user2,1404183160444.5446f1f888398e97c0ec3cc30da5bf97., current region memstore size 111.0m
2014-06-30 19:54:54,745 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1404182803191/sceplus-vm48.almaden.ibm.com%2C60020%2C1404182803191.1404183292027 with entries=103, filesize=64.1m; new WAL /hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1404182803191/sceplus-vm48.almaden.ibm.com%2C60020%2C1404182803191.1404183294633
2014-06-30 19:54:54,755 DEBUG [MemStoreFlusher.1] util.FSUtils: DFS Client does not support most favored nodes create; using default create
2014-06-30 19:54:55,173 INFO  [RpcServer.handler=14,port=60020] regionserver.MemStoreFlusher: Blocking updates on sceplus-vm48.almaden.ibm.com,60020,1404182803191: the global memstore size 386.8m is >= than blocking 386.7m size
2014-06-30 19:54:55,178 INFO  [RpcServer.handler=4,port=60020] regionserver.MemStoreFlusher: Blocking updates on sceplus-vm48.almaden.ibm.com,60020,1404182803191: the global memstore size 388.3m is >= than blocking 386.7m size
2014-06-30 19:54:55,188 INFO  [RpcServer.handler=5,port=60020] regionserver.MemStoreFlusher: Blocking updates on sceplus-vm48.almaden.ibm.com,60020,1404182803191: the global memstore size 388.3m is >= than blocking 386.7m size
2014-06-30 19:54:55,195 INFO  [RpcServer.handler=47,port=60020] regionserver.MemStoreFlusher: Blocking updates on sceplus-vm48.almaden.ibm.com,60020,1404182803191: the global memstore size 391.8m is >= than blocking 386.7m size
2014-06-30 19:54:55,196 INFO  [RpcServer.handler=46,port=60020] regionserver.MemStoreFlusher: Blocking updates on sceplus-vm48.almaden.ibm.com,60020,1404182803191: the global memstore size 391.8m is >= than blocking 386.7m size
2014-06-30 19:54:55,206 INFO  [RpcServer.handler=23,port=60020] regionserver.MemStoreFlusher: Blocking updates on sceplus-vm48.almaden.ibm.com,60020,1404182803191: the global memstore size 391.8m is >= than blocking 386.7m size
2014-06-30 19:54:55,221 INFO  [RpcServer.handler=0,port=60020] regionserver.MemStoreFlusher: Blocking updates on sceplus-vm48.almaden.ibm.com,60020,1404182803191: the global memstore size 393.4m is >= than blocking 386.7m size
2014-06-30 19:54:55,222 INFO  [RpcServer.handler=45,port=60020] regionserver.MemStoreFlusher: Blocking updates on sceplus-vm48.almaden.ibm.com,60020,1404182803191: the global memstore size 393.4m is >= than blocking 386.7m size
2014-06-30 19:54:55,223 INFO  [RpcServer.handler=41,port=60020] regionserver.MemStoreFlusher: Blocking updates on sceplus-vm48.almaden.ibm.com,60020,1404182803191: the global memstore size 395.1m is >= than blocking 386.7m size
2014-06-30 19:54:55,225 INFO  [RpcServer.handler=33,port=60020] regionserver.MemStoreFlusher: Blocking updates on sceplus-vm48.almaden.ibm.com,60020,1404182803191: the global memstore size 395.1m is >= than blocking 386.7m size
2014-06-30 19:54:55,227 INFO  [RpcServer.handler=24,port=60020] regionserver.MemStoreFlusher: Blocking updates on sceplus-vm48.almaden.ibm.com,60020,1404182803191: the global memstore size 396.8m is >= than blocking 386.7m size
2014-06-30 19:54:55,234 INFO  [RpcServer.handler=37,port=60020] regionserver.MemStoreFlusher: Blocking updates on sceplus-vm48.almaden.ibm.com,60020,1404182803191: the global memstore size 400.0m is >= than blocking 386.7m size
2014-06-30 19:54:55,234 DEBUG [MemStoreFlusher.0] util.FSUtils: DFS Client does not support most favored nodes create; using default create
2014-06-30 19:54:55,236 INFO  [RpcServer.handler=19,port=60020] regionserver.MemStoreFlusher: Blocking updates on sceplus-vm48.almaden.ibm.com,60020,1404182803191: the global memstore size 400.0m is >= than blocking 386.7m size
2014-06-30 19:54:55,236 INFO  [RpcServer.handler=8,port=60020] regionserver.MemStoreFlusher: Blocking updates on sceplus-vm48.almaden.ibm.com,60020,1404182803191: the global memstore size 400.0m is >= than blocking 386.7m size
2014-06-30 19:54:55,243 INFO  [RpcServer.handler=20,port=60020] regionserver.MemStoreFlusher: Blocking updates on sceplus-vm48.almaden.ibm.com,60020,1404182803191: the global memstore size 400.0m is >= than blocking 386.7m size
2014-06-30 19:54:55,259 INFO  [RpcServer.handler=13,port=60020] regionserver.MemStoreFlusher: Blocking updates on sceplus-vm48.almaden.ibm.com,60020,1404182803191: the global memstore size 400.0m is >= than blocking 386.7m size
2014-06-30 19:54:55,286 INFO  [RpcServer.handler=25,port=60020] regionserver.MemStoreFlusher: Blocking updates on sceplus-vm48.almaden.ibm.com,60020,1404182803191: the global memstore size 400.0m is >= than blocking 386.7m size
2014-06-30 19:54:55,315 INFO  [RpcServer.handler=43,port=60020] regionserver.MemStoreFlusher: Blocking updates on sceplus-vm48.almaden.ibm.com,60020,1404182803191: the global memstore size 400.0m is >= than blocking 386.7m size
2014-06-30 19:54:57,249 INFO  [JvmPauseMonitor] util.JvmPauseMonitor: Detected pause in JVM or host machine (eg GC): pause of approximately 1627ms
GC pool 'ParNew' had collection(s): count=1 time=0ms
GC pool 'ConcurrentMarkSweep' had collection(s): count=1 time=1925ms
2014-06-30 19:54:57,268 INFO  [RpcServer.handler=28,port=60020] regionserver.MemStoreFlusher: Blocking updates on sceplus-vm48.almaden.ibm.com,60020,1404182803191: the global memstore size 400.0m is >= than blocking 386.7m size
2014-06-30 19:54:57,269 INFO  [RpcServer.handler=49,port=60020] regionserver.MemStoreFlusher: Blocking updates on sceplus-vm48.almaden.ibm.com,60020,1404182803191: the global memstore size 400.0m is >= than blocking 386.7m size
2014-06-30 19:54:57,281 INFO  [RpcServer.handler=10,port=60020] regionserver.MemStoreFlusher: Blocking updates on sceplus-vm48.almaden.ibm.com,60020,1404182803191: the global memstore size 400.0m is >= than blocking 386.7m size
2014-06-30 19:54:57,285 INFO  [RpcServer.handler=17,port=60020] regionserver.MemStoreFlusher: Blocking updates on sceplus-vm48.almaden.ibm.com,60020,1404182803191: the global memstore size 400.0m is >= than blocking 386.7m size
2014-06-30 19:54:57,294 INFO  [RpcServer.handler=18,port=60020] regionserver.MemStoreFlusher: Blocking updates on sceplus-vm48.almaden.ibm.com,60020,1404182803191: the global memstore size 400.0m is >= than blocking 386.7m size
2014-06-30 19:54:57,306 INFO  [RpcServer.handler=26,port=60020] regionserver.MemStoreFlusher: Blocking updates on sceplus-vm48.almaden.ibm.com,60020,1404182803191: the global memstore size 400.0m is >= than blocking 386.7m size
2014-06-30 19:54:57,317 INFO  [RpcServer.handler=32,port=60020] regionserver.MemStoreFlusher: Blocking updates on sceplus-vm48.almaden.ibm.com,60020,1404182803191: the global memstore size 400.0m is >= than blocking 386.7m size
2014-06-30 19:54:57,335 INFO  [RpcServer.handler=22,port=60020] regionserver.MemStoreFlusher: Blocking updates on sceplus-vm48.almaden.ibm.com,60020,1404182803191: the global memstore size 400.0m is >= than blocking 386.7m size
2014-06-30 19:54:57,346 INFO  [RpcServer.handler=34,port=60020] regionserver.MemStoreFlusher: Blocking updates on sceplus-vm48.almaden.ibm.com,60020,1404182803191: the global memstore size 400.0m is >= than blocking 386.7m size
2014-06-30 19:54:57,474 INFO  [RpcServer.handler=30,port=60020] regionserver.MemStoreFlusher: Blocking updates on sceplus-vm48.almaden.ibm.com,60020,1404182803191: the global memstore size 400.0m is >= than blocking 386.7m size
2014-06-30 19:54:57,495 INFO  [RpcServer.handler=48,port=60020] regionserver.MemStoreFlusher: Blocking updates on sceplus-vm48.almaden.ibm.com,60020,1404182803191: the global memstore size 400.0m is >= than blocking 386.7m size
2014-06-30 19:54:57,512 INFO  [RpcServer.handler=16,port=60020] regionserver.MemStoreFlusher: Blocking updates on sceplus-vm48.almaden.ibm.com,60020,1404182803191: the global memstore size 400.0m is >= than blocking 386.7m size
2014-06-30 19:54:57,527 INFO  [RpcServer.handler=44,port=60020] regionserver.MemStoreFlusher: Blocking updates on sceplus-vm48.almaden.ibm.com,60020,1404182803191: the global memstore size 400.0m is >= than blocking 386.7m size
2014-06-30 19:54:57,815 INFO  [RpcServer.handler=42,port=60020] regionserver.MemStoreFlusher: Blocking updates on sceplus-vm48.almaden.ibm.com,60020,1404182803191: the global memstore size 400.0m is >= than blocking 386.7m size
2014-06-30 19:54:57,825 INFO  [RpcServer.handler=3,port=60020] regionserver.MemStoreFlusher: Blocking updates on sceplus-vm48.almaden.ibm.com,60020,1404182803191: the global memstore size 400.0m is >= than blocking 386.7m size
2014-06-30 19:54:57,840 INFO  [RpcServer.handler=27,port=60020] regionserver.MemStoreFlusher: Blocking updates on sceplus-vm48.almaden.ibm.com,60020,1404182803191: the global memstore size 400.0m is >= than blocking 386.7m size
2014-06-30 19:54:57,844 INFO  [RpcServer.handler=36,port=60020] regionserver.MemStoreFlusher: Blocking updates on sceplus-vm48.almaden.ibm.com,60020,1404182803191: the global memstore size 400.0m is >= than blocking 386.7m size
2014-06-30 19:54:57,844 INFO  [RpcServer.handler=2,port=60020] regionserver.MemStoreFlusher: Blocking updates on sceplus-vm48.almaden.ibm.com,60020,1404182803191: the global memstore size 400.0m is >= than blocking 386.7m size
2014-06-30 19:54:57,856 INFO  [RpcServer.handler=7,port=60020] regionserver.MemStoreFlusher: Blocking updates on sceplus-vm48.almaden.ibm.com,60020,1404182803191: the global memstore size 400.0m is >= than blocking 386.7m size
2014-06-30 19:54:57,868 INFO  [RpcServer.handler=15,port=60020] regionserver.MemStoreFlusher: Blocking updates on sceplus-vm48.almaden.ibm.com,60020,1404182803191: the global memstore size 400.0m is >= than blocking 386.7m size
2014-06-30 19:54:57,878 INFO  [RpcServer.handler=40,port=60020] regionserver.MemStoreFlusher: Blocking updates on sceplus-vm48.almaden.ibm.com,60020,1404182803191: the global memstore size 400.0m is >= than blocking 386.7m size
2014-06-30 19:54:57,903 INFO  [RpcServer.handler=39,port=60020] regionserver.MemStoreFlusher: Blocking updates on sceplus-vm48.almaden.ibm.com,60020,1404182803191: the global memstore size 400.0m is >= than blocking 386.7m size
2014-06-30 19:54:57,922 INFO  [RpcServer.handler=29,port=60020] regionserver.MemStoreFlusher: Blocking updates on sceplus-vm48.almaden.ibm.com,60020,1404182803191: the global memstore size 400.0m is >= than blocking 386.7m size
2014-06-30 19:54:57,942 INFO  [RpcServer.handler=38,port=60020] regionserver.MemStoreFlusher: Blocking updates on sceplus-vm48.almaden.ibm.com,60020,1404182803191: the global memstore size 400.0m is >= than blocking 386.7m size
2014-06-30 19:54:57,963 INFO  [RpcServer.handler=21,port=60020] regionserver.MemStoreFlusher: Blocking updates on sceplus-vm48.almaden.ibm.com,60020,1404182803191: the global memstore size 400.0m is >= than blocking 386.7m size
2014-06-30 19:54:57,985 INFO  [RpcServer.handler=35,port=60020] regionserver.MemStoreFlusher: Blocking updates on sceplus-vm48.almaden.ibm.com,60020,1404182803191: the global memstore size 400.0m is >= than blocking 386.7m size
2014-06-30 19:54:58,005 INFO  [RpcServer.handler=11,port=60020] regionserver.MemStoreFlusher: Blocking updates on sceplus-vm48.almaden.ibm.com,60020,1404182803191: the global memstore size 400.0m is >= than blocking 386.7m size
2014-06-30 19:54:59,771 INFO  [JvmPauseMonitor] util.JvmPauseMonitor: Detected pause in JVM or host machine (eg GC): pause of approximately 1477ms
GC pool 'ParNew' had collection(s): count=1 time=0ms
GC pool 'ConcurrentMarkSweep' had collection(s): count=2 time=1770ms
2014-06-30 19:54:59,781 INFO  [RpcServer.handler=1,port=60020] regionserver.MemStoreFlusher: Blocking updates on sceplus-vm48.almaden.ibm.com,60020,1404182803191: the global memstore size 400.0m is >= than blocking 386.7m size
2014-06-30 19:54:59,792 INFO  [RpcServer.handler=31,port=60020] regionserver.MemStoreFlusher: Blocking updates on sceplus-vm48.almaden.ibm.com,60020,1404182803191: the global memstore size 400.0m is >= than blocking 386.7m size
2014-06-30 19:54:59,803 INFO  [RpcServer.handler=6,port=60020] regionserver.MemStoreFlusher: Blocking updates on sceplus-vm48.almaden.ibm.com,60020,1404182803191: the global memstore size 400.0m is >= than blocking 386.7m size
2014-06-30 19:54:59,804 INFO  [RpcServer.handler=12,port=60020] regionserver.MemStoreFlusher: Blocking updates on sceplus-vm48.almaden.ibm.com,60020,1404182803191: the global memstore size 400.0m is >= than blocking 386.7m size
2014-06-30 19:54:59,805 INFO  [RpcServer.handler=9,port=60020] regionserver.MemStoreFlusher: Blocking updates on sceplus-vm48.almaden.ibm.com,60020,1404182803191: the global memstore size 400.0m is >= than blocking 386.7m size
2014-06-30 19:55:00,133 INFO  [MemStoreFlusher.1] regionserver.DefaultStoreFlusher: Flushed, sequenceid=391, memsize=123.3m, hasBloomFilter=true, into tmp file hdfs://master:54310/hbase/data/default/usertable/28938d267c1c61f7feb582b85e4c172f/.tmp/5e35681f42734d87ad30ea719516990f
2014-06-30 19:55:00,142 INFO  [MemStoreFlusher.0] regionserver.DefaultStoreFlusher: Flushed, sequenceid=395, memsize=107.9m, hasBloomFilter=true, into tmp file hdfs://master:54310/hbase/data/default/usertable/5446f1f888398e97c0ec3cc30da5bf97/.tmp/bb59d24afb2a4ba4b8ab45700e1bcb78
2014-06-30 19:55:00,146 DEBUG [MemStoreFlusher.1] regionserver.HRegionFileSystem: Committing store file hdfs://master:54310/hbase/data/default/usertable/28938d267c1c61f7feb582b85e4c172f/.tmp/5e35681f42734d87ad30ea719516990f as hdfs://master:54310/hbase/data/default/usertable/28938d267c1c61f7feb582b85e4c172f/family/5e35681f42734d87ad30ea719516990f
2014-06-30 19:55:00,151 DEBUG [MemStoreFlusher.0] regionserver.HRegionFileSystem: Committing store file hdfs://master:54310/hbase/data/default/usertable/5446f1f888398e97c0ec3cc30da5bf97/.tmp/bb59d24afb2a4ba4b8ab45700e1bcb78 as hdfs://master:54310/hbase/data/default/usertable/5446f1f888398e97c0ec3cc30da5bf97/family/bb59d24afb2a4ba4b8ab45700e1bcb78
2014-06-30 19:55:00,154 INFO  [MemStoreFlusher.1] regionserver.HStore: Added hdfs://master:54310/hbase/data/default/usertable/28938d267c1c61f7feb582b85e4c172f/family/5e35681f42734d87ad30ea719516990f, entries=449020, sequenceid=391, filesize=66.9m
2014-06-30 19:55:00,154 INFO  [MemStoreFlusher.1] regionserver.HRegion: Finished memstore flush of ~129.8m/136106000, currentsize=3.0m/3191040 for region usertable,user8,1404183160444.28938d267c1c61f7feb582b85e4c172f. in 5598ms, sequenceid=391, compaction requested=true
2014-06-30 19:55:00,154 DEBUG [MemStoreFlusher.1] regionserver.CompactSplitThread: Small Compaction requested: system; Because: MemStoreFlusher.1; compaction_queue=(0:1), split_queue=0, merge_queue=0
2014-06-30 19:55:00,155 WARN  [RpcServer.handler=9,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 351ms
2014-06-30 19:55:00,155 INFO  [RpcServer.handler=9,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server sceplus-vm48.almaden.ibm.com,60020,1404182803191
2014-06-30 19:55:00,155 WARN  [RpcServer.handler=12,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 351ms
2014-06-30 19:55:00,155 INFO  [RpcServer.handler=12,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server sceplus-vm48.almaden.ibm.com,60020,1404182803191
2014-06-30 19:55:00,155 WARN  [RpcServer.handler=6,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 352ms
2014-06-30 19:55:00,155 INFO  [RpcServer.handler=6,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server sceplus-vm48.almaden.ibm.com,60020,1404182803191
2014-06-30 19:55:00,159 WARN  [RpcServer.handler=31,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 367ms
2014-06-30 19:55:00,159 INFO  [RpcServer.handler=31,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server sceplus-vm48.almaden.ibm.com,60020,1404182803191
2014-06-30 19:55:00,159 WARN  [RpcServer.handler=1,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 378ms
2014-06-30 19:55:00,159 INFO  [RpcServer.handler=1,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server sceplus-vm48.almaden.ibm.com,60020,1404182803191
2014-06-30 19:55:00,159 WARN  [RpcServer.handler=11,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 2154ms
2014-06-30 19:55:00,159 INFO  [RpcServer.handler=11,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server sceplus-vm48.almaden.ibm.com,60020,1404182803191
2014-06-30 19:55:00,160 WARN  [RpcServer.handler=35,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 2175ms
2014-06-30 19:55:00,160 INFO  [RpcServer.handler=35,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server sceplus-vm48.almaden.ibm.com,60020,1404182803191
2014-06-30 19:55:00,160 INFO  [MemStoreFlusher.0] regionserver.HStore: Added hdfs://master:54310/hbase/data/default/usertable/5446f1f888398e97c0ec3cc30da5bf97/family/bb59d24afb2a4ba4b8ab45700e1bcb78, entries=393030, sequenceid=395, filesize=58.6m
2014-06-30 19:55:00,160 INFO  [MemStoreFlusher.0] regionserver.HRegion: Finished memstore flush of ~114.4m/119910640, currentsize=7.8m/8173280 for region usertable,user2,1404183160444.5446f1f888398e97c0ec3cc30da5bf97. in 5471ms, sequenceid=395, compaction requested=false
2014-06-30 19:55:00,164 WARN  [RpcServer.handler=21,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 2201ms
2014-06-30 19:55:00,164 INFO  [RpcServer.handler=21,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server sceplus-vm48.almaden.ibm.com,60020,1404182803191
2014-06-30 19:55:00,164 WARN  [RpcServer.handler=38,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 2222ms
2014-06-30 19:55:00,164 INFO  [RpcServer.handler=38,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server sceplus-vm48.almaden.ibm.com,60020,1404182803191
2014-06-30 19:55:00,164 WARN  [RpcServer.handler=29,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 2242ms
2014-06-30 19:55:00,167 INFO  [RpcServer.handler=29,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server sceplus-vm48.almaden.ibm.com,60020,1404182803191
2014-06-30 19:55:00,170 WARN  [RpcServer.handler=39,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 2266ms
2014-06-30 19:55:00,171 INFO  [RpcServer.handler=39,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server sceplus-vm48.almaden.ibm.com,60020,1404182803191
2014-06-30 19:55:00,172 WARN  [RpcServer.handler=40,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 2294ms
2014-06-30 19:55:00,172 INFO  [RpcServer.handler=40,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server sceplus-vm48.almaden.ibm.com,60020,1404182803191
2014-06-30 19:55:00,172 WARN  [RpcServer.handler=15,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 2304ms
2014-06-30 19:55:00,173 INFO  [RpcServer.handler=15,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server sceplus-vm48.almaden.ibm.com,60020,1404182803191
2014-06-30 19:55:00,174 WARN  [RpcServer.handler=7,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 2318ms
2014-06-30 19:55:00,174 INFO  [RpcServer.handler=7,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server sceplus-vm48.almaden.ibm.com,60020,1404182803191
2014-06-30 19:55:00,174 WARN  [RpcServer.handler=2,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 2330ms
2014-06-30 19:55:00,180 INFO  [RpcServer.handler=2,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server sceplus-vm48.almaden.ibm.com,60020,1404182803191
2014-06-30 19:55:00,184 WARN  [RpcServer.handler=36,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 2340ms
2014-06-30 19:55:00,185 INFO  [RpcServer.handler=36,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server sceplus-vm48.almaden.ibm.com,60020,1404182803191
2014-06-30 19:55:00,185 WARN  [RpcServer.handler=27,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 2345ms
2014-06-30 19:55:00,185 INFO  [RpcServer.handler=27,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server sceplus-vm48.almaden.ibm.com,60020,1404182803191
2014-06-30 19:55:00,185 WARN  [RpcServer.handler=3,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 2360ms
2014-06-30 19:55:00,185 INFO  [RpcServer.handler=3,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server sceplus-vm48.almaden.ibm.com,60020,1404182803191
2014-06-30 19:55:00,185 WARN  [RpcServer.handler=42,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 2370ms
2014-06-30 19:55:00,188 INFO  [RpcServer.handler=42,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server sceplus-vm48.almaden.ibm.com,60020,1404182803191
2014-06-30 19:55:00,189 WARN  [RpcServer.handler=44,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 2662ms
2014-06-30 19:55:00,189 INFO  [RpcServer.handler=44,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server sceplus-vm48.almaden.ibm.com,60020,1404182803191
2014-06-30 19:55:00,189 WARN  [RpcServer.handler=16,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 2677ms
2014-06-30 19:55:00,189 INFO  [RpcServer.handler=16,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server sceplus-vm48.almaden.ibm.com,60020,1404182803191
2014-06-30 19:55:00,189 WARN  [RpcServer.handler=48,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 2694ms
2014-06-30 19:55:00,189 INFO  [RpcServer.handler=48,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server sceplus-vm48.almaden.ibm.com,60020,1404182803191
2014-06-30 19:55:00,190 WARN  [RpcServer.handler=30,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 2716ms
2014-06-30 19:55:00,190 INFO  [RpcServer.handler=30,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server sceplus-vm48.almaden.ibm.com,60020,1404182803191
2014-06-30 19:55:00,190 WARN  [RpcServer.handler=34,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 2844ms
2014-06-30 19:55:00,190 INFO  [RpcServer.handler=34,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server sceplus-vm48.almaden.ibm.com,60020,1404182803191
2014-06-30 19:55:00,190 WARN  [RpcServer.handler=22,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 2856ms
2014-06-30 19:55:00,190 INFO  [RpcServer.handler=22,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server sceplus-vm48.almaden.ibm.com,60020,1404182803191
2014-06-30 19:55:00,190 WARN  [RpcServer.handler=32,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 2873ms
2014-06-30 19:55:00,194 INFO  [RpcServer.handler=32,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server sceplus-vm48.almaden.ibm.com,60020,1404182803191
2014-06-30 19:55:00,194 WARN  [RpcServer.handler=26,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 2888ms
2014-06-30 19:55:00,195 INFO  [RpcServer.handler=26,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server sceplus-vm48.almaden.ibm.com,60020,1404182803191
2014-06-30 19:55:00,196 WARN  [RpcServer.handler=18,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 2901ms
2014-06-30 19:55:00,196 INFO  [RpcServer.handler=18,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server sceplus-vm48.almaden.ibm.com,60020,1404182803191
2014-06-30 19:55:00,196 WARN  [RpcServer.handler=17,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 2911ms
2014-06-30 19:55:00,201 INFO  [RpcServer.handler=17,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server sceplus-vm48.almaden.ibm.com,60020,1404182803191
2014-06-30 19:55:00,201 WARN  [RpcServer.handler=10,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 2920ms
2014-06-30 19:55:00,210 INFO  [RpcServer.handler=10,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server sceplus-vm48.almaden.ibm.com,60020,1404182803191
2014-06-30 19:55:00,211 WARN  [RpcServer.handler=49,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 2942ms
2014-06-30 19:55:00,212 INFO  [RpcServer.handler=49,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server sceplus-vm48.almaden.ibm.com,60020,1404182803191
2014-06-30 19:55:00,214 WARN  [RpcServer.handler=28,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 2946ms
2014-06-30 19:55:00,214 INFO  [RpcServer.handler=28,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server sceplus-vm48.almaden.ibm.com,60020,1404182803191
2014-06-30 19:55:00,214 WARN  [RpcServer.handler=43,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 4899ms
2014-06-30 19:55:00,215 INFO  [RpcServer.handler=43,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server sceplus-vm48.almaden.ibm.com,60020,1404182803191
2014-06-30 19:55:00,215 WARN  [RpcServer.handler=25,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 4929ms
2014-06-30 19:55:00,222 INFO  [RpcServer.handler=25,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server sceplus-vm48.almaden.ibm.com,60020,1404182803191
2014-06-30 19:55:00,222 WARN  [RpcServer.handler=13,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 4963ms
2014-06-30 19:55:00,222 INFO  [RpcServer.handler=13,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server sceplus-vm48.almaden.ibm.com,60020,1404182803191
2014-06-30 19:55:00,222 WARN  [RpcServer.handler=20,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 4979ms
2014-06-30 19:55:00,223 INFO  [RpcServer.handler=20,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server sceplus-vm48.almaden.ibm.com,60020,1404182803191
2014-06-30 19:55:00,223 WARN  [RpcServer.handler=45,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5001ms
2014-06-30 19:55:00,224 INFO  [RpcServer.handler=45,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server sceplus-vm48.almaden.ibm.com,60020,1404182803191
2014-06-30 19:55:00,224 WARN  [RpcServer.handler=8,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 4988ms
2014-06-30 19:55:00,224 INFO  [RpcServer.handler=8,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server sceplus-vm48.almaden.ibm.com,60020,1404182803191
2014-06-30 19:55:00,224 WARN  [RpcServer.handler=19,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 4988ms
2014-06-30 19:55:00,224 INFO  [RpcServer.handler=19,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server sceplus-vm48.almaden.ibm.com,60020,1404182803191
2014-06-30 19:55:00,224 WARN  [RpcServer.handler=37,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 4990ms
2014-06-30 19:55:00,224 INFO  [RpcServer.handler=37,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server sceplus-vm48.almaden.ibm.com,60020,1404182803191
2014-06-30 19:55:00,225 WARN  [RpcServer.handler=24,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 4998ms
2014-06-30 19:55:00,225 INFO  [RpcServer.handler=24,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server sceplus-vm48.almaden.ibm.com,60020,1404182803191
2014-06-30 19:55:00,225 WARN  [RpcServer.handler=33,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-06-30 19:55:00,225 INFO  [RpcServer.handler=33,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server sceplus-vm48.almaden.ibm.com,60020,1404182803191
2014-06-30 19:55:00,226 WARN  [RpcServer.handler=41,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5002ms
2014-06-30 19:55:00,226 INFO  [RpcServer.handler=41,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server sceplus-vm48.almaden.ibm.com,60020,1404182803191
2014-06-30 19:55:00,235 WARN  [RpcServer.handler=0,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5005ms
2014-06-30 19:55:00,235 INFO  [RpcServer.handler=0,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server sceplus-vm48.almaden.ibm.com,60020,1404182803191
2014-06-30 19:55:00,240 WARN  [RpcServer.handler=23,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5029ms
2014-06-30 19:55:00,251 INFO  [RpcServer.handler=23,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server sceplus-vm48.almaden.ibm.com,60020,1404182803191
2014-06-30 19:55:00,251 WARN  [RpcServer.handler=46,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5055ms
2014-06-30 19:55:00,251 INFO  [RpcServer.handler=46,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server sceplus-vm48.almaden.ibm.com,60020,1404182803191
2014-06-30 19:55:00,251 WARN  [RpcServer.handler=47,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5056ms
2014-06-30 19:55:00,252 INFO  [RpcServer.handler=47,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server sceplus-vm48.almaden.ibm.com,60020,1404182803191
2014-06-30 19:55:00,253 WARN  [RpcServer.handler=5,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5066ms
2014-06-30 19:55:00,254 INFO  [RpcServer.handler=5,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server sceplus-vm48.almaden.ibm.com,60020,1404182803191
2014-06-30 19:55:00,256 WARN  [RpcServer.handler=4,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5078ms
2014-06-30 19:55:00,256 INFO  [RpcServer.handler=4,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server sceplus-vm48.almaden.ibm.com,60020,1404182803191
2014-06-30 19:55:00,259 WARN  [RpcServer.handler=14,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5085ms
2014-06-30 19:55:00,259 INFO  [RpcServer.handler=14,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server sceplus-vm48.almaden.ibm.com,60020,1404182803191
2014-06-30 19:55:02,111 INFO  [JvmPauseMonitor] util.JvmPauseMonitor: Detected pause in JVM or host machine (eg GC): pause of approximately 1339ms
GC pool 'ConcurrentMarkSweep' had collection(s): count=1 time=1706ms
2014-06-30 19:55:04,160 INFO  [JvmPauseMonitor] util.JvmPauseMonitor: Detected pause in JVM or host machine (eg GC): pause of approximately 1548ms
GC pool 'ParNew' had collection(s): count=1 time=0ms
GC pool 'ConcurrentMarkSweep' had collection(s): count=1 time=1851ms
2014-06-30 19:55:04,238 DEBUG [regionserver60020-smallCompactions-1404182848217] regionserver.HRegionFileSystem: Committing store file hdfs://master:54310/hbase/data/default/usertable/1d66ac045cc0407c0fa426db565fcbba/.tmp/5f0613413be6424184c334d89b1ade67 as hdfs://master:54310/hbase/data/default/usertable/1d66ac045cc0407c0fa426db565fcbba/family/5f0613413be6424184c334d89b1ade67
2014-06-30 19:55:04,520 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-06-30 19:55:04,522 DEBUG [regionserver60020-smallCompactions-1404182848217] regionserver.HStore: Removing store files after compaction...
2014-06-30 19:55:04,532 DEBUG [regionserver60020-smallCompactions-1404182848217] backup.HFileArchiver: Finished archiving from class org.apache.hadoop.hbase.backup.HFileArchiver$FileableStoreFile, file:hdfs://master:54310/hbase/data/default/usertable/1d66ac045cc0407c0fa426db565fcbba/family/243baa5247984fdcbd140990e4091ee9, to hdfs://master:54310/hbase/archive/data/default/usertable/1d66ac045cc0407c0fa426db565fcbba/family/243baa5247984fdcbd140990e4091ee9
2014-06-30 19:55:04,542 DEBUG [regionserver60020-smallCompactions-1404182848217] backup.HFileArchiver: Finished archiving from class org.apache.hadoop.hbase.backup.HFileArchiver$FileableStoreFile, file:hdfs://master:54310/hbase/data/default/usertable/1d66ac045cc0407c0fa426db565fcbba/family/0cbbf83c0b3e483c9e50f7fc7fc4676b, to hdfs://master:54310/hbase/archive/data/default/usertable/1d66ac045cc0407c0fa426db565fcbba/family/0cbbf83c0b3e483c9e50f7fc7fc4676b
2014-06-30 19:55:04,577 DEBUG [regionserver60020-smallCompactions-1404182848217] backup.HFileArchiver: Finished archiving from class org.apache.hadoop.hbase.backup.HFileArchiver$FileableStoreFile, file:hdfs://master:54310/hbase/data/default/usertable/1d66ac045cc0407c0fa426db565fcbba/family/59a9ee279a4e4b8ba5064a989e7a1042, to hdfs://master:54310/hbase/archive/data/default/usertable/1d66ac045cc0407c0fa426db565fcbba/family/59a9ee279a4e4b8ba5064a989e7a1042
2014-06-30 19:55:04,577 INFO  [regionserver60020-smallCompactions-1404182848217] regionserver.HStore: Completed major compaction of 3 file(s) in family of usertable,user5,1404183160444.1d66ac045cc0407c0fa426db565fcbba. into 5f0613413be6424184c334d89b1ade67(size=112.3m), total size for store is 112.3m. This selection was in queue for 0sec, and took 11sec to execute.
2014-06-30 19:55:04,577 INFO  [regionserver60020-smallCompactions-1404182848217] regionserver.CompactSplitThread: Completed compaction: Request = regionName=usertable,user5,1404183160444.1d66ac045cc0407c0fa426db565fcbba., storeName=family, fileCount=3, fileSize=148.2m, priority=7, time=3435410677582; duration=11sec
2014-06-30 19:55:04,577 DEBUG [regionserver60020-smallCompactions-1404182848217] regionserver.CompactSplitThread: CompactSplitThread Status: compaction_queue=(0:1), split_queue=0, merge_queue=0
2014-06-30 19:55:04,577 DEBUG [regionserver60020-smallCompactions-1404182848217] compactions.RatioBasedCompactionPolicy: Selecting compaction from 3 store files, 0 compacting, 3 eligible, 10 blocking
2014-06-30 19:55:04,578 DEBUG [regionserver60020-smallCompactions-1404182848217] compactions.ExploringCompactionPolicy: Exploring compaction algorithm has selected 3 files of size 169426232 starting at candidate #0 after considering 1 permutations with 1 in ratio
2014-06-30 19:55:04,578 DEBUG [regionserver60020-smallCompactions-1404182848217] regionserver.HStore: 28938d267c1c61f7feb582b85e4c172f - family: Initiating major compaction
2014-06-30 19:55:04,578 INFO  [regionserver60020-smallCompactions-1404182848217] regionserver.HRegion: Starting compaction on family in region usertable,user8,1404183160444.28938d267c1c61f7feb582b85e4c172f.
2014-06-30 19:55:04,578 INFO  [regionserver60020-smallCompactions-1404182848217] regionserver.HStore: Starting compaction of 3 file(s) in family of usertable,user8,1404183160444.28938d267c1c61f7feb582b85e4c172f. into tmpdir=hdfs://master:54310/hbase/data/default/usertable/28938d267c1c61f7feb582b85e4c172f/.tmp, totalSize=161.6m
2014-06-30 19:55:04,578 DEBUG [regionserver60020-smallCompactions-1404182848217] compactions.Compactor: Compacting hdfs://master:54310/hbase/data/default/usertable/28938d267c1c61f7feb582b85e4c172f/family/3a42b3247d78465eb0d13f74ec7379cf, keycount=35922, bloomtype=ROW, size=53.2m, encoding=NONE, seqNum=242, earliestPutTs=1404183223191
2014-06-30 19:55:04,578 DEBUG [regionserver60020-smallCompactions-1404182848217] compactions.Compactor: Compacting hdfs://master:54310/hbase/data/default/usertable/28938d267c1c61f7feb582b85e4c172f/family/cbe61450a0bc4936abb02a53218a6e89, keycount=27778, bloomtype=ROW, size=41.4m, encoding=NONE, seqNum=308, earliestPutTs=1404183272525
2014-06-30 19:55:04,578 DEBUG [regionserver60020-smallCompactions-1404182848217] compactions.Compactor: Compacting hdfs://master:54310/hbase/data/default/usertable/28938d267c1c61f7feb582b85e4c172f/family/5e35681f42734d87ad30ea719516990f, keycount=44902, bloomtype=ROW, size=66.9m, encoding=NONE, seqNum=391, earliestPutTs=1404183280937
2014-06-30 19:55:04,586 DEBUG [regionserver60020-smallCompactions-1404182848217] util.FSUtils: DFS Client does not support most favored nodes create; using default create
2014-06-30 19:55:07,320 INFO  [JvmPauseMonitor] util.JvmPauseMonitor: Detected pause in JVM or host machine (eg GC): pause of approximately 2659ms
GC pool 'ConcurrentMarkSweep' had collection(s): count=1 time=2668ms
2014-06-30 19:55:07,345 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 1999 synced till here 1981
2014-06-30 19:55:07,448 WARN  [RpcServer.handler=19,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":12642,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.58:49972","starttimems":1404183294806,"queuetimems":1,"class":"HRegionServer","responsesize":16943,"method":"Multi"}
2014-06-30 19:55:07,573 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1404182803191/sceplus-vm48.almaden.ibm.com%2C60020%2C1404182803191.1404183294633 with entries=108, filesize=83.8m; new WAL /hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1404182803191/sceplus-vm48.almaden.ibm.com%2C60020%2C1404182803191.1404183304525
2014-06-30 19:55:07,573 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1404182803191/sceplus-vm48.almaden.ibm.com%2C60020%2C1404182803191.1404183277094
2014-06-30 19:55:07,573 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1404182803191/sceplus-vm48.almaden.ibm.com%2C60020%2C1404182803191.1404183282342
2014-06-30 19:55:09,923 INFO  [JvmPauseMonitor] util.JvmPauseMonitor: Detected pause in JVM or host machine (eg GC): pause of approximately 1601ms
GC pool 'ConcurrentMarkSweep' had collection(s): count=1 time=2052ms
2014-06-30 19:55:12,495 INFO  [JvmPauseMonitor] util.JvmPauseMonitor: Detected pause in JVM or host machine (eg GC): pause of approximately 2072ms
GC pool 'ConcurrentMarkSweep' had collection(s): count=1 time=2286ms
2014-06-30 19:55:12,536 WARN  [RpcServer.handler=9,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":12735,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.58:49972","starttimems":1404183299801,"queuetimems":0,"class":"HRegionServer","responsesize":16566,"method":"Multi"}
2014-06-30 19:55:12,537 WARN  [RpcServer.handler=9,port=60020] ipc.RpcServer: RpcServer.respondercallId: 1311 service: ClientService methodName: Multi size: 2.9m connection: 9.1.143.58:49972: output error
2014-06-30 19:55:12,537 WARN  [RpcServer.handler=9,port=60020] ipc.RpcServer: RpcServer.handler=9,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-06-30 19:55:12,538 WARN  [RpcServer.handler=35,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":14553,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.58:49972","starttimems":1404183297982,"queuetimems":1,"class":"HRegionServer","responsesize":16204,"method":"Multi"}
2014-06-30 19:55:12,539 WARN  [RpcServer.handler=35,port=60020] ipc.RpcServer: RpcServer.respondercallId: 1315 service: ClientService methodName: Multi size: 2.8m connection: 9.1.143.58:49972: output error
2014-06-30 19:55:12,540 WARN  [RpcServer.handler=35,port=60020] ipc.RpcServer: RpcServer.handler=35,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-06-30 19:55:12,541 WARN  [RpcServer.handler=33,port=60020] ipc.RpcServer: RpcServer.respondercallId: 1332 service: ClientService methodName: Multi size: 209.7k connection: 9.1.143.58:49972: output error
2014-06-30 19:55:12,541 WARN  [RpcServer.handler=33,port=60020] ipc.RpcServer: RpcServer.handler=33,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-06-30 19:55:12,541 WARN  [RpcServer.handler=24,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":17756,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.58:49972","starttimems":1404183294785,"queuetimems":0,"class":"HRegionServer","responsesize":16760,"method":"Multi"}
2014-06-30 19:55:12,541 WARN  [RpcServer.handler=46,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":17883,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.58:49972","starttimems":1404183294658,"queuetimems":5,"class":"HRegionServer","responsesize":17169,"method":"Multi"}
2014-06-30 19:55:12,541 WARN  [RpcServer.handler=24,port=60020] ipc.RpcServer: RpcServer.respondercallId: 1258 service: ClientService methodName: Multi size: 2.9m connection: 9.1.143.58:49972: output error
2014-06-30 19:55:12,541 WARN  [RpcServer.handler=24,port=60020] ipc.RpcServer: RpcServer.handler=24,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-06-30 19:55:12,542 WARN  [RpcServer.handler=46,port=60020] ipc.RpcServer: RpcServer.respondercallId: 1250 service: ClientService methodName: Multi size: 3.0m connection: 9.1.143.58:49972: output error
2014-06-30 19:55:12,542 WARN  [RpcServer.handler=46,port=60020] ipc.RpcServer: RpcServer.handler=46,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-06-30 19:55:14,719 INFO  [JvmPauseMonitor] util.JvmPauseMonitor: Detected pause in JVM or host machine (eg GC): pause of approximately 1723ms
GC pool 'ConcurrentMarkSweep' had collection(s): count=1 time=2019ms
2014-06-30 19:55:17,204 INFO  [JvmPauseMonitor] util.JvmPauseMonitor: Detected pause in JVM or host machine (eg GC): pause of approximately 1985ms
GC pool 'ConcurrentMarkSweep' had collection(s): count=1 time=2123ms
2014-06-30 19:55:17,527 WARN  [RpcServer.handler=39,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":19631,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.58:49972","starttimems":1404183297896,"queuetimems":0,"class":"HRegionServer","responsesize":16074,"method":"Multi"}
2014-06-30 19:55:17,528 WARN  [RpcServer.handler=39,port=60020] ipc.RpcServer: RpcServer.respondercallId: 1282 service: ClientService methodName: Multi size: 2.8m connection: 9.1.143.58:49972: output error
2014-06-30 19:55:17,528 WARN  [RpcServer.handler=39,port=60020] ipc.RpcServer: RpcServer.handler=39,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-06-30 19:55:17,529 WARN  [RpcServer.handler=45,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":22382,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.58:49972","starttimems":1404183295141,"queuetimems":0,"class":"HRegionServer","responsesize":17088,"method":"Multi"}
2014-06-30 19:55:17,529 WARN  [RpcServer.handler=45,port=60020] ipc.RpcServer: RpcServer.respondercallId: 1256 service: ClientService methodName: Multi size: 3.0m connection: 9.1.143.58:49972: output error
2014-06-30 19:55:17,529 WARN  [RpcServer.handler=45,port=60020] ipc.RpcServer: RpcServer.handler=45,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-06-30 19:55:17,530 WARN  [RpcServer.handler=5,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":22350,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.58:49972","starttimems":1404183295180,"queuetimems":2,"class":"HRegionServer","responsesize":15114,"method":"Multi"}
2014-06-30 19:55:17,530 WARN  [RpcServer.handler=5,port=60020] ipc.RpcServer: RpcServer.respondercallId: 1254 service: ClientService methodName: Multi size: 2.6m connection: 9.1.143.58:49972: output error
2014-06-30 19:55:17,530 WARN  [RpcServer.handler=5,port=60020] ipc.RpcServer: RpcServer.handler=5,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-06-30 19:55:17,537 WARN  [RpcServer.handler=35,port=60020] ipc.RpcServer: RpcServer.respondercallId: 1389 service: ClientService methodName: Multi size: 209.7k connection: 9.1.143.58:50135: output error
2014-06-30 19:55:17,538 WARN  [RpcServer.handler=35,port=60020] ipc.RpcServer: RpcServer.handler=35,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-06-30 19:55:17,538 WARN  [RpcServer.handler=44,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":20013,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.58:49972","starttimems":1404183297525,"queuetimems":0,"class":"HRegionServer","responsesize":15967,"method":"Multi"}
2014-06-30 19:55:17,538 WARN  [RpcServer.handler=44,port=60020] ipc.RpcServer: RpcServer.respondercallId: 1295 service: ClientService methodName: Multi size: 2.8m connection: 9.1.143.58:49972: output error
2014-06-30 19:55:17,538 WARN  [RpcServer.handler=44,port=60020] ipc.RpcServer: RpcServer.handler=44,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-06-30 19:55:17,542 WARN  [RpcServer.handler=8,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":22362,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.58:49972","starttimems":1404183295164,"queuetimems":0,"class":"HRegionServer","responsesize":17388,"method":"Multi"}
2014-06-30 19:55:17,543 WARN  [RpcServer.handler=8,port=60020] ipc.RpcServer: RpcServer.respondercallId: 1255 service: ClientService methodName: Multi size: 3.0m connection: 9.1.143.58:49972: output error
2014-06-30 19:55:17,543 WARN  [RpcServer.handler=8,port=60020] ipc.RpcServer: RpcServer.handler=8,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-06-30 19:55:17,551 WARN  [RpcServer.handler=26,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":20248,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.58:49972","starttimems":1404183297303,"queuetimems":0,"class":"HRegionServer","responsesize":15676,"method":"Multi"}
2014-06-30 19:55:17,551 WARN  [RpcServer.handler=26,port=60020] ipc.RpcServer: RpcServer.respondercallId: 1263 service: ClientService methodName: Multi size: 2.7m connection: 9.1.143.58:49972: output error
2014-06-30 19:55:17,552 WARN  [RpcServer.handler=26,port=60020] ipc.RpcServer: RpcServer.handler=26,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-06-30 19:55:17,562 WARN  [RpcServer.handler=9,port=60020] ipc.RpcServer: RpcServer.respondercallId: 1386 service: ClientService methodName: Multi size: 176.0k connection: 9.1.143.58:50135: output error
2014-06-30 19:55:17,562 WARN  [RpcServer.handler=48,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":20063,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.58:49972","starttimems":1404183297492,"queuetimems":0,"class":"HRegionServer","responsesize":15142,"method":"Multi"}
2014-06-30 19:55:17,562 WARN  [RpcServer.handler=9,port=60020] ipc.RpcServer: RpcServer.handler=9,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-06-30 19:55:17,566 WARN  [RpcServer.handler=1,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":17785,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.58:49972","starttimems":1404183299778,"queuetimems":1,"class":"HRegionServer","responsesize":16268,"method":"Multi"}
2014-06-30 19:55:17,569 DEBUG [MemStoreFlusher.1] regionserver.MemStoreFlusher: Flush thread woke up because memory above low water=367.3m
2014-06-30 19:55:17,570 WARN  [RpcServer.handler=48,port=60020] ipc.RpcServer: RpcServer.respondercallId: 1298 service: ClientService methodName: Multi size: 2.6m connection: 9.1.143.58:49972: output error
2014-06-30 19:55:17,570 WARN  [RpcServer.handler=48,port=60020] ipc.RpcServer: RpcServer.handler=48,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-06-30 19:55:17,574 INFO  [MemStoreFlusher.1] regionserver.MemStoreFlusher: Flush of region usertable,user7,1404183160444.994b975d2c4cd5bbe532bddfb5e4ac46. due to global heap pressure
2014-06-30 19:55:17,576 WARN  [RpcServer.handler=1,port=60020] ipc.RpcServer: RpcServer.respondercallId: 1313 service: ClientService methodName: Multi size: 2.8m connection: 9.1.143.58:49972: output error
2014-06-30 19:55:17,578 WARN  [RpcServer.handler=1,port=60020] ipc.RpcServer: RpcServer.handler=1,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-06-30 19:55:17,583 DEBUG [MemStoreFlusher.1] regionserver.HRegion: Started memstore flush for usertable,user7,1404183160444.994b975d2c4cd5bbe532bddfb5e4ac46., current region memstore size 122.0m
2014-06-30 19:55:17,584 WARN  [RpcServer.handler=31,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":17791,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.58:49972","starttimems":1404183299789,"queuetimems":1,"class":"HRegionServer","responsesize":16114,"method":"Multi"}
2014-06-30 19:55:17,589 WARN  [RpcServer.handler=31,port=60020] ipc.RpcServer: RpcServer.respondercallId: 1312 service: ClientService methodName: Multi size: 2.8m connection: 9.1.143.58:49972: output error
2014-06-30 19:55:17,591 WARN  [RpcServer.handler=31,port=60020] ipc.RpcServer: RpcServer.handler=31,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-06-30 19:55:17,614 WARN  [RpcServer.handler=11,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":19609,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.58:49972","starttimems":1404183298003,"queuetimems":0,"class":"HRegionServer","responsesize":16666,"method":"Multi"}
2014-06-30 19:55:17,619 WARN  [RpcServer.handler=11,port=60020] ipc.RpcServer: RpcServer.respondercallId: 1314 service: ClientService methodName: Multi size: 2.9m connection: 9.1.143.58:49972: output error
2014-06-30 19:55:19,751 INFO  [JvmPauseMonitor] util.JvmPauseMonitor: Detected pause in JVM or host machine (eg GC): pause of approximately 2046ms
GC pool 'ConcurrentMarkSweep' had collection(s): count=1 time=2275ms
2014-06-30 19:55:19,775 WARN  [RpcServer.handler=11,port=60020] ipc.RpcServer: RpcServer.handler=11,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-06-30 19:55:19,981 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-06-30 19:55:19,986 DEBUG [RpcServer.handler=25,port=60020] regionserver.HRegion: Flush requested on usertable,user7,1404183160444.994b975d2c4cd5bbe532bddfb5e4ac46.
2014-06-30 19:55:19,987 DEBUG [MemStoreFlusher.0] regionserver.HRegion: NOT flushing memstore for region usertable,user7,1404183160444.994b975d2c4cd5bbe532bddfb5e4ac46., flushing=true, writesEnabled=true
2014-06-30 19:55:22,054 INFO  [JvmPauseMonitor] util.JvmPauseMonitor: Detected pause in JVM or host machine (eg GC): pause of approximately 1802ms
GC pool 'ConcurrentMarkSweep' had collection(s): count=1 time=2057ms
2014-06-30 19:55:22,056 INFO  [RpcServer.handler=25,port=60020] regionserver.MemStoreFlusher: Blocking updates on sceplus-vm48.almaden.ibm.com,60020,1404182803191: the global memstore size 390.2m is >= than blocking 386.7m size
2014-06-30 19:55:22,056 DEBUG [MemStoreFlusher.0] regionserver.MemStoreFlusher: Flush thread woke up because memory above low water=367.3m
2014-06-30 19:55:22,056 INFO  [MemStoreFlusher.0] regionserver.MemStoreFlusher: Flush of region usertable,user5,1404183160444.1d66ac045cc0407c0fa426db565fcbba. due to global heap pressure
2014-06-30 19:55:22,056 DEBUG [MemStoreFlusher.0] regionserver.HRegion: Started memstore flush for usertable,user5,1404183160444.1d66ac045cc0407c0fa426db565fcbba., current region memstore size 110.5m
2014-06-30 19:55:22,056 INFO  [RpcServer.handler=37,port=60020] regionserver.MemStoreFlusher: Blocking updates on sceplus-vm48.almaden.ibm.com,60020,1404182803191: the global memstore size 390.2m is >= than blocking 386.7m size
2014-06-30 19:55:22,165 INFO  [RpcServer.handler=2,port=60020] regionserver.MemStoreFlusher: Blocking updates on sceplus-vm48.almaden.ibm.com,60020,1404182803191: the global memstore size 390.2m is >= than blocking 386.7m size
2014-06-30 19:55:22,166 INFO  [RpcServer.handler=0,port=60020] regionserver.MemStoreFlusher: Blocking updates on sceplus-vm48.almaden.ibm.com,60020,1404182803191: the global memstore size 390.2m is >= than blocking 386.7m size
2014-06-30 19:55:22,167 INFO  [RpcServer.handler=17,port=60020] regionserver.MemStoreFlusher: Blocking updates on sceplus-vm48.almaden.ibm.com,60020,1404182803191: the global memstore size 390.2m is >= than blocking 386.7m size
2014-06-30 19:55:22,169 INFO  [RpcServer.handler=38,port=60020] regionserver.MemStoreFlusher: Blocking updates on sceplus-vm48.almaden.ibm.com,60020,1404182803191: the global memstore size 390.2m is >= than blocking 386.7m size
2014-06-30 19:55:22,170 INFO  [RpcServer.handler=49,port=60020] regionserver.MemStoreFlusher: Blocking updates on sceplus-vm48.almaden.ibm.com,60020,1404182803191: the global memstore size 390.2m is >= than blocking 386.7m size
2014-06-30 19:55:22,176 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 2119 synced till here 2093
2014-06-30 19:55:24,595 INFO  [JvmPauseMonitor] util.JvmPauseMonitor: Detected pause in JVM or host machine (eg GC): pause of approximately 2040ms
GC pool 'ConcurrentMarkSweep' had collection(s): count=1 time=2348ms
2014-06-30 19:55:24,596 WARN  [RpcServer.handler=30,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":27125,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.58:49972","starttimems":1404183297471,"queuetimems":0,"class":"HRegionServer","responsesize":15077,"method":"Multi"}
2014-06-30 19:55:24,597 WARN  [RpcServer.handler=30,port=60020] ipc.RpcServer: RpcServer.respondercallId: 1299 service: ClientService methodName: Multi size: 2.6m connection: 9.1.143.58:49972: output error
2014-06-30 19:55:24,597 WARN  [RpcServer.handler=30,port=60020] ipc.RpcServer: RpcServer.handler=30,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-06-30 19:55:24,766 WARN  [RpcServer.handler=31,port=60020] ipc.RpcServer: RpcServer.respondercallId: 1424 service: ClientService methodName: Multi size: 217.2k connection: 9.1.143.58:50137: output error
2014-06-30 19:55:24,766 WARN  [RpcServer.handler=22,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":27434,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.58:49972","starttimems":1404183297332,"queuetimems":1,"class":"HRegionServer","responsesize":16853,"method":"Multi"}
2014-06-30 19:55:24,766 WARN  [RpcServer.handler=22,port=60020] ipc.RpcServer: RpcServer.respondercallId: 1302 service: ClientService methodName: Multi size: 2.9m connection: 9.1.143.58:49972: output error
2014-06-30 19:55:24,766 WARN  [RpcServer.handler=31,port=60020] ipc.RpcServer: RpcServer.handler=31,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-06-30 19:55:24,766 WARN  [RpcServer.handler=22,port=60020] ipc.RpcServer: RpcServer.handler=22,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-06-30 19:55:24,766 WARN  [RpcServer.handler=48,port=60020] ipc.RpcServer: RpcServer.respondercallId: 1426 service: ClientService methodName: Multi size: 176.0k connection: 9.1.143.58:50137: output error
2014-06-30 19:55:24,767 WARN  [RpcServer.handler=34,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":27422,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.58:49972","starttimems":1404183297344,"queuetimems":1,"class":"HRegionServer","responsesize":15384,"method":"Multi"}
2014-06-30 19:55:24,767 WARN  [RpcServer.handler=34,port=60020] ipc.RpcServer: RpcServer.respondercallId: 1301 service: ClientService methodName: Multi size: 2.7m connection: 9.1.143.58:49972: output error
2014-06-30 19:55:24,767 WARN  [RpcServer.handler=34,port=60020] ipc.RpcServer: RpcServer.handler=34,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-06-30 19:55:24,767 WARN  [RpcServer.handler=40,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":26891,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.58:49972","starttimems":1404183297876,"queuetimems":1,"class":"HRegionServer","responsesize":16408,"method":"Multi"}
2014-06-30 19:55:24,767 WARN  [RpcServer.handler=42,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":26954,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.58:49972","starttimems":1404183297813,"queuetimems":1,"class":"HRegionServer","responsesize":15712,"method":"Multi"}
2014-06-30 19:55:24,771 WARN  [RpcServer.handler=48,port=60020] ipc.RpcServer: RpcServer.handler=48,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-06-30 19:55:24,774 WARN  [RpcServer.handler=39,port=60020] ipc.RpcServer: RpcServer.respondercallId: 1415 service: ClientService methodName: Multi size: 218.5k connection: 9.1.143.58:50137: output error
2014-06-30 19:55:24,774 WARN  [RpcServer.handler=39,port=60020] ipc.RpcServer: RpcServer.handler=39,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-06-30 19:55:24,775 WARN  [RpcServer.handler=47,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":29576,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.58:49972","starttimems":1404183295191,"queuetimems":1,"class":"HRegionServer","responsesize":15737,"method":"Multi"}
2014-06-30 19:55:24,780 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1404182803191/sceplus-vm48.almaden.ibm.com%2C60020%2C1404182803191.1404183304525 with entries=120, filesize=95.3m; new WAL /hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1404182803191/sceplus-vm48.almaden.ibm.com%2C60020%2C1404182803191.1404183319982
2014-06-30 19:55:24,771 INFO  [RpcServer.handler=30,port=60020] regionserver.MemStoreFlusher: Blocking updates on sceplus-vm48.almaden.ibm.com,60020,1404182803191: the global memstore size 411.9m is >= than blocking 386.7m size
2014-06-30 19:55:24,771 WARN  [RpcServer.handler=40,port=60020] ipc.RpcServer: RpcServer.respondercallId: 1285 service: ClientService methodName: Multi size: 2.9m connection: 9.1.143.58:49972: output error
2014-06-30 19:55:24,782 INFO  [RpcServer.handler=33,port=60020] regionserver.MemStoreFlusher: Blocking updates on sceplus-vm48.almaden.ibm.com,60020,1404182803191: the global memstore size 450.5m is >= than blocking 386.7m size
2014-06-30 19:55:24,782 WARN  [RpcServer.handler=40,port=60020] ipc.RpcServer: RpcServer.handler=40,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-06-30 19:55:24,782 WARN  [RpcServer.handler=47,port=60020] ipc.RpcServer: RpcServer.respondercallId: 1253 service: ClientService methodName: Multi size: 2.8m connection: 9.1.143.58:49972: output error
2014-06-30 19:55:24,782 INFO  [RpcServer.handler=39,port=60020] regionserver.MemStoreFlusher: Blocking updates on sceplus-vm48.almaden.ibm.com,60020,1404182803191: the global memstore size 450.5m is >= than blocking 386.7m size
2014-06-30 19:55:24,782 WARN  [RpcServer.handler=47,port=60020] ipc.RpcServer: RpcServer.handler=47,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-06-30 19:55:24,782 WARN  [RpcServer.handler=42,port=60020] ipc.RpcServer: RpcServer.respondercallId: 1293 service: ClientService methodName: Multi size: 2.7m connection: 9.1.143.58:49972: output error
2014-06-30 19:55:24,782 WARN  [RpcServer.handler=42,port=60020] ipc.RpcServer: RpcServer.handler=42,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-06-30 19:55:24,783 INFO  [RpcServer.handler=15,port=60020] regionserver.MemStoreFlusher: Blocking updates on sceplus-vm48.almaden.ibm.com,60020,1404182803191: the global memstore size 450.5m is >= than blocking 386.7m size
2014-06-30 19:55:24,786 INFO  [RpcServer.handler=27,port=60020] regionserver.MemStoreFlusher: Blocking updates on sceplus-vm48.almaden.ibm.com,60020,1404182803191: the global memstore size 450.5m is >= than blocking 386.7m size
2014-06-30 19:55:24,786 INFO  [RpcServer.handler=48,port=60020] regionserver.MemStoreFlusher: Blocking updates on sceplus-vm48.almaden.ibm.com,60020,1404182803191: the global memstore size 450.5m is >= than blocking 386.7m size
2014-06-30 19:55:24,786 INFO  [RpcServer.handler=5,port=60020] regionserver.MemStoreFlusher: Blocking updates on sceplus-vm48.almaden.ibm.com,60020,1404182803191: the global memstore size 450.5m is >= than blocking 386.7m size
2014-06-30 19:55:24,787 INFO  [RpcServer.handler=29,port=60020] regionserver.MemStoreFlusher: Blocking updates on sceplus-vm48.almaden.ibm.com,60020,1404182803191: the global memstore size 450.5m is >= than blocking 386.7m size
2014-06-30 19:55:24,787 INFO  [RpcServer.handler=22,port=60020] regionserver.MemStoreFlusher: Blocking updates on sceplus-vm48.almaden.ibm.com,60020,1404182803191: the global memstore size 450.5m is >= than blocking 386.7m size
2014-06-30 19:55:24,787 INFO  [RpcServer.handler=14,port=60020] regionserver.MemStoreFlusher: Blocking updates on sceplus-vm48.almaden.ibm.com,60020,1404182803191: the global memstore size 451.9m is >= than blocking 386.7m size
2014-06-30 19:55:24,787 INFO  [RpcServer.handler=34,port=60020] regionserver.MemStoreFlusher: Blocking updates on sceplus-vm48.almaden.ibm.com,60020,1404182803191: the global memstore size 451.9m is >= than blocking 386.7m size
2014-06-30 19:55:24,787 INFO  [RpcServer.handler=24,port=60020] regionserver.MemStoreFlusher: Blocking updates on sceplus-vm48.almaden.ibm.com,60020,1404182803191: the global memstore size 451.9m is >= than blocking 386.7m size
2014-06-30 19:55:24,787 INFO  [RpcServer.handler=46,port=60020] regionserver.MemStoreFlusher: Blocking updates on sceplus-vm48.almaden.ibm.com,60020,1404182803191: the global memstore size 451.9m is >= than blocking 386.7m size
2014-06-30 19:55:24,787 INFO  [RpcServer.handler=31,port=60020] regionserver.MemStoreFlusher: Blocking updates on sceplus-vm48.almaden.ibm.com,60020,1404182803191: the global memstore size 451.9m is >= than blocking 386.7m size
2014-06-30 19:55:24,788 INFO  [RpcServer.handler=44,port=60020] regionserver.MemStoreFlusher: Blocking updates on sceplus-vm48.almaden.ibm.com,60020,1404182803191: the global memstore size 451.9m is >= than blocking 386.7m size
2014-06-30 19:55:24,788 INFO  [RpcServer.handler=1,port=60020] regionserver.MemStoreFlusher: Blocking updates on sceplus-vm48.almaden.ibm.com,60020,1404182803191: the global memstore size 451.9m is >= than blocking 386.7m size
2014-06-30 19:55:24,834 INFO  [RpcServer.handler=20,port=60020] regionserver.MemStoreFlusher: Blocking updates on sceplus-vm48.almaden.ibm.com,60020,1404182803191: the global memstore size 451.9m is >= than blocking 386.7m size
2014-06-30 19:55:24,839 INFO  [RpcServer.handler=36,port=60020] regionserver.MemStoreFlusher: Blocking updates on sceplus-vm48.almaden.ibm.com,60020,1404182803191: the global memstore size 451.9m is >= than blocking 386.7m size
2014-06-30 19:55:24,840 INFO  [RpcServer.handler=12,port=60020] regionserver.MemStoreFlusher: Blocking updates on sceplus-vm48.almaden.ibm.com,60020,1404182803191: the global memstore size 451.9m is >= than blocking 386.7m size
2014-06-30 19:55:24,840 INFO  [RpcServer.handler=32,port=60020] regionserver.MemStoreFlusher: Blocking updates on sceplus-vm48.almaden.ibm.com,60020,1404182803191: the global memstore size 451.9m is >= than blocking 386.7m size
2014-06-30 19:55:24,841 INFO  [RpcServer.handler=42,port=60020] regionserver.MemStoreFlusher: Blocking updates on sceplus-vm48.almaden.ibm.com,60020,1404182803191: the global memstore size 451.9m is >= than blocking 386.7m size
2014-06-30 19:55:24,849 INFO  [RpcServer.handler=19,port=60020] regionserver.MemStoreFlusher: Blocking updates on sceplus-vm48.almaden.ibm.com,60020,1404182803191: the global memstore size 451.9m is >= than blocking 386.7m size
2014-06-30 19:55:24,852 INFO  [RpcServer.handler=23,port=60020] regionserver.MemStoreFlusher: Blocking updates on sceplus-vm48.almaden.ibm.com,60020,1404182803191: the global memstore size 451.9m is >= than blocking 386.7m size
2014-06-30 19:55:24,876 INFO  [RpcServer.handler=6,port=60020] regionserver.MemStoreFlusher: Blocking updates on sceplus-vm48.almaden.ibm.com,60020,1404182803191: the global memstore size 451.9m is >= than blocking 386.7m size
2014-06-30 19:55:24,900 DEBUG [MemStoreFlusher.1] util.FSUtils: DFS Client does not support most favored nodes create; using default create
2014-06-30 19:55:24,902 INFO  [RpcServer.handler=43,port=60020] regionserver.MemStoreFlusher: Blocking updates on sceplus-vm48.almaden.ibm.com,60020,1404182803191: the global memstore size 451.9m is >= than blocking 386.7m size
2014-06-30 19:55:24,903 INFO  [RpcServer.handler=7,port=60020] regionserver.MemStoreFlusher: Blocking updates on sceplus-vm48.almaden.ibm.com,60020,1404182803191: the global memstore size 451.9m is >= than blocking 386.7m size
2014-06-30 19:55:24,908 INFO  [RpcServer.handler=21,port=60020] regionserver.MemStoreFlusher: Blocking updates on sceplus-vm48.almaden.ibm.com,60020,1404182803191: the global memstore size 451.9m is >= than blocking 386.7m size
2014-06-30 19:55:24,913 INFO  [RpcServer.handler=8,port=60020] regionserver.MemStoreFlusher: Blocking updates on sceplus-vm48.almaden.ibm.com,60020,1404182803191: the global memstore size 451.9m is >= than blocking 386.7m size
2014-06-30 19:55:24,917 DEBUG [MemStoreFlusher.0] util.FSUtils: DFS Client does not support most favored nodes create; using default create
2014-06-30 19:55:24,918 INFO  [RpcServer.handler=18,port=60020] regionserver.MemStoreFlusher: Blocking updates on sceplus-vm48.almaden.ibm.com,60020,1404182803191: the global memstore size 451.9m is >= than blocking 386.7m size
2014-06-30 19:55:24,922 INFO  [RpcServer.handler=28,port=60020] regionserver.MemStoreFlusher: Blocking updates on sceplus-vm48.almaden.ibm.com,60020,1404182803191: the global memstore size 451.9m is >= than blocking 386.7m size
2014-06-30 19:55:27,279 WARN  [RpcServer.handler=49,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5109ms
2014-06-30 19:55:27,279 WARN  [RpcServer.handler=25,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5224ms
2014-06-30 19:55:27,280 WARN  [RpcServer.handler=37,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5224ms
2014-06-30 19:55:27,280 INFO  [JvmPauseMonitor] util.JvmPauseMonitor: Detected pause in JVM or host machine (eg GC): pause of approximately 2184ms
GC pool 'ConcurrentMarkSweep' had collection(s): count=1 time=2517ms
2014-06-30 19:55:27,282 WARN  [RpcServer.handler=2,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5226ms
2014-06-30 19:55:27,282 WARN  [RpcServer.handler=17,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5115ms
2014-06-30 19:55:27,282 WARN  [RpcServer.handler=0,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5117ms
2014-06-30 19:55:27,283 WARN  [RpcServer.handler=38,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5114ms
2014-06-30 19:55:27,283 INFO  [RpcServer.handler=16,port=60020] regionserver.MemStoreFlusher: Blocking updates on sceplus-vm48.almaden.ibm.com,60020,1404182803191: the global memstore size 451.9m is >= than blocking 386.7m size
2014-06-30 19:55:27,283 INFO  [RpcServer.handler=26,port=60020] regionserver.MemStoreFlusher: Blocking updates on sceplus-vm48.almaden.ibm.com,60020,1404182803191: the global memstore size 451.9m is >= than blocking 386.7m size
2014-06-30 19:55:27,283 INFO  [RpcServer.handler=3,port=60020] regionserver.MemStoreFlusher: Blocking updates on sceplus-vm48.almaden.ibm.com,60020,1404182803191: the global memstore size 451.9m is >= than blocking 386.7m size
2014-06-30 19:55:27,284 INFO  [RpcServer.handler=9,port=60020] regionserver.MemStoreFlusher: Blocking updates on sceplus-vm48.almaden.ibm.com,60020,1404182803191: the global memstore size 451.9m is >= than blocking 386.7m size
2014-06-30 19:55:27,284 INFO  [RpcServer.handler=40,port=60020] regionserver.MemStoreFlusher: Blocking updates on sceplus-vm48.almaden.ibm.com,60020,1404182803191: the global memstore size 451.9m is >= than blocking 386.7m size
2014-06-30 19:55:27,284 INFO  [RpcServer.handler=4,port=60020] regionserver.MemStoreFlusher: Blocking updates on sceplus-vm48.almaden.ibm.com,60020,1404182803191: the global memstore size 451.9m is >= than blocking 386.7m size
2014-06-30 19:55:27,285 INFO  [RpcServer.handler=10,port=60020] regionserver.MemStoreFlusher: Blocking updates on sceplus-vm48.almaden.ibm.com,60020,1404182803191: the global memstore size 451.9m is >= than blocking 386.7m size
2014-06-30 19:55:27,285 INFO  [RpcServer.handler=35,port=60020] regionserver.MemStoreFlusher: Blocking updates on sceplus-vm48.almaden.ibm.com,60020,1404182803191: the global memstore size 451.9m is >= than blocking 386.7m size
2014-06-30 19:55:27,285 INFO  [RpcServer.handler=47,port=60020] regionserver.MemStoreFlusher: Blocking updates on sceplus-vm48.almaden.ibm.com,60020,1404182803191: the global memstore size 451.9m is >= than blocking 386.7m size
2014-06-30 19:55:27,286 INFO  [RpcServer.handler=13,port=60020] regionserver.MemStoreFlusher: Blocking updates on sceplus-vm48.almaden.ibm.com,60020,1404182803191: the global memstore size 451.9m is >= than blocking 386.7m size
2014-06-30 19:55:27,287 INFO  [RpcServer.handler=41,port=60020] regionserver.MemStoreFlusher: Blocking updates on sceplus-vm48.almaden.ibm.com,60020,1404182803191: the global memstore size 451.9m is >= than blocking 386.7m size
2014-06-30 19:55:27,287 INFO  [RpcServer.handler=11,port=60020] regionserver.MemStoreFlusher: Blocking updates on sceplus-vm48.almaden.ibm.com,60020,1404182803191: the global memstore size 451.9m is >= than blocking 386.7m size
2014-06-30 19:55:27,288 INFO  [RpcServer.handler=45,port=60020] regionserver.MemStoreFlusher: Blocking updates on sceplus-vm48.almaden.ibm.com,60020,1404182803191: the global memstore size 451.9m is >= than blocking 386.7m size
2014-06-30 19:55:29,858 WARN  [RpcServer.handler=23,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5006ms
2014-06-30 19:55:29,858 WARN  [RpcServer.handler=20,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5024ms
2014-06-30 19:55:29,858 WARN  [RpcServer.handler=46,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5083ms
2014-06-30 19:55:29,859 WARN  [RpcServer.handler=36,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5020ms
2014-06-30 19:55:29,859 WARN  [RpcServer.handler=32,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5019ms
2014-06-30 19:55:29,859 WARN  [RpcServer.handler=12,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5019ms
2014-06-30 19:55:29,859 WARN  [RpcServer.handler=27,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5078ms
2014-06-30 19:55:29,859 WARN  [RpcServer.handler=30,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5088ms
2014-06-30 19:55:29,859 WARN  [RpcServer.handler=42,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5018ms
2014-06-30 19:55:29,860 WARN  [RpcServer.handler=19,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5011ms
2014-06-30 19:55:29,860 INFO  [JvmPauseMonitor] util.JvmPauseMonitor: Detected pause in JVM or host machine (eg GC): pause of approximately 2079ms
GC pool 'ConcurrentMarkSweep' had collection(s): count=1 time=2349ms
2014-06-30 19:55:29,860 WARN  [RpcServer.handler=33,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5079ms
2014-06-30 19:55:29,860 WARN  [RpcServer.handler=39,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5079ms
2014-06-30 19:55:29,860 WARN  [RpcServer.handler=15,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5088ms
2014-06-30 19:55:29,860 WARN  [RpcServer.handler=48,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5081ms
2014-06-30 19:55:29,861 WARN  [RpcServer.handler=5,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5081ms
2014-06-30 19:55:29,861 WARN  [RpcServer.handler=29,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5083ms
2014-06-30 19:55:29,861 WARN  [RpcServer.handler=22,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5084ms
2014-06-30 19:55:29,861 WARN  [RpcServer.handler=14,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5086ms
2014-06-30 19:55:29,861 WARN  [RpcServer.handler=34,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5086ms
2014-06-30 19:55:29,861 WARN  [RpcServer.handler=24,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5086ms
2014-06-30 19:55:29,861 WARN  [RpcServer.handler=1,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5089ms
2014-06-30 19:55:29,861 WARN  [RpcServer.handler=31,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5086ms
2014-06-30 19:55:29,861 WARN  [RpcServer.handler=44,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5088ms
2014-06-30 19:55:29,876 WARN  [RpcServer.handler=6,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-06-30 19:55:29,902 WARN  [RpcServer.handler=43,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-06-30 19:55:29,904 WARN  [RpcServer.handler=7,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5001ms
2014-06-30 19:55:29,908 WARN  [RpcServer.handler=21,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-06-30 19:55:29,913 WARN  [RpcServer.handler=8,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-06-30 19:55:29,918 WARN  [RpcServer.handler=18,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-06-30 19:55:29,922 WARN  [RpcServer.handler=28,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-06-30 19:55:32,002 INFO  [JvmPauseMonitor] util.JvmPauseMonitor: Detected pause in JVM or host machine (eg GC): pause of approximately 1642ms
GC pool 'ConcurrentMarkSweep' had collection(s): count=1 time=1898ms
2014-06-30 19:55:34,598 INFO  [JvmPauseMonitor] util.JvmPauseMonitor: Detected pause in JVM or host machine (eg GC): pause of approximately 2095ms
GC pool 'ConcurrentMarkSweep' had collection(s): count=1 time=2340ms
2014-06-30 19:55:34,601 WARN  [RpcServer.handler=45,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 7312ms
2014-06-30 19:55:34,601 WARN  [RpcServer.handler=49,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 12431ms
2014-06-30 19:55:34,601 WARN  [RpcServer.handler=25,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 12546ms
2014-06-30 19:55:34,602 WARN  [RpcServer.handler=37,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 12545ms
2014-06-30 19:55:34,604 WARN  [RpcServer.handler=2,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 12548ms
2014-06-30 19:55:34,604 WARN  [RpcServer.handler=17,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 12437ms
2014-06-30 19:55:34,604 WARN  [RpcServer.handler=0,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 12439ms
2014-06-30 19:55:34,604 WARN  [RpcServer.handler=38,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 12435ms
2014-06-30 19:55:34,604 WARN  [RpcServer.handler=16,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 7322ms
2014-06-30 19:55:34,605 WARN  [RpcServer.handler=26,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 7322ms
2014-06-30 19:55:34,605 WARN  [RpcServer.handler=3,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 7323ms
2014-06-30 19:55:34,605 WARN  [RpcServer.handler=9,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 7324ms
2014-06-30 19:55:34,606 WARN  [RpcServer.handler=40,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 7323ms
2014-06-30 19:55:34,606 WARN  [RpcServer.handler=4,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 7322ms
2014-06-30 19:55:34,606 WARN  [RpcServer.handler=10,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 7322ms
2014-06-30 19:55:34,606 WARN  [RpcServer.handler=35,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 7321ms
2014-06-30 19:55:34,606 WARN  [RpcServer.handler=47,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 7322ms
2014-06-30 19:55:34,606 WARN  [RpcServer.handler=13,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 7321ms
2014-06-30 19:55:34,607 WARN  [RpcServer.handler=41,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 7322ms
2014-06-30 19:55:34,607 WARN  [RpcServer.handler=11,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 7322ms
2014-06-30 19:55:34,858 WARN  [RpcServer.handler=23,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10006ms
2014-06-30 19:55:34,859 WARN  [RpcServer.handler=20,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10025ms
2014-06-30 19:55:34,859 WARN  [RpcServer.handler=36,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10020ms
2014-06-30 19:55:34,859 WARN  [RpcServer.handler=46,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10084ms
2014-06-30 19:55:34,859 WARN  [RpcServer.handler=12,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10019ms
2014-06-30 19:55:34,860 WARN  [RpcServer.handler=32,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10019ms
2014-06-30 19:55:34,860 WARN  [RpcServer.handler=30,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10089ms
2014-06-30 19:55:34,860 WARN  [RpcServer.handler=27,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10079ms
2014-06-30 19:55:34,860 WARN  [RpcServer.handler=42,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10019ms
2014-06-30 19:55:34,861 WARN  [RpcServer.handler=19,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10011ms
2014-06-30 19:55:34,862 WARN  [RpcServer.handler=33,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10080ms
2014-06-30 19:55:34,862 WARN  [RpcServer.handler=44,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10089ms
2014-06-30 19:55:34,862 WARN  [RpcServer.handler=31,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10087ms
2014-06-30 19:55:34,862 WARN  [RpcServer.handler=1,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10090ms
2014-06-30 19:55:34,862 WARN  [RpcServer.handler=24,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10087ms
2014-06-30 19:55:34,863 WARN  [RpcServer.handler=34,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10088ms
2014-06-30 19:55:34,863 WARN  [RpcServer.handler=14,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10088ms
2014-06-30 19:55:34,863 WARN  [RpcServer.handler=22,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10086ms
2014-06-30 19:55:34,863 WARN  [RpcServer.handler=29,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10085ms
2014-06-30 19:55:34,863 WARN  [RpcServer.handler=5,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10084ms
2014-06-30 19:55:34,864 WARN  [RpcServer.handler=48,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10085ms
2014-06-30 19:55:34,864 WARN  [RpcServer.handler=15,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10092ms
2014-06-30 19:55:34,864 WARN  [RpcServer.handler=39,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10083ms
2014-06-30 19:55:34,877 WARN  [RpcServer.handler=6,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10000ms
2014-06-30 19:55:34,903 WARN  [RpcServer.handler=43,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10000ms
2014-06-30 19:55:34,904 WARN  [RpcServer.handler=7,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10001ms
2014-06-30 19:55:34,909 WARN  [RpcServer.handler=21,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10000ms
2014-06-30 19:55:34,913 WARN  [RpcServer.handler=8,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10000ms
2014-06-30 19:55:34,919 WARN  [RpcServer.handler=18,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10000ms
2014-06-30 19:55:34,922 WARN  [RpcServer.handler=28,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10000ms
2014-06-30 19:55:37,061 INFO  [JvmPauseMonitor] util.JvmPauseMonitor: Detected pause in JVM or host machine (eg GC): pause of approximately 1962ms
GC pool 'ConcurrentMarkSweep' had collection(s): count=1 time=2283ms
2014-06-30 19:55:37,292 INFO  [MemStoreFlusher.0] regionserver.DefaultStoreFlusher: Flushed, sequenceid=449, memsize=89.0m, hasBloomFilter=true, into tmp file hdfs://master:54310/hbase/data/default/usertable/1d66ac045cc0407c0fa426db565fcbba/.tmp/6b715f472bf34bf8ab1f0abab79d0595
2014-06-30 19:55:37,305 DEBUG [MemStoreFlusher.0] regionserver.HRegionFileSystem: Committing store file hdfs://master:54310/hbase/data/default/usertable/1d66ac045cc0407c0fa426db565fcbba/.tmp/6b715f472bf34bf8ab1f0abab79d0595 as hdfs://master:54310/hbase/data/default/usertable/1d66ac045cc0407c0fa426db565fcbba/family/6b715f472bf34bf8ab1f0abab79d0595
2014-06-30 19:55:37,318 INFO  [MemStoreFlusher.0] regionserver.HStore: Added hdfs://master:54310/hbase/data/default/usertable/1d66ac045cc0407c0fa426db565fcbba/family/6b715f472bf34bf8ab1f0abab79d0595, entries=324010, sequenceid=449, filesize=48.3m
2014-06-30 19:55:37,318 INFO  [MemStoreFlusher.0] regionserver.HRegion: Finished memstore flush of ~115.1m/120671520, currentsize=0.0/0 for region usertable,user5,1404183160444.1d66ac045cc0407c0fa426db565fcbba. in 15262ms, sequenceid=449, compaction requested=false
2014-06-30 19:55:37,318 WARN  [RpcServer.handler=28,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 12396ms
2014-06-30 19:55:37,318 INFO  [RpcServer.handler=28,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server sceplus-vm48.almaden.ibm.com,60020,1404182803191
2014-06-30 19:55:37,318 WARN  [RpcServer.handler=18,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 12400ms
2014-06-30 19:55:37,319 INFO  [RpcServer.handler=18,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server sceplus-vm48.almaden.ibm.com,60020,1404182803191
2014-06-30 19:55:37,319 WARN  [RpcServer.handler=8,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 12406ms
2014-06-30 19:55:37,319 INFO  [RpcServer.handler=8,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server sceplus-vm48.almaden.ibm.com,60020,1404182803191
2014-06-30 19:55:37,319 WARN  [RpcServer.handler=21,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 12411ms
2014-06-30 19:55:37,319 INFO  [RpcServer.handler=21,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server sceplus-vm48.almaden.ibm.com,60020,1404182803191
2014-06-30 19:55:37,323 WARN  [RpcServer.handler=7,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 12419ms
2014-06-30 19:55:37,323 INFO  [RpcServer.handler=7,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server sceplus-vm48.almaden.ibm.com,60020,1404182803191
2014-06-30 19:55:37,323 WARN  [RpcServer.handler=43,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 12421ms
2014-06-30 19:55:37,323 INFO  [RpcServer.handler=43,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server sceplus-vm48.almaden.ibm.com,60020,1404182803191
2014-06-30 19:55:37,324 WARN  [RpcServer.handler=6,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 12447ms
2014-06-30 19:55:37,324 INFO  [RpcServer.handler=6,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server sceplus-vm48.almaden.ibm.com,60020,1404182803191
2014-06-30 19:55:37,324 WARN  [RpcServer.handler=39,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 12543ms
2014-06-30 19:55:37,324 INFO  [RpcServer.handler=39,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server sceplus-vm48.almaden.ibm.com,60020,1404182803191
2014-06-30 19:55:37,324 WARN  [RpcServer.handler=15,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 12552ms
2014-06-30 19:55:37,324 INFO  [RpcServer.handler=15,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server sceplus-vm48.almaden.ibm.com,60020,1404182803191
2014-06-30 19:55:37,324 WARN  [RpcServer.handler=48,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 12545ms
2014-06-30 19:55:37,325 INFO  [RpcServer.handler=48,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server sceplus-vm48.almaden.ibm.com,60020,1404182803191
2014-06-30 19:55:37,325 WARN  [RpcServer.handler=5,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 12546ms
2014-06-30 19:55:37,326 INFO  [RpcServer.handler=5,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server sceplus-vm48.almaden.ibm.com,60020,1404182803191
2014-06-30 19:55:37,331 WARN  [RpcServer.handler=29,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 12553ms
2014-06-30 19:55:37,331 INFO  [RpcServer.handler=29,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server sceplus-vm48.almaden.ibm.com,60020,1404182803191
2014-06-30 19:55:37,331 WARN  [RpcServer.handler=22,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 12554ms
2014-06-30 19:55:37,331 INFO  [RpcServer.handler=22,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server sceplus-vm48.almaden.ibm.com,60020,1404182803191
2014-06-30 19:55:37,331 WARN  [RpcServer.handler=14,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 12556ms
2014-06-30 19:55:37,331 INFO  [RpcServer.handler=14,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server sceplus-vm48.almaden.ibm.com,60020,1404182803191
2014-06-30 19:55:37,335 WARN  [RpcServer.handler=34,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 12556ms
2014-06-30 19:55:37,335 INFO  [RpcServer.handler=34,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server sceplus-vm48.almaden.ibm.com,60020,1404182803191
2014-06-30 19:55:37,336 WARN  [RpcServer.handler=24,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 12561ms
2014-06-30 19:55:37,336 INFO  [RpcServer.handler=24,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server sceplus-vm48.almaden.ibm.com,60020,1404182803191
2014-06-30 19:55:37,336 WARN  [RpcServer.handler=1,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 12564ms
2014-06-30 19:55:37,336 INFO  [RpcServer.handler=1,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server sceplus-vm48.almaden.ibm.com,60020,1404182803191
2014-06-30 19:55:37,350 WARN  [RpcServer.handler=31,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 12561ms
2014-06-30 19:55:37,354 INFO  [RpcServer.handler=31,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server sceplus-vm48.almaden.ibm.com,60020,1404182803191
2014-06-30 19:55:37,354 WARN  [RpcServer.handler=44,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 12581ms
2014-06-30 19:55:37,355 INFO  [RpcServer.handler=44,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server sceplus-vm48.almaden.ibm.com,60020,1404182803191
2014-06-30 19:55:37,358 WARN  [RpcServer.handler=33,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 12577ms
2014-06-30 19:55:37,359 INFO  [RpcServer.handler=33,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server sceplus-vm48.almaden.ibm.com,60020,1404182803191
2014-06-30 19:55:37,359 WARN  [RpcServer.handler=19,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 12510ms
2014-06-30 19:55:37,363 INFO  [RpcServer.handler=19,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server sceplus-vm48.almaden.ibm.com,60020,1404182803191
2014-06-30 19:55:37,366 WARN  [RpcServer.handler=42,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 12525ms
2014-06-30 19:55:37,367 INFO  [RpcServer.handler=42,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server sceplus-vm48.almaden.ibm.com,60020,1404182803191
2014-06-30 19:55:37,372 WARN  [RpcServer.handler=27,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 12591ms
2014-06-30 19:55:37,372 INFO  [RpcServer.handler=27,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server sceplus-vm48.almaden.ibm.com,60020,1404182803191
2014-06-30 19:55:37,372 WARN  [RpcServer.handler=30,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 12601ms
2014-06-30 19:55:37,373 INFO  [RpcServer.handler=30,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server sceplus-vm48.almaden.ibm.com,60020,1404182803191
2014-06-30 19:55:37,373 WARN  [RpcServer.handler=32,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 12533ms
2014-06-30 19:55:37,373 INFO  [RpcServer.handler=32,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server sceplus-vm48.almaden.ibm.com,60020,1404182803191
2014-06-30 19:55:37,373 WARN  [RpcServer.handler=12,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 12533ms
2014-06-30 19:55:37,373 INFO  [RpcServer.handler=12,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server sceplus-vm48.almaden.ibm.com,60020,1404182803191
2014-06-30 19:55:37,373 WARN  [RpcServer.handler=46,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 12598ms
2014-06-30 19:55:37,373 INFO  [RpcServer.handler=46,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server sceplus-vm48.almaden.ibm.com,60020,1404182803191
2014-06-30 19:55:37,373 WARN  [RpcServer.handler=36,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 12534ms
2014-06-30 19:55:37,374 INFO  [RpcServer.handler=36,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server sceplus-vm48.almaden.ibm.com,60020,1404182803191
2014-06-30 19:55:37,374 WARN  [RpcServer.handler=20,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 12540ms
2014-06-30 19:55:37,374 INFO  [RpcServer.handler=20,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server sceplus-vm48.almaden.ibm.com,60020,1404182803191
2014-06-30 19:55:37,374 WARN  [RpcServer.handler=23,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 12522ms
2014-06-30 19:55:37,374 INFO  [RpcServer.handler=23,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server sceplus-vm48.almaden.ibm.com,60020,1404182803191
2014-06-30 19:55:37,374 WARN  [RpcServer.handler=11,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10089ms
2014-06-30 19:55:37,375 INFO  [RpcServer.handler=11,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server sceplus-vm48.almaden.ibm.com,60020,1404182803191
2014-06-30 19:55:37,375 WARN  [RpcServer.handler=41,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10090ms
2014-06-30 19:55:37,375 INFO  [RpcServer.handler=41,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server sceplus-vm48.almaden.ibm.com,60020,1404182803191
2014-06-30 19:55:37,382 WARN  [RpcServer.handler=13,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10090ms
2014-06-30 19:55:37,385 INFO  [RpcServer.handler=13,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server sceplus-vm48.almaden.ibm.com,60020,1404182803191
2014-06-30 19:55:37,398 WARN  [RpcServer.handler=47,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10114ms
2014-06-30 19:55:37,398 INFO  [RpcServer.handler=47,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server sceplus-vm48.almaden.ibm.com,60020,1404182803191
2014-06-30 19:55:37,398 WARN  [RpcServer.handler=35,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10113ms
2014-06-30 19:55:37,399 INFO  [RpcServer.handler=35,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server sceplus-vm48.almaden.ibm.com,60020,1404182803191
2014-06-30 19:55:37,402 WARN  [RpcServer.handler=10,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10115ms
2014-06-30 19:55:37,402 INFO  [RpcServer.handler=10,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server sceplus-vm48.almaden.ibm.com,60020,1404182803191
2014-06-30 19:55:37,405 WARN  [RpcServer.handler=4,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10119ms
2014-06-30 19:55:37,407 INFO  [RpcServer.handler=4,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server sceplus-vm48.almaden.ibm.com,60020,1404182803191
2014-06-30 19:55:37,407 WARN  [RpcServer.handler=40,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10124ms
2014-06-30 19:55:37,407 INFO  [RpcServer.handler=40,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server sceplus-vm48.almaden.ibm.com,60020,1404182803191
2014-06-30 19:55:37,407 WARN  [RpcServer.handler=9,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10126ms
2014-06-30 19:55:37,407 INFO  [RpcServer.handler=9,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server sceplus-vm48.almaden.ibm.com,60020,1404182803191
2014-06-30 19:55:37,410 WARN  [RpcServer.handler=3,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10125ms
2014-06-30 19:55:37,410 INFO  [RpcServer.handler=3,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server sceplus-vm48.almaden.ibm.com,60020,1404182803191
2014-06-30 19:55:37,410 WARN  [RpcServer.handler=26,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10127ms
2014-06-30 19:55:37,410 INFO  [RpcServer.handler=26,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server sceplus-vm48.almaden.ibm.com,60020,1404182803191
2014-06-30 19:55:37,411 WARN  [RpcServer.handler=16,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10129ms
2014-06-30 19:55:37,414 INFO  [RpcServer.handler=16,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server sceplus-vm48.almaden.ibm.com,60020,1404182803191
2014-06-30 19:55:37,415 WARN  [RpcServer.handler=38,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 15245ms
2014-06-30 19:55:37,419 INFO  [RpcServer.handler=38,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server sceplus-vm48.almaden.ibm.com,60020,1404182803191
2014-06-30 19:55:37,420 WARN  [RpcServer.handler=0,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 15254ms
2014-06-30 19:55:37,429 INFO  [RpcServer.handler=0,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server sceplus-vm48.almaden.ibm.com,60020,1404182803191
2014-06-30 19:55:37,429 WARN  [RpcServer.handler=17,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 15262ms
2014-06-30 19:55:37,429 INFO  [RpcServer.handler=17,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server sceplus-vm48.almaden.ibm.com,60020,1404182803191
2014-06-30 19:55:37,431 WARN  [RpcServer.handler=2,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 15375ms
2014-06-30 19:55:37,431 INFO  [RpcServer.handler=2,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server sceplus-vm48.almaden.ibm.com,60020,1404182803191
2014-06-30 19:55:37,431 WARN  [RpcServer.handler=37,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 15375ms
2014-06-30 19:55:37,432 INFO  [RpcServer.handler=37,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server sceplus-vm48.almaden.ibm.com,60020,1404182803191
2014-06-30 19:55:37,432 WARN  [RpcServer.handler=25,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 15377ms
2014-06-30 19:55:37,432 INFO  [RpcServer.handler=25,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server sceplus-vm48.almaden.ibm.com,60020,1404182803191
2014-06-30 19:55:37,432 WARN  [RpcServer.handler=49,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 15262ms
2014-06-30 19:55:37,437 INFO  [RpcServer.handler=49,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server sceplus-vm48.almaden.ibm.com,60020,1404182803191
2014-06-30 19:55:37,439 WARN  [RpcServer.handler=45,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10151ms
2014-06-30 19:55:37,439 INFO  [RpcServer.handler=45,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server sceplus-vm48.almaden.ibm.com,60020,1404182803191
2014-06-30 19:55:39,740 INFO  [JvmPauseMonitor] util.JvmPauseMonitor: Detected pause in JVM or host machine (eg GC): pause of approximately 2178ms
GC pool 'ConcurrentMarkSweep' had collection(s): count=1 time=2458ms
2014-06-30 19:55:39,985 WARN  [RpcServer.handler=15,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":42120,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.58:49972","starttimems":1404183297865,"queuetimems":0,"class":"HRegionServer","responsesize":16791,"method":"Multi"}
2014-06-30 19:55:39,998 WARN  [RpcServer.handler=15,port=60020] ipc.RpcServer: RpcServer.respondercallId: 1286 service: ClientService methodName: Multi size: 2.9m connection: 9.1.143.58:49972: output error
2014-06-30 19:55:39,999 WARN  [RpcServer.handler=15,port=60020] ipc.RpcServer: RpcServer.handler=15,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-06-30 19:55:40,072 WARN  [RpcServer.handler=21,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":42110,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.58:49972","starttimems":1404183297961,"queuetimems":1,"class":"HRegionServer","responsesize":15658,"method":"Multi"}
2014-06-30 19:55:40,072 WARN  [RpcServer.handler=21,port=60020] ipc.RpcServer: RpcServer.respondercallId: 1316 service: ClientService methodName: Multi size: 2.7m connection: 9.1.143.58:49972: output error
2014-06-30 19:55:40,072 WARN  [RpcServer.handler=21,port=60020] ipc.RpcServer: RpcServer.handler=21,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-06-30 19:55:42,211 INFO  [JvmPauseMonitor] util.JvmPauseMonitor: Detected pause in JVM or host machine (eg GC): pause of approximately 1970ms
GC pool 'ConcurrentMarkSweep' had collection(s): count=1 time=2205ms
2014-06-30 19:55:42,284 WARN  [RpcServer.handler=7,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":44430,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.58:49972","starttimems":1404183297853,"queuetimems":0,"class":"HRegionServer","responsesize":15056,"method":"Multi"}
2014-06-30 19:55:42,284 WARN  [RpcServer.handler=7,port=60020] ipc.RpcServer: RpcServer.respondercallId: 1287 service: ClientService methodName: Multi size: 2.6m connection: 9.1.143.58:49972: output error
2014-06-30 19:55:42,284 WARN  [RpcServer.handler=7,port=60020] ipc.RpcServer: RpcServer.handler=7,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-06-30 19:55:44,554 INFO  [JvmPauseMonitor] util.JvmPauseMonitor: Detected pause in JVM or host machine (eg GC): pause of approximately 1842ms
GC pool 'ConcurrentMarkSweep' had collection(s): count=1 time=2092ms
2014-06-30 19:55:44,583 WARN  [RpcServer.handler=12,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":42290,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.58:49972","starttimems":1404183302292,"queuetimems":2425,"class":"HRegionServer","responsesize":15077,"method":"Multi"}
2014-06-30 19:55:44,583 WARN  [RpcServer.handler=12,port=60020] ipc.RpcServer: RpcServer.respondercallId: 1321 service: ClientService methodName: Multi size: 2.6m connection: 9.1.143.58:49972: output error
2014-06-30 19:55:44,583 WARN  [RpcServer.handler=12,port=60020] ipc.RpcServer: RpcServer.handler=12,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-06-30 19:55:44,587 WARN  [RpcServer.handler=36,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":42296,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.58:49972","starttimems":1404183302291,"queuetimems":2470,"class":"HRegionServer","responsesize":15142,"method":"Multi"}
2014-06-30 19:55:44,587 WARN  [RpcServer.handler=36,port=60020] ipc.RpcServer: RpcServer.respondercallId: 1320 service: ClientService methodName: Multi size: 2.6m connection: 9.1.143.58:49972: output error
2014-06-30 19:55:44,588 WARN  [RpcServer.handler=36,port=60020] ipc.RpcServer: RpcServer.handler=36,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-06-30 19:55:46,956 DEBUG [MemStoreFlusher.0] regionserver.MemStoreFlusher: Flush thread woke up because memory above low water=367.3m
2014-06-30 19:55:46,956 INFO  [MemStoreFlusher.0] regionserver.MemStoreFlusher: Flush of region usertable,user2,1404183160444.5446f1f888398e97c0ec3cc30da5bf97. due to global heap pressure
2014-06-30 19:55:46,957 DEBUG [MemStoreFlusher.0] regionserver.HRegion: Started memstore flush for usertable,user2,1404183160444.5446f1f888398e97c0ec3cc30da5bf97., current region memstore size 114.5m
2014-06-30 19:55:46,957 INFO  [JvmPauseMonitor] util.JvmPauseMonitor: Detected pause in JVM or host machine (eg GC): pause of approximately 1902ms
GC pool 'ConcurrentMarkSweep' had collection(s): count=1 time=2161ms
2014-06-30 19:55:46,962 WARN  [RpcServer.handler=42,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":22180,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.58:50138","starttimems":1404183324782,"queuetimems":2594,"class":"HRegionServer","responsesize":1100,"method":"Multi"}
2014-06-30 19:55:46,963 WARN  [RpcServer.handler=42,port=60020] ipc.RpcServer: RpcServer.respondercallId: 1477 service: ClientService methodName: Multi size: 218.5k connection: 9.1.143.58:50138: output error
2014-06-30 19:55:46,963 WARN  [RpcServer.handler=42,port=60020] ipc.RpcServer: RpcServer.handler=42,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-06-30 19:55:46,990 INFO  [MemStoreFlusher.1] regionserver.DefaultStoreFlusher: Flushed, sequenceid=442, memsize=136.8m, hasBloomFilter=true, into tmp file hdfs://master:54310/hbase/data/default/usertable/994b975d2c4cd5bbe532bddfb5e4ac46/.tmp/35621afe2710412b9c89a3a40e6904ce
2014-06-30 19:55:47,013 DEBUG [MemStoreFlusher.1] regionserver.HRegionFileSystem: Committing store file hdfs://master:54310/hbase/data/default/usertable/994b975d2c4cd5bbe532bddfb5e4ac46/.tmp/35621afe2710412b9c89a3a40e6904ce as hdfs://master:54310/hbase/data/default/usertable/994b975d2c4cd5bbe532bddfb5e4ac46/family/35621afe2710412b9c89a3a40e6904ce
2014-06-30 19:55:47,028 INFO  [MemStoreFlusher.1] regionserver.HStore: Added hdfs://master:54310/hbase/data/default/usertable/994b975d2c4cd5bbe532bddfb5e4ac46/family/35621afe2710412b9c89a3a40e6904ce, entries=497900, sequenceid=442, filesize=74.2m
2014-06-30 19:55:47,029 INFO  [MemStoreFlusher.1] regionserver.HRegion: Finished memstore flush of ~163.7m/171659280, currentsize=7.9m/8259520 for region usertable,user7,1404183160444.994b975d2c4cd5bbe532bddfb5e4ac46. in 29449ms, sequenceid=442, compaction requested=true
2014-06-30 19:55:47,029 DEBUG [MemStoreFlusher.1] regionserver.CompactSplitThread: Small Compaction requested: system; Because: MemStoreFlusher.1; compaction_queue=(0:1), split_queue=0, merge_queue=0
2014-06-30 19:55:47,175 WARN  [RpcServer.handler=43,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":51862,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.58:49972","starttimems":1404183295313,"queuetimems":1,"class":"HRegionServer","responsesize":15614,"method":"Multi"}
2014-06-30 19:55:47,175 WARN  [RpcServer.handler=25,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":51894,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.58:49972","starttimems":1404183295281,"queuetimems":2,"class":"HRegionServer","responsesize":16268,"method":"Multi"}
2014-06-30 19:55:47,176 WARN  [RpcServer.handler=23,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":51971,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.58:49972","starttimems":1404183295204,"queuetimems":1,"class":"HRegionServer","responsesize":16204,"method":"Multi"}
2014-06-30 19:55:47,176 WARN  [RpcServer.handler=43,port=60020] ipc.RpcServer: RpcServer.respondercallId: 1269 service: ClientService methodName: Multi size: 2.7m connection: 9.1.143.58:49972: output error
2014-06-30 19:55:47,176 WARN  [RpcServer.handler=43,port=60020] ipc.RpcServer: RpcServer.handler=43,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-06-30 19:55:47,176 WARN  [RpcServer.handler=23,port=60020] ipc.RpcServer: RpcServer.respondercallId: 1279 service: ClientService methodName: Multi size: 2.8m connection: 9.1.143.58:49972: output error
2014-06-30 19:55:47,176 WARN  [RpcServer.handler=23,port=60020] ipc.RpcServer: RpcServer.handler=23,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-06-30 19:55:47,176 WARN  [RpcServer.handler=25,port=60020] ipc.RpcServer: RpcServer.respondercallId: 1270 service: ClientService methodName: Multi size: 2.8m connection: 9.1.143.58:49972: output error
2014-06-30 19:55:47,176 WARN  [RpcServer.handler=25,port=60020] ipc.RpcServer: RpcServer.handler=25,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-06-30 19:55:47,176 WARN  [RpcServer.handler=46,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":34633,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.58:50135","starttimems":1404183312542,"queuetimems":2581,"class":"HRegionServer","responsesize":16566,"method":"Multi"}
2014-06-30 19:55:47,179 WARN  [RpcServer.handler=46,port=60020] ipc.RpcServer: RpcServer.respondercallId: 1391 service: ClientService methodName: Multi size: 2.9m connection: 9.1.143.58:50135: output error
2014-06-30 19:55:47,179 WARN  [RpcServer.handler=46,port=60020] ipc.RpcServer: RpcServer.handler=46,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-06-30 19:55:47,187 WARN  [RpcServer.handler=3,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":49365,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.58:49972","starttimems":1404183297822,"queuetimems":0,"class":"HRegionServer","responsesize":16063,"method":"Multi"}
2014-06-30 19:55:47,188 WARN  [RpcServer.handler=41,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":42592,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.58:49972","starttimems":1404183304596,"queuetimems":2470,"class":"HRegionServer","responsesize":16268,"method":"Multi"}
2014-06-30 19:55:47,188 WARN  [RpcServer.handler=16,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":49678,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.58:49972","starttimems":1404183297510,"queuetimems":1,"class":"HRegionServer","responsesize":15489,"method":"Multi"}
2014-06-30 19:55:47,188 WARN  [RpcServer.handler=3,port=60020] ipc.RpcServer: RpcServer.respondercallId: 1291 service: ClientService methodName: Multi size: 2.8m connection: 9.1.143.58:49972: output error
2014-06-30 19:55:47,189 WARN  [RpcServer.handler=3,port=60020] ipc.RpcServer: RpcServer.handler=3,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-06-30 19:55:47,189 WARN  [RpcServer.handler=13,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":51932,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.58:49972","starttimems":1404183295256,"queuetimems":1,"class":"HRegionServer","responsesize":14654,"method":"Multi"}
2014-06-30 19:55:47,190 WARN  [RpcServer.handler=19,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":39741,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.58:49972","starttimems":1404183307448,"queuetimems":5295,"class":"HRegionServer","responsesize":16566,"method":"Multi"}
2014-06-30 19:55:47,190 WARN  [RpcServer.handler=13,port=60020] ipc.RpcServer: RpcServer.respondercallId: 1271 service: ClientService methodName: Multi size: 2.6m connection: 9.1.143.58:49972: output error
2014-06-30 19:55:47,190 WARN  [RpcServer.handler=24,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":34647,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.58:50135","starttimems":1404183312542,"queuetimems":2597,"class":"HRegionServer","responsesize":16114,"method":"Multi"}
2014-06-30 19:55:47,190 WARN  [RpcServer.handler=13,port=60020] ipc.RpcServer: RpcServer.handler=13,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-06-30 19:55:47,191 WARN  [RpcServer.handler=33,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":34649,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.58:50135","starttimems":1404183312541,"queuetimems":2605,"class":"HRegionServer","responsesize":16268,"method":"Multi"}
2014-06-30 19:55:49,088 WARN  [RpcServer.handler=20,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":53847,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.58:49972","starttimems":1404183295241,"queuetimems":0,"class":"HRegionServer","responsesize":14017,"method":"Multi"}
2014-06-30 19:55:49,087 WARN  [RpcServer.handler=4,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":41635,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.58:49972","starttimems":1404183307448,"queuetimems":5307,"class":"HRegionServer","responsesize":16114,"method":"Multi"}
2014-06-30 19:55:49,083 WARN  [RpcServer.handler=18,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":51792,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.58:49972","starttimems":1404183297291,"queuetimems":1,"class":"HRegionServer","responsesize":15624,"method":"Multi"}
2014-06-30 19:55:49,083 WARN  [RpcServer.handler=37,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":53851,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.58:49972","starttimems":1404183295231,"queuetimems":1,"class":"HRegionServer","responsesize":15915,"method":"Multi"}
2014-06-30 19:55:49,079 WARN  [RpcServer.handler=32,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":51764,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.58:49972","starttimems":1404183297315,"queuetimems":1,"class":"HRegionServer","responsesize":15737,"method":"Multi"}
2014-06-30 19:55:49,079 WARN  [RpcServer.handler=19,port=60020] ipc.RpcServer: RpcServer.respondercallId: 1336 service: ClientService methodName: Multi size: 2.9m connection: 9.1.143.58:49972: output error
2014-06-30 19:55:49,079 WARN  [RpcServer.handler=27,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":51246,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.58:49972","starttimems":1404183297832,"queuetimems":0,"class":"HRegionServer","responsesize":15083,"method":"Multi"}
2014-06-30 19:55:49,077 INFO  [JvmPauseMonitor] util.JvmPauseMonitor: Detected pause in JVM or host machine (eg GC): pause of approximately 1620ms
GC pool 'ConcurrentMarkSweep' had collection(s): count=1 time=1882ms
2014-06-30 19:55:47,193 WARN  [RpcServer.handler=17,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":49913,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.58:49972","starttimems":1404183297280,"queuetimems":2,"class":"HRegionServer","responsesize":15078,"method":"Multi"}
2014-06-30 19:55:47,193 WARN  [RpcServer.handler=6,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":44901,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.58:49972","starttimems":1404183302292,"queuetimems":2452,"class":"HRegionServer","responsesize":16408,"method":"Multi"}
2014-06-30 19:55:47,191 WARN  [RpcServer.handler=24,port=60020] ipc.RpcServer: RpcServer.respondercallId: 1392 service: ClientService methodName: Multi size: 2.8m connection: 9.1.143.58:50135: output error
2014-06-30 19:55:49,089 WARN  [RpcServer.handler=32,port=60020] ipc.RpcServer: RpcServer.respondercallId: 1305 service: ClientService methodName: Multi size: 2.8m connection: 9.1.143.58:49972: output error
2014-06-30 19:55:49,089 WARN  [RpcServer.handler=19,port=60020] ipc.RpcServer: RpcServer.handler=19,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-06-30 19:55:49,090 WARN  [RpcServer.handler=32,port=60020] ipc.RpcServer: RpcServer.handler=32,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-06-30 19:55:49,090 WARN  [RpcServer.handler=24,port=60020] ipc.RpcServer: RpcServer.handler=24,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-06-30 19:55:49,090 WARN  [RpcServer.handler=33,port=60020] ipc.RpcServer: RpcServer.respondercallId: 1390 service: ClientService methodName: Multi size: 2.8m connection: 9.1.143.58:50135: output error
2014-06-30 19:55:49,090 WARN  [RpcServer.handler=16,port=60020] ipc.RpcServer: RpcServer.respondercallId: 1296 service: ClientService methodName: Multi size: 2.7m connection: 9.1.143.58:49972: output error
2014-06-30 19:55:49,091 WARN  [RpcServer.handler=33,port=60020] ipc.RpcServer: RpcServer.handler=33,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-06-30 19:55:49,091 WARN  [RpcServer.handler=16,port=60020] ipc.RpcServer: RpcServer.handler=16,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-06-30 19:55:49,091 WARN  [RpcServer.handler=41,port=60020] ipc.RpcServer: RpcServer.respondercallId: 1333 service: ClientService methodName: Multi size: 2.8m connection: 9.1.143.58:49972: output error
2014-06-30 19:55:49,091 WARN  [RpcServer.handler=41,port=60020] ipc.RpcServer: RpcServer.handler=41,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-06-30 19:55:49,134 WARN  [RpcServer.handler=2,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":51292,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.58:49972","starttimems":1404183297842,"queuetimems":1,"class":"HRegionServer","responsesize":15624,"method":"Multi"}
2014-06-30 19:55:49,134 WARN  [RpcServer.handler=14,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":46843,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.58:49972","starttimems":1404183302291,"queuetimems":2460,"class":"HRegionServer","responsesize":15712,"method":"Multi"}
2014-06-30 19:55:49,134 WARN  [RpcServer.handler=0,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":53916,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.58:49972","starttimems":1404183295218,"queuetimems":0,"class":"HRegionServer","responsesize":16666,"method":"Multi"}
2014-06-30 19:55:49,150 WARN  [RpcServer.handler=10,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":44899,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.58:49972","starttimems":1404183302292,"queuetimems":2433,"class":"HRegionServer","responsesize":15967,"method":"Multi"}
2014-06-30 19:55:49,157 WARN  [RpcServer.handler=6,port=60020] ipc.RpcServer: RpcServer.respondercallId: 1324 service: ClientService methodName: Multi size: 2.9m connection: 9.1.143.58:49972: output error
2014-06-30 19:55:49,157 WARN  [RpcServer.handler=6,port=60020] ipc.RpcServer: RpcServer.handler=6,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-06-30 19:55:49,158 WARN  [RpcServer.handler=17,port=60020] ipc.RpcServer: RpcServer.respondercallId: 1266 service: ClientService methodName: Multi size: 2.6m connection: 9.1.143.58:49972: output error
2014-06-30 19:55:49,158 WARN  [RpcServer.handler=17,port=60020] ipc.RpcServer: RpcServer.handler=17,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-06-30 19:55:49,158 WARN  [RpcServer.handler=27,port=60020] ipc.RpcServer: RpcServer.respondercallId: 1290 service: ClientService methodName: Multi size: 2.6m connection: 9.1.143.58:49972: output error
2014-06-30 19:55:49,158 WARN  [RpcServer.handler=27,port=60020] ipc.RpcServer: RpcServer.handler=27,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-06-30 19:55:49,166 WARN  [RpcServer.handler=37,port=60020] ipc.RpcServer: RpcServer.respondercallId: 1274 service: ClientService methodName: Multi size: 2.8m connection: 9.1.143.58:49972: output error
2014-06-30 19:55:49,166 WARN  [RpcServer.handler=37,port=60020] ipc.RpcServer: RpcServer.handler=37,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-06-30 19:55:49,167 WARN  [RpcServer.handler=18,port=60020] ipc.RpcServer: RpcServer.respondercallId: 1264 service: ClientService methodName: Multi size: 2.7m connection: 9.1.143.58:49972: output error
2014-06-30 19:55:49,167 WARN  [RpcServer.handler=18,port=60020] ipc.RpcServer: RpcServer.handler=18,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-06-30 19:55:49,167 WARN  [RpcServer.handler=4,port=60020] ipc.RpcServer: RpcServer.respondercallId: 1337 service: ClientService methodName: Multi size: 2.8m connection: 9.1.143.58:49972: output error
2014-06-30 19:55:49,167 WARN  [RpcServer.handler=4,port=60020] ipc.RpcServer: RpcServer.handler=4,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-06-30 19:55:49,167 WARN  [RpcServer.handler=20,port=60020] ipc.RpcServer: RpcServer.respondercallId: 1272 service: ClientService methodName: Multi size: 2.5m connection: 9.1.143.58:49972: output error
2014-06-30 19:55:49,167 WARN  [RpcServer.handler=20,port=60020] ipc.RpcServer: RpcServer.handler=20,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-06-30 19:55:49,167 WARN  [RpcServer.handler=10,port=60020] ipc.RpcServer: RpcServer.respondercallId: 1322 service: ClientService methodName: Multi size: 2.8m connection: 9.1.143.58:49972: output error
2014-06-30 19:55:49,167 WARN  [RpcServer.handler=10,port=60020] ipc.RpcServer: RpcServer.handler=10,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-06-30 19:55:49,168 WARN  [RpcServer.handler=0,port=60020] ipc.RpcServer: RpcServer.respondercallId: 1277 service: ClientService methodName: Multi size: 2.9m connection: 9.1.143.58:49972: output error
2014-06-30 19:55:49,168 WARN  [RpcServer.handler=0,port=60020] ipc.RpcServer: RpcServer.handler=0,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-06-30 19:55:49,168 WARN  [RpcServer.handler=14,port=60020] ipc.RpcServer: RpcServer.respondercallId: 1325 service: ClientService methodName: Multi size: 2.7m connection: 9.1.143.58:49972: output error
2014-06-30 19:55:49,168 WARN  [RpcServer.handler=14,port=60020] ipc.RpcServer: RpcServer.handler=14,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-06-30 19:55:49,168 WARN  [RpcServer.handler=2,port=60020] ipc.RpcServer: RpcServer.respondercallId: 1289 service: ClientService methodName: Multi size: 2.7m connection: 9.1.143.58:49972: output error
2014-06-30 19:55:49,168 WARN  [RpcServer.handler=2,port=60020] ipc.RpcServer: RpcServer.handler=2,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-06-30 19:55:49,253 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-06-30 19:55:49,278 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 2228 synced till here 2195
2014-06-30 19:55:49,330 DEBUG [MemStoreFlusher.0] util.FSUtils: DFS Client does not support most favored nodes create; using default create
2014-06-30 19:55:51,295 INFO  [JvmPauseMonitor] util.JvmPauseMonitor: Detected pause in JVM or host machine (eg GC): pause of approximately 1704ms
GC pool 'ConcurrentMarkSweep' had collection(s): count=1 time=1883ms
2014-06-30 19:55:51,430 WARN  [RpcServer.handler=49,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":49136,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.58:49972","starttimems":1404183302292,"queuetimems":2442,"class":"HRegionServer","responsesize":16791,"method":"Multi"}
2014-06-30 19:55:51,457 WARN  [RpcServer.handler=38,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":53488,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.58:49972","starttimems":1404183297940,"queuetimems":0,"class":"HRegionServer","responsesize":16900,"method":"Multi"}
2014-06-30 19:55:51,459 WARN  [RpcServer.handler=49,port=60020] ipc.RpcServer: RpcServer.respondercallId: 1323 service: ClientService methodName: Multi size: 2.9m connection: 9.1.143.58:49972: output error
2014-06-30 19:55:51,457 WARN  [RpcServer.handler=28,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":54162,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.58:49972","starttimems":1404183297266,"queuetimems":1,"class":"HRegionServer","responsesize":17167,"method":"Multi"}
2014-06-30 19:55:51,459 WARN  [RpcServer.handler=49,port=60020] ipc.RpcServer: RpcServer.handler=49,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-06-30 19:55:51,459 WARN  [RpcServer.handler=29,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":53538,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.58:49972","starttimems":1404183297920,"queuetimems":1,"class":"HRegionServer","responsesize":17012,"method":"Multi"}
2014-06-30 19:55:51,459 WARN  [RpcServer.handler=28,port=60020] ipc.RpcServer: RpcServer.respondercallId: 1268 service: ClientService methodName: Multi size: 3.0m connection: 9.1.143.58:49972: output error
2014-06-30 19:55:51,459 WARN  [RpcServer.handler=28,port=60020] ipc.RpcServer: RpcServer.handler=28,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-06-30 19:55:51,459 WARN  [RpcServer.handler=29,port=60020] ipc.RpcServer: RpcServer.respondercallId: 1281 service: ClientService methodName: Multi size: 3.0m connection: 9.1.143.58:49972: output error
2014-06-30 19:55:51,460 WARN  [RpcServer.handler=29,port=60020] ipc.RpcServer: RpcServer.handler=29,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-06-30 19:55:51,460 WARN  [RpcServer.handler=38,port=60020] ipc.RpcServer: RpcServer.respondercallId: 1280 service: ClientService methodName: Multi size: 3.0m connection: 9.1.143.58:49972: output error
2014-06-30 19:55:51,460 WARN  [RpcServer.handler=38,port=60020] ipc.RpcServer: RpcServer.handler=38,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-06-30 19:55:51,474 DEBUG [RpcServer.handler=37,port=60020] regionserver.HRegion: Flush requested on usertable,user2,1404183160444.5446f1f888398e97c0ec3cc30da5bf97.
2014-06-30 19:55:51,475 DEBUG [MemStoreFlusher.1] regionserver.HRegion: NOT flushing memstore for region usertable,user2,1404183160444.5446f1f888398e97c0ec3cc30da5bf97., flushing=true, writesEnabled=true
2014-06-30 19:55:51,513 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1404182803191/sceplus-vm48.almaden.ibm.com%2C60020%2C1404182803191.1404183319982 with entries=109, filesize=86.3m; new WAL /hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1404182803191/sceplus-vm48.almaden.ibm.com%2C60020%2C1404182803191.1404183349254
2014-06-30 19:55:51,521 WARN  [RpcServer.handler=5,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":33990,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.58:50137","starttimems":1404183317530,"queuetimems":2786,"class":"HRegionServer","responsesize":16791,"method":"Multi"}
2014-06-30 19:55:51,521 WARN  [RpcServer.handler=5,port=60020] ipc.RpcServer: RpcServer.respondercallId: 1423 service: ClientService methodName: Multi size: 2.9m connection: 9.1.143.58:50137: output error
2014-06-30 19:55:51,521 WARN  [RpcServer.handler=5,port=60020] ipc.RpcServer: RpcServer.handler=5,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-06-30 19:55:53,829 INFO  [JvmPauseMonitor] util.JvmPauseMonitor: Detected pause in JVM or host machine (eg GC): pause of approximately 2033ms
GC pool 'ConcurrentMarkSweep' had collection(s): count=1 time=2057ms
2014-06-30 19:55:53,848 WARN  [RpcServer.handler=9,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":36282,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.58:50137","starttimems":1404183317565,"queuetimems":190,"class":"HRegionServer","responsesize":16666,"method":"Multi"}
2014-06-30 19:55:53,848 WARN  [RpcServer.handler=22,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":29076,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.58:50138","starttimems":1404183324771,"queuetimems":4955,"class":"HRegionServer","responsesize":15624,"method":"Multi"}
2014-06-30 19:55:53,848 WARN  [RpcServer.handler=9,port=60020] ipc.RpcServer: RpcServer.respondercallId: 1418 service: ClientService methodName: Multi size: 2.9m connection: 9.1.143.58:50137: output error
2014-06-30 19:55:53,848 WARN  [RpcServer.handler=35,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":36309,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.58:50137","starttimems":1404183317538,"queuetimems":2749,"class":"HRegionServer","responsesize":15712,"method":"Multi"}
2014-06-30 19:55:53,849 WARN  [RpcServer.handler=9,port=60020] ipc.RpcServer: RpcServer.handler=9,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-06-30 19:55:53,849 WARN  [RpcServer.handler=22,port=60020] ipc.RpcServer: RpcServer.respondercallId: 1475 service: ClientService methodName: Multi size: 2.7m connection: 9.1.143.58:50138: output error
2014-06-30 19:55:53,849 WARN  [RpcServer.handler=22,port=60020] ipc.RpcServer: RpcServer.handler=22,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-06-30 19:55:53,849 WARN  [RpcServer.handler=35,port=60020] ipc.RpcServer: RpcServer.respondercallId: 1422 service: ClientService methodName: Multi size: 2.7m connection: 9.1.143.58:50137: output error
2014-06-30 19:55:53,849 WARN  [RpcServer.handler=35,port=60020] ipc.RpcServer: RpcServer.handler=35,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-06-30 19:55:53,849 WARN  [RpcServer.handler=45,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":36320,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.58:50137","starttimems":1404183317529,"queuetimems":2795,"class":"HRegionServer","responsesize":15142,"method":"Multi"}
2014-06-30 19:55:53,849 WARN  [RpcServer.handler=45,port=60020] ipc.RpcServer: RpcServer.respondercallId: 1417 service: ClientService methodName: Multi size: 2.6m connection: 9.1.143.58:50137: output error
2014-06-30 19:55:53,849 WARN  [RpcServer.handler=45,port=60020] ipc.RpcServer: RpcServer.handler=45,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-06-30 19:55:54,043 DEBUG [regionserver60020-smallCompactions-1404182848217] regionserver.HRegionFileSystem: Committing store file hdfs://master:54310/hbase/data/default/usertable/28938d267c1c61f7feb582b85e4c172f/.tmp/8c88397781b449829d9e7935422bfa7e as hdfs://master:54310/hbase/data/default/usertable/28938d267c1c61f7feb582b85e4c172f/family/8c88397781b449829d9e7935422bfa7e
2014-06-30 19:55:54,046 WARN  [RpcServer.handler=26,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":36493,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.58:50137","starttimems":1404183317552,"queuetimems":2732,"class":"HRegionServer","responsesize":16408,"method":"Multi"}
2014-06-30 19:55:54,046 WARN  [RpcServer.handler=26,port=60020] ipc.RpcServer: RpcServer.respondercallId: 1419 service: ClientService methodName: Multi size: 2.9m connection: 9.1.143.58:50137: output error
2014-06-30 19:55:54,046 WARN  [RpcServer.handler=26,port=60020] ipc.RpcServer: RpcServer.handler=26,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-06-30 19:55:54,047 WARN  [RpcServer.handler=39,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":29273,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.58:50138","starttimems":1404183324774,"queuetimems":4806,"class":"HRegionServer","responsesize":15384,"method":"Multi"}
2014-06-30 19:55:54,047 WARN  [RpcServer.handler=1,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":36467,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.58:50137","starttimems":1404183317580,"queuetimems":189,"class":"HRegionServer","responsesize":16204,"method":"Multi"}
2014-06-30 19:55:54,047 WARN  [RpcServer.handler=39,port=60020] ipc.RpcServer: RpcServer.respondercallId: 1472 service: ClientService methodName: Multi size: 2.7m connection: 9.1.143.58:50138: output error
2014-06-30 19:55:54,047 WARN  [RpcServer.handler=1,port=60020] ipc.RpcServer: RpcServer.respondercallId: 1425 service: ClientService methodName: Multi size: 2.8m connection: 9.1.143.58:50137: output error
2014-06-30 19:55:54,047 WARN  [RpcServer.handler=1,port=60020] ipc.RpcServer: RpcServer.handler=1,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-06-30 19:55:54,048 WARN  [RpcServer.handler=8,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":36504,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.58:50137","starttimems":1404183317543,"queuetimems":2733,"class":"HRegionServer","responsesize":15967,"method":"Multi"}
2014-06-30 19:55:54,048 WARN  [RpcServer.handler=48,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":29274,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.58:50138","starttimems":1404183324773,"queuetimems":4918,"class":"HRegionServer","responsesize":16074,"method":"Multi"}
2014-06-30 19:55:54,048 WARN  [RpcServer.handler=8,port=60020] ipc.RpcServer: RpcServer.respondercallId: 1420 service: ClientService methodName: Multi size: 2.8m connection: 9.1.143.58:50137: output error
2014-06-30 19:55:54,048 WARN  [RpcServer.handler=8,port=60020] ipc.RpcServer: RpcServer.handler=8,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-06-30 19:55:54,048 WARN  [RpcServer.handler=44,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":36508,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.58:50137","starttimems":1404183317538,"queuetimems":2739,"class":"HRegionServer","responsesize":15077,"method":"Multi"}
2014-06-30 19:55:54,048 WARN  [RpcServer.handler=44,port=60020] ipc.RpcServer: RpcServer.respondercallId: 1421 service: ClientService methodName: Multi size: 2.6m connection: 9.1.143.58:50137: output error
2014-06-30 19:55:54,048 WARN  [RpcServer.handler=44,port=60020] ipc.RpcServer: RpcServer.handler=44,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-06-30 19:55:54,051 WARN  [RpcServer.handler=39,port=60020] ipc.RpcServer: RpcServer.handler=39,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-06-30 19:55:54,051 WARN  [RpcServer.handler=48,port=60020] ipc.RpcServer: RpcServer.respondercallId: 1473 service: ClientService methodName: Multi size: 2.8m connection: 9.1.143.58:50138: output error
2014-06-30 19:55:54,051 WARN  [RpcServer.handler=48,port=60020] ipc.RpcServer: RpcServer.handler=48,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-06-30 19:55:54,062 DEBUG [MemStoreFlusher.1] regionserver.MemStoreFlusher: Flush thread woke up because memory above low water=367.3m
2014-06-30 19:55:54,063 INFO  [MemStoreFlusher.1] regionserver.MemStoreFlusher: Flush of region usertable,user8,1404183160444.28938d267c1c61f7feb582b85e4c172f. due to global heap pressure
2014-06-30 19:55:54,063 DEBUG [MemStoreFlusher.1] regionserver.HRegion: Started memstore flush for usertable,user8,1404183160444.28938d267c1c61f7feb582b85e4c172f., current region memstore size 108.2m
2014-06-30 19:55:54,082 INFO  [RpcServer.handler=33,port=60020] regionserver.MemStoreFlusher: Blocking updates on sceplus-vm48.almaden.ibm.com,60020,1404182803191: the global memstore size 390.4m is >= than blocking 386.7m size
2014-06-30 19:55:54,083 INFO  [RpcServer.handler=20,port=60020] regionserver.MemStoreFlusher: Blocking updates on sceplus-vm48.almaden.ibm.com,60020,1404182803191: the global memstore size 390.4m is >= than blocking 386.7m size
2014-06-30 19:55:54,083 INFO  [RpcServer.handler=13,port=60020] regionserver.MemStoreFlusher: Blocking updates on sceplus-vm48.almaden.ibm.com,60020,1404182803191: the global memstore size 390.4m is >= than blocking 386.7m size
2014-06-30 19:55:54,083 INFO  [RpcServer.handler=4,port=60020] regionserver.MemStoreFlusher: Blocking updates on sceplus-vm48.almaden.ibm.com,60020,1404182803191: the global memstore size 390.4m is >= than blocking 386.7m size
2014-06-30 19:55:54,154 INFO  [RpcServer.handler=0,port=60020] regionserver.MemStoreFlusher: Blocking updates on sceplus-vm48.almaden.ibm.com,60020,1404182803191: the global memstore size 393.5m is >= than blocking 386.7m size
2014-06-30 19:55:54,155 WARN  [RpcServer.handler=30,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":29557,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.58:50138","starttimems":1404183324597,"queuetimems":4813,"class":"HRegionServer","responsesize":15056,"method":"Multi"}
2014-06-30 19:55:54,155 WARN  [RpcServer.handler=47,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":29369,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.58:50138","starttimems":1404183324786,"queuetimems":2598,"class":"HRegionServer","responsesize":16063,"method":"Multi"}
2014-06-30 19:55:54,155 WARN  [RpcServer.handler=31,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":29384,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.58:50138","starttimems":1404183324771,"queuetimems":4974,"class":"HRegionServer","responsesize":15489,"method":"Multi"}
2014-06-30 19:55:54,155 WARN  [RpcServer.handler=30,port=60020] ipc.RpcServer: RpcServer.respondercallId: 1469 service: ClientService methodName: Multi size: 2.6m connection: 9.1.143.58:50138: output error
2014-06-30 19:55:54,155 WARN  [RpcServer.handler=30,port=60020] ipc.RpcServer: RpcServer.handler=30,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-06-30 19:55:54,155 WARN  [RpcServer.handler=47,port=60020] ipc.RpcServer: RpcServer.respondercallId: 1470 service: ClientService methodName: Multi size: 2.8m connection: 9.1.143.58:50138: output error
2014-06-30 19:55:54,155 INFO  [RpcServer.handler=21,port=60020] regionserver.MemStoreFlusher: Blocking updates on sceplus-vm48.almaden.ibm.com,60020,1404182803191: the global memstore size 404.2m is >= than blocking 386.7m size
2014-06-30 19:55:54,155 WARN  [RpcServer.handler=47,port=60020] ipc.RpcServer: RpcServer.handler=47,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-06-30 19:55:54,156 INFO  [RpcServer.handler=7,port=60020] regionserver.MemStoreFlusher: Blocking updates on sceplus-vm48.almaden.ibm.com,60020,1404182803191: the global memstore size 405.6m is >= than blocking 386.7m size
2014-06-30 19:55:54,156 INFO  [RpcServer.handler=40,port=60020] regionserver.MemStoreFlusher: Blocking updates on sceplus-vm48.almaden.ibm.com,60020,1404182803191: the global memstore size 405.6m is >= than blocking 386.7m size
2014-06-30 19:55:54,156 INFO  [RpcServer.handler=15,port=60020] regionserver.MemStoreFlusher: Blocking updates on sceplus-vm48.almaden.ibm.com,60020,1404182803191: the global memstore size 405.6m is >= than blocking 386.7m size
2014-06-30 19:55:54,156 WARN  [RpcServer.handler=31,port=60020] ipc.RpcServer: RpcServer.respondercallId: 1476 service: ClientService methodName: Multi size: 2.7m connection: 9.1.143.58:50138: output error
2014-06-30 19:55:54,156 WARN  [RpcServer.handler=31,port=60020] ipc.RpcServer: RpcServer.handler=31,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-06-30 19:55:54,156 WARN  [RpcServer.handler=34,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":29385,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.58:50138","starttimems":1404183324771,"queuetimems":4935,"class":"HRegionServer","responsesize":15083,"method":"Multi"}
2014-06-30 19:55:54,156 WARN  [RpcServer.handler=34,port=60020] ipc.RpcServer: RpcServer.respondercallId: 1474 service: ClientService methodName: Multi size: 2.6m connection: 9.1.143.58:50138: output error
2014-06-30 19:55:54,156 WARN  [RpcServer.handler=34,port=60020] ipc.RpcServer: RpcServer.handler=34,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-06-30 19:55:54,157 INFO  [RpcServer.handler=42,port=60020] regionserver.MemStoreFlusher: Blocking updates on sceplus-vm48.almaden.ibm.com,60020,1404182803191: the global memstore size 405.6m is >= than blocking 386.7m size
2014-06-30 19:55:54,172 INFO  [MemStoreFlusher.0] regionserver.DefaultStoreFlusher: Flushed, sequenceid=472, memsize=57.4m, hasBloomFilter=true, into tmp file hdfs://master:54310/hbase/data/default/usertable/5446f1f888398e97c0ec3cc30da5bf97/.tmp/6b88676d2d074964ae09ff1f408a9663
2014-06-30 19:55:54,183 DEBUG [MemStoreFlusher.0] regionserver.HRegionFileSystem: Committing store file hdfs://master:54310/hbase/data/default/usertable/5446f1f888398e97c0ec3cc30da5bf97/.tmp/6b88676d2d074964ae09ff1f408a9663 as hdfs://master:54310/hbase/data/default/usertable/5446f1f888398e97c0ec3cc30da5bf97/family/6b88676d2d074964ae09ff1f408a9663
2014-06-30 19:55:54,190 INFO  [MemStoreFlusher.0] regionserver.HStore: Added hdfs://master:54310/hbase/data/default/usertable/5446f1f888398e97c0ec3cc30da5bf97/family/6b88676d2d074964ae09ff1f408a9663, entries=208960, sequenceid=472, filesize=31.1m
2014-06-30 19:55:54,190 INFO  [MemStoreFlusher.0] regionserver.HRegion: Finished memstore flush of ~117.6m/123344320, currentsize=28.2m/29565200 for region usertable,user2,1404183160444.5446f1f888398e97c0ec3cc30da5bf97. in 7233ms, sequenceid=472, compaction requested=true
2014-06-30 19:55:54,191 DEBUG [MemStoreFlusher.0] regionserver.CompactSplitThread: Small Compaction requested: system; Because: MemStoreFlusher.0; compaction_queue=(0:2), split_queue=0, merge_queue=0
2014-06-30 19:55:54,191 WARN  [RpcServer.handler=42,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 34ms
2014-06-30 19:55:54,191 INFO  [RpcServer.handler=42,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server sceplus-vm48.almaden.ibm.com,60020,1404182803191
2014-06-30 19:55:54,191 WARN  [RpcServer.handler=15,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 36ms
2014-06-30 19:55:54,191 INFO  [RpcServer.handler=15,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server sceplus-vm48.almaden.ibm.com,60020,1404182803191
2014-06-30 19:55:54,191 WARN  [RpcServer.handler=40,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 35ms
2014-06-30 19:55:54,191 INFO  [RpcServer.handler=40,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server sceplus-vm48.almaden.ibm.com,60020,1404182803191
2014-06-30 19:55:54,191 WARN  [RpcServer.handler=7,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 37ms
2014-06-30 19:55:54,191 INFO  [RpcServer.handler=7,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server sceplus-vm48.almaden.ibm.com,60020,1404182803191
2014-06-30 19:55:54,193 WARN  [RpcServer.handler=21,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 38ms
2014-06-30 19:55:54,193 INFO  [RpcServer.handler=21,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server sceplus-vm48.almaden.ibm.com,60020,1404182803191
2014-06-30 19:55:54,193 WARN  [RpcServer.handler=0,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 39ms
2014-06-30 19:55:54,193 INFO  [RpcServer.handler=0,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server sceplus-vm48.almaden.ibm.com,60020,1404182803191
2014-06-30 19:55:54,195 WARN  [RpcServer.handler=4,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 112ms
2014-06-30 19:55:54,195 INFO  [RpcServer.handler=4,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server sceplus-vm48.almaden.ibm.com,60020,1404182803191
2014-06-30 19:55:54,195 WARN  [RpcServer.handler=13,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 112ms
2014-06-30 19:55:54,195 INFO  [RpcServer.handler=13,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server sceplus-vm48.almaden.ibm.com,60020,1404182803191
2014-06-30 19:55:54,195 WARN  [RpcServer.handler=20,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 112ms
2014-06-30 19:55:54,195 INFO  [RpcServer.handler=20,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server sceplus-vm48.almaden.ibm.com,60020,1404182803191
2014-06-30 19:55:54,195 WARN  [RpcServer.handler=33,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 113ms
2014-06-30 19:55:54,196 INFO  [RpcServer.handler=33,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server sceplus-vm48.almaden.ibm.com,60020,1404182803191
2014-06-30 19:55:54,259 DEBUG [regionserver60020-smallCompactions-1404182848217] regionserver.HStore: Removing store files after compaction...
2014-06-30 19:55:54,260 WARN  [RpcServer.handler=11,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":34484,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.58:50138","starttimems":1404183319776,"queuetimems":4,"class":"HRegionServer","responsesize":17012,"method":"Multi"}
2014-06-30 19:55:54,261 WARN  [RpcServer.handler=11,port=60020] ipc.RpcServer: RpcServer.respondercallId: 1468 service: ClientService methodName: Multi size: 3.0m connection: 9.1.143.58:50138: output error
2014-06-30 19:55:54,261 WARN  [RpcServer.handler=11,port=60020] ipc.RpcServer: RpcServer.handler=11,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-06-30 19:55:54,270 DEBUG [regionserver60020-smallCompactions-1404182848217] backup.HFileArchiver: Finished archiving from class org.apache.hadoop.hbase.backup.HFileArchiver$FileableStoreFile, file:hdfs://master:54310/hbase/data/default/usertable/28938d267c1c61f7feb582b85e4c172f/family/3a42b3247d78465eb0d13f74ec7379cf, to hdfs://master:54310/hbase/archive/data/default/usertable/28938d267c1c61f7feb582b85e4c172f/family/3a42b3247d78465eb0d13f74ec7379cf
2014-06-30 19:55:54,282 DEBUG [regionserver60020-smallCompactions-1404182848217] backup.HFileArchiver: Finished archiving from class org.apache.hadoop.hbase.backup.HFileArchiver$FileableStoreFile, file:hdfs://master:54310/hbase/data/default/usertable/28938d267c1c61f7feb582b85e4c172f/family/cbe61450a0bc4936abb02a53218a6e89, to hdfs://master:54310/hbase/archive/data/default/usertable/28938d267c1c61f7feb582b85e4c172f/family/cbe61450a0bc4936abb02a53218a6e89
2014-06-30 19:55:54,304 DEBUG [regionserver60020-smallCompactions-1404182848217] backup.HFileArchiver: Finished archiving from class org.apache.hadoop.hbase.backup.HFileArchiver$FileableStoreFile, file:hdfs://master:54310/hbase/data/default/usertable/28938d267c1c61f7feb582b85e4c172f/family/5e35681f42734d87ad30ea719516990f, to hdfs://master:54310/hbase/archive/data/default/usertable/28938d267c1c61f7feb582b85e4c172f/family/5e35681f42734d87ad30ea719516990f
2014-06-30 19:55:54,306 INFO  [regionserver60020-smallCompactions-1404182848217] regionserver.HStore: Completed major compaction of 3 file(s) in family of usertable,user8,1404183160444.28938d267c1c61f7feb582b85e4c172f. into 8c88397781b449829d9e7935422bfa7e(size=125.8m), total size for store is 125.8m. This selection was in queue for 0sec, and took 49sec to execute.
2014-06-30 19:55:54,327 INFO  [regionserver60020-smallCompactions-1404182848217] regionserver.CompactSplitThread: Completed compaction: Request = regionName=usertable,user8,1404183160444.28938d267c1c61f7feb582b85e4c172f., storeName=family, fileCount=3, fileSize=161.6m, priority=7, time=3446935460412; duration=49sec
2014-06-30 19:55:54,331 DEBUG [regionserver60020-smallCompactions-1404182848217] regionserver.CompactSplitThread: CompactSplitThread Status: compaction_queue=(0:2), split_queue=0, merge_queue=0
2014-06-30 19:55:54,331 DEBUG [regionserver60020-smallCompactions-1404182848217] compactions.RatioBasedCompactionPolicy: Selecting compaction from 3 store files, 0 compacting, 3 eligible, 10 blocking
2014-06-30 19:55:54,338 DEBUG [regionserver60020-smallCompactions-1404182848217] compactions.ExploringCompactionPolicy: Exploring compaction algorithm has selected 3 files of size 203537823 starting at candidate #0 after considering 1 permutations with 1 in ratio
2014-06-30 19:55:54,348 DEBUG [regionserver60020-smallCompactions-1404182848217] regionserver.HStore: 994b975d2c4cd5bbe532bddfb5e4ac46 - family: Initiating major compaction
2014-06-30 19:55:54,348 INFO  [regionserver60020-smallCompactions-1404182848217] regionserver.HRegion: Starting compaction on family in region usertable,user7,1404183160444.994b975d2c4cd5bbe532bddfb5e4ac46.
2014-06-30 19:55:54,353 INFO  [regionserver60020-smallCompactions-1404182848217] regionserver.HStore: Starting compaction of 3 file(s) in family of usertable,user7,1404183160444.994b975d2c4cd5bbe532bddfb5e4ac46. into tmpdir=hdfs://master:54310/hbase/data/default/usertable/994b975d2c4cd5bbe532bddfb5e4ac46/.tmp, totalSize=194.1m
2014-06-30 19:55:54,354 DEBUG [regionserver60020-smallCompactions-1404182848217] compactions.Compactor: Compacting hdfs://master:54310/hbase/data/default/usertable/994b975d2c4cd5bbe532bddfb5e4ac46/family/0c1ed825a8e94b1daf3c71a2cd6714ae, keycount=35922, bloomtype=ROW, size=53.2m, encoding=NONE, seqNum=254, earliestPutTs=1404183223115
2014-06-30 19:55:54,354 DEBUG [regionserver60020-smallCompactions-1404182848217] compactions.Compactor: Compacting hdfs://master:54310/hbase/data/default/usertable/994b975d2c4cd5bbe532bddfb5e4ac46/family/b49a3c92980d4a0684a89071ca69d4a7, keycount=44737, bloomtype=ROW, size=66.7m, encoding=NONE, seqNum=337, earliestPutTs=1404183272112
2014-06-30 19:55:54,354 DEBUG [regionserver60020-smallCompactions-1404182848217] compactions.Compactor: Compacting hdfs://master:54310/hbase/data/default/usertable/994b975d2c4cd5bbe532bddfb5e4ac46/family/35621afe2710412b9c89a3a40e6904ce, keycount=49790, bloomtype=ROW, size=74.2m, encoding=NONE, seqNum=442, earliestPutTs=1404183287312
2014-06-30 19:55:54,383 DEBUG [regionserver60020-smallCompactions-1404182848217] util.FSUtils: DFS Client does not support most favored nodes create; using default create
2014-06-30 19:55:56,707 INFO  [JvmPauseMonitor] util.JvmPauseMonitor: Detected pause in JVM or host machine (eg GC): pause of approximately 1874ms
GC pool 'ConcurrentMarkSweep' had collection(s): count=1 time=2293ms
2014-06-30 19:55:56,709 DEBUG [MemStoreFlusher.1] util.FSUtils: DFS Client does not support most favored nodes create; using default create
2014-06-30 19:55:56,747 WARN  [RpcServer.handler=15,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":16748,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.58:50155","starttimems":1404183339999,"queuetimems":240,"class":"HRegionServer","responsesize":16900,"method":"Multi"}
2014-06-30 19:55:56,747 WARN  [RpcServer.handler=21,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":14530,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.58:50155","starttimems":1404183342217,"queuetimems":0,"class":"HRegionServer","responsesize":17012,"method":"Multi"}
2014-06-30 19:55:56,747 WARN  [RpcServer.handler=40,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":31965,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.58:50138","starttimems":1404183324782,"queuetimems":2606,"class":"HRegionServer","responsesize":16853,"method":"Multi"}
2014-06-30 19:55:56,748 WARN  [RpcServer.handler=40,port=60020] ipc.RpcServer: RpcServer.respondercallId: 1471 service: ClientService methodName: Multi size: 2.9m connection: 9.1.143.58:50138: output error
2014-06-30 19:55:56,748 WARN  [RpcServer.handler=40,port=60020] ipc.RpcServer: RpcServer.handler=40,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-06-30 19:55:56,781 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-06-30 19:55:56,812 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 2319 synced till here 2305
2014-06-30 19:55:56,853 WARN  [RpcServer.handler=7,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":14568,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.58:50155","starttimems":1404183342284,"queuetimems":56,"class":"HRegionServer","responsesize":16853,"method":"Multi"}
2014-06-30 19:55:56,853 DEBUG [RpcServer.handler=46,port=60020] regionserver.HRegion: Flush requested on usertable,user8,1404183160444.28938d267c1c61f7feb582b85e4c172f.
2014-06-30 19:55:56,853 DEBUG [MemStoreFlusher.0] regionserver.HRegion: NOT flushing memstore for region usertable,user8,1404183160444.28938d267c1c61f7feb582b85e4c172f., flushing=true, writesEnabled=true
2014-06-30 19:55:56,880 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1404182803191/sceplus-vm48.almaden.ibm.com%2C60020%2C1404182803191.1404183349254 with entries=91, filesize=71.9m; new WAL /hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1404182803191/sceplus-vm48.almaden.ibm.com%2C60020%2C1404182803191.1404183356781
2014-06-30 19:55:57,346 INFO  [MemStoreFlusher.1] regionserver.DefaultStoreFlusher: Flushed, sequenceid=472, memsize=63.0m, hasBloomFilter=true, into tmp file hdfs://master:54310/hbase/data/default/usertable/28938d267c1c61f7feb582b85e4c172f/.tmp/4ee3e63f7e1b44b891e842efc382a974
2014-06-30 19:55:57,355 DEBUG [MemStoreFlusher.1] regionserver.HRegionFileSystem: Committing store file hdfs://master:54310/hbase/data/default/usertable/28938d267c1c61f7feb582b85e4c172f/.tmp/4ee3e63f7e1b44b891e842efc382a974 as hdfs://master:54310/hbase/data/default/usertable/28938d267c1c61f7feb582b85e4c172f/family/4ee3e63f7e1b44b891e842efc382a974
2014-06-30 19:55:57,362 INFO  [MemStoreFlusher.1] regionserver.HStore: Added hdfs://master:54310/hbase/data/default/usertable/28938d267c1c61f7feb582b85e4c172f/family/4ee3e63f7e1b44b891e842efc382a974, entries=229270, sequenceid=472, filesize=34.2m
2014-06-30 19:55:57,362 INFO  [MemStoreFlusher.1] regionserver.HRegion: Finished memstore flush of ~121.8m/127689600, currentsize=29.5m/30953120 for region usertable,user8,1404183160444.28938d267c1c61f7feb582b85e4c172f. in 3299ms, sequenceid=472, compaction requested=false
2014-06-30 19:55:59,676 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-06-30 19:55:59,716 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 2415 synced till here 2404
2014-06-30 19:55:59,882 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1404182803191/sceplus-vm48.almaden.ibm.com%2C60020%2C1404182803191.1404183356781 with entries=96, filesize=75.0m; new WAL /hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1404182803191/sceplus-vm48.almaden.ibm.com%2C60020%2C1404182803191.1404183359676
2014-06-30 19:55:59,942 DEBUG [MemStoreFlusher.1] regionserver.MemStoreFlusher: Flush thread woke up because memory above low water=367.3m
2014-06-30 19:55:59,942 INFO  [MemStoreFlusher.1] regionserver.MemStoreFlusher: Flush of region usertable,user7,1404183160444.994b975d2c4cd5bbe532bddfb5e4ac46. due to global heap pressure
2014-06-30 19:55:59,942 DEBUG [MemStoreFlusher.1] regionserver.HRegion: Started memstore flush for usertable,user7,1404183160444.994b975d2c4cd5bbe532bddfb5e4ac46., current region memstore size 103.3m
2014-06-30 19:56:00,164 INFO  [RpcServer.handler=40,port=60020] regionserver.MemStoreFlusher: Blocking updates on sceplus-vm48.almaden.ibm.com,60020,1404182803191: the global memstore size 386.7m is >= than blocking 386.7m size
2014-06-30 19:56:00,168 INFO  [RpcServer.handler=46,port=60020] regionserver.MemStoreFlusher: Blocking updates on sceplus-vm48.almaden.ibm.com,60020,1404182803191: the global memstore size 386.7m is >= than blocking 386.7m size
2014-06-30 19:56:00,182 INFO  [RpcServer.handler=10,port=60020] regionserver.MemStoreFlusher: Blocking updates on sceplus-vm48.almaden.ibm.com,60020,1404182803191: the global memstore size 386.7m is >= than blocking 386.7m size
2014-06-30 19:56:00,188 DEBUG [MemStoreFlusher.1] util.FSUtils: DFS Client does not support most favored nodes create; using default create
2014-06-30 19:56:00,214 INFO  [RpcServer.handler=19,port=60020] regionserver.MemStoreFlusher: Blocking updates on sceplus-vm48.almaden.ibm.com,60020,1404182803191: the global memstore size 386.7m is >= than blocking 386.7m size
2014-06-30 19:56:00,215 INFO  [RpcServer.handler=25,port=60020] regionserver.MemStoreFlusher: Blocking updates on sceplus-vm48.almaden.ibm.com,60020,1404182803191: the global memstore size 386.7m is >= than blocking 386.7m size
2014-06-30 19:56:00,238 INFO  [RpcServer.handler=16,port=60020] regionserver.MemStoreFlusher: Blocking updates on sceplus-vm48.almaden.ibm.com,60020,1404182803191: the global memstore size 386.7m is >= than blocking 386.7m size
2014-06-30 19:56:00,262 INFO  [RpcServer.handler=24,port=60020] regionserver.MemStoreFlusher: Blocking updates on sceplus-vm48.almaden.ibm.com,60020,1404182803191: the global memstore size 386.7m is >= than blocking 386.7m size
2014-06-30 19:56:00,273 INFO  [RpcServer.handler=37,port=60020] regionserver.MemStoreFlusher: Blocking updates on sceplus-vm48.almaden.ibm.com,60020,1404182803191: the global memstore size 386.7m is >= than blocking 386.7m size
2014-06-30 19:56:00,285 INFO  [RpcServer.handler=41,port=60020] regionserver.MemStoreFlusher: Blocking updates on sceplus-vm48.almaden.ibm.com,60020,1404182803191: the global memstore size 386.7m is >= than blocking 386.7m size
2014-06-30 19:56:00,297 INFO  [RpcServer.handler=2,port=60020] regionserver.MemStoreFlusher: Blocking updates on sceplus-vm48.almaden.ibm.com,60020,1404182803191: the global memstore size 386.7m is >= than blocking 386.7m size
2014-06-30 19:56:00,318 INFO  [RpcServer.handler=23,port=60020] regionserver.MemStoreFlusher: Blocking updates on sceplus-vm48.almaden.ibm.com,60020,1404182803191: the global memstore size 386.7m is >= than blocking 386.7m size
2014-06-30 19:56:00,327 INFO  [RpcServer.handler=17,port=60020] regionserver.MemStoreFlusher: Blocking updates on sceplus-vm48.almaden.ibm.com,60020,1404182803191: the global memstore size 386.7m is >= than blocking 386.7m size
2014-06-30 19:56:00,334 INFO  [RpcServer.handler=33,port=60020] regionserver.MemStoreFlusher: Blocking updates on sceplus-vm48.almaden.ibm.com,60020,1404182803191: the global memstore size 386.7m is >= than blocking 386.7m size
2014-06-30 19:56:00,334 INFO  [RpcServer.handler=0,port=60020] regionserver.MemStoreFlusher: Blocking updates on sceplus-vm48.almaden.ibm.com,60020,1404182803191: the global memstore size 386.7m is >= than blocking 386.7m size
2014-06-30 19:56:00,345 INFO  [RpcServer.handler=13,port=60020] regionserver.MemStoreFlusher: Blocking updates on sceplus-vm48.almaden.ibm.com,60020,1404182803191: the global memstore size 386.7m is >= than blocking 386.7m size
2014-06-30 19:56:01,211 INFO  [MemStoreFlusher.1] regionserver.DefaultStoreFlusher: Flushed, sequenceid=511, memsize=67.7m, hasBloomFilter=true, into tmp file hdfs://master:54310/hbase/data/default/usertable/994b975d2c4cd5bbe532bddfb5e4ac46/.tmp/54b03d51827347ce888d58f01e2cdc1c
2014-06-30 19:56:01,223 DEBUG [MemStoreFlusher.1] regionserver.HRegionFileSystem: Committing store file hdfs://master:54310/hbase/data/default/usertable/994b975d2c4cd5bbe532bddfb5e4ac46/.tmp/54b03d51827347ce888d58f01e2cdc1c as hdfs://master:54310/hbase/data/default/usertable/994b975d2c4cd5bbe532bddfb5e4ac46/family/54b03d51827347ce888d58f01e2cdc1c
2014-06-30 19:56:01,232 INFO  [MemStoreFlusher.1] regionserver.HStore: Added hdfs://master:54310/hbase/data/default/usertable/994b975d2c4cd5bbe532bddfb5e4ac46/family/54b03d51827347ce888d58f01e2cdc1c, entries=246570, sequenceid=511, filesize=36.8m
2014-06-30 19:56:01,232 INFO  [MemStoreFlusher.1] regionserver.HRegion: Finished memstore flush of ~104.9m/110010640, currentsize=1.6m/1687600 for region usertable,user7,1404183160444.994b975d2c4cd5bbe532bddfb5e4ac46. in 1290ms, sequenceid=511, compaction requested=false
2014-06-30 19:56:01,232 WARN  [RpcServer.handler=13,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 888ms
2014-06-30 19:56:01,232 INFO  [RpcServer.handler=13,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server sceplus-vm48.almaden.ibm.com,60020,1404182803191
2014-06-30 19:56:01,232 WARN  [RpcServer.handler=0,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 898ms
2014-06-30 19:56:01,232 INFO  [RpcServer.handler=0,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server sceplus-vm48.almaden.ibm.com,60020,1404182803191
2014-06-30 19:56:01,232 WARN  [RpcServer.handler=33,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 898ms
2014-06-30 19:56:01,232 INFO  [RpcServer.handler=33,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server sceplus-vm48.almaden.ibm.com,60020,1404182803191
2014-06-30 19:56:01,232 WARN  [RpcServer.handler=17,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 905ms
2014-06-30 19:56:01,233 INFO  [RpcServer.handler=17,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server sceplus-vm48.almaden.ibm.com,60020,1404182803191
2014-06-30 19:56:01,234 WARN  [RpcServer.handler=23,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 916ms
2014-06-30 19:56:01,234 INFO  [RpcServer.handler=23,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server sceplus-vm48.almaden.ibm.com,60020,1404182803191
2014-06-30 19:56:01,234 WARN  [RpcServer.handler=2,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 937ms
2014-06-30 19:56:01,235 INFO  [RpcServer.handler=2,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server sceplus-vm48.almaden.ibm.com,60020,1404182803191
2014-06-30 19:56:01,235 WARN  [RpcServer.handler=41,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 950ms
2014-06-30 19:56:01,235 INFO  [RpcServer.handler=41,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server sceplus-vm48.almaden.ibm.com,60020,1404182803191
2014-06-30 19:56:01,236 WARN  [RpcServer.handler=37,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 963ms
2014-06-30 19:56:01,236 INFO  [RpcServer.handler=37,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server sceplus-vm48.almaden.ibm.com,60020,1404182803191
2014-06-30 19:56:01,243 WARN  [RpcServer.handler=24,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 981ms
2014-06-30 19:56:01,243 INFO  [RpcServer.handler=24,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server sceplus-vm48.almaden.ibm.com,60020,1404182803191
2014-06-30 19:56:01,243 WARN  [RpcServer.handler=16,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 1005ms
2014-06-30 19:56:01,243 INFO  [RpcServer.handler=16,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server sceplus-vm48.almaden.ibm.com,60020,1404182803191
2014-06-30 19:56:01,243 WARN  [RpcServer.handler=25,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 1028ms
2014-06-30 19:56:01,243 INFO  [RpcServer.handler=25,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server sceplus-vm48.almaden.ibm.com,60020,1404182803191
2014-06-30 19:56:01,244 WARN  [RpcServer.handler=19,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 1030ms
2014-06-30 19:56:01,244 INFO  [RpcServer.handler=19,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server sceplus-vm48.almaden.ibm.com,60020,1404182803191
2014-06-30 19:56:01,244 WARN  [RpcServer.handler=10,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 1062ms
2014-06-30 19:56:01,244 INFO  [RpcServer.handler=10,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server sceplus-vm48.almaden.ibm.com,60020,1404182803191
2014-06-30 19:56:01,245 WARN  [RpcServer.handler=46,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 1077ms
2014-06-30 19:56:01,245 INFO  [RpcServer.handler=46,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server sceplus-vm48.almaden.ibm.com,60020,1404182803191
2014-06-30 19:56:01,245 WARN  [RpcServer.handler=40,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 1081ms
2014-06-30 19:56:01,245 INFO  [RpcServer.handler=40,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server sceplus-vm48.almaden.ibm.com,60020,1404182803191
2014-06-30 19:56:03,056 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-06-30 19:56:03,070 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 2510 synced till here 2499
2014-06-30 19:56:03,132 DEBUG [MemStoreFlusher.0] regionserver.MemStoreFlusher: Flush thread woke up because memory above low water=367.3m
2014-06-30 19:56:03,135 INFO  [MemStoreFlusher.0] regionserver.MemStoreFlusher: Flush of region usertable,user5,1404183160444.1d66ac045cc0407c0fa426db565fcbba. due to global heap pressure
2014-06-30 19:56:03,135 DEBUG [MemStoreFlusher.0] regionserver.HRegion: Started memstore flush for usertable,user5,1404183160444.1d66ac045cc0407c0fa426db565fcbba., current region memstore size 124.0m
2014-06-30 19:56:03,139 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1404182803191/sceplus-vm48.almaden.ibm.com%2C60020%2C1404182803191.1404183359676 with entries=95, filesize=69.7m; new WAL /hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1404182803191/sceplus-vm48.almaden.ibm.com%2C60020%2C1404182803191.1404183363056
2014-06-30 19:56:03,228 INFO  [RpcServer.handler=18,port=60020] regionserver.MemStoreFlusher: Blocking updates on sceplus-vm48.almaden.ibm.com,60020,1404182803191: the global memstore size 391.2m is >= than blocking 386.7m size
2014-06-30 19:56:03,252 INFO  [RpcServer.handler=42,port=60020] regionserver.MemStoreFlusher: Blocking updates on sceplus-vm48.almaden.ibm.com,60020,1404182803191: the global memstore size 391.2m is >= than blocking 386.7m size
2014-06-30 19:56:03,274 INFO  [RpcServer.handler=36,port=60020] regionserver.MemStoreFlusher: Blocking updates on sceplus-vm48.almaden.ibm.com,60020,1404182803191: the global memstore size 391.2m is >= than blocking 386.7m size
2014-06-30 19:56:03,274 INFO  [RpcServer.handler=15,port=60020] regionserver.MemStoreFlusher: Blocking updates on sceplus-vm48.almaden.ibm.com,60020,1404182803191: the global memstore size 391.2m is >= than blocking 386.7m size
2014-06-30 19:56:03,275 INFO  [RpcServer.handler=45,port=60020] regionserver.MemStoreFlusher: Blocking updates on sceplus-vm48.almaden.ibm.com,60020,1404182803191: the global memstore size 391.2m is >= than blocking 386.7m size
2014-06-30 19:56:03,293 INFO  [RpcServer.handler=22,port=60020] regionserver.MemStoreFlusher: Blocking updates on sceplus-vm48.almaden.ibm.com,60020,1404182803191: the global memstore size 391.2m is >= than blocking 386.7m size
2014-06-30 19:56:03,318 DEBUG [MemStoreFlusher.0] util.FSUtils: DFS Client does not support most favored nodes create; using default create
2014-06-30 19:56:03,402 DEBUG [regionserver60020-smallCompactions-1404182848217] regionserver.HRegionFileSystem: Committing store file hdfs://master:54310/hbase/data/default/usertable/994b975d2c4cd5bbe532bddfb5e4ac46/.tmp/204ac70e73ae438abf77926eb42a0572 as hdfs://master:54310/hbase/data/default/usertable/994b975d2c4cd5bbe532bddfb5e4ac46/family/204ac70e73ae438abf77926eb42a0572
2014-06-30 19:56:03,413 DEBUG [regionserver60020-smallCompactions-1404182848217] regionserver.HStore: Removing store files after compaction...
2014-06-30 19:56:03,417 DEBUG [regionserver60020-smallCompactions-1404182848217] backup.HFileArchiver: Finished archiving from class org.apache.hadoop.hbase.backup.HFileArchiver$FileableStoreFile, file:hdfs://master:54310/hbase/data/default/usertable/994b975d2c4cd5bbe532bddfb5e4ac46/family/0c1ed825a8e94b1daf3c71a2cd6714ae, to hdfs://master:54310/hbase/archive/data/default/usertable/994b975d2c4cd5bbe532bddfb5e4ac46/family/0c1ed825a8e94b1daf3c71a2cd6714ae
2014-06-30 19:56:03,419 DEBUG [regionserver60020-smallCompactions-1404182848217] backup.HFileArchiver: Finished archiving from class org.apache.hadoop.hbase.backup.HFileArchiver$FileableStoreFile, file:hdfs://master:54310/hbase/data/default/usertable/994b975d2c4cd5bbe532bddfb5e4ac46/family/b49a3c92980d4a0684a89071ca69d4a7, to hdfs://master:54310/hbase/archive/data/default/usertable/994b975d2c4cd5bbe532bddfb5e4ac46/family/b49a3c92980d4a0684a89071ca69d4a7
2014-06-30 19:56:03,420 DEBUG [regionserver60020-smallCompactions-1404182848217] backup.HFileArchiver: Finished archiving from class org.apache.hadoop.hbase.backup.HFileArchiver$FileableStoreFile, file:hdfs://master:54310/hbase/data/default/usertable/994b975d2c4cd5bbe532bddfb5e4ac46/family/35621afe2710412b9c89a3a40e6904ce, to hdfs://master:54310/hbase/archive/data/default/usertable/994b975d2c4cd5bbe532bddfb5e4ac46/family/35621afe2710412b9c89a3a40e6904ce
2014-06-30 19:56:03,421 INFO  [regionserver60020-smallCompactions-1404182848217] regionserver.HStore: Completed major compaction of 3 file(s) in family of usertable,user7,1404183160444.994b975d2c4cd5bbe532bddfb5e4ac46. into 204ac70e73ae438abf77926eb42a0572(size=135.2m), total size for store is 172.0m. This selection was in queue for 0sec, and took 9sec to execute.
2014-06-30 19:56:03,421 INFO  [regionserver60020-smallCompactions-1404182848217] regionserver.CompactSplitThread: Completed compaction: Request = regionName=usertable,user7,1404183160444.994b975d2c4cd5bbe532bddfb5e4ac46., storeName=family, fileCount=3, fileSize=194.1m, priority=7, time=3496704608677; duration=9sec
2014-06-30 19:56:03,421 DEBUG [regionserver60020-smallCompactions-1404182848217] regionserver.CompactSplitThread: CompactSplitThread Status: compaction_queue=(0:1), split_queue=0, merge_queue=0
2014-06-30 19:56:03,421 DEBUG [regionserver60020-smallCompactions-1404182848217] compactions.RatioBasedCompactionPolicy: Selecting compaction from 3 store files, 0 compacting, 3 eligible, 10 blocking
2014-06-30 19:56:03,421 DEBUG [regionserver60020-smallCompactions-1404182848217] compactions.ExploringCompactionPolicy: Exploring compaction algorithm has selected 3 files of size 172525953 starting at candidate #0 after considering 1 permutations with 1 in ratio
2014-06-30 19:56:03,421 DEBUG [regionserver60020-smallCompactions-1404182848217] regionserver.HStore: 5446f1f888398e97c0ec3cc30da5bf97 - family: Initiating major compaction
2014-06-30 19:56:03,421 INFO  [regionserver60020-smallCompactions-1404182848217] regionserver.HRegion: Starting compaction on family in region usertable,user2,1404183160444.5446f1f888398e97c0ec3cc30da5bf97.
2014-06-30 19:56:03,421 INFO  [regionserver60020-smallCompactions-1404182848217] regionserver.HStore: Starting compaction of 3 file(s) in family of usertable,user2,1404183160444.5446f1f888398e97c0ec3cc30da5bf97. into tmpdir=hdfs://master:54310/hbase/data/default/usertable/5446f1f888398e97c0ec3cc30da5bf97/.tmp, totalSize=164.5m
2014-06-30 19:56:03,422 DEBUG [regionserver60020-smallCompactions-1404182848217] compactions.Compactor: Compacting hdfs://master:54310/hbase/data/default/usertable/5446f1f888398e97c0ec3cc30da5bf97/family/abd1f6805378412a8e3d65c12d64c335, keycount=50504, bloomtype=ROW, size=74.8m, encoding=NONE, seqNum=322, earliestPutTs=1404183222023
2014-06-30 19:56:03,422 DEBUG [regionserver60020-smallCompactions-1404182848217] compactions.Compactor: Compacting hdfs://master:54310/hbase/data/default/usertable/5446f1f888398e97c0ec3cc30da5bf97/family/bb59d24afb2a4ba4b8ab45700e1bcb78, keycount=39303, bloomtype=ROW, size=58.6m, encoding=NONE, seqNum=395, earliestPutTs=1404183284870
2014-06-30 19:56:03,422 DEBUG [regionserver60020-smallCompactions-1404182848217] compactions.Compactor: Compacting hdfs://master:54310/hbase/data/default/usertable/5446f1f888398e97c0ec3cc30da5bf97/family/6b88676d2d074964ae09ff1f408a9663, keycount=20896, bloomtype=ROW, size=31.1m, encoding=NONE, seqNum=472, earliestPutTs=1404183294792
2014-06-30 19:56:03,427 DEBUG [regionserver60020-smallCompactions-1404182848217] util.FSUtils: DFS Client does not support most favored nodes create; using default create
2014-06-30 19:56:03,623 INFO  [RpcServer.handler=35,port=60020] regionserver.MemStoreFlusher: Blocking updates on sceplus-vm48.almaden.ibm.com,60020,1404182803191: the global memstore size 391.2m is >= than blocking 386.7m size
2014-06-30 19:56:03,695 INFO  [RpcServer.handler=9,port=60020] regionserver.MemStoreFlusher: Blocking updates on sceplus-vm48.almaden.ibm.com,60020,1404182803191: the global memstore size 391.2m is >= than blocking 386.7m size
2014-06-30 19:56:03,969 INFO  [RpcServer.handler=5,port=60020] regionserver.MemStoreFlusher: Blocking updates on sceplus-vm48.almaden.ibm.com,60020,1404182803191: the global memstore size 391.2m is >= than blocking 386.7m size
2014-06-30 19:56:04,147 INFO  [MemStoreFlusher.0] regionserver.DefaultStoreFlusher: Flushed, sequenceid=532, memsize=72.5m, hasBloomFilter=true, into tmp file hdfs://master:54310/hbase/data/default/usertable/1d66ac045cc0407c0fa426db565fcbba/.tmp/cad5a72f2c9949378c3fa437b4b8a718
2014-06-30 19:56:04,156 DEBUG [MemStoreFlusher.0] regionserver.HRegionFileSystem: Committing store file hdfs://master:54310/hbase/data/default/usertable/1d66ac045cc0407c0fa426db565fcbba/.tmp/cad5a72f2c9949378c3fa437b4b8a718 as hdfs://master:54310/hbase/data/default/usertable/1d66ac045cc0407c0fa426db565fcbba/family/cad5a72f2c9949378c3fa437b4b8a718
2014-06-30 19:56:04,162 INFO  [RpcServer.handler=26,port=60020] regionserver.MemStoreFlusher: Blocking updates on sceplus-vm48.almaden.ibm.com,60020,1404182803191: the global memstore size 391.2m is >= than blocking 386.7m size
2014-06-30 19:56:04,164 INFO  [MemStoreFlusher.0] regionserver.HStore: Added hdfs://master:54310/hbase/data/default/usertable/1d66ac045cc0407c0fa426db565fcbba/family/cad5a72f2c9949378c3fa437b4b8a718, entries=263960, sequenceid=532, filesize=39.3m
2014-06-30 19:56:04,165 INFO  [MemStoreFlusher.0] regionserver.HRegion: Finished memstore flush of ~125.5m/131614880, currentsize=0.0/0 for region usertable,user5,1404183160444.1d66ac045cc0407c0fa426db565fcbba. in 1030ms, sequenceid=532, compaction requested=true
2014-06-30 19:56:04,165 DEBUG [MemStoreFlusher.0] regionserver.CompactSplitThread: Small Compaction requested: system; Because: MemStoreFlusher.0; compaction_queue=(0:1), split_queue=0, merge_queue=0
2014-06-30 19:56:04,165 WARN  [RpcServer.handler=26,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 3ms
2014-06-30 19:56:04,165 INFO  [RpcServer.handler=26,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server sceplus-vm48.almaden.ibm.com,60020,1404182803191
2014-06-30 19:56:04,165 WARN  [RpcServer.handler=5,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 196ms
2014-06-30 19:56:04,165 INFO  [RpcServer.handler=5,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server sceplus-vm48.almaden.ibm.com,60020,1404182803191
2014-06-30 19:56:04,165 WARN  [RpcServer.handler=9,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 470ms
2014-06-30 19:56:04,166 INFO  [RpcServer.handler=9,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server sceplus-vm48.almaden.ibm.com,60020,1404182803191
2014-06-30 19:56:04,166 WARN  [RpcServer.handler=35,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 543ms
2014-06-30 19:56:04,166 INFO  [RpcServer.handler=35,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server sceplus-vm48.almaden.ibm.com,60020,1404182803191
2014-06-30 19:56:04,166 WARN  [RpcServer.handler=22,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 873ms
2014-06-30 19:56:04,166 INFO  [RpcServer.handler=22,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server sceplus-vm48.almaden.ibm.com,60020,1404182803191
2014-06-30 19:56:04,167 WARN  [RpcServer.handler=45,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 891ms
2014-06-30 19:56:04,167 INFO  [RpcServer.handler=45,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server sceplus-vm48.almaden.ibm.com,60020,1404182803191
2014-06-30 19:56:04,167 WARN  [RpcServer.handler=15,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 893ms
2014-06-30 19:56:04,167 INFO  [RpcServer.handler=15,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server sceplus-vm48.almaden.ibm.com,60020,1404182803191
2014-06-30 19:56:04,167 WARN  [RpcServer.handler=36,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 893ms
2014-06-30 19:56:04,167 INFO  [RpcServer.handler=36,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server sceplus-vm48.almaden.ibm.com,60020,1404182803191
2014-06-30 19:56:04,168 WARN  [RpcServer.handler=42,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 916ms
2014-06-30 19:56:04,168 INFO  [RpcServer.handler=42,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server sceplus-vm48.almaden.ibm.com,60020,1404182803191
2014-06-30 19:56:04,168 WARN  [RpcServer.handler=18,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 940ms
2014-06-30 19:56:04,168 INFO  [RpcServer.handler=18,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server sceplus-vm48.almaden.ibm.com,60020,1404182803191
2014-06-30 19:56:06,597 INFO  [JvmPauseMonitor] util.JvmPauseMonitor: Detected pause in JVM or host machine (eg GC): pause of approximately 1314ms
GC pool 'ParNew' had collection(s): count=1 time=0ms
GC pool 'ConcurrentMarkSweep' had collection(s): count=2 time=1548ms
2014-06-30 19:56:06,669 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-06-30 19:56:06,683 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 2606 synced till here 2595
2014-06-30 19:56:06,710 DEBUG [MemStoreFlusher.1] regionserver.MemStoreFlusher: Flush thread woke up because memory above low water=367.3m
2014-06-30 19:56:06,711 INFO  [MemStoreFlusher.1] regionserver.MemStoreFlusher: Flush of region usertable,user2,1404183160444.5446f1f888398e97c0ec3cc30da5bf97. due to global heap pressure
2014-06-30 19:56:06,711 DEBUG [MemStoreFlusher.1] regionserver.HRegion: Started memstore flush for usertable,user2,1404183160444.5446f1f888398e97c0ec3cc30da5bf97., current region memstore size 120.9m
2014-06-30 19:56:06,727 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1404182803191/sceplus-vm48.almaden.ibm.com%2C60020%2C1404182803191.1404183363056 with entries=96, filesize=69.0m; new WAL /hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1404182803191/sceplus-vm48.almaden.ibm.com%2C60020%2C1404182803191.1404183366669
2014-06-30 19:56:06,792 INFO  [RpcServer.handler=32,port=60020] regionserver.MemStoreFlusher: Blocking updates on sceplus-vm48.almaden.ibm.com,60020,1404182803191: the global memstore size 388.9m is >= than blocking 386.7m size
2014-06-30 19:56:06,792 INFO  [RpcServer.handler=37,port=60020] regionserver.MemStoreFlusher: Blocking updates on sceplus-vm48.almaden.ibm.com,60020,1404182803191: the global memstore size 388.9m is >= than blocking 386.7m size
2014-06-30 19:56:06,792 INFO  [RpcServer.handler=34,port=60020] regionserver.MemStoreFlusher: Blocking updates on sceplus-vm48.almaden.ibm.com,60020,1404182803191: the global memstore size 388.9m is >= than blocking 386.7m size
2014-06-30 19:56:06,792 INFO  [RpcServer.handler=33,port=60020] regionserver.MemStoreFlusher: Blocking updates on sceplus-vm48.almaden.ibm.com,60020,1404182803191: the global memstore size 388.9m is >= than blocking 386.7m size
2014-06-30 19:56:06,793 INFO  [RpcServer.handler=21,port=60020] regionserver.MemStoreFlusher: Blocking updates on sceplus-vm48.almaden.ibm.com,60020,1404182803191: the global memstore size 388.9m is >= than blocking 386.7m size
2014-06-30 19:56:06,793 INFO  [RpcServer.handler=49,port=60020] regionserver.MemStoreFlusher: Blocking updates on sceplus-vm48.almaden.ibm.com,60020,1404182803191: the global memstore size 388.9m is >= than blocking 386.7m size
2014-06-30 19:56:06,794 INFO  [RpcServer.handler=23,port=60020] regionserver.MemStoreFlusher: Blocking updates on sceplus-vm48.almaden.ibm.com,60020,1404182803191: the global memstore size 388.9m is >= than blocking 386.7m size
2014-06-30 19:56:06,821 INFO  [RpcServer.handler=20,port=60020] regionserver.MemStoreFlusher: Blocking updates on sceplus-vm48.almaden.ibm.com,60020,1404182803191: the global memstore size 390.4m is >= than blocking 386.7m size
2014-06-30 19:56:06,976 INFO  [RpcServer.handler=19,port=60020] regionserver.MemStoreFlusher: Blocking updates on sceplus-vm48.almaden.ibm.com,60020,1404182803191: the global memstore size 395.3m is >= than blocking 386.7m size
2014-06-30 19:56:06,977 INFO  [RpcServer.handler=4,port=60020] regionserver.MemStoreFlusher: Blocking updates on sceplus-vm48.almaden.ibm.com,60020,1404182803191: the global memstore size 395.3m is >= than blocking 386.7m size
2014-06-30 19:56:06,977 INFO  [RpcServer.handler=25,port=60020] regionserver.MemStoreFlusher: Blocking updates on sceplus-vm48.almaden.ibm.com,60020,1404182803191: the global memstore size 395.3m is >= than blocking 386.7m size
2014-06-30 19:56:07,045 DEBUG [MemStoreFlusher.1] util.FSUtils: DFS Client does not support most favored nodes create; using default create
2014-06-30 19:56:07,827 INFO  [MemStoreFlusher.1] regionserver.DefaultStoreFlusher: Flushed, sequenceid=555, memsize=96.1m, hasBloomFilter=true, into tmp file hdfs://master:54310/hbase/data/default/usertable/5446f1f888398e97c0ec3cc30da5bf97/.tmp/d386e992ead34e91b89d6f9e2dda27c1
2014-06-30 19:56:07,836 DEBUG [MemStoreFlusher.1] regionserver.HRegionFileSystem: Committing store file hdfs://master:54310/hbase/data/default/usertable/5446f1f888398e97c0ec3cc30da5bf97/.tmp/d386e992ead34e91b89d6f9e2dda27c1 as hdfs://master:54310/hbase/data/default/usertable/5446f1f888398e97c0ec3cc30da5bf97/family/d386e992ead34e91b89d6f9e2dda27c1
2014-06-30 19:56:07,842 INFO  [MemStoreFlusher.1] regionserver.HStore: Added hdfs://master:54310/hbase/data/default/usertable/5446f1f888398e97c0ec3cc30da5bf97/family/d386e992ead34e91b89d6f9e2dda27c1, entries=349960, sequenceid=555, filesize=52.2m
2014-06-30 19:56:07,843 INFO  [MemStoreFlusher.1] regionserver.HRegion: Finished memstore flush of ~127.2m/133359600, currentsize=0.0/0 for region usertable,user2,1404183160444.5446f1f888398e97c0ec3cc30da5bf97. in 1132ms, sequenceid=555, compaction requested=false
2014-06-30 19:56:07,843 WARN  [RpcServer.handler=25,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 866ms
2014-06-30 19:56:07,843 INFO  [RpcServer.handler=25,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server sceplus-vm48.almaden.ibm.com,60020,1404182803191
2014-06-30 19:56:07,843 WARN  [RpcServer.handler=4,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 867ms
2014-06-30 19:56:07,843 INFO  [RpcServer.handler=4,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server sceplus-vm48.almaden.ibm.com,60020,1404182803191
2014-06-30 19:56:07,843 WARN  [RpcServer.handler=19,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 867ms
2014-06-30 19:56:07,843 INFO  [RpcServer.handler=19,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server sceplus-vm48.almaden.ibm.com,60020,1404182803191
2014-06-30 19:56:07,843 WARN  [RpcServer.handler=20,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 1022ms
2014-06-30 19:56:07,843 INFO  [RpcServer.handler=20,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server sceplus-vm48.almaden.ibm.com,60020,1404182803191
2014-06-30 19:56:07,843 WARN  [RpcServer.handler=23,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 1049ms
2014-06-30 19:56:07,844 INFO  [RpcServer.handler=23,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server sceplus-vm48.almaden.ibm.com,60020,1404182803191
2014-06-30 19:56:07,844 WARN  [RpcServer.handler=49,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 1051ms
2014-06-30 19:56:07,844 INFO  [RpcServer.handler=49,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server sceplus-vm48.almaden.ibm.com,60020,1404182803191
2014-06-30 19:56:07,844 WARN  [RpcServer.handler=21,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 1051ms
2014-06-30 19:56:07,844 INFO  [RpcServer.handler=21,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server sceplus-vm48.almaden.ibm.com,60020,1404182803191
2014-06-30 19:56:07,844 WARN  [RpcServer.handler=33,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 1052ms
2014-06-30 19:56:07,844 INFO  [RpcServer.handler=33,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server sceplus-vm48.almaden.ibm.com,60020,1404182803191
2014-06-30 19:56:07,844 WARN  [RpcServer.handler=34,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 1052ms
2014-06-30 19:56:07,844 INFO  [RpcServer.handler=34,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server sceplus-vm48.almaden.ibm.com,60020,1404182803191
2014-06-30 19:56:07,844 WARN  [RpcServer.handler=37,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 1052ms
2014-06-30 19:56:07,845 INFO  [RpcServer.handler=37,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server sceplus-vm48.almaden.ibm.com,60020,1404182803191
2014-06-30 19:56:07,845 WARN  [RpcServer.handler=32,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 1053ms
2014-06-30 19:56:07,845 INFO  [RpcServer.handler=32,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server sceplus-vm48.almaden.ibm.com,60020,1404182803191
2014-06-30 19:56:08,000 DEBUG [RpcServer.handler=37,port=60020] regionserver.HRegion: Flush requested on usertable,user8,1404183160444.28938d267c1c61f7feb582b85e4c172f.
2014-06-30 19:56:08,001 DEBUG [MemStoreFlusher.0] regionserver.HRegion: Started memstore flush for usertable,user8,1404183160444.28938d267c1c61f7feb582b85e4c172f., current region memstore size 128.6m
2014-06-30 19:56:08,290 DEBUG [MemStoreFlusher.0] util.FSUtils: DFS Client does not support most favored nodes create; using default create
2014-06-30 19:56:08,756 DEBUG [regionserver60020-smallCompactions-1404182848217] regionserver.HRegionFileSystem: Committing store file hdfs://master:54310/hbase/data/default/usertable/5446f1f888398e97c0ec3cc30da5bf97/.tmp/a0169492de6b4f00aec2ed8adddb6971 as hdfs://master:54310/hbase/data/default/usertable/5446f1f888398e97c0ec3cc30da5bf97/family/a0169492de6b4f00aec2ed8adddb6971
2014-06-30 19:56:08,791 DEBUG [regionserver60020-smallCompactions-1404182848217] regionserver.HStore: Removing store files after compaction...
2014-06-30 19:56:08,797 DEBUG [regionserver60020-smallCompactions-1404182848217] backup.HFileArchiver: Finished archiving from class org.apache.hadoop.hbase.backup.HFileArchiver$FileableStoreFile, file:hdfs://master:54310/hbase/data/default/usertable/5446f1f888398e97c0ec3cc30da5bf97/family/abd1f6805378412a8e3d65c12d64c335, to hdfs://master:54310/hbase/archive/data/default/usertable/5446f1f888398e97c0ec3cc30da5bf97/family/abd1f6805378412a8e3d65c12d64c335
2014-06-30 19:56:08,799 DEBUG [regionserver60020-smallCompactions-1404182848217] backup.HFileArchiver: Finished archiving from class org.apache.hadoop.hbase.backup.HFileArchiver$FileableStoreFile, file:hdfs://master:54310/hbase/data/default/usertable/5446f1f888398e97c0ec3cc30da5bf97/family/bb59d24afb2a4ba4b8ab45700e1bcb78, to hdfs://master:54310/hbase/archive/data/default/usertable/5446f1f888398e97c0ec3cc30da5bf97/family/bb59d24afb2a4ba4b8ab45700e1bcb78
2014-06-30 19:56:08,802 DEBUG [regionserver60020-smallCompactions-1404182848217] backup.HFileArchiver: Finished archiving from class org.apache.hadoop.hbase.backup.HFileArchiver$FileableStoreFile, file:hdfs://master:54310/hbase/data/default/usertable/5446f1f888398e97c0ec3cc30da5bf97/family/6b88676d2d074964ae09ff1f408a9663, to hdfs://master:54310/hbase/archive/data/default/usertable/5446f1f888398e97c0ec3cc30da5bf97/family/6b88676d2d074964ae09ff1f408a9663
2014-06-30 19:56:08,803 INFO  [regionserver60020-smallCompactions-1404182848217] regionserver.HStore: Completed major compaction of 3 file(s) in family of usertable,user2,1404183160444.5446f1f888398e97c0ec3cc30da5bf97. into a0169492de6b4f00aec2ed8adddb6971(size=135.7m), total size for store is 187.8m. This selection was in queue for 0sec, and took 5sec to execute.
2014-06-30 19:56:08,803 INFO  [regionserver60020-smallCompactions-1404182848217] regionserver.CompactSplitThread: Completed compaction: Request = regionName=usertable,user2,1404183160444.5446f1f888398e97c0ec3cc30da5bf97., storeName=family, fileCount=3, fileSize=164.5m, priority=7, time=3505778909795; duration=5sec
2014-06-30 19:56:08,803 DEBUG [regionserver60020-smallCompactions-1404182848217] regionserver.CompactSplitThread: CompactSplitThread Status: compaction_queue=(0:1), split_queue=0, merge_queue=0
2014-06-30 19:56:08,803 DEBUG [regionserver60020-smallCompactions-1404182848217] compactions.RatioBasedCompactionPolicy: Selecting compaction from 3 store files, 0 compacting, 3 eligible, 10 blocking
2014-06-30 19:56:08,804 DEBUG [regionserver60020-smallCompactions-1404182848217] compactions.ExploringCompactionPolicy: Exploring compaction algorithm has selected 0 files of size 0 starting at candidate #-1 after considering 1 permutations with 0 in ratio
2014-06-30 19:56:08,804 DEBUG [regionserver60020-smallCompactions-1404182848217] compactions.RatioBasedCompactionPolicy: Not compacting files because we only have 0 files ready for compaction. Need 3 to initiate.
2014-06-30 19:56:08,804 DEBUG [regionserver60020-smallCompactions-1404182848217] regionserver.CompactSplitThread: Not compacting usertable,user5,1404183160444.1d66ac045cc0407c0fa426db565fcbba. because compaction request was cancelled
2014-06-30 19:56:08,831 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-06-30 19:56:08,848 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 2689 synced till here 2685
2014-06-30 19:56:08,965 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1404182803191/sceplus-vm48.almaden.ibm.com%2C60020%2C1404182803191.1404183366669 with entries=83, filesize=62.9m; new WAL /hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1404182803191/sceplus-vm48.almaden.ibm.com%2C60020%2C1404182803191.1404183368832
2014-06-30 19:56:09,016 DEBUG [MemStoreFlusher.1] regionserver.MemStoreFlusher: Flush thread woke up because memory above low water=367.3m
2014-06-30 19:56:09,017 INFO  [MemStoreFlusher.1] regionserver.MemStoreFlusher: Flush of region usertable,user7,1404183160444.994b975d2c4cd5bbe532bddfb5e4ac46. due to global heap pressure
2014-06-30 19:56:09,017 DEBUG [MemStoreFlusher.1] regionserver.HRegion: Started memstore flush for usertable,user7,1404183160444.994b975d2c4cd5bbe532bddfb5e4ac46., current region memstore size 81.6m
2014-06-30 19:56:10,333 INFO  [JvmPauseMonitor] util.JvmPauseMonitor: Detected pause in JVM or host machine (eg GC): pause of approximately 1070ms
GC pool 'ParNew' had collection(s): count=1 time=0ms
GC pool 'ConcurrentMarkSweep' had collection(s): count=2 time=1301ms
2014-06-30 19:56:10,357 DEBUG [MemStoreFlusher.1] util.FSUtils: DFS Client does not support most favored nodes create; using default create
2014-06-30 19:56:10,385 INFO  [RpcServer.handler=8,port=60020] regionserver.MemStoreFlusher: Blocking updates on sceplus-vm48.almaden.ibm.com,60020,1404182803191: the global memstore size 388.1m is >= than blocking 386.7m size
2014-06-30 19:56:10,386 INFO  [RpcServer.handler=17,port=60020] regionserver.MemStoreFlusher: Blocking updates on sceplus-vm48.almaden.ibm.com,60020,1404182803191: the global memstore size 388.1m is >= than blocking 386.7m size
2014-06-30 19:56:10,397 INFO  [RpcServer.handler=16,port=60020] regionserver.MemStoreFlusher: Blocking updates on sceplus-vm48.almaden.ibm.com,60020,1404182803191: the global memstore size 390.1m is >= than blocking 386.7m size
2014-06-30 19:56:10,397 INFO  [RpcServer.handler=40,port=60020] regionserver.MemStoreFlusher: Blocking updates on sceplus-vm48.almaden.ibm.com,60020,1404182803191: the global memstore size 390.1m is >= than blocking 386.7m size
2014-06-30 19:56:10,406 INFO  [RpcServer.handler=2,port=60020] regionserver.MemStoreFlusher: Blocking updates on sceplus-vm48.almaden.ibm.com,60020,1404182803191: the global memstore size 390.1m is >= than blocking 386.7m size
2014-06-30 19:56:10,407 INFO  [RpcServer.handler=21,port=60020] regionserver.MemStoreFlusher: Blocking updates on sceplus-vm48.almaden.ibm.com,60020,1404182803191: the global memstore size 390.1m is >= than blocking 386.7m size
2014-06-30 19:56:10,407 INFO  [RpcServer.handler=0,port=60020] regionserver.MemStoreFlusher: Blocking updates on sceplus-vm48.almaden.ibm.com,60020,1404182803191: the global memstore size 390.1m is >= than blocking 386.7m size
2014-06-30 19:56:10,407 INFO  [RpcServer.handler=23,port=60020] regionserver.MemStoreFlusher: Blocking updates on sceplus-vm48.almaden.ibm.com,60020,1404182803191: the global memstore size 390.1m is >= than blocking 386.7m size
2014-06-30 19:56:10,409 INFO  [RpcServer.handler=25,port=60020] regionserver.MemStoreFlusher: Blocking updates on sceplus-vm48.almaden.ibm.com,60020,1404182803191: the global memstore size 390.1m is >= than blocking 386.7m size
2014-06-30 19:56:10,409 INFO  [RpcServer.handler=34,port=60020] regionserver.MemStoreFlusher: Blocking updates on sceplus-vm48.almaden.ibm.com,60020,1404182803191: the global memstore size 390.1m is >= than blocking 386.7m size
2014-06-30 19:56:10,415 INFO  [RpcServer.handler=31,port=60020] regionserver.MemStoreFlusher: Blocking updates on sceplus-vm48.almaden.ibm.com,60020,1404182803191: the global memstore size 393.4m is >= than blocking 386.7m size
2014-06-30 19:56:10,415 INFO  [RpcServer.handler=11,port=60020] regionserver.MemStoreFlusher: Blocking updates on sceplus-vm48.almaden.ibm.com,60020,1404182803191: the global memstore size 393.4m is >= than blocking 386.7m size
2014-06-30 19:56:10,425 INFO  [RpcServer.handler=10,port=60020] regionserver.MemStoreFlusher: Blocking updates on sceplus-vm48.almaden.ibm.com,60020,1404182803191: the global memstore size 396.6m is >= than blocking 386.7m size
2014-06-30 19:56:10,465 INFO  [RpcServer.handler=33,port=60020] regionserver.MemStoreFlusher: Blocking updates on sceplus-vm48.almaden.ibm.com,60020,1404182803191: the global memstore size 396.6m is >= than blocking 386.7m size
2014-06-30 19:56:10,481 INFO  [RpcServer.handler=4,port=60020] regionserver.MemStoreFlusher: Blocking updates on sceplus-vm48.almaden.ibm.com,60020,1404182803191: the global memstore size 396.6m is >= than blocking 386.7m size
2014-06-30 19:56:10,586 INFO  [RpcServer.handler=49,port=60020] regionserver.MemStoreFlusher: Blocking updates on sceplus-vm48.almaden.ibm.com,60020,1404182803191: the global memstore size 396.6m is >= than blocking 386.7m size
2014-06-30 19:56:10,651 INFO  [RpcServer.handler=20,port=60020] regionserver.MemStoreFlusher: Blocking updates on sceplus-vm48.almaden.ibm.com,60020,1404182803191: the global memstore size 396.6m is >= than blocking 386.7m size
2014-06-30 19:56:10,901 INFO  [RpcServer.handler=19,port=60020] regionserver.MemStoreFlusher: Blocking updates on sceplus-vm48.almaden.ibm.com,60020,1404182803191: the global memstore size 396.6m is >= than blocking 386.7m size
2014-06-30 19:56:11,042 INFO  [MemStoreFlusher.0] regionserver.DefaultStoreFlusher: Flushed, sequenceid=556, memsize=95.9m, hasBloomFilter=true, into tmp file hdfs://master:54310/hbase/data/default/usertable/28938d267c1c61f7feb582b85e4c172f/.tmp/04113e6551ba48c5bfd046d960978314
2014-06-30 19:56:11,051 DEBUG [MemStoreFlusher.0] regionserver.HRegionFileSystem: Committing store file hdfs://master:54310/hbase/data/default/usertable/28938d267c1c61f7feb582b85e4c172f/.tmp/04113e6551ba48c5bfd046d960978314 as hdfs://master:54310/hbase/data/default/usertable/28938d267c1c61f7feb582b85e4c172f/family/04113e6551ba48c5bfd046d960978314
2014-06-30 19:56:11,060 INFO  [MemStoreFlusher.0] regionserver.HStore: Added hdfs://master:54310/hbase/data/default/usertable/28938d267c1c61f7feb582b85e4c172f/family/04113e6551ba48c5bfd046d960978314, entries=349090, sequenceid=556, filesize=52.0m
2014-06-30 19:56:11,060 INFO  [MemStoreFlusher.0] regionserver.HRegion: Finished memstore flush of ~128.6m/134817760, currentsize=20.6m/21559040 for region usertable,user8,1404183160444.28938d267c1c61f7feb582b85e4c172f. in 3059ms, sequenceid=556, compaction requested=true
2014-06-30 19:56:11,061 DEBUG [MemStoreFlusher.0] regionserver.CompactSplitThread: Small Compaction requested: system; Because: MemStoreFlusher.0; compaction_queue=(0:0), split_queue=0, merge_queue=0
2014-06-30 19:56:11,061 DEBUG [regionserver60020-smallCompactions-1404182848217] compactions.RatioBasedCompactionPolicy: Selecting compaction from 3 store files, 0 compacting, 3 eligible, 10 blocking
2014-06-30 19:56:11,061 WARN  [RpcServer.handler=19,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 161ms
2014-06-30 19:56:11,061 DEBUG [regionserver60020-smallCompactions-1404182848217] compactions.ExploringCompactionPolicy: Exploring compaction algorithm has selected 0 files of size 0 starting at candidate #-1 after considering 1 permutations with 0 in ratio
2014-06-30 19:56:11,061 DEBUG [regionserver60020-smallCompactions-1404182848217] compactions.RatioBasedCompactionPolicy: Not compacting files because we only have 0 files ready for compaction. Need 3 to initiate.
2014-06-30 19:56:11,061 INFO  [RpcServer.handler=19,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server sceplus-vm48.almaden.ibm.com,60020,1404182803191
2014-06-30 19:56:11,061 DEBUG [regionserver60020-smallCompactions-1404182848217] regionserver.CompactSplitThread: Not compacting usertable,user8,1404183160444.28938d267c1c61f7feb582b85e4c172f. because compaction request was cancelled
2014-06-30 19:56:11,061 WARN  [RpcServer.handler=20,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 410ms
2014-06-30 19:56:11,061 INFO  [RpcServer.handler=20,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server sceplus-vm48.almaden.ibm.com,60020,1404182803191
2014-06-30 19:56:11,062 WARN  [RpcServer.handler=49,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 476ms
2014-06-30 19:56:11,062 INFO  [RpcServer.handler=49,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server sceplus-vm48.almaden.ibm.com,60020,1404182803191
2014-06-30 19:56:11,062 WARN  [RpcServer.handler=4,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 582ms
2014-06-30 19:56:11,062 INFO  [RpcServer.handler=4,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server sceplus-vm48.almaden.ibm.com,60020,1404182803191
2014-06-30 19:56:11,062 WARN  [RpcServer.handler=33,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 598ms
2014-06-30 19:56:11,062 INFO  [RpcServer.handler=33,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server sceplus-vm48.almaden.ibm.com,60020,1404182803191
2014-06-30 19:56:11,062 WARN  [RpcServer.handler=10,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 637ms
2014-06-30 19:56:11,062 INFO  [RpcServer.handler=10,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server sceplus-vm48.almaden.ibm.com,60020,1404182803191
2014-06-30 19:56:11,062 WARN  [RpcServer.handler=11,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 647ms
2014-06-30 19:56:11,062 INFO  [RpcServer.handler=11,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server sceplus-vm48.almaden.ibm.com,60020,1404182803191
2014-06-30 19:56:11,063 WARN  [RpcServer.handler=31,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 647ms
2014-06-30 19:56:11,063 INFO  [RpcServer.handler=31,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server sceplus-vm48.almaden.ibm.com,60020,1404182803191
2014-06-30 19:56:11,066 WARN  [RpcServer.handler=34,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 657ms
2014-06-30 19:56:11,066 INFO  [RpcServer.handler=34,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server sceplus-vm48.almaden.ibm.com,60020,1404182803191
2014-06-30 19:56:11,067 WARN  [RpcServer.handler=25,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 658ms
2014-06-30 19:56:11,067 INFO  [RpcServer.handler=25,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server sceplus-vm48.almaden.ibm.com,60020,1404182803191
2014-06-30 19:56:11,067 WARN  [RpcServer.handler=23,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 660ms
2014-06-30 19:56:11,067 INFO  [RpcServer.handler=23,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server sceplus-vm48.almaden.ibm.com,60020,1404182803191
2014-06-30 19:56:11,067 WARN  [RpcServer.handler=0,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 660ms
2014-06-30 19:56:11,067 INFO  [RpcServer.handler=0,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server sceplus-vm48.almaden.ibm.com,60020,1404182803191
2014-06-30 19:56:11,068 WARN  [RpcServer.handler=21,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 661ms
2014-06-30 19:56:11,069 INFO  [RpcServer.handler=21,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server sceplus-vm48.almaden.ibm.com,60020,1404182803191
2014-06-30 19:56:11,069 WARN  [RpcServer.handler=2,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 663ms
2014-06-30 19:56:11,069 INFO  [RpcServer.handler=2,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server sceplus-vm48.almaden.ibm.com,60020,1404182803191
2014-06-30 19:56:11,069 WARN  [RpcServer.handler=40,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 672ms
2014-06-30 19:56:11,069 INFO  [RpcServer.handler=40,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server sceplus-vm48.almaden.ibm.com,60020,1404182803191
2014-06-30 19:56:11,070 WARN  [RpcServer.handler=16,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 673ms
2014-06-30 19:56:11,070 INFO  [RpcServer.handler=16,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server sceplus-vm48.almaden.ibm.com,60020,1404182803191
2014-06-30 19:56:11,071 WARN  [RpcServer.handler=17,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 685ms
2014-06-30 19:56:11,071 INFO  [RpcServer.handler=17,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server sceplus-vm48.almaden.ibm.com,60020,1404182803191
2014-06-30 19:56:11,076 WARN  [RpcServer.handler=8,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 691ms
2014-06-30 19:56:11,076 INFO  [RpcServer.handler=8,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server sceplus-vm48.almaden.ibm.com,60020,1404182803191
2014-06-30 19:56:11,119 INFO  [MemStoreFlusher.1] regionserver.DefaultStoreFlusher: Flushed, sequenceid=565, memsize=69.0m, hasBloomFilter=true, into tmp file hdfs://master:54310/hbase/data/default/usertable/994b975d2c4cd5bbe532bddfb5e4ac46/.tmp/e20161fb9a30414facd313203b4b327e
2014-06-30 19:56:11,150 DEBUG [MemStoreFlusher.1] regionserver.HRegionFileSystem: Committing store file hdfs://master:54310/hbase/data/default/usertable/994b975d2c4cd5bbe532bddfb5e4ac46/.tmp/e20161fb9a30414facd313203b4b327e as hdfs://master:54310/hbase/data/default/usertable/994b975d2c4cd5bbe532bddfb5e4ac46/family/e20161fb9a30414facd313203b4b327e
2014-06-30 19:56:11,187 INFO  [MemStoreFlusher.1] regionserver.HStore: Added hdfs://master:54310/hbase/data/default/usertable/994b975d2c4cd5bbe532bddfb5e4ac46/family/e20161fb9a30414facd313203b4b327e, entries=251240, sequenceid=565, filesize=37.4m
2014-06-30 19:56:11,187 INFO  [MemStoreFlusher.1] regionserver.HRegion: Finished memstore flush of ~81.6m/85577760, currentsize=6.5m/6790960 for region usertable,user7,1404183160444.994b975d2c4cd5bbe532bddfb5e4ac46. in 2170ms, sequenceid=565, compaction requested=true
2014-06-30 19:56:11,190 DEBUG [regionserver60020-smallCompactions-1404182848217] compactions.RatioBasedCompactionPolicy: Selecting compaction from 3 store files, 0 compacting, 3 eligible, 10 blocking
2014-06-30 19:56:11,191 DEBUG [regionserver60020-smallCompactions-1404182848217] compactions.ExploringCompactionPolicy: Exploring compaction algorithm has selected 0 files of size 0 starting at candidate #-1 after considering 1 permutations with 0 in ratio
2014-06-30 19:56:11,191 DEBUG [regionserver60020-smallCompactions-1404182848217] compactions.RatioBasedCompactionPolicy: Not compacting files because we only have 0 files ready for compaction. Need 3 to initiate.
2014-06-30 19:56:11,191 DEBUG [regionserver60020-smallCompactions-1404182848217] regionserver.CompactSplitThread: Not compacting usertable,user7,1404183160444.994b975d2c4cd5bbe532bddfb5e4ac46. because compaction request was cancelled
2014-06-30 19:56:11,194 DEBUG [MemStoreFlusher.1] regionserver.CompactSplitThread: Small Compaction requested: system; Because: MemStoreFlusher.1; compaction_queue=(0:0), split_queue=0, merge_queue=0
2014-06-30 19:56:11,707 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-06-30 19:56:11,721 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 2784 synced till here 2781
2014-06-30 19:56:11,816 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1404182803191/sceplus-vm48.almaden.ibm.com%2C60020%2C1404182803191.1404183368832 with entries=95, filesize=65.5m; new WAL /hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1404182803191/sceplus-vm48.almaden.ibm.com%2C60020%2C1404182803191.1404183371707
2014-06-30 19:56:14,058 DEBUG [MemStoreFlusher.1] regionserver.MemStoreFlusher: Flush thread woke up because memory above low water=367.3m
2014-06-30 19:56:14,058 INFO  [MemStoreFlusher.1] regionserver.MemStoreFlusher: Flush of region usertable,user5,1404183160444.1d66ac045cc0407c0fa426db565fcbba. due to global heap pressure
2014-06-30 19:56:14,059 DEBUG [MemStoreFlusher.1] regionserver.HRegion: Started memstore flush for usertable,user5,1404183160444.1d66ac045cc0407c0fa426db565fcbba., current region memstore size 103.2m
2014-06-30 19:56:14,179 DEBUG [MemStoreFlusher.1] util.FSUtils: DFS Client does not support most favored nodes create; using default create
2014-06-30 19:56:14,206 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-06-30 19:56:14,218 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 2883 synced till here 2874
2014-06-30 19:56:14,221 INFO  [RpcServer.handler=16,port=60020] regionserver.MemStoreFlusher: Blocking updates on sceplus-vm48.almaden.ibm.com,60020,1404182803191: the global memstore size 390.1m is >= than blocking 386.7m size
2014-06-30 19:56:14,222 INFO  [RpcServer.handler=10,port=60020] regionserver.MemStoreFlusher: Blocking updates on sceplus-vm48.almaden.ibm.com,60020,1404182803191: the global memstore size 393.8m is >= than blocking 386.7m size
2014-06-30 19:56:14,223 INFO  [RpcServer.handler=4,port=60020] regionserver.MemStoreFlusher: Blocking updates on sceplus-vm48.almaden.ibm.com,60020,1404182803191: the global memstore size 393.8m is >= than blocking 386.7m size
2014-06-30 19:56:14,223 INFO  [RpcServer.handler=18,port=60020] regionserver.MemStoreFlusher: Blocking updates on sceplus-vm48.almaden.ibm.com,60020,1404182803191: the global memstore size 393.8m is >= than blocking 386.7m size
2014-06-30 19:56:14,223 INFO  [RpcServer.handler=40,port=60020] regionserver.MemStoreFlusher: Blocking updates on sceplus-vm48.almaden.ibm.com,60020,1404182803191: the global memstore size 393.8m is >= than blocking 386.7m size
2014-06-30 19:56:14,223 INFO  [RpcServer.handler=44,port=60020] regionserver.MemStoreFlusher: Blocking updates on sceplus-vm48.almaden.ibm.com,60020,1404182803191: the global memstore size 393.8m is >= than blocking 386.7m size
2014-06-30 19:56:14,225 INFO  [RpcServer.handler=43,port=60020] regionserver.MemStoreFlusher: Blocking updates on sceplus-vm48.almaden.ibm.com,60020,1404182803191: the global memstore size 393.8m is >= than blocking 386.7m size
2014-06-30 19:56:14,227 INFO  [RpcServer.handler=27,port=60020] regionserver.MemStoreFlusher: Blocking updates on sceplus-vm48.almaden.ibm.com,60020,1404182803191: the global memstore size 393.8m is >= than blocking 386.7m size
2014-06-30 19:56:14,228 INFO  [RpcServer.handler=12,port=60020] regionserver.MemStoreFlusher: Blocking updates on sceplus-vm48.almaden.ibm.com,60020,1404182803191: the global memstore size 393.8m is >= than blocking 386.7m size
2014-06-30 19:56:14,229 INFO  [RpcServer.handler=14,port=60020] regionserver.MemStoreFlusher: Blocking updates on sceplus-vm48.almaden.ibm.com,60020,1404182803191: the global memstore size 393.8m is >= than blocking 386.7m size
2014-06-30 19:56:14,259 INFO  [RpcServer.handler=17,port=60020] regionserver.MemStoreFlusher: Blocking updates on sceplus-vm48.almaden.ibm.com,60020,1404182803191: the global memstore size 403.5m is >= than blocking 386.7m size
2014-06-30 19:56:14,259 INFO  [RpcServer.handler=19,port=60020] regionserver.MemStoreFlusher: Blocking updates on sceplus-vm48.almaden.ibm.com,60020,1404182803191: the global memstore size 405.0m is >= than blocking 386.7m size
2014-06-30 19:56:14,261 INFO  [RpcServer.handler=39,port=60020] regionserver.MemStoreFlusher: Blocking updates on sceplus-vm48.almaden.ibm.com,60020,1404182803191: the global memstore size 406.6m is >= than blocking 386.7m size
2014-06-30 19:56:14,262 INFO  [RpcServer.handler=8,port=60020] regionserver.MemStoreFlusher: Blocking updates on sceplus-vm48.almaden.ibm.com,60020,1404182803191: the global memstore size 406.6m is >= than blocking 386.7m size
2014-06-30 19:56:14,264 INFO  [RpcServer.handler=37,port=60020] regionserver.MemStoreFlusher: Blocking updates on sceplus-vm48.almaden.ibm.com,60020,1404182803191: the global memstore size 406.6m is >= than blocking 386.7m size
2014-06-30 19:56:14,264 INFO  [RpcServer.handler=32,port=60020] regionserver.MemStoreFlusher: Blocking updates on sceplus-vm48.almaden.ibm.com,60020,1404182803191: the global memstore size 406.6m is >= than blocking 386.7m size
2014-06-30 19:56:14,265 INFO  [RpcServer.handler=22,port=60020] regionserver.MemStoreFlusher: Blocking updates on sceplus-vm48.almaden.ibm.com,60020,1404182803191: the global memstore size 406.6m is >= than blocking 386.7m size
2014-06-30 19:56:14,265 INFO  [RpcServer.handler=47,port=60020] regionserver.MemStoreFlusher: Blocking updates on sceplus-vm48.almaden.ibm.com,60020,1404182803191: the global memstore size 406.6m is >= than blocking 386.7m size
2014-06-30 19:56:14,266 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1404182803191/sceplus-vm48.almaden.ibm.com%2C60020%2C1404182803191.1404183371707 with entries=99, filesize=73.8m; new WAL /hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1404182803191/sceplus-vm48.almaden.ibm.com%2C60020%2C1404182803191.1404183374206
2014-06-30 19:56:14,267 INFO  [RpcServer.handler=23,port=60020] regionserver.MemStoreFlusher: Blocking updates on sceplus-vm48.almaden.ibm.com,60020,1404182803191: the global memstore size 406.6m is >= than blocking 386.7m size
2014-06-30 19:56:14,293 INFO  [RpcServer.handler=30,port=60020] regionserver.MemStoreFlusher: Blocking updates on sceplus-vm48.almaden.ibm.com,60020,1404182803191: the global memstore size 409.8m is >= than blocking 386.7m size
2014-06-30 19:56:14,320 INFO  [RpcServer.handler=36,port=60020] regionserver.MemStoreFlusher: Blocking updates on sceplus-vm48.almaden.ibm.com,60020,1404182803191: the global memstore size 409.8m is >= than blocking 386.7m size
2014-06-30 19:56:14,340 INFO  [RpcServer.handler=9,port=60020] regionserver.MemStoreFlusher: Blocking updates on sceplus-vm48.almaden.ibm.com,60020,1404182803191: the global memstore size 409.8m is >= than blocking 386.7m size
2014-06-30 19:56:14,645 INFO  [RpcServer.handler=29,port=60020] regionserver.MemStoreFlusher: Blocking updates on sceplus-vm48.almaden.ibm.com,60020,1404182803191: the global memstore size 409.8m is >= than blocking 386.7m size
2014-06-30 19:56:14,653 INFO  [RpcServer.handler=6,port=60020] regionserver.MemStoreFlusher: Blocking updates on sceplus-vm48.almaden.ibm.com,60020,1404182803191: the global memstore size 409.8m is >= than blocking 386.7m size
2014-06-30 19:56:15,098 INFO  [MemStoreFlusher.1] regionserver.DefaultStoreFlusher: Flushed, sequenceid=601, memsize=90.3m, hasBloomFilter=true, into tmp file hdfs://master:54310/hbase/data/default/usertable/1d66ac045cc0407c0fa426db565fcbba/.tmp/8637c175dd8148efa0604a6fba62e54c
2014-06-30 19:56:15,108 DEBUG [MemStoreFlusher.1] regionserver.HRegionFileSystem: Committing store file hdfs://master:54310/hbase/data/default/usertable/1d66ac045cc0407c0fa426db565fcbba/.tmp/8637c175dd8148efa0604a6fba62e54c as hdfs://master:54310/hbase/data/default/usertable/1d66ac045cc0407c0fa426db565fcbba/family/8637c175dd8148efa0604a6fba62e54c
2014-06-30 19:56:15,115 INFO  [MemStoreFlusher.1] regionserver.HStore: Added hdfs://master:54310/hbase/data/default/usertable/1d66ac045cc0407c0fa426db565fcbba/family/8637c175dd8148efa0604a6fba62e54c, entries=328920, sequenceid=601, filesize=49.0m
2014-06-30 19:56:15,116 INFO  [MemStoreFlusher.1] regionserver.HRegion: Finished memstore flush of ~108.0m/113198080, currentsize=9.8m/10272880 for region usertable,user5,1404183160444.1d66ac045cc0407c0fa426db565fcbba. in 1057ms, sequenceid=601, compaction requested=true
2014-06-30 19:56:15,116 DEBUG [MemStoreFlusher.1] regionserver.CompactSplitThread: Small Compaction requested: system; Because: MemStoreFlusher.1; compaction_queue=(0:0), split_queue=0, merge_queue=0
2014-06-30 19:56:15,116 DEBUG [regionserver60020-smallCompactions-1404182848217] compactions.RatioBasedCompactionPolicy: Selecting compaction from 4 store files, 0 compacting, 4 eligible, 10 blocking
2014-06-30 19:56:15,116 WARN  [RpcServer.handler=6,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 463ms
2014-06-30 19:56:15,116 INFO  [RpcServer.handler=6,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server sceplus-vm48.almaden.ibm.com,60020,1404182803191
2014-06-30 19:56:15,116 DEBUG [regionserver60020-smallCompactions-1404182848217] compactions.ExploringCompactionPolicy: Exploring compaction algorithm has selected 4 files of size 261139860 starting at candidate #0 after considering 3 permutations with 2 in ratio
2014-06-30 19:56:15,117 WARN  [RpcServer.handler=29,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 471ms
2014-06-30 19:56:15,117 DEBUG [regionserver60020-smallCompactions-1404182848217] regionserver.HStore: 1d66ac045cc0407c0fa426db565fcbba - family: Initiating major compaction
2014-06-30 19:56:15,117 INFO  [RpcServer.handler=29,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server sceplus-vm48.almaden.ibm.com,60020,1404182803191
2014-06-30 19:56:15,117 INFO  [regionserver60020-smallCompactions-1404182848217] regionserver.HRegion: Starting compaction on family in region usertable,user5,1404183160444.1d66ac045cc0407c0fa426db565fcbba.
2014-06-30 19:56:15,117 WARN  [RpcServer.handler=9,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 777ms
2014-06-30 19:56:15,117 INFO  [RpcServer.handler=9,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server sceplus-vm48.almaden.ibm.com,60020,1404182803191
2014-06-30 19:56:15,117 WARN  [RpcServer.handler=36,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 797ms
2014-06-30 19:56:15,117 INFO  [RpcServer.handler=36,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server sceplus-vm48.almaden.ibm.com,60020,1404182803191
2014-06-30 19:56:15,117 WARN  [RpcServer.handler=30,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 824ms
2014-06-30 19:56:15,117 INFO  [RpcServer.handler=30,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server sceplus-vm48.almaden.ibm.com,60020,1404182803191
2014-06-30 19:56:15,117 INFO  [regionserver60020-smallCompactions-1404182848217] regionserver.HStore: Starting compaction of 4 file(s) in family of usertable,user5,1404183160444.1d66ac045cc0407c0fa426db565fcbba. into tmpdir=hdfs://master:54310/hbase/data/default/usertable/1d66ac045cc0407c0fa426db565fcbba/.tmp, totalSize=249.0m
2014-06-30 19:56:15,117 WARN  [RpcServer.handler=23,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 850ms
2014-06-30 19:56:15,117 DEBUG [regionserver60020-smallCompactions-1404182848217] compactions.Compactor: Compacting hdfs://master:54310/hbase/data/default/usertable/1d66ac045cc0407c0fa426db565fcbba/family/5f0613413be6424184c334d89b1ade67, keycount=75840, bloomtype=ROW, size=112.3m, encoding=NONE, seqNum=372, earliestPutTs=1404183222083
2014-06-30 19:56:15,117 INFO  [RpcServer.handler=23,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server sceplus-vm48.almaden.ibm.com,60020,1404182803191
2014-06-30 19:56:15,117 DEBUG [regionserver60020-smallCompactions-1404182848217] compactions.Compactor: Compacting hdfs://master:54310/hbase/data/default/usertable/1d66ac045cc0407c0fa426db565fcbba/family/6b715f472bf34bf8ab1f0abab79d0595, keycount=32401, bloomtype=ROW, size=48.3m, encoding=NONE, seqNum=449, earliestPutTs=1404183292061
2014-06-30 19:56:15,118 DEBUG [regionserver60020-smallCompactions-1404182848217] compactions.Compactor: Compacting hdfs://master:54310/hbase/data/default/usertable/1d66ac045cc0407c0fa426db565fcbba/family/cad5a72f2c9949378c3fa437b4b8a718, keycount=26396, bloomtype=ROW, size=39.3m, encoding=NONE, seqNum=532, earliestPutTs=1404183347000
2014-06-30 19:56:15,118 DEBUG [regionserver60020-smallCompactions-1404182848217] compactions.Compactor: Compacting hdfs://master:54310/hbase/data/default/usertable/1d66ac045cc0407c0fa426db565fcbba/family/8637c175dd8148efa0604a6fba62e54c, keycount=32892, bloomtype=ROW, size=49.0m, encoding=NONE, seqNum=601, earliestPutTs=1404183364375
2014-06-30 19:56:15,118 WARN  [RpcServer.handler=47,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 853ms
2014-06-30 19:56:15,118 INFO  [RpcServer.handler=47,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server sceplus-vm48.almaden.ibm.com,60020,1404182803191
2014-06-30 19:56:15,118 WARN  [RpcServer.handler=22,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 853ms
2014-06-30 19:56:15,118 INFO  [RpcServer.handler=22,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server sceplus-vm48.almaden.ibm.com,60020,1404182803191
2014-06-30 19:56:15,119 WARN  [RpcServer.handler=32,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 855ms
2014-06-30 19:56:15,119 INFO  [RpcServer.handler=32,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server sceplus-vm48.almaden.ibm.com,60020,1404182803191
2014-06-30 19:56:15,122 WARN  [RpcServer.handler=37,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 859ms
2014-06-30 19:56:15,122 INFO  [RpcServer.handler=37,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server sceplus-vm48.almaden.ibm.com,60020,1404182803191
2014-06-30 19:56:15,122 WARN  [RpcServer.handler=8,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 860ms
2014-06-30 19:56:15,123 INFO  [RpcServer.handler=8,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server sceplus-vm48.almaden.ibm.com,60020,1404182803191
2014-06-30 19:56:15,129 WARN  [RpcServer.handler=39,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 868ms
2014-06-30 19:56:15,129 INFO  [RpcServer.handler=39,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server sceplus-vm48.almaden.ibm.com,60020,1404182803191
2014-06-30 19:56:15,129 WARN  [RpcServer.handler=19,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 870ms
2014-06-30 19:56:15,129 INFO  [RpcServer.handler=19,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server sceplus-vm48.almaden.ibm.com,60020,1404182803191
2014-06-30 19:56:15,129 WARN  [RpcServer.handler=17,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 870ms
2014-06-30 19:56:15,129 INFO  [RpcServer.handler=17,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server sceplus-vm48.almaden.ibm.com,60020,1404182803191
2014-06-30 19:56:15,129 WARN  [RpcServer.handler=14,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 900ms
2014-06-30 19:56:15,130 INFO  [RpcServer.handler=14,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server sceplus-vm48.almaden.ibm.com,60020,1404182803191
2014-06-30 19:56:15,130 WARN  [RpcServer.handler=12,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 902ms
2014-06-30 19:56:15,130 INFO  [RpcServer.handler=12,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server sceplus-vm48.almaden.ibm.com,60020,1404182803191
2014-06-30 19:56:15,130 WARN  [RpcServer.handler=27,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 903ms
2014-06-30 19:56:15,130 INFO  [RpcServer.handler=27,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server sceplus-vm48.almaden.ibm.com,60020,1404182803191
2014-06-30 19:56:15,130 WARN  [RpcServer.handler=43,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 905ms
2014-06-30 19:56:15,130 INFO  [RpcServer.handler=43,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server sceplus-vm48.almaden.ibm.com,60020,1404182803191
2014-06-30 19:56:15,131 WARN  [RpcServer.handler=44,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 908ms
2014-06-30 19:56:15,131 INFO  [RpcServer.handler=44,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server sceplus-vm48.almaden.ibm.com,60020,1404182803191
2014-06-30 19:56:15,135 WARN  [RpcServer.handler=40,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 911ms
2014-06-30 19:56:15,135 DEBUG [regionserver60020-smallCompactions-1404182848217] util.FSUtils: DFS Client does not support most favored nodes create; using default create
2014-06-30 19:56:15,135 INFO  [RpcServer.handler=40,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server sceplus-vm48.almaden.ibm.com,60020,1404182803191
2014-06-30 19:56:15,139 WARN  [RpcServer.handler=18,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 916ms
2014-06-30 19:56:15,139 INFO  [RpcServer.handler=18,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server sceplus-vm48.almaden.ibm.com,60020,1404182803191
2014-06-30 19:56:15,139 WARN  [RpcServer.handler=4,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 916ms
2014-06-30 19:56:15,140 INFO  [RpcServer.handler=4,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server sceplus-vm48.almaden.ibm.com,60020,1404182803191
2014-06-30 19:56:15,145 WARN  [RpcServer.handler=10,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 924ms
2014-06-30 19:56:15,145 INFO  [RpcServer.handler=10,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server sceplus-vm48.almaden.ibm.com,60020,1404182803191
2014-06-30 19:56:15,146 WARN  [RpcServer.handler=16,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 925ms
2014-06-30 19:56:15,146 INFO  [RpcServer.handler=16,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server sceplus-vm48.almaden.ibm.com,60020,1404182803191
2014-06-30 19:56:17,324 DEBUG [MemStoreFlusher.0] regionserver.MemStoreFlusher: Flush thread woke up because memory above low water=367.3m
2014-06-30 19:56:17,324 INFO  [MemStoreFlusher.0] regionserver.MemStoreFlusher: Flush of region usertable,user2,1404183160444.5446f1f888398e97c0ec3cc30da5bf97. due to global heap pressure
2014-06-30 19:56:17,325 DEBUG [MemStoreFlusher.0] regionserver.HRegion: Started memstore flush for usertable,user2,1404183160444.5446f1f888398e97c0ec3cc30da5bf97., current region memstore size 104.8m
2014-06-30 19:56:17,419 INFO  [RpcServer.handler=46,port=60020] regionserver.MemStoreFlusher: Blocking updates on sceplus-vm48.almaden.ibm.com,60020,1404182803191: the global memstore size 393.2m is >= than blocking 386.7m size
2014-06-30 19:56:17,420 INFO  [RpcServer.handler=2,port=60020] regionserver.MemStoreFlusher: Blocking updates on sceplus-vm48.almaden.ibm.com,60020,1404182803191: the global memstore size 393.2m is >= than blocking 386.7m size
2014-06-30 19:56:17,421 INFO  [RpcServer.handler=13,port=60020] regionserver.MemStoreFlusher: Blocking updates on sceplus-vm48.almaden.ibm.com,60020,1404182803191: the global memstore size 393.2m is >= than blocking 386.7m size
2014-06-30 19:56:17,421 INFO  [RpcServer.handler=45,port=60020] regionserver.MemStoreFlusher: Blocking updates on sceplus-vm48.almaden.ibm.com,60020,1404182803191: the global memstore size 393.2m is >= than blocking 386.7m size
2014-06-30 19:56:17,422 INFO  [RpcServer.handler=42,port=60020] regionserver.MemStoreFlusher: Blocking updates on sceplus-vm48.almaden.ibm.com,60020,1404182803191: the global memstore size 393.2m is >= than blocking 386.7m size
2014-06-30 19:56:17,425 INFO  [RpcServer.handler=31,port=60020] regionserver.MemStoreFlusher: Blocking updates on sceplus-vm48.almaden.ibm.com,60020,1404182803191: the global memstore size 394.8m is >= than blocking 386.7m size
2014-06-30 19:56:17,469 INFO  [RpcServer.handler=24,port=60020] regionserver.MemStoreFlusher: Blocking updates on sceplus-vm48.almaden.ibm.com,60020,1404182803191: the global memstore size 406.0m is >= than blocking 386.7m size
2014-06-30 19:56:17,470 INFO  [RpcServer.handler=3,port=60020] regionserver.MemStoreFlusher: Blocking updates on sceplus-vm48.almaden.ibm.com,60020,1404182803191: the global memstore size 406.0m is >= than blocking 386.7m size
2014-06-30 19:56:17,470 INFO  [RpcServer.handler=25,port=60020] regionserver.MemStoreFlusher: Blocking updates on sceplus-vm48.almaden.ibm.com,60020,1404182803191: the global memstore size 406.0m is >= than blocking 386.7m size
2014-06-30 19:56:17,471 INFO  [RpcServer.handler=11,port=60020] regionserver.MemStoreFlusher: Blocking updates on sceplus-vm48.almaden.ibm.com,60020,1404182803191: the global memstore size 406.0m is >= than blocking 386.7m size
2014-06-30 19:56:17,525 DEBUG [MemStoreFlusher.0] util.FSUtils: DFS Client does not support most favored nodes create; using default create
2014-06-30 19:56:17,554 INFO  [RpcServer.handler=41,port=60020] regionserver.MemStoreFlusher: Blocking updates on sceplus-vm48.almaden.ibm.com,60020,1404182803191: the global memstore size 406.0m is >= than blocking 386.7m size
2014-06-30 19:56:17,575 INFO  [RpcServer.handler=38,port=60020] regionserver.MemStoreFlusher: Blocking updates on sceplus-vm48.almaden.ibm.com,60020,1404182803191: the global memstore size 406.0m is >= than blocking 386.7m size
2014-06-30 19:56:17,593 INFO  [RpcServer.handler=5,port=60020] regionserver.MemStoreFlusher: Blocking updates on sceplus-vm48.almaden.ibm.com,60020,1404182803191: the global memstore size 406.0m is >= than blocking 386.7m size
2014-06-30 19:56:17,615 INFO  [RpcServer.handler=20,port=60020] regionserver.MemStoreFlusher: Blocking updates on sceplus-vm48.almaden.ibm.com,60020,1404182803191: the global memstore size 406.0m is >= than blocking 386.7m size
2014-06-30 19:56:17,630 INFO  [RpcServer.handler=49,port=60020] regionserver.MemStoreFlusher: Blocking updates on sceplus-vm48.almaden.ibm.com,60020,1404182803191: the global memstore size 406.0m is >= than blocking 386.7m size
2014-06-30 19:56:17,648 INFO  [RpcServer.handler=15,port=60020] regionserver.MemStoreFlusher: Blocking updates on sceplus-vm48.almaden.ibm.com,60020,1404182803191: the global memstore size 406.0m is >= than blocking 386.7m size
2014-06-30 19:56:19,301 INFO  [JvmPauseMonitor] util.JvmPauseMonitor: Detected pause in JVM or host machine (eg GC): pause of approximately 1499ms
GC pool 'ParNew' had collection(s): count=1 time=0ms
GC pool 'ConcurrentMarkSweep' had collection(s): count=1 time=1642ms
2014-06-30 19:56:19,309 INFO  [RpcServer.handler=0,port=60020] regionserver.MemStoreFlusher: Blocking updates on sceplus-vm48.almaden.ibm.com,60020,1404182803191: the global memstore size 406.0m is >= than blocking 386.7m size
2014-06-30 19:56:19,672 INFO  [RpcServer.handler=33,port=60020] regionserver.MemStoreFlusher: Blocking updates on sceplus-vm48.almaden.ibm.com,60020,1404182803191: the global memstore size 406.0m is >= than blocking 386.7m size
2014-06-30 19:56:19,707 INFO  [RpcServer.handler=28,port=60020] regionserver.MemStoreFlusher: Blocking updates on sceplus-vm48.almaden.ibm.com,60020,1404182803191: the global memstore size 406.0m is >= than blocking 386.7m size
2014-06-30 19:56:19,719 INFO  [RpcServer.handler=35,port=60020] regionserver.MemStoreFlusher: Blocking updates on sceplus-vm48.almaden.ibm.com,60020,1404182803191: the global memstore size 406.0m is >= than blocking 386.7m size
2014-06-30 19:56:19,737 INFO  [RpcServer.handler=34,port=60020] regionserver.MemStoreFlusher: Blocking updates on sceplus-vm48.almaden.ibm.com,60020,1404182803191: the global memstore size 406.0m is >= than blocking 386.7m size
2014-06-30 19:56:19,759 INFO  [RpcServer.handler=9,port=60020] regionserver.MemStoreFlusher: Blocking updates on sceplus-vm48.almaden.ibm.com,60020,1404182803191: the global memstore size 406.0m is >= than blocking 386.7m size
2014-06-30 19:56:19,776 INFO  [RpcServer.handler=47,port=60020] regionserver.MemStoreFlusher: Blocking updates on sceplus-vm48.almaden.ibm.com,60020,1404182803191: the global memstore size 406.0m is >= than blocking 386.7m size
2014-06-30 19:56:19,798 INFO  [RpcServer.handler=39,port=60020] regionserver.MemStoreFlusher: Blocking updates on sceplus-vm48.almaden.ibm.com,60020,1404182803191: the global memstore size 406.0m is >= than blocking 386.7m size
2014-06-30 19:56:19,977 INFO  [RpcServer.handler=14,port=60020] regionserver.MemStoreFlusher: Blocking updates on sceplus-vm48.almaden.ibm.com,60020,1404182803191: the global memstore size 406.0m is >= than blocking 386.7m size
2014-06-30 19:56:19,988 INFO  [RpcServer.handler=12,port=60020] regionserver.MemStoreFlusher: Blocking updates on sceplus-vm48.almaden.ibm.com,60020,1404182803191: the global memstore size 406.0m is >= than blocking 386.7m size
2014-06-30 19:56:19,995 INFO  [RpcServer.handler=27,port=60020] regionserver.MemStoreFlusher: Blocking updates on sceplus-vm48.almaden.ibm.com,60020,1404182803191: the global memstore size 406.0m is >= than blocking 386.7m size
2014-06-30 19:56:20,004 INFO  [RpcServer.handler=26,port=60020] regionserver.MemStoreFlusher: Blocking updates on sceplus-vm48.almaden.ibm.com,60020,1404182803191: the global memstore size 406.0m is >= than blocking 386.7m size
2014-06-30 19:56:20,130 INFO  [MemStoreFlusher.0] regionserver.DefaultStoreFlusher: Flushed, sequenceid=626, memsize=85.3m, hasBloomFilter=true, into tmp file hdfs://master:54310/hbase/data/default/usertable/5446f1f888398e97c0ec3cc30da5bf97/.tmp/896fee37320d41b9a9316a4e242fc409
2014-06-30 19:56:20,139 DEBUG [MemStoreFlusher.0] regionserver.HRegionFileSystem: Committing store file hdfs://master:54310/hbase/data/default/usertable/5446f1f888398e97c0ec3cc30da5bf97/.tmp/896fee37320d41b9a9316a4e242fc409 as hdfs://master:54310/hbase/data/default/usertable/5446f1f888398e97c0ec3cc30da5bf97/family/896fee37320d41b9a9316a4e242fc409
2014-06-30 19:56:20,147 INFO  [MemStoreFlusher.0] regionserver.HStore: Added hdfs://master:54310/hbase/data/default/usertable/5446f1f888398e97c0ec3cc30da5bf97/family/896fee37320d41b9a9316a4e242fc409, entries=310740, sequenceid=626, filesize=46.3m
2014-06-30 19:56:20,148 INFO  [MemStoreFlusher.0] regionserver.HRegion: Finished memstore flush of ~111.3m/116716800, currentsize=0.0/0 for region usertable,user2,1404183160444.5446f1f888398e97c0ec3cc30da5bf97. in 2823ms, sequenceid=626, compaction requested=true
2014-06-30 19:56:20,148 DEBUG [MemStoreFlusher.0] regionserver.CompactSplitThread: Small Compaction requested: system; Because: MemStoreFlusher.0; compaction_queue=(0:1), split_queue=0, merge_queue=0
2014-06-30 19:56:20,148 WARN  [RpcServer.handler=26,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 144ms
2014-06-30 19:56:20,148 INFO  [RpcServer.handler=26,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server sceplus-vm48.almaden.ibm.com,60020,1404182803191
2014-06-30 19:56:20,149 WARN  [RpcServer.handler=27,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 154ms
2014-06-30 19:56:20,150 INFO  [RpcServer.handler=27,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server sceplus-vm48.almaden.ibm.com,60020,1404182803191
2014-06-30 19:56:20,150 WARN  [RpcServer.handler=12,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 162ms
2014-06-30 19:56:20,150 INFO  [RpcServer.handler=12,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server sceplus-vm48.almaden.ibm.com,60020,1404182803191
2014-06-30 19:56:20,150 WARN  [RpcServer.handler=14,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 173ms
2014-06-30 19:56:20,150 INFO  [RpcServer.handler=14,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server sceplus-vm48.almaden.ibm.com,60020,1404182803191
2014-06-30 19:56:20,150 WARN  [RpcServer.handler=39,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 352ms
2014-06-30 19:56:20,150 INFO  [RpcServer.handler=39,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server sceplus-vm48.almaden.ibm.com,60020,1404182803191
2014-06-30 19:56:20,150 WARN  [RpcServer.handler=47,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 374ms
2014-06-30 19:56:20,150 INFO  [RpcServer.handler=47,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server sceplus-vm48.almaden.ibm.com,60020,1404182803191
2014-06-30 19:56:20,150 WARN  [RpcServer.handler=9,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 391ms
2014-06-30 19:56:20,150 INFO  [RpcServer.handler=9,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server sceplus-vm48.almaden.ibm.com,60020,1404182803191
2014-06-30 19:56:20,152 WARN  [RpcServer.handler=34,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 415ms
2014-06-30 19:56:20,152 INFO  [RpcServer.handler=34,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server sceplus-vm48.almaden.ibm.com,60020,1404182803191
2014-06-30 19:56:20,152 WARN  [RpcServer.handler=35,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 433ms
2014-06-30 19:56:20,153 INFO  [RpcServer.handler=35,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server sceplus-vm48.almaden.ibm.com,60020,1404182803191
2014-06-30 19:56:20,153 WARN  [RpcServer.handler=28,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 446ms
2014-06-30 19:56:20,153 INFO  [RpcServer.handler=28,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server sceplus-vm48.almaden.ibm.com,60020,1404182803191
2014-06-30 19:56:20,153 WARN  [RpcServer.handler=33,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 481ms
2014-06-30 19:56:20,154 INFO  [RpcServer.handler=33,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server sceplus-vm48.almaden.ibm.com,60020,1404182803191
2014-06-30 19:56:20,155 WARN  [RpcServer.handler=0,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 845ms
2014-06-30 19:56:20,155 INFO  [RpcServer.handler=0,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server sceplus-vm48.almaden.ibm.com,60020,1404182803191
2014-06-30 19:56:20,158 WARN  [RpcServer.handler=15,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 2510ms
2014-06-30 19:56:20,158 INFO  [RpcServer.handler=15,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server sceplus-vm48.almaden.ibm.com,60020,1404182803191
2014-06-30 19:56:20,160 WARN  [RpcServer.handler=49,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 2531ms
2014-06-30 19:56:20,160 INFO  [RpcServer.handler=49,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server sceplus-vm48.almaden.ibm.com,60020,1404182803191
2014-06-30 19:56:20,160 WARN  [RpcServer.handler=20,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 2545ms
2014-06-30 19:56:20,160 INFO  [RpcServer.handler=20,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server sceplus-vm48.almaden.ibm.com,60020,1404182803191
2014-06-30 19:56:20,162 WARN  [RpcServer.handler=5,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 2569ms
2014-06-30 19:56:20,163 INFO  [RpcServer.handler=5,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server sceplus-vm48.almaden.ibm.com,60020,1404182803191
2014-06-30 19:56:20,163 WARN  [RpcServer.handler=38,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 2588ms
2014-06-30 19:56:20,164 INFO  [RpcServer.handler=38,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server sceplus-vm48.almaden.ibm.com,60020,1404182803191
2014-06-30 19:56:20,164 WARN  [RpcServer.handler=41,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 2610ms
2014-06-30 19:56:20,164 INFO  [RpcServer.handler=41,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server sceplus-vm48.almaden.ibm.com,60020,1404182803191
2014-06-30 19:56:20,164 WARN  [RpcServer.handler=11,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 2694ms
2014-06-30 19:56:20,164 INFO  [RpcServer.handler=11,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server sceplus-vm48.almaden.ibm.com,60020,1404182803191
2014-06-30 19:56:20,164 WARN  [RpcServer.handler=25,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 2695ms
2014-06-30 19:56:20,164 INFO  [RpcServer.handler=25,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server sceplus-vm48.almaden.ibm.com,60020,1404182803191
2014-06-30 19:56:20,166 WARN  [RpcServer.handler=3,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 2697ms
2014-06-30 19:56:20,166 INFO  [RpcServer.handler=3,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server sceplus-vm48.almaden.ibm.com,60020,1404182803191
2014-06-30 19:56:20,174 WARN  [RpcServer.handler=24,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 2705ms
2014-06-30 19:56:20,174 INFO  [RpcServer.handler=24,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server sceplus-vm48.almaden.ibm.com,60020,1404182803191
2014-06-30 19:56:20,175 WARN  [RpcServer.handler=31,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 2749ms
2014-06-30 19:56:20,175 INFO  [RpcServer.handler=31,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server sceplus-vm48.almaden.ibm.com,60020,1404182803191
2014-06-30 19:56:20,175 WARN  [RpcServer.handler=42,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 2753ms
2014-06-30 19:56:20,175 INFO  [RpcServer.handler=42,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server sceplus-vm48.almaden.ibm.com,60020,1404182803191
2014-06-30 19:56:20,177 WARN  [RpcServer.handler=45,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 2755ms
2014-06-30 19:56:20,177 INFO  [RpcServer.handler=45,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server sceplus-vm48.almaden.ibm.com,60020,1404182803191
2014-06-30 19:56:20,177 WARN  [RpcServer.handler=13,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 2756ms
2014-06-30 19:56:20,177 INFO  [RpcServer.handler=13,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server sceplus-vm48.almaden.ibm.com,60020,1404182803191
2014-06-30 19:56:20,177 WARN  [RpcServer.handler=2,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 2757ms
2014-06-30 19:56:20,177 INFO  [RpcServer.handler=2,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server sceplus-vm48.almaden.ibm.com,60020,1404182803191
2014-06-30 19:56:20,186 WARN  [RpcServer.handler=46,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 2767ms
2014-06-30 19:56:20,186 INFO  [RpcServer.handler=46,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server sceplus-vm48.almaden.ibm.com,60020,1404182803191
2014-06-30 19:56:20,241 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-06-30 19:56:20,279 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 2986 synced till here 2964
2014-06-30 19:56:20,602 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1404182803191/sceplus-vm48.almaden.ibm.com%2C60020%2C1404182803191.1404183374206 with entries=103, filesize=80.9m; new WAL /hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1404182803191/sceplus-vm48.almaden.ibm.com%2C60020%2C1404182803191.1404183380242
2014-06-30 19:56:22,607 INFO  [JvmPauseMonitor] util.JvmPauseMonitor: Detected pause in JVM or host machine (eg GC): pause of approximately 1805ms
GC pool 'ParNew' had collection(s): count=2 time=191ms
GC pool 'ConcurrentMarkSweep' had collection(s): count=2 time=1859ms
2014-06-30 19:56:22,802 DEBUG [MemStoreFlusher.1] regionserver.MemStoreFlusher: Flush thread woke up because memory above low water=367.3m
2014-06-30 19:56:22,803 INFO  [MemStoreFlusher.1] regionserver.MemStoreFlusher: Flush of region usertable,user8,1404183160444.28938d267c1c61f7feb582b85e4c172f. due to global heap pressure
2014-06-30 19:56:22,803 DEBUG [MemStoreFlusher.1] regionserver.HRegion: Started memstore flush for usertable,user8,1404183160444.28938d267c1c61f7feb582b85e4c172f., current region memstore size 103.9m
2014-06-30 19:56:22,890 INFO  [RpcServer.handler=5,port=60020] regionserver.MemStoreFlusher: Blocking updates on sceplus-vm48.almaden.ibm.com,60020,1404182803191: the global memstore size 387.9m is >= than blocking 386.7m size
2014-06-30 19:56:22,890 INFO  [RpcServer.handler=20,port=60020] regionserver.MemStoreFlusher: Blocking updates on sceplus-vm48.almaden.ibm.com,60020,1404182803191: the global memstore size 387.9m is >= than blocking 386.7m size
2014-06-30 19:56:22,890 INFO  [RpcServer.handler=23,port=60020] regionserver.MemStoreFlusher: Blocking updates on sceplus-vm48.almaden.ibm.com,60020,1404182803191: the global memstore size 387.9m is >= than blocking 386.7m size
2014-06-30 19:56:22,894 INFO  [RpcServer.handler=15,port=60020] regionserver.MemStoreFlusher: Blocking updates on sceplus-vm48.almaden.ibm.com,60020,1404182803191: the global memstore size 400.9m is >= than blocking 386.7m size
2014-06-30 19:56:22,895 INFO  [RpcServer.handler=37,port=60020] regionserver.MemStoreFlusher: Blocking updates on sceplus-vm48.almaden.ibm.com,60020,1404182803191: the global memstore size 400.9m is >= than blocking 386.7m size
2014-06-30 19:56:22,895 INFO  [RpcServer.handler=38,port=60020] regionserver.MemStoreFlusher: Blocking updates on sceplus-vm48.almaden.ibm.com,60020,1404182803191: the global memstore size 400.9m is >= than blocking 386.7m size
2014-06-30 19:56:22,895 INFO  [RpcServer.handler=41,port=60020] regionserver.MemStoreFlusher: Blocking updates on sceplus-vm48.almaden.ibm.com,60020,1404182803191: the global memstore size 400.9m is >= than blocking 386.7m size
2014-06-30 19:56:22,895 INFO  [RpcServer.handler=34,port=60020] regionserver.MemStoreFlusher: Blocking updates on sceplus-vm48.almaden.ibm.com,60020,1404182803191: the global memstore size 400.9m is >= than blocking 386.7m size
2014-06-30 19:56:22,896 INFO  [RpcServer.handler=48,port=60020] regionserver.MemStoreFlusher: Blocking updates on sceplus-vm48.almaden.ibm.com,60020,1404182803191: the global memstore size 400.9m is >= than blocking 386.7m size
2014-06-30 19:56:24,883 INFO  [JvmPauseMonitor] util.JvmPauseMonitor: Detected pause in JVM or host machine (eg GC): pause of approximately 1775ms
GC pool 'ParNew' had collection(s): count=1 time=0ms
GC pool 'ConcurrentMarkSweep' had collection(s): count=1 time=1962ms
2014-06-30 19:56:24,893 INFO  [RpcServer.handler=18,port=60020] regionserver.MemStoreFlusher: Blocking updates on sceplus-vm48.almaden.ibm.com,60020,1404182803191: the global memstore size 400.9m is >= than blocking 386.7m size
2014-06-30 19:56:24,902 INFO  [RpcServer.handler=10,port=60020] regionserver.MemStoreFlusher: Blocking updates on sceplus-vm48.almaden.ibm.com,60020,1404182803191: the global memstore size 400.9m is >= than blocking 386.7m size
2014-06-30 19:56:24,913 INFO  [RpcServer.handler=19,port=60020] regionserver.MemStoreFlusher: Blocking updates on sceplus-vm48.almaden.ibm.com,60020,1404182803191: the global memstore size 400.9m is >= than blocking 386.7m size
2014-06-30 19:56:24,923 INFO  [RpcServer.handler=43,port=60020] regionserver.MemStoreFlusher: Blocking updates on sceplus-vm48.almaden.ibm.com,60020,1404182803191: the global memstore size 400.9m is >= than blocking 386.7m size
2014-06-30 19:56:24,932 INFO  [RpcServer.handler=17,port=60020] regionserver.MemStoreFlusher: Blocking updates on sceplus-vm48.almaden.ibm.com,60020,1404182803191: the global memstore size 400.9m is >= than blocking 386.7m size
2014-06-30 19:56:24,942 INFO  [RpcServer.handler=21,port=60020] regionserver.MemStoreFlusher: Blocking updates on sceplus-vm48.almaden.ibm.com,60020,1404182803191: the global memstore size 400.9m is >= than blocking 386.7m size
2014-06-30 19:56:24,944 INFO  [RpcServer.handler=35,port=60020] regionserver.MemStoreFlusher: Blocking updates on sceplus-vm48.almaden.ibm.com,60020,1404182803191: the global memstore size 410.6m is >= than blocking 386.7m size
2014-06-30 19:56:24,944 INFO  [RpcServer.handler=27,port=60020] regionserver.MemStoreFlusher: Blocking updates on sceplus-vm48.almaden.ibm.com,60020,1404182803191: the global memstore size 410.6m is >= than blocking 386.7m size
2014-06-30 19:56:24,947 INFO  [RpcServer.handler=30,port=60020] regionserver.MemStoreFlusher: Blocking updates on sceplus-vm48.almaden.ibm.com,60020,1404182803191: the global memstore size 421.5m is >= than blocking 386.7m size
2014-06-30 19:56:24,948 INFO  [RpcServer.handler=32,port=60020] regionserver.MemStoreFlusher: Blocking updates on sceplus-vm48.almaden.ibm.com,60020,1404182803191: the global memstore size 423.0m is >= than blocking 386.7m size
2014-06-30 19:56:24,948 INFO  [RpcServer.handler=28,port=60020] regionserver.MemStoreFlusher: Blocking updates on sceplus-vm48.almaden.ibm.com,60020,1404182803191: the global memstore size 423.0m is >= than blocking 386.7m size
2014-06-30 19:56:24,949 INFO  [RpcServer.handler=22,port=60020] regionserver.MemStoreFlusher: Blocking updates on sceplus-vm48.almaden.ibm.com,60020,1404182803191: the global memstore size 423.0m is >= than blocking 386.7m size
2014-06-30 19:56:24,950 INFO  [RpcServer.handler=0,port=60020] regionserver.MemStoreFlusher: Blocking updates on sceplus-vm48.almaden.ibm.com,60020,1404182803191: the global memstore size 423.0m is >= than blocking 386.7m size
2014-06-30 19:56:24,950 INFO  [RpcServer.handler=14,port=60020] regionserver.MemStoreFlusher: Blocking updates on sceplus-vm48.almaden.ibm.com,60020,1404182803191: the global memstore size 423.0m is >= than blocking 386.7m size
2014-06-30 19:56:24,951 INFO  [RpcServer.handler=39,port=60020] regionserver.MemStoreFlusher: Blocking updates on sceplus-vm48.almaden.ibm.com,60020,1404182803191: the global memstore size 424.6m is >= than blocking 386.7m size
2014-06-30 19:56:24,951 INFO  [RpcServer.handler=47,port=60020] regionserver.MemStoreFlusher: Blocking updates on sceplus-vm48.almaden.ibm.com,60020,1404182803191: the global memstore size 424.6m is >= than blocking 386.7m size
2014-06-30 19:56:24,951 INFO  [RpcServer.handler=40,port=60020] regionserver.MemStoreFlusher: Blocking updates on sceplus-vm48.almaden.ibm.com,60020,1404182803191: the global memstore size 424.6m is >= than blocking 386.7m size
2014-06-30 19:56:24,952 INFO  [RpcServer.handler=7,port=60020] regionserver.MemStoreFlusher: Blocking updates on sceplus-vm48.almaden.ibm.com,60020,1404182803191: the global memstore size 424.6m is >= than blocking 386.7m size
2014-06-30 19:56:24,953 INFO  [RpcServer.handler=8,port=60020] regionserver.MemStoreFlusher: Blocking updates on sceplus-vm48.almaden.ibm.com,60020,1404182803191: the global memstore size 424.6m is >= than blocking 386.7m size
2014-06-30 19:56:24,953 INFO  [RpcServer.handler=1,port=60020] regionserver.MemStoreFlusher: Blocking updates on sceplus-vm48.almaden.ibm.com,60020,1404182803191: the global memstore size 424.6m is >= than blocking 386.7m size
2014-06-30 19:56:24,955 INFO  [RpcServer.handler=4,port=60020] regionserver.MemStoreFlusher: Blocking updates on sceplus-vm48.almaden.ibm.com,60020,1404182803191: the global memstore size 430.8m is >= than blocking 386.7m size
2014-06-30 19:56:24,956 INFO  [RpcServer.handler=16,port=60020] regionserver.MemStoreFlusher: Blocking updates on sceplus-vm48.almaden.ibm.com,60020,1404182803191: the global memstore size 430.8m is >= than blocking 386.7m size
2014-06-30 19:56:24,957 INFO  [RpcServer.handler=9,port=60020] regionserver.MemStoreFlusher: Blocking updates on sceplus-vm48.almaden.ibm.com,60020,1404182803191: the global memstore size 430.8m is >= than blocking 386.7m size
2014-06-30 19:56:24,957 INFO  [RpcServer.handler=49,port=60020] regionserver.MemStoreFlusher: Blocking updates on sceplus-vm48.almaden.ibm.com,60020,1404182803191: the global memstore size 430.8m is >= than blocking 386.7m size
2014-06-30 19:56:24,957 INFO  [RpcServer.handler=33,port=60020] regionserver.MemStoreFlusher: Blocking updates on sceplus-vm48.almaden.ibm.com,60020,1404182803191: the global memstore size 430.8m is >= than blocking 386.7m size
2014-06-30 19:56:24,957 INFO  [RpcServer.handler=12,port=60020] regionserver.MemStoreFlusher: Blocking updates on sceplus-vm48.almaden.ibm.com,60020,1404182803191: the global memstore size 430.8m is >= than blocking 386.7m size
2014-06-30 19:56:25,026 DEBUG [MemStoreFlusher.1] util.FSUtils: DFS Client does not support most favored nodes create; using default create
2014-06-30 19:56:27,058 INFO  [JvmPauseMonitor] util.JvmPauseMonitor: Detected pause in JVM or host machine (eg GC): pause of approximately 1675ms
GC pool 'ParNew' had collection(s): count=1 time=0ms
GC pool 'ConcurrentMarkSweep' had collection(s): count=1 time=1735ms
2014-06-30 19:56:27,083 INFO  [RpcServer.handler=44,port=60020] regionserver.MemStoreFlusher: Blocking updates on sceplus-vm48.almaden.ibm.com,60020,1404182803191: the global memstore size 430.8m is >= than blocking 386.7m size
2014-06-30 19:56:27,093 INFO  [RpcServer.handler=29,port=60020] regionserver.MemStoreFlusher: Blocking updates on sceplus-vm48.almaden.ibm.com,60020,1404182803191: the global memstore size 430.8m is >= than blocking 386.7m size
2014-06-30 19:56:27,102 INFO  [RpcServer.handler=6,port=60020] regionserver.MemStoreFlusher: Blocking updates on sceplus-vm48.almaden.ibm.com,60020,1404182803191: the global memstore size 430.8m is >= than blocking 386.7m size
2014-06-30 19:56:27,455 INFO  [MemStoreFlusher.1] regionserver.DefaultStoreFlusher: Flushed, sequenceid=629, memsize=89.3m, hasBloomFilter=true, into tmp file hdfs://master:54310/hbase/data/default/usertable/28938d267c1c61f7feb582b85e4c172f/.tmp/3f8f1c17a86848db9e7be2613b5186a6
2014-06-30 19:56:27,464 DEBUG [MemStoreFlusher.1] regionserver.HRegionFileSystem: Committing store file hdfs://master:54310/hbase/data/default/usertable/28938d267c1c61f7feb582b85e4c172f/.tmp/3f8f1c17a86848db9e7be2613b5186a6 as hdfs://master:54310/hbase/data/default/usertable/28938d267c1c61f7feb582b85e4c172f/family/3f8f1c17a86848db9e7be2613b5186a6
2014-06-30 19:56:27,473 INFO  [MemStoreFlusher.1] regionserver.HStore: Added hdfs://master:54310/hbase/data/default/usertable/28938d267c1c61f7feb582b85e4c172f/family/3f8f1c17a86848db9e7be2613b5186a6, entries=325270, sequenceid=629, filesize=48.5m
2014-06-30 19:56:27,474 INFO  [MemStoreFlusher.1] regionserver.HRegion: Finished memstore flush of ~115.2m/120788960, currentsize=0.0/0 for region usertable,user8,1404183160444.28938d267c1c61f7feb582b85e4c172f. in 4671ms, sequenceid=629, compaction requested=true
2014-06-30 19:56:27,474 DEBUG [MemStoreFlusher.1] regionserver.CompactSplitThread: Small Compaction requested: system; Because: MemStoreFlusher.1; compaction_queue=(0:2), split_queue=0, merge_queue=0
2014-06-30 19:56:27,474 WARN  [RpcServer.handler=6,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 372ms
2014-06-30 19:56:27,474 INFO  [RpcServer.handler=6,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server sceplus-vm48.almaden.ibm.com,60020,1404182803191
2014-06-30 19:56:27,474 WARN  [RpcServer.handler=29,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 381ms
2014-06-30 19:56:27,474 INFO  [RpcServer.handler=29,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server sceplus-vm48.almaden.ibm.com,60020,1404182803191
2014-06-30 19:56:27,474 WARN  [RpcServer.handler=44,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 391ms
2014-06-30 19:56:27,474 INFO  [RpcServer.handler=44,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server sceplus-vm48.almaden.ibm.com,60020,1404182803191
2014-06-30 19:56:27,475 WARN  [RpcServer.handler=12,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 2518ms
2014-06-30 19:56:27,475 INFO  [RpcServer.handler=12,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server sceplus-vm48.almaden.ibm.com,60020,1404182803191
2014-06-30 19:56:27,475 WARN  [RpcServer.handler=33,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 2518ms
2014-06-30 19:56:27,475 INFO  [RpcServer.handler=33,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server sceplus-vm48.almaden.ibm.com,60020,1404182803191
2014-06-30 19:56:27,476 WARN  [RpcServer.handler=49,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 2518ms
2014-06-30 19:56:27,476 INFO  [RpcServer.handler=49,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server sceplus-vm48.almaden.ibm.com,60020,1404182803191
2014-06-30 19:56:27,476 WARN  [RpcServer.handler=9,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 2519ms
2014-06-30 19:56:27,476 INFO  [RpcServer.handler=9,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server sceplus-vm48.almaden.ibm.com,60020,1404182803191
2014-06-30 19:56:27,476 WARN  [RpcServer.handler=16,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 2520ms
2014-06-30 19:56:27,476 INFO  [RpcServer.handler=16,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server sceplus-vm48.almaden.ibm.com,60020,1404182803191
2014-06-30 19:56:27,476 WARN  [RpcServer.handler=4,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 2522ms
2014-06-30 19:56:27,476 INFO  [RpcServer.handler=4,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server sceplus-vm48.almaden.ibm.com,60020,1404182803191
2014-06-30 19:56:27,477 WARN  [RpcServer.handler=1,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 2523ms
2014-06-30 19:56:27,477 INFO  [RpcServer.handler=1,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server sceplus-vm48.almaden.ibm.com,60020,1404182803191
2014-06-30 19:56:27,478 WARN  [RpcServer.handler=8,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 2526ms
2014-06-30 19:56:27,478 INFO  [RpcServer.handler=8,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server sceplus-vm48.almaden.ibm.com,60020,1404182803191
2014-06-30 19:56:27,478 WARN  [RpcServer.handler=7,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 2526ms
2014-06-30 19:56:27,478 INFO  [RpcServer.handler=7,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server sceplus-vm48.almaden.ibm.com,60020,1404182803191
2014-06-30 19:56:27,479 WARN  [RpcServer.handler=40,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 2527ms
2014-06-30 19:56:27,479 INFO  [RpcServer.handler=40,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server sceplus-vm48.almaden.ibm.com,60020,1404182803191
2014-06-30 19:56:27,486 WARN  [RpcServer.handler=47,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 2535ms
2014-06-30 19:56:27,486 INFO  [RpcServer.handler=47,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server sceplus-vm48.almaden.ibm.com,60020,1404182803191
2014-06-30 19:56:27,487 WARN  [RpcServer.handler=39,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 2536ms
2014-06-30 19:56:27,487 INFO  [RpcServer.handler=39,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server sceplus-vm48.almaden.ibm.com,60020,1404182803191
2014-06-30 19:56:27,487 WARN  [RpcServer.handler=14,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 2537ms
2014-06-30 19:56:27,487 INFO  [RpcServer.handler=14,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server sceplus-vm48.almaden.ibm.com,60020,1404182803191
2014-06-30 19:56:27,487 WARN  [RpcServer.handler=0,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 2537ms
2014-06-30 19:56:27,488 INFO  [RpcServer.handler=0,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server sceplus-vm48.almaden.ibm.com,60020,1404182803191
2014-06-30 19:56:27,488 WARN  [RpcServer.handler=22,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 2539ms
2014-06-30 19:56:27,488 INFO  [RpcServer.handler=22,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server sceplus-vm48.almaden.ibm.com,60020,1404182803191
2014-06-30 19:56:27,498 WARN  [RpcServer.handler=28,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 2550ms
2014-06-30 19:56:27,498 INFO  [RpcServer.handler=28,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server sceplus-vm48.almaden.ibm.com,60020,1404182803191
2014-06-30 19:56:27,498 WARN  [RpcServer.handler=32,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 2550ms
2014-06-30 19:56:27,498 INFO  [RpcServer.handler=32,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server sceplus-vm48.almaden.ibm.com,60020,1404182803191
2014-06-30 19:56:27,498 WARN  [RpcServer.handler=30,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 2551ms
2014-06-30 19:56:27,498 INFO  [RpcServer.handler=30,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server sceplus-vm48.almaden.ibm.com,60020,1404182803191
2014-06-30 19:56:27,499 WARN  [RpcServer.handler=27,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 2555ms
2014-06-30 19:56:27,499 INFO  [RpcServer.handler=27,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server sceplus-vm48.almaden.ibm.com,60020,1404182803191
2014-06-30 19:56:27,506 WARN  [RpcServer.handler=35,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 2562ms
2014-06-30 19:56:27,506 INFO  [RpcServer.handler=35,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server sceplus-vm48.almaden.ibm.com,60020,1404182803191
2014-06-30 19:56:27,507 WARN  [RpcServer.handler=21,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 2564ms
2014-06-30 19:56:27,507 INFO  [RpcServer.handler=21,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server sceplus-vm48.almaden.ibm.com,60020,1404182803191
2014-06-30 19:56:27,510 WARN  [RpcServer.handler=17,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 2578ms
2014-06-30 19:56:27,510 INFO  [RpcServer.handler=17,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server sceplus-vm48.almaden.ibm.com,60020,1404182803191
2014-06-30 19:56:27,511 WARN  [RpcServer.handler=43,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 2587ms
2014-06-30 19:56:27,511 INFO  [RpcServer.handler=43,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server sceplus-vm48.almaden.ibm.com,60020,1404182803191
2014-06-30 19:56:27,511 WARN  [RpcServer.handler=19,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 2598ms
2014-06-30 19:56:27,511 INFO  [RpcServer.handler=19,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server sceplus-vm48.almaden.ibm.com,60020,1404182803191
2014-06-30 19:56:27,514 WARN  [RpcServer.handler=10,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 2612ms
2014-06-30 19:56:27,515 INFO  [RpcServer.handler=10,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server sceplus-vm48.almaden.ibm.com,60020,1404182803191
2014-06-30 19:56:27,517 WARN  [RpcServer.handler=18,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 2624ms
2014-06-30 19:56:27,517 INFO  [RpcServer.handler=18,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server sceplus-vm48.almaden.ibm.com,60020,1404182803191
2014-06-30 19:56:27,518 WARN  [RpcServer.handler=48,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 4622ms
2014-06-30 19:56:27,518 INFO  [RpcServer.handler=48,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server sceplus-vm48.almaden.ibm.com,60020,1404182803191
2014-06-30 19:56:27,518 WARN  [RpcServer.handler=34,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 4623ms
2014-06-30 19:56:27,518 INFO  [RpcServer.handler=34,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server sceplus-vm48.almaden.ibm.com,60020,1404182803191
2014-06-30 19:56:27,520 WARN  [RpcServer.handler=41,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 4625ms
2014-06-30 19:56:27,520 INFO  [RpcServer.handler=41,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server sceplus-vm48.almaden.ibm.com,60020,1404182803191
2014-06-30 19:56:27,520 WARN  [RpcServer.handler=38,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 4625ms
2014-06-30 19:56:27,520 INFO  [RpcServer.handler=38,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server sceplus-vm48.almaden.ibm.com,60020,1404182803191
2014-06-30 19:56:27,520 WARN  [RpcServer.handler=37,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 4626ms
2014-06-30 19:56:27,520 INFO  [RpcServer.handler=37,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server sceplus-vm48.almaden.ibm.com,60020,1404182803191
2014-06-30 19:56:27,520 WARN  [RpcServer.handler=15,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 4627ms
2014-06-30 19:56:27,521 INFO  [RpcServer.handler=15,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server sceplus-vm48.almaden.ibm.com,60020,1404182803191
2014-06-30 19:56:27,521 WARN  [RpcServer.handler=23,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 4631ms
2014-06-30 19:56:27,521 INFO  [RpcServer.handler=23,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server sceplus-vm48.almaden.ibm.com,60020,1404182803191
2014-06-30 19:56:27,521 WARN  [RpcServer.handler=20,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 4631ms
2014-06-30 19:56:27,521 INFO  [RpcServer.handler=20,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server sceplus-vm48.almaden.ibm.com,60020,1404182803191
2014-06-30 19:56:27,521 WARN  [RpcServer.handler=5,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 4631ms
2014-06-30 19:56:27,521 INFO  [RpcServer.handler=5,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server sceplus-vm48.almaden.ibm.com,60020,1404182803191
2014-06-30 19:56:30,184 INFO  [JvmPauseMonitor] util.JvmPauseMonitor: Detected pause in JVM or host machine (eg GC): pause of approximately 2111ms
GC pool 'ConcurrentMarkSweep' had collection(s): count=1 time=2457ms
2014-06-30 19:56:30,184 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-06-30 19:56:30,207 WARN  [RpcServer.handler=33,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":10537,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.58:50155","starttimems":1404183379670,"queuetimems":0,"class":"HRegionServer","responsesize":13985,"method":"Multi"}
2014-06-30 19:56:30,253 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 3089 synced till here 3064
2014-06-30 19:56:30,343 WARN  [RpcServer.handler=12,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":10359,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.58:50155","starttimems":1404183379984,"queuetimems":1,"class":"HRegionServer","responsesize":15676,"method":"Multi"}
2014-06-30 19:56:30,344 WARN  [RpcServer.handler=12,port=60020] ipc.RpcServer: RpcServer.respondercallId: 1935 service: ClientService methodName: Multi size: 2.7m connection: 9.1.143.58:50155: output error
2014-06-30 19:56:30,347 WARN  [RpcServer.handler=12,port=60020] ipc.RpcServer: RpcServer.handler=12,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-06-30 19:56:30,447 WARN  [RpcServer.handler=49,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":12819,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.58:50155","starttimems":1404183377627,"queuetimems":0,"class":"HRegionServer","responsesize":16176,"method":"Multi"}
2014-06-30 19:56:30,447 WARN  [RpcServer.handler=9,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":10690,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.58:50155","starttimems":1404183379756,"queuetimems":0,"class":"HRegionServer","responsesize":16380,"method":"Multi"}
2014-06-30 19:56:30,463 WARN  [RpcServer.handler=9,port=60020] ipc.RpcServer: RpcServer.respondercallId: 1929 service: ClientService methodName: Multi size: 2.9m connection: 9.1.143.58:50155: output error
2014-06-30 19:56:30,463 WARN  [RpcServer.handler=9,port=60020] ipc.RpcServer: RpcServer.handler=9,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-06-30 19:56:30,463 WARN  [RpcServer.handler=49,port=60020] ipc.RpcServer: RpcServer.respondercallId: 1920 service: ClientService methodName: Multi size: 2.8m connection: 9.1.143.58:50155: output error
2014-06-30 19:56:30,463 WARN  [RpcServer.handler=49,port=60020] ipc.RpcServer: RpcServer.handler=49,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-06-30 19:56:30,551 WARN  [RpcServer.handler=47,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":10777,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.58:50155","starttimems":1404183379774,"queuetimems":1,"class":"HRegionServer","responsesize":15915,"method":"Multi"}
2014-06-30 19:56:30,552 WARN  [RpcServer.handler=47,port=60020] ipc.RpcServer: RpcServer.respondercallId: 1933 service: ClientService methodName: Multi size: 2.8m connection: 9.1.143.58:50155: output error
2014-06-30 19:56:30,552 WARN  [RpcServer.handler=47,port=60020] ipc.RpcServer: RpcServer.handler=47,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-06-30 19:56:30,552 WARN  [RpcServer.handler=28,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":10848,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.58:50155","starttimems":1404183379704,"queuetimems":1,"class":"HRegionServer","responsesize":16591,"method":"Multi"}
2014-06-30 19:56:30,553 WARN  [RpcServer.handler=28,port=60020] ipc.RpcServer: RpcServer.respondercallId: 1928 service: ClientService methodName: Multi size: 2.9m connection: 9.1.143.58:50155: output error
2014-06-30 19:56:30,553 WARN  [RpcServer.handler=28,port=60020] ipc.RpcServer: RpcServer.handler=28,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-06-30 19:56:30,553 WARN  [RpcServer.handler=34,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":10818,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.58:50155","starttimems":1404183379735,"queuetimems":0,"class":"HRegionServer","responsesize":16500,"method":"Multi"}
2014-06-30 19:56:30,554 WARN  [RpcServer.handler=34,port=60020] ipc.RpcServer: RpcServer.respondercallId: 1930 service: ClientService methodName: Multi size: 2.9m connection: 9.1.143.58:50155: output error
2014-06-30 19:56:30,554 WARN  [RpcServer.handler=34,port=60020] ipc.RpcServer: RpcServer.handler=34,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-06-30 19:56:30,554 WARN  [RpcServer.handler=0,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":11246,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.58:50155","starttimems":1404183379307,"queuetimems":0,"class":"HRegionServer","responsesize":15988,"method":"Multi"}
2014-06-30 19:56:30,554 WARN  [RpcServer.handler=0,port=60020] ipc.RpcServer: RpcServer.respondercallId: 1924 service: ClientService methodName: Multi size: 2.8m connection: 9.1.143.58:50155: output error
2014-06-30 19:56:30,554 WARN  [RpcServer.handler=0,port=60020] ipc.RpcServer: RpcServer.handler=0,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-06-30 19:56:30,554 DEBUG [MemStoreFlusher.1] regionserver.MemStoreFlusher: Flush thread woke up because memory above low water=367.3m
2014-06-30 19:56:30,554 INFO  [MemStoreFlusher.1] regionserver.MemStoreFlusher: Flush of region usertable,user7,1404183160444.994b975d2c4cd5bbe532bddfb5e4ac46. due to global heap pressure
2014-06-30 19:56:30,555 DEBUG [MemStoreFlusher.1] regionserver.HRegion: Started memstore flush for usertable,user7,1404183160444.994b975d2c4cd5bbe532bddfb5e4ac46., current region memstore size 123.9m
2014-06-30 19:56:30,555 WARN  [RpcServer.handler=4,port=60020] ipc.RpcServer: RpcServer.respondercallId: 1944 service: ClientService methodName: Multi size: 2.9m connection: 9.1.143.58:50155: output error
2014-06-30 19:56:30,555 WARN  [RpcServer.handler=4,port=60020] ipc.RpcServer: RpcServer.handler=4,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-06-30 19:56:30,555 WARN  [RpcServer.handler=7,port=60020] ipc.RpcServer: RpcServer.respondercallId: 1940 service: ClientService methodName: Multi size: 2.8m connection: 9.1.143.58:50155: output error
2014-06-30 19:56:30,555 WARN  [RpcServer.handler=7,port=60020] ipc.RpcServer: RpcServer.handler=7,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-06-30 19:56:30,556 WARN  [RpcServer.handler=1,port=60020] ipc.RpcServer: RpcServer.respondercallId: 1946 service: ClientService methodName: Multi size: 2.9m connection: 9.1.143.58:50155: output error
2014-06-30 19:56:30,556 WARN  [RpcServer.handler=1,port=60020] ipc.RpcServer: RpcServer.handler=1,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-06-30 19:56:30,567 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1404182803191/sceplus-vm48.almaden.ibm.com%2C60020%2C1404182803191.1404183380242 with entries=103, filesize=88.5m; new WAL /hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1404182803191/sceplus-vm48.almaden.ibm.com%2C60020%2C1404182803191.1404183390185
2014-06-30 19:56:32,576 INFO  [JvmPauseMonitor] util.JvmPauseMonitor: Detected pause in JVM or host machine (eg GC): pause of approximately 1891ms
GC pool 'ParNew' had collection(s): count=1 time=0ms
GC pool 'ConcurrentMarkSweep' had collection(s): count=1 time=2096ms
2014-06-30 19:56:32,588 WARN  [RpcServer.handler=14,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":12614,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.58:50155","starttimems":1404183379974,"queuetimems":0,"class":"HRegionServer","responsesize":17167,"method":"Multi"}
2014-06-30 19:56:32,588 WARN  [RpcServer.handler=39,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":12792,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.58:50155","starttimems":1404183379796,"queuetimems":0,"class":"HRegionServer","responsesize":17169,"method":"Multi"}
2014-06-30 19:56:32,589 WARN  [RpcServer.handler=14,port=60020] ipc.RpcServer: RpcServer.respondercallId: 1936 service: ClientService methodName: Multi size: 3.0m connection: 9.1.143.58:50155: output error
2014-06-30 19:56:32,589 WARN  [RpcServer.handler=14,port=60020] ipc.RpcServer: RpcServer.handler=14,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-06-30 19:56:32,590 WARN  [RpcServer.handler=39,port=60020] ipc.RpcServer: RpcServer.respondercallId: 1932 service: ClientService methodName: Multi size: 3.0m connection: 9.1.143.58:50155: output error
2014-06-30 19:56:32,590 WARN  [RpcServer.handler=39,port=60020] ipc.RpcServer: RpcServer.handler=39,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-06-30 19:56:32,730 INFO  [RpcServer.handler=10,port=60020] regionserver.MemStoreFlusher: Blocking updates on sceplus-vm48.almaden.ibm.com,60020,1404182803191: the global memstore size 389.6m is >= than blocking 386.7m size
2014-06-30 19:56:32,731 INFO  [RpcServer.handler=19,port=60020] regionserver.MemStoreFlusher: Blocking updates on sceplus-vm48.almaden.ibm.com,60020,1404182803191: the global memstore size 392.8m is >= than blocking 386.7m size
2014-06-30 19:56:32,735 INFO  [RpcServer.handler=29,port=60020] regionserver.MemStoreFlusher: Blocking updates on sceplus-vm48.almaden.ibm.com,60020,1404182803191: the global memstore size 392.8m is >= than blocking 386.7m size
2014-06-30 19:56:32,738 INFO  [RpcServer.handler=26,port=60020] regionserver.MemStoreFlusher: Blocking updates on sceplus-vm48.almaden.ibm.com,60020,1404182803191: the global memstore size 394.4m is >= than blocking 386.7m size
2014-06-30 19:56:32,739 INFO  [RpcServer.handler=18,port=60020] regionserver.MemStoreFlusher: Blocking updates on sceplus-vm48.almaden.ibm.com,60020,1404182803191: the global memstore size 394.4m is >= than blocking 386.7m size
2014-06-30 19:56:32,742 INFO  [RpcServer.handler=17,port=60020] regionserver.MemStoreFlusher: Blocking updates on sceplus-vm48.almaden.ibm.com,60020,1404182803191: the global memstore size 396.0m is >= than blocking 386.7m size
2014-06-30 19:56:32,742 INFO  [RpcServer.handler=43,port=60020] regionserver.MemStoreFlusher: Blocking updates on sceplus-vm48.almaden.ibm.com,60020,1404182803191: the global memstore size 396.0m is >= than blocking 386.7m size
2014-06-30 19:56:32,742 INFO  [RpcServer.handler=21,port=60020] regionserver.MemStoreFlusher: Blocking updates on sceplus-vm48.almaden.ibm.com,60020,1404182803191: the global memstore size 396.0m is >= than blocking 386.7m size
2014-06-30 19:56:32,756 INFO  [RpcServer.handler=45,port=60020] regionserver.MemStoreFlusher: Blocking updates on sceplus-vm48.almaden.ibm.com,60020,1404182803191: the global memstore size 396.0m is >= than blocking 386.7m size
2014-06-30 19:56:32,774 INFO  [RpcServer.handler=35,port=60020] regionserver.MemStoreFlusher: Blocking updates on sceplus-vm48.almaden.ibm.com,60020,1404182803191: the global memstore size 396.0m is >= than blocking 386.7m size
2014-06-30 19:56:34,690 INFO  [JvmPauseMonitor] util.JvmPauseMonitor: Detected pause in JVM or host machine (eg GC): pause of approximately 1614ms
GC pool 'ParNew' had collection(s): count=1 time=0ms
GC pool 'ConcurrentMarkSweep' had collection(s): count=1 time=1917ms
2014-06-30 19:56:34,775 DEBUG [RpcServer.handler=41,port=60020] regionserver.HRegion: Flush requested on usertable,user7,1404183160444.994b975d2c4cd5bbe532bddfb5e4ac46.
2014-06-30 19:56:34,775 WARN  [RpcServer.handler=38,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":17202,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.58:50155","starttimems":1404183377573,"queuetimems":1,"class":"HRegionServer","responsesize":16294,"method":"Multi"}
2014-06-30 19:56:34,776 DEBUG [MemStoreFlusher.0] regionserver.HRegion: NOT flushing memstore for region usertable,user7,1404183160444.994b975d2c4cd5bbe532bddfb5e4ac46., flushing=true, writesEnabled=true
2014-06-30 19:56:34,776 WARN  [RpcServer.handler=38,port=60020] ipc.RpcServer: RpcServer.respondercallId: 1923 service: ClientService methodName: Multi size: 2.8m connection: 9.1.143.58:50155: output error
2014-06-30 19:56:34,776 WARN  [RpcServer.handler=38,port=60020] ipc.RpcServer: RpcServer.handler=38,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-06-30 19:56:34,776 WARN  [RpcServer.handler=35,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 2002ms
2014-06-30 19:56:34,776 DEBUG [MemStoreFlusher.0] regionserver.MemStoreFlusher: Flush thread woke up because memory above low water=367.3m
2014-06-30 19:56:34,776 INFO  [MemStoreFlusher.0] regionserver.MemStoreFlusher: Flush of region usertable,user5,1404183160444.1d66ac045cc0407c0fa426db565fcbba. due to global heap pressure
2014-06-30 19:56:34,776 DEBUG [MemStoreFlusher.0] regionserver.HRegion: Started memstore flush for usertable,user5,1404183160444.1d66ac045cc0407c0fa426db565fcbba., current region memstore size 100.0m
2014-06-30 19:56:34,777 WARN  [RpcServer.handler=15,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":17131,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.58:50155","starttimems":1404183377645,"queuetimems":0,"class":"HRegionServer","responsesize":16341,"method":"Multi"}
2014-06-30 19:56:34,777 WARN  [RpcServer.handler=20,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":17164,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.58:50155","starttimems":1404183377612,"queuetimems":0,"class":"HRegionServer","responsesize":16670,"method":"Multi"}
2014-06-30 19:56:34,777 WARN  [RpcServer.handler=15,port=60020] ipc.RpcServer: RpcServer.respondercallId: 1919 service: ClientService methodName: Multi size: 2.9m connection: 9.1.143.58:50155: output error
2014-06-30 19:56:34,777 WARN  [RpcServer.handler=8,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":14076,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.58:50155","starttimems":1404183380700,"queuetimems":0,"class":"HRegionServer","responsesize":15988,"method":"Multi"}
2014-06-30 19:56:34,777 WARN  [RpcServer.handler=20,port=60020] ipc.RpcServer: RpcServer.respondercallId: 1921 service: ClientService methodName: Multi size: 2.9m connection: 9.1.143.58:50155: output error
2014-06-30 19:56:34,777 WARN  [RpcServer.handler=20,port=60020] ipc.RpcServer: RpcServer.handler=20,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-06-30 19:56:34,777 WARN  [RpcServer.handler=8,port=60020] ipc.RpcServer: RpcServer.respondercallId: 1945 service: ClientService methodName: Multi size: 2.8m connection: 9.1.143.58:50155: output error
2014-06-30 19:56:34,777 WARN  [RpcServer.handler=8,port=60020] ipc.RpcServer: RpcServer.handler=8,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-06-30 19:56:34,777 WARN  [RpcServer.handler=15,port=60020] ipc.RpcServer: RpcServer.handler=15,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-06-30 19:56:34,778 INFO  [RpcServer.handler=40,port=60020] regionserver.MemStoreFlusher: Blocking updates on sceplus-vm48.almaden.ibm.com,60020,1404182803191: the global memstore size 415.1m is >= than blocking 386.7m size
2014-06-30 19:56:34,778 INFO  [RpcServer.handler=36,port=60020] regionserver.MemStoreFlusher: Blocking updates on sceplus-vm48.almaden.ibm.com,60020,1404182803191: the global memstore size 419.9m is >= than blocking 386.7m size
2014-06-30 19:56:34,779 INFO  [RpcServer.handler=32,port=60020] regionserver.MemStoreFlusher: Blocking updates on sceplus-vm48.almaden.ibm.com,60020,1404182803191: the global memstore size 421.6m is >= than blocking 386.7m size
2014-06-30 19:56:34,779 WARN  [RpcServer.handler=45,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 2023ms
2014-06-30 19:56:34,779 WARN  [RpcServer.handler=21,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 2044ms
2014-06-30 19:56:34,779 WARN  [RpcServer.handler=43,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 2049ms
2014-06-30 19:56:34,780 WARN  [RpcServer.handler=17,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 2038ms
2014-06-30 19:56:34,780 WARN  [RpcServer.handler=18,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 2041ms
2014-06-30 19:56:34,780 WARN  [RpcServer.handler=26,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 2042ms
2014-06-30 19:56:34,780 WARN  [RpcServer.handler=29,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 2045ms
2014-06-30 19:56:34,780 WARN  [RpcServer.handler=19,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 2049ms
2014-06-30 19:56:34,780 WARN  [RpcServer.handler=10,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 2050ms
2014-06-30 19:56:34,781 INFO  [RpcServer.handler=42,port=60020] regionserver.MemStoreFlusher: Blocking updates on sceplus-vm48.almaden.ibm.com,60020,1404182803191: the global memstore size 421.6m is >= than blocking 386.7m size
2014-06-30 19:56:34,781 INFO  [RpcServer.handler=25,port=60020] regionserver.MemStoreFlusher: Blocking updates on sceplus-vm48.almaden.ibm.com,60020,1404182803191: the global memstore size 421.6m is >= than blocking 386.7m size
2014-06-30 19:56:34,781 INFO  [RpcServer.handler=41,port=60020] regionserver.MemStoreFlusher: Blocking updates on sceplus-vm48.almaden.ibm.com,60020,1404182803191: the global memstore size 421.6m is >= than blocking 386.7m size
2014-06-30 19:56:34,781 INFO  [RpcServer.handler=3,port=60020] regionserver.MemStoreFlusher: Blocking updates on sceplus-vm48.almaden.ibm.com,60020,1404182803191: the global memstore size 421.6m is >= than blocking 386.7m size
2014-06-30 19:56:34,782 INFO  [RpcServer.handler=48,port=60020] regionserver.MemStoreFlusher: Blocking updates on sceplus-vm48.almaden.ibm.com,60020,1404182803191: the global memstore size 421.6m is >= than blocking 386.7m size
2014-06-30 19:56:34,782 INFO  [RpcServer.handler=24,port=60020] regionserver.MemStoreFlusher: Blocking updates on sceplus-vm48.almaden.ibm.com,60020,1404182803191: the global memstore size 421.6m is >= than blocking 386.7m size
2014-06-30 19:56:34,782 INFO  [RpcServer.handler=23,port=60020] regionserver.MemStoreFlusher: Blocking updates on sceplus-vm48.almaden.ibm.com,60020,1404182803191: the global memstore size 423.3m is >= than blocking 386.7m size
2014-06-30 19:56:34,783 WARN  [RpcServer.handler=5,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":17193,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.58:50155","starttimems":1404183377590,"queuetimems":1,"class":"HRegionServer","responsesize":16533,"method":"Multi"}
2014-06-30 19:56:34,786 INFO  [RpcServer.handler=22,port=60020] regionserver.MemStoreFlusher: Blocking updates on sceplus-vm48.almaden.ibm.com,60020,1404182803191: the global memstore size 423.3m is >= than blocking 386.7m size
2014-06-30 19:56:34,786 WARN  [RpcServer.handler=5,port=60020] ipc.RpcServer: RpcServer.respondercallId: 1922 service: ClientService methodName: Multi size: 2.9m connection: 9.1.143.58:50155: output error
2014-06-30 19:56:34,786 INFO  [RpcServer.handler=37,port=60020] regionserver.MemStoreFlusher: Blocking updates on sceplus-vm48.almaden.ibm.com,60020,1404182803191: the global memstore size 424.9m is >= than blocking 386.7m size
2014-06-30 19:56:34,786 WARN  [RpcServer.handler=5,port=60020] ipc.RpcServer: RpcServer.handler=5,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-06-30 19:56:34,786 INFO  [RpcServer.handler=30,port=60020] regionserver.MemStoreFlusher: Blocking updates on sceplus-vm48.almaden.ibm.com,60020,1404182803191: the global memstore size 424.9m is >= than blocking 386.7m size
2014-06-30 19:56:34,828 INFO  [RpcServer.handler=11,port=60020] regionserver.MemStoreFlusher: Blocking updates on sceplus-vm48.almaden.ibm.com,60020,1404182803191: the global memstore size 437.8m is >= than blocking 386.7m size
2014-06-30 19:56:34,829 INFO  [RpcServer.handler=6,port=60020] regionserver.MemStoreFlusher: Blocking updates on sceplus-vm48.almaden.ibm.com,60020,1404182803191: the global memstore size 437.8m is >= than blocking 386.7m size
2014-06-30 19:56:34,829 INFO  [RpcServer.handler=2,port=60020] regionserver.MemStoreFlusher: Blocking updates on sceplus-vm48.almaden.ibm.com,60020,1404182803191: the global memstore size 437.8m is >= than blocking 386.7m size
2014-06-30 19:56:34,830 INFO  [RpcServer.handler=13,port=60020] regionserver.MemStoreFlusher: Blocking updates on sceplus-vm48.almaden.ibm.com,60020,1404182803191: the global memstore size 437.8m is >= than blocking 386.7m size
2014-06-30 19:56:34,830 INFO  [RpcServer.handler=31,port=60020] regionserver.MemStoreFlusher: Blocking updates on sceplus-vm48.almaden.ibm.com,60020,1404182803191: the global memstore size 437.8m is >= than blocking 386.7m size
2014-06-30 19:56:34,831 INFO  [RpcServer.handler=44,port=60020] regionserver.MemStoreFlusher: Blocking updates on sceplus-vm48.almaden.ibm.com,60020,1404182803191: the global memstore size 437.8m is >= than blocking 386.7m size
2014-06-30 19:56:34,831 INFO  [RpcServer.handler=16,port=60020] regionserver.MemStoreFlusher: Blocking updates on sceplus-vm48.almaden.ibm.com,60020,1404182803191: the global memstore size 437.8m is >= than blocking 386.7m size
2014-06-30 19:56:34,832 INFO  [RpcServer.handler=46,port=60020] regionserver.MemStoreFlusher: Blocking updates on sceplus-vm48.almaden.ibm.com,60020,1404182803191: the global memstore size 437.8m is >= than blocking 386.7m size
2014-06-30 19:56:34,873 DEBUG [MemStoreFlusher.0] util.FSUtils: DFS Client does not support most favored nodes create; using default create
2014-06-30 19:56:34,895 DEBUG [MemStoreFlusher.1] util.FSUtils: DFS Client does not support most favored nodes create; using default create
2014-06-30 19:56:37,594 INFO  [JvmPauseMonitor] util.JvmPauseMonitor: Detected pause in JVM or host machine (eg GC): pause of approximately 1902ms
GC pool 'ConcurrentMarkSweep' had collection(s): count=1 time=2253ms
2014-06-30 19:56:37,599 INFO  [RpcServer.handler=27,port=60020] regionserver.MemStoreFlusher: Blocking updates on sceplus-vm48.almaden.ibm.com,60020,1404182803191: the global memstore size 437.8m is >= than blocking 386.7m size
2014-06-30 19:56:37,610 INFO  [RpcServer.handler=33,port=60020] regionserver.MemStoreFlusher: Blocking updates on sceplus-vm48.almaden.ibm.com,60020,1404182803191: the global memstore size 437.8m is >= than blocking 386.7m size
2014-06-30 19:56:37,618 INFO  [RpcServer.handler=12,port=60020] regionserver.MemStoreFlusher: Blocking updates on sceplus-vm48.almaden.ibm.com,60020,1404182803191: the global memstore size 437.8m is >= than blocking 386.7m size
2014-06-30 19:56:37,627 INFO  [RpcServer.handler=9,port=60020] regionserver.MemStoreFlusher: Blocking updates on sceplus-vm48.almaden.ibm.com,60020,1404182803191: the global memstore size 437.8m is >= than blocking 386.7m size
2014-06-30 19:56:37,684 INFO  [MemStoreFlusher.0] regionserver.DefaultStoreFlusher: Flushed, sequenceid=665, memsize=42.2m, hasBloomFilter=true, into tmp file hdfs://master:54310/hbase/data/default/usertable/1d66ac045cc0407c0fa426db565fcbba/.tmp/2aa952bdfe9a4c1da775b8b46dd807f3
2014-06-30 19:56:37,693 DEBUG [MemStoreFlusher.0] regionserver.HRegionFileSystem: Committing store file hdfs://master:54310/hbase/data/default/usertable/1d66ac045cc0407c0fa426db565fcbba/.tmp/2aa952bdfe9a4c1da775b8b46dd807f3 as hdfs://master:54310/hbase/data/default/usertable/1d66ac045cc0407c0fa426db565fcbba/family/2aa952bdfe9a4c1da775b8b46dd807f3
2014-06-30 19:56:37,700 INFO  [MemStoreFlusher.0] regionserver.HStore: Added hdfs://master:54310/hbase/data/default/usertable/1d66ac045cc0407c0fa426db565fcbba/family/2aa952bdfe9a4c1da775b8b46dd807f3, entries=153730, sequenceid=665, filesize=22.9m
2014-06-30 19:56:37,700 INFO  [MemStoreFlusher.0] regionserver.HRegion: Finished memstore flush of ~100.0m/104861920, currentsize=0.0/0 for region usertable,user5,1404183160444.1d66ac045cc0407c0fa426db565fcbba. in 2924ms, sequenceid=665, compaction requested=false
2014-06-30 19:56:37,700 WARN  [RpcServer.handler=9,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 73ms
2014-06-30 19:56:37,700 INFO  [RpcServer.handler=9,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server sceplus-vm48.almaden.ibm.com,60020,1404182803191
2014-06-30 19:56:37,700 WARN  [RpcServer.handler=12,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 82ms
2014-06-30 19:56:37,700 INFO  [RpcServer.handler=12,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server sceplus-vm48.almaden.ibm.com,60020,1404182803191
2014-06-30 19:56:37,701 WARN  [RpcServer.handler=33,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 91ms
2014-06-30 19:56:37,701 INFO  [RpcServer.handler=33,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server sceplus-vm48.almaden.ibm.com,60020,1404182803191
2014-06-30 19:56:37,701 WARN  [RpcServer.handler=27,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 102ms
2014-06-30 19:56:37,701 INFO  [RpcServer.handler=27,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server sceplus-vm48.almaden.ibm.com,60020,1404182803191
2014-06-30 19:56:37,701 WARN  [RpcServer.handler=46,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 2869ms
2014-06-30 19:56:37,701 INFO  [RpcServer.handler=46,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server sceplus-vm48.almaden.ibm.com,60020,1404182803191
2014-06-30 19:56:37,702 WARN  [RpcServer.handler=16,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 2870ms
2014-06-30 19:56:37,702 INFO  [RpcServer.handler=16,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server sceplus-vm48.almaden.ibm.com,60020,1404182803191
2014-06-30 19:56:37,702 WARN  [RpcServer.handler=44,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 2872ms
2014-06-30 19:56:37,702 INFO  [RpcServer.handler=44,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server sceplus-vm48.almaden.ibm.com,60020,1404182803191
2014-06-30 19:56:37,702 WARN  [RpcServer.handler=31,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 2872ms
2014-06-30 19:56:37,702 INFO  [RpcServer.handler=31,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server sceplus-vm48.almaden.ibm.com,60020,1404182803191
2014-06-30 19:56:37,702 WARN  [RpcServer.handler=13,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 2872ms
2014-06-30 19:56:37,702 INFO  [RpcServer.handler=13,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server sceplus-vm48.almaden.ibm.com,60020,1404182803191
2014-06-30 19:56:37,705 WARN  [RpcServer.handler=2,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 2876ms
2014-06-30 19:56:37,705 INFO  [RpcServer.handler=2,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server sceplus-vm48.almaden.ibm.com,60020,1404182803191
2014-06-30 19:56:37,718 WARN  [RpcServer.handler=6,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 2889ms
2014-06-30 19:56:37,718 INFO  [RpcServer.handler=6,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server sceplus-vm48.almaden.ibm.com,60020,1404182803191
2014-06-30 19:56:37,718 WARN  [RpcServer.handler=11,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 2890ms
2014-06-30 19:56:37,718 INFO  [RpcServer.handler=11,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server sceplus-vm48.almaden.ibm.com,60020,1404182803191
2014-06-30 19:56:37,718 WARN  [RpcServer.handler=30,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 2932ms
2014-06-30 19:56:37,719 INFO  [RpcServer.handler=30,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server sceplus-vm48.almaden.ibm.com,60020,1404182803191
2014-06-30 19:56:37,719 WARN  [RpcServer.handler=37,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 2936ms
2014-06-30 19:56:37,719 INFO  [RpcServer.handler=37,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server sceplus-vm48.almaden.ibm.com,60020,1404182803191
2014-06-30 19:56:37,732 WARN  [RpcServer.handler=22,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 2949ms
2014-06-30 19:56:37,733 INFO  [RpcServer.handler=22,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server sceplus-vm48.almaden.ibm.com,60020,1404182803191
2014-06-30 19:56:37,733 WARN  [RpcServer.handler=23,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 2951ms
2014-06-30 19:56:37,733 INFO  [RpcServer.handler=23,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server sceplus-vm48.almaden.ibm.com,60020,1404182803191
2014-06-30 19:56:37,742 WARN  [RpcServer.handler=24,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 2960ms
2014-06-30 19:56:37,742 INFO  [RpcServer.handler=24,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server sceplus-vm48.almaden.ibm.com,60020,1404182803191
2014-06-30 19:56:37,743 WARN  [RpcServer.handler=48,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 2961ms
2014-06-30 19:56:37,743 INFO  [RpcServer.handler=48,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server sceplus-vm48.almaden.ibm.com,60020,1404182803191
2014-06-30 19:56:37,743 WARN  [RpcServer.handler=3,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 2967ms
2014-06-30 19:56:37,743 INFO  [RpcServer.handler=3,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server sceplus-vm48.almaden.ibm.com,60020,1404182803191
2014-06-30 19:56:37,750 WARN  [RpcServer.handler=41,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 2972ms
2014-06-30 19:56:37,750 INFO  [RpcServer.handler=41,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server sceplus-vm48.almaden.ibm.com,60020,1404182803191
2014-06-30 19:56:37,758 WARN  [RpcServer.handler=25,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 2978ms
2014-06-30 19:56:37,758 INFO  [RpcServer.handler=25,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server sceplus-vm48.almaden.ibm.com,60020,1404182803191
2014-06-30 19:56:37,766 WARN  [RpcServer.handler=42,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 2986ms
2014-06-30 19:56:37,766 INFO  [RpcServer.handler=42,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server sceplus-vm48.almaden.ibm.com,60020,1404182803191
2014-06-30 19:56:37,767 WARN  [RpcServer.handler=10,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5037ms
2014-06-30 19:56:37,767 INFO  [RpcServer.handler=10,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server sceplus-vm48.almaden.ibm.com,60020,1404182803191
2014-06-30 19:56:37,768 WARN  [RpcServer.handler=19,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5037ms
2014-06-30 19:56:37,768 INFO  [RpcServer.handler=19,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server sceplus-vm48.almaden.ibm.com,60020,1404182803191
2014-06-30 19:56:37,770 WARN  [RpcServer.handler=29,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5035ms
2014-06-30 19:56:37,770 INFO  [RpcServer.handler=29,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server sceplus-vm48.almaden.ibm.com,60020,1404182803191
2014-06-30 19:56:37,770 WARN  [RpcServer.handler=26,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5032ms
2014-06-30 19:56:37,771 INFO  [RpcServer.handler=26,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server sceplus-vm48.almaden.ibm.com,60020,1404182803191
2014-06-30 19:56:37,774 WARN  [RpcServer.handler=18,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5035ms
2014-06-30 19:56:37,774 INFO  [RpcServer.handler=18,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server sceplus-vm48.almaden.ibm.com,60020,1404182803191
2014-06-30 19:56:37,782 WARN  [RpcServer.handler=17,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5040ms
2014-06-30 19:56:37,782 INFO  [RpcServer.handler=17,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server sceplus-vm48.almaden.ibm.com,60020,1404182803191
2014-06-30 19:56:37,783 WARN  [RpcServer.handler=43,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5053ms
2014-06-30 19:56:37,783 INFO  [RpcServer.handler=43,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server sceplus-vm48.almaden.ibm.com,60020,1404182803191
2014-06-30 19:56:37,789 WARN  [RpcServer.handler=21,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5054ms
2014-06-30 19:56:37,789 INFO  [RpcServer.handler=21,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server sceplus-vm48.almaden.ibm.com,60020,1404182803191
2014-06-30 19:56:37,789 WARN  [RpcServer.handler=45,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5033ms
2014-06-30 19:56:37,792 INFO  [RpcServer.handler=45,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server sceplus-vm48.almaden.ibm.com,60020,1404182803191
2014-06-30 19:56:37,798 WARN  [RpcServer.handler=32,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 3016ms
2014-06-30 19:56:37,798 INFO  [RpcServer.handler=32,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server sceplus-vm48.almaden.ibm.com,60020,1404182803191
2014-06-30 19:56:37,798 WARN  [RpcServer.handler=36,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 3020ms
2014-06-30 19:56:37,798 INFO  [RpcServer.handler=36,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server sceplus-vm48.almaden.ibm.com,60020,1404182803191
2014-06-30 19:56:37,798 WARN  [RpcServer.handler=40,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 3020ms
2014-06-30 19:56:37,798 INFO  [RpcServer.handler=40,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server sceplus-vm48.almaden.ibm.com,60020,1404182803191
2014-06-30 19:56:37,798 WARN  [RpcServer.handler=35,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5024ms
2014-06-30 19:56:37,798 INFO  [RpcServer.handler=35,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server sceplus-vm48.almaden.ibm.com,60020,1404182803191
2014-06-30 19:56:37,830 WARN  [RpcServer.handler=16,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":12876,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.58:50155","starttimems":1404183384953,"queuetimems":1,"class":"HRegionServer","responsesize":15988,"method":"Multi"}
2014-06-30 19:56:37,830 WARN  [RpcServer.handler=16,port=60020] ipc.RpcServer: RpcServer.respondercallId: 1956 service: ClientService methodName: Multi size: 2.8m connection: 9.1.143.58:50155: output error
2014-06-30 19:56:37,830 WARN  [RpcServer.handler=16,port=60020] ipc.RpcServer: RpcServer.handler=16,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-06-30 19:56:39,655 INFO  [JvmPauseMonitor] util.JvmPauseMonitor: Detected pause in JVM or host machine (eg GC): pause of approximately 1560ms
GC pool 'ParNew' had collection(s): count=1 time=0ms
GC pool 'ConcurrentMarkSweep' had collection(s): count=1 time=1805ms
2014-06-30 19:56:39,708 WARN  [RpcServer.handler=44,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":12627,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.58:50155","starttimems":1404183387081,"queuetimems":0,"class":"HRegionServer","responsesize":16591,"method":"Multi"}
2014-06-30 19:56:39,708 WARN  [RpcServer.handler=6,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":12608,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.58:50155","starttimems":1404183387100,"queuetimems":0,"class":"HRegionServer","responsesize":16380,"method":"Multi"}
2014-06-30 19:56:39,709 WARN  [RpcServer.handler=44,port=60020] ipc.RpcServer: RpcServer.respondercallId: 1964 service: ClientService methodName: Multi size: 2.9m connection: 9.1.143.58:50155: output error
2014-06-30 19:56:39,709 WARN  [RpcServer.handler=44,port=60020] ipc.RpcServer: RpcServer.handler=44,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-06-30 19:56:39,709 WARN  [RpcServer.handler=6,port=60020] ipc.RpcServer: RpcServer.respondercallId: 1965 service: ClientService methodName: Multi size: 2.9m connection: 9.1.143.58:50155: output error
2014-06-30 19:56:39,709 WARN  [RpcServer.handler=6,port=60020] ipc.RpcServer: RpcServer.handler=6,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-06-30 19:56:39,710 WARN  [RpcServer.handler=23,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":18969,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.58:50155","starttimems":1404183380740,"queuetimems":0,"class":"HRegionServer","responsesize":16176,"method":"Multi"}
2014-06-30 19:56:39,710 WARN  [RpcServer.handler=48,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":17087,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.58:50155","starttimems":1404183382622,"queuetimems":0,"class":"HRegionServer","responsesize":16294,"method":"Multi"}
2014-06-30 19:56:39,710 WARN  [RpcServer.handler=41,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":22157,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.58:50155","starttimems":1404183377552,"queuetimems":0,"class":"HRegionServer","responsesize":16181,"method":"Multi"}
2014-06-30 19:56:39,710 WARN  [RpcServer.handler=23,port=60020] ipc.RpcServer: RpcServer.respondercallId: 1943 service: ClientService methodName: Multi size: 2.8m connection: 9.1.143.58:50155: output error
2014-06-30 19:56:39,710 WARN  [RpcServer.handler=23,port=60020] ipc.RpcServer: RpcServer.handler=23,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-06-30 19:56:39,710 WARN  [RpcServer.handler=41,port=60020] ipc.RpcServer: RpcServer.respondercallId: 1918 service: ClientService methodName: Multi size: 2.8m connection: 9.1.143.58:50155: output error
2014-06-30 19:56:39,710 WARN  [RpcServer.handler=41,port=60020] ipc.RpcServer: RpcServer.handler=41,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-06-30 19:56:39,710 WARN  [RpcServer.handler=48,port=60020] ipc.RpcServer: RpcServer.respondercallId: 1941 service: ClientService methodName: Multi size: 2.8m connection: 9.1.143.58:50155: output error
2014-06-30 19:56:39,710 WARN  [RpcServer.handler=48,port=60020] ipc.RpcServer: RpcServer.handler=48,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-06-30 19:56:39,710 WARN  [RpcServer.handler=37,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":18949,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.58:50155","starttimems":1404183380761,"queuetimems":0,"class":"HRegionServer","responsesize":16533,"method":"Multi"}
2014-06-30 19:56:39,711 WARN  [RpcServer.handler=37,port=60020] ipc.RpcServer: RpcServer.respondercallId: 1942 service: ClientService methodName: Multi size: 2.9m connection: 9.1.143.58:50155: output error
2014-06-30 19:56:39,711 WARN  [RpcServer.handler=37,port=60020] ipc.RpcServer: RpcServer.handler=37,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-06-30 19:56:39,714 DEBUG [MemStoreFlusher.0] regionserver.MemStoreFlusher: Flush thread woke up because memory above low water=367.3m
2014-06-30 19:56:39,714 INFO  [MemStoreFlusher.0] regionserver.MemStoreFlusher: Flush of region usertable,user9,1404183160445.9638edaa7b212ccfbe14e9984bd5a5a3. due to global heap pressure
2014-06-30 19:56:39,715 DEBUG [MemStoreFlusher.0] regionserver.HRegion: Started memstore flush for usertable,user9,1404183160445.9638edaa7b212ccfbe14e9984bd5a5a3., current region memstore size 87.6m
2014-06-30 19:56:39,848 DEBUG [regionserver60020-smallCompactions-1404182848217] regionserver.HRegionFileSystem: Committing store file hdfs://master:54310/hbase/data/default/usertable/1d66ac045cc0407c0fa426db565fcbba/.tmp/d11e8b5302b34cc1b594362696cae425 as hdfs://master:54310/hbase/data/default/usertable/1d66ac045cc0407c0fa426db565fcbba/family/d11e8b5302b34cc1b594362696cae425
2014-06-30 19:56:39,892 WARN  [RpcServer.handler=30,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":17111,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.58:50155","starttimems":1404183382781,"queuetimems":0,"class":"HRegionServer","responsesize":16380,"method":"Multi"}
2014-06-30 19:56:39,893 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-06-30 19:56:39,893 WARN  [RpcServer.handler=30,port=60020] ipc.RpcServer: RpcServer.respondercallId: 1949 service: ClientService methodName: Multi size: 2.9m connection: 9.1.143.58:50155: output error
2014-06-30 19:56:39,893 WARN  [RpcServer.handler=30,port=60020] ipc.RpcServer: RpcServer.handler=30,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-06-30 19:56:39,894 WARN  [RpcServer.handler=40,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":17042,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.58:50155","starttimems":1404183382851,"queuetimems":1,"class":"HRegionServer","responsesize":16900,"method":"Multi"}
2014-06-30 19:56:39,894 WARN  [RpcServer.handler=40,port=60020] ipc.RpcServer: RpcServer.respondercallId: 1952 service: ClientService methodName: Multi size: 3.0m connection: 9.1.143.58:50155: output error
2014-06-30 19:56:39,894 WARN  [RpcServer.handler=32,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":17093,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.58:50155","starttimems":1404183382801,"queuetimems":0,"class":"HRegionServer","responsesize":16500,"method":"Multi"}
2014-06-30 19:56:39,894 WARN  [RpcServer.handler=40,port=60020] ipc.RpcServer: RpcServer.handler=40,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-06-30 19:56:39,894 WARN  [RpcServer.handler=32,port=60020] ipc.RpcServer: RpcServer.respondercallId: 1951 service: ClientService methodName: Multi size: 2.9m connection: 9.1.143.58:50155: output error
2014-06-30 19:56:39,894 WARN  [RpcServer.handler=32,port=60020] ipc.RpcServer: RpcServer.handler=32,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-06-30 19:56:39,901 INFO  [RpcServer.handler=17,port=60020] regionserver.MemStoreFlusher: Blocking updates on sceplus-vm48.almaden.ibm.com,60020,1404182803191: the global memstore size 391.7m is >= than blocking 386.7m size
2014-06-30 19:56:39,902 INFO  [RpcServer.handler=45,port=60020] regionserver.MemStoreFlusher: Blocking updates on sceplus-vm48.almaden.ibm.com,60020,1404182803191: the global memstore size 393.3m is >= than blocking 386.7m size
2014-06-30 19:56:39,904 INFO  [RpcServer.handler=27,port=60020] regionserver.MemStoreFlusher: Blocking updates on sceplus-vm48.almaden.ibm.com,60020,1404182803191: the global memstore size 393.3m is >= than blocking 386.7m size
2014-06-30 19:56:39,904 INFO  [RpcServer.handler=31,port=60020] regionserver.MemStoreFlusher: Blocking updates on sceplus-vm48.almaden.ibm.com,60020,1404182803191: the global memstore size 396.3m is >= than blocking 386.7m size
2014-06-30 19:56:39,905 WARN  [RpcServer.handler=22,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":17083,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.58:50155","starttimems":1404183382821,"queuetimems":1,"class":"HRegionServer","responsesize":16591,"method":"Multi"}
2014-06-30 19:56:39,905 WARN  [RpcServer.handler=22,port=60020] ipc.RpcServer: RpcServer.respondercallId: 1950 service: ClientService methodName: Multi size: 2.9m connection: 9.1.143.58:50155: output error
2014-06-30 19:56:39,905 WARN  [RpcServer.handler=22,port=60020] ipc.RpcServer: RpcServer.handler=22,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-06-30 19:56:39,905 INFO  [RpcServer.handler=43,port=60020] regionserver.MemStoreFlusher: Blocking updates on sceplus-vm48.almaden.ibm.com,60020,1404182803191: the global memstore size 396.3m is >= than blocking 386.7m size
2014-06-30 19:56:39,907 INFO  [RpcServer.handler=46,port=60020] regionserver.MemStoreFlusher: Blocking updates on sceplus-vm48.almaden.ibm.com,60020,1404182803191: the global memstore size 401.1m is >= than blocking 386.7m size
2014-06-30 19:56:39,910 INFO  [RpcServer.handler=33,port=60020] regionserver.MemStoreFlusher: Blocking updates on sceplus-vm48.almaden.ibm.com,60020,1404182803191: the global memstore size 401.1m is >= than blocking 386.7m size
2014-06-30 19:56:39,910 INFO  [RpcServer.handler=2,port=60020] regionserver.MemStoreFlusher: Blocking updates on sceplus-vm48.almaden.ibm.com,60020,1404182803191: the global memstore size 404.3m is >= than blocking 386.7m size
2014-06-30 19:56:39,911 INFO  [RpcServer.handler=21,port=60020] regionserver.MemStoreFlusher: Blocking updates on sceplus-vm48.almaden.ibm.com,60020,1404182803191: the global memstore size 404.3m is >= than blocking 386.7m size
2014-06-30 19:56:39,913 INFO  [RpcServer.handler=12,port=60020] regionserver.MemStoreFlusher: Blocking updates on sceplus-vm48.almaden.ibm.com,60020,1404182803191: the global memstore size 404.3m is >= than blocking 386.7m size
2014-06-30 19:56:39,914 INFO  [RpcServer.handler=11,port=60020] regionserver.MemStoreFlusher: Blocking updates on sceplus-vm48.almaden.ibm.com,60020,1404182803191: the global memstore size 404.3m is >= than blocking 386.7m size
2014-06-30 19:56:39,915 INFO  [RpcServer.handler=13,port=60020] regionserver.MemStoreFlusher: Blocking updates on sceplus-vm48.almaden.ibm.com,60020,1404182803191: the global memstore size 404.3m is >= than blocking 386.7m size
2014-06-30 19:56:39,917 INFO  [MemStoreFlusher.1] regionserver.DefaultStoreFlusher: Flushed, sequenceid=653, memsize=79.2m, hasBloomFilter=true, into tmp file hdfs://master:54310/hbase/data/default/usertable/994b975d2c4cd5bbe532bddfb5e4ac46/.tmp/0e2a219931684eadbfeb39bf53323383
2014-06-30 19:56:39,925 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 3178 synced till here 3171
2014-06-30 19:56:39,932 DEBUG [MemStoreFlusher.1] regionserver.HRegionFileSystem: Committing store file hdfs://master:54310/hbase/data/default/usertable/994b975d2c4cd5bbe532bddfb5e4ac46/.tmp/0e2a219931684eadbfeb39bf53323383 as hdfs://master:54310/hbase/data/default/usertable/994b975d2c4cd5bbe532bddfb5e4ac46/family/0e2a219931684eadbfeb39bf53323383
2014-06-30 19:56:39,939 INFO  [MemStoreFlusher.1] regionserver.HStore: Added hdfs://master:54310/hbase/data/default/usertable/994b975d2c4cd5bbe532bddfb5e4ac46/family/0e2a219931684eadbfeb39bf53323383, entries=288370, sequenceid=653, filesize=43.0m
2014-06-30 19:56:39,940 INFO  [MemStoreFlusher.1] regionserver.HRegion: Finished memstore flush of ~136.7m/143331120, currentsize=24.0m/25135840 for region usertable,user7,1404183160444.994b975d2c4cd5bbe532bddfb5e4ac46. in 9385ms, sequenceid=653, compaction requested=true
2014-06-30 19:56:39,940 DEBUG [MemStoreFlusher.1] regionserver.CompactSplitThread: Small Compaction requested: system; Because: MemStoreFlusher.1; compaction_queue=(0:3), split_queue=0, merge_queue=0
2014-06-30 19:56:39,940 WARN  [RpcServer.handler=13,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 25ms
2014-06-30 19:56:39,940 INFO  [RpcServer.handler=13,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server sceplus-vm48.almaden.ibm.com,60020,1404182803191
2014-06-30 19:56:39,940 WARN  [RpcServer.handler=11,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 26ms
2014-06-30 19:56:39,940 INFO  [RpcServer.handler=11,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server sceplus-vm48.almaden.ibm.com,60020,1404182803191
2014-06-30 19:56:39,940 WARN  [RpcServer.handler=12,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 27ms
2014-06-30 19:56:39,940 INFO  [RpcServer.handler=12,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server sceplus-vm48.almaden.ibm.com,60020,1404182803191
2014-06-30 19:56:39,941 WARN  [RpcServer.handler=21,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 31ms
2014-06-30 19:56:39,941 INFO  [RpcServer.handler=21,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server sceplus-vm48.almaden.ibm.com,60020,1404182803191
2014-06-30 19:56:39,941 WARN  [RpcServer.handler=2,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 31ms
2014-06-30 19:56:39,941 INFO  [RpcServer.handler=2,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server sceplus-vm48.almaden.ibm.com,60020,1404182803191
2014-06-30 19:56:39,941 WARN  [RpcServer.handler=33,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 31ms
2014-06-30 19:56:39,941 INFO  [RpcServer.handler=33,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server sceplus-vm48.almaden.ibm.com,60020,1404182803191
2014-06-30 19:56:39,945 WARN  [RpcServer.handler=46,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 38ms
2014-06-30 19:56:39,945 INFO  [RpcServer.handler=46,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server sceplus-vm48.almaden.ibm.com,60020,1404182803191
2014-06-30 19:56:39,945 WARN  [RpcServer.handler=43,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 40ms
2014-06-30 19:56:39,945 INFO  [RpcServer.handler=43,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server sceplus-vm48.almaden.ibm.com,60020,1404182803191
2014-06-30 19:56:39,945 WARN  [RpcServer.handler=31,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 41ms
2014-06-30 19:56:39,945 INFO  [RpcServer.handler=31,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server sceplus-vm48.almaden.ibm.com,60020,1404182803191
2014-06-30 19:56:39,945 WARN  [RpcServer.handler=27,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 41ms
2014-06-30 19:56:39,945 INFO  [RpcServer.handler=27,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server sceplus-vm48.almaden.ibm.com,60020,1404182803191
2014-06-30 19:56:39,945 WARN  [RpcServer.handler=45,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 43ms
2014-06-30 19:56:39,945 INFO  [RpcServer.handler=45,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server sceplus-vm48.almaden.ibm.com,60020,1404182803191
2014-06-30 19:56:39,945 WARN  [RpcServer.handler=17,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 44ms
2014-06-30 19:56:39,945 INFO  [RpcServer.handler=17,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server sceplus-vm48.almaden.ibm.com,60020,1404182803191
2014-06-30 19:56:39,966 DEBUG [regionserver60020-smallCompactions-1404182848217] regionserver.HStore: Removing store files after compaction...
2014-06-30 19:56:39,967 WARN  [RpcServer.handler=10,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":15067,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.58:50155","starttimems":1404183384900,"queuetimems":0,"class":"HRegionServer","responsesize":16341,"method":"Multi"}
2014-06-30 19:56:39,968 WARN  [RpcServer.handler=10,port=60020] ipc.RpcServer: RpcServer.respondercallId: 1961 service: ClientService methodName: Multi size: 2.9m connection: 9.1.143.58:50155: output error
2014-06-30 19:56:39,968 WARN  [RpcServer.handler=10,port=60020] ipc.RpcServer: RpcServer.handler=10,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-06-30 19:56:39,968 WARN  [RpcServer.handler=29,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":12877,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.58:50155","starttimems":1404183387091,"queuetimems":1,"class":"HRegionServer","responsesize":16500,"method":"Multi"}
2014-06-30 19:56:39,968 WARN  [RpcServer.handler=29,port=60020] ipc.RpcServer: RpcServer.respondercallId: 1966 service: ClientService methodName: Multi size: 2.9m connection: 9.1.143.58:50155: output error
2014-06-30 19:56:39,968 WARN  [RpcServer.handler=29,port=60020] ipc.RpcServer: RpcServer.handler=29,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-06-30 19:56:39,969 WARN  [RpcServer.handler=19,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":15058,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.58:50155","starttimems":1404183384911,"queuetimems":1,"class":"HRegionServer","responsesize":16294,"method":"Multi"}
2014-06-30 19:56:39,969 WARN  [RpcServer.handler=19,port=60020] ipc.RpcServer: RpcServer.respondercallId: 1960 service: ClientService methodName: Multi size: 2.8m connection: 9.1.143.58:50155: output error
2014-06-30 19:56:39,969 WARN  [RpcServer.handler=19,port=60020] ipc.RpcServer: RpcServer.handler=19,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-06-30 19:56:39,969 WARN  [RpcServer.handler=18,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":15076,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.58:50155","starttimems":1404183384891,"queuetimems":1,"class":"HRegionServer","responsesize":16533,"method":"Multi"}
2014-06-30 19:56:39,970 WARN  [RpcServer.handler=18,port=60020] ipc.RpcServer: RpcServer.respondercallId: 1955 service: ClientService methodName: Multi size: 2.9m connection: 9.1.143.58:50155: output error
2014-06-30 19:56:39,970 WARN  [RpcServer.handler=18,port=60020] ipc.RpcServer: RpcServer.handler=18,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-06-30 19:56:39,972 DEBUG [regionserver60020-smallCompactions-1404182848217] backup.HFileArchiver: Finished archiving from class org.apache.hadoop.hbase.backup.HFileArchiver$FileableStoreFile, file:hdfs://master:54310/hbase/data/default/usertable/1d66ac045cc0407c0fa426db565fcbba/family/5f0613413be6424184c334d89b1ade67, to hdfs://master:54310/hbase/archive/data/default/usertable/1d66ac045cc0407c0fa426db565fcbba/family/5f0613413be6424184c334d89b1ade67
2014-06-30 19:56:39,975 DEBUG [regionserver60020-smallCompactions-1404182848217] backup.HFileArchiver: Finished archiving from class org.apache.hadoop.hbase.backup.HFileArchiver$FileableStoreFile, file:hdfs://master:54310/hbase/data/default/usertable/1d66ac045cc0407c0fa426db565fcbba/family/6b715f472bf34bf8ab1f0abab79d0595, to hdfs://master:54310/hbase/archive/data/default/usertable/1d66ac045cc0407c0fa426db565fcbba/family/6b715f472bf34bf8ab1f0abab79d0595
2014-06-30 19:56:39,977 DEBUG [regionserver60020-smallCompactions-1404182848217] backup.HFileArchiver: Finished archiving from class org.apache.hadoop.hbase.backup.HFileArchiver$FileableStoreFile, file:hdfs://master:54310/hbase/data/default/usertable/1d66ac045cc0407c0fa426db565fcbba/family/cad5a72f2c9949378c3fa437b4b8a718, to hdfs://master:54310/hbase/archive/data/default/usertable/1d66ac045cc0407c0fa426db565fcbba/family/cad5a72f2c9949378c3fa437b4b8a718
2014-06-30 19:56:39,978 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1404182803191/sceplus-vm48.almaden.ibm.com%2C60020%2C1404182803191.1404183390185 with entries=89, filesize=74.4m; new WAL /hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1404182803191/sceplus-vm48.almaden.ibm.com%2C60020%2C1404182803191.1404183399893
2014-06-30 19:56:39,979 DEBUG [regionserver60020-smallCompactions-1404182848217] backup.HFileArchiver: Finished archiving from class org.apache.hadoop.hbase.backup.HFileArchiver$FileableStoreFile, file:hdfs://master:54310/hbase/data/default/usertable/1d66ac045cc0407c0fa426db565fcbba/family/8637c175dd8148efa0604a6fba62e54c, to hdfs://master:54310/hbase/archive/data/default/usertable/1d66ac045cc0407c0fa426db565fcbba/family/8637c175dd8148efa0604a6fba62e54c
2014-06-30 19:56:39,979 INFO  [regionserver60020-smallCompactions-1404182848217] regionserver.HStore: Completed major compaction of 4 file(s) in family of usertable,user5,1404183160444.1d66ac045cc0407c0fa426db565fcbba. into d11e8b5302b34cc1b594362696cae425(size=181.7m), total size for store is 204.6m. This selection was in queue for 0sec, and took 24sec to execute.
2014-06-30 19:56:39,979 INFO  [regionserver60020-smallCompactions-1404182848217] regionserver.CompactSplitThread: Completed compaction: Request = regionName=usertable,user5,1404183160444.1d66ac045cc0407c0fa426db565fcbba., storeName=family, fileCount=4, fileSize=249.0m, priority=6, time=3517474354432; duration=24sec
2014-06-30 19:56:39,980 DEBUG [regionserver60020-smallCompactions-1404182848217] regionserver.CompactSplitThread: CompactSplitThread Status: compaction_queue=(0:3), split_queue=0, merge_queue=0
2014-06-30 19:56:39,980 DEBUG [regionserver60020-smallCompactions-1404182848217] compactions.RatioBasedCompactionPolicy: Selecting compaction from 4 store files, 0 compacting, 4 eligible, 10 blocking
2014-06-30 19:56:39,980 DEBUG [regionserver60020-smallCompactions-1404182848217] compactions.ExploringCompactionPolicy: Exploring compaction algorithm has selected 4 files of size 273212543 starting at candidate #0 after considering 3 permutations with 2 in ratio
2014-06-30 19:56:39,980 DEBUG [regionserver60020-smallCompactions-1404182848217] regionserver.HStore: 28938d267c1c61f7feb582b85e4c172f - family: Initiating major compaction
2014-06-30 19:56:39,980 INFO  [regionserver60020-smallCompactions-1404182848217] regionserver.HRegion: Starting compaction on family in region usertable,user8,1404183160444.28938d267c1c61f7feb582b85e4c172f.
2014-06-30 19:56:39,982 INFO  [regionserver60020-smallCompactions-1404182848217] regionserver.HStore: Starting compaction of 4 file(s) in family of usertable,user8,1404183160444.28938d267c1c61f7feb582b85e4c172f. into tmpdir=hdfs://master:54310/hbase/data/default/usertable/28938d267c1c61f7feb582b85e4c172f/.tmp, totalSize=260.6m
2014-06-30 19:56:39,982 DEBUG [regionserver60020-smallCompactions-1404182848217] compactions.Compactor: Compacting hdfs://master:54310/hbase/data/default/usertable/28938d267c1c61f7feb582b85e4c172f/family/8c88397781b449829d9e7935422bfa7e, keycount=84955, bloomtype=ROW, size=125.8m, encoding=NONE, seqNum=391, earliestPutTs=1404183223191
2014-06-30 19:56:39,982 DEBUG [regionserver60020-smallCompactions-1404182848217] compactions.Compactor: Compacting hdfs://master:54310/hbase/data/default/usertable/28938d267c1c61f7feb582b85e4c172f/family/4ee3e63f7e1b44b891e842efc382a974, keycount=22927, bloomtype=ROW, size=34.2m, encoding=NONE, seqNum=472, earliestPutTs=1404183294803
2014-06-30 19:56:39,982 DEBUG [regionserver60020-smallCompactions-1404182848217] compactions.Compactor: Compacting hdfs://master:54310/hbase/data/default/usertable/28938d267c1c61f7feb582b85e4c172f/family/04113e6551ba48c5bfd046d960978314, keycount=34909, bloomtype=ROW, size=52.0m, encoding=NONE, seqNum=556, earliestPutTs=1404183354383
2014-06-30 19:56:39,982 DEBUG [regionserver60020-smallCompactions-1404182848217] compactions.Compactor: Compacting hdfs://master:54310/hbase/data/default/usertable/28938d267c1c61f7feb582b85e4c172f/family/3f8f1c17a86848db9e7be2613b5186a6, keycount=32527, bloomtype=ROW, size=48.5m, encoding=NONE, seqNum=629, earliestPutTs=1404183368282
2014-06-30 19:56:39,993 DEBUG [regionserver60020-smallCompactions-1404182848217] util.FSUtils: DFS Client does not support most favored nodes create; using default create
2014-06-30 19:56:39,999 DEBUG [MemStoreFlusher.0] util.FSUtils: DFS Client does not support most favored nodes create; using default create
2014-06-30 19:56:41,390 INFO  [JvmPauseMonitor] util.JvmPauseMonitor: Detected pause in JVM or host machine (eg GC): pause of approximately 1235ms
GC pool 'ParNew' had collection(s): count=1 time=0ms
GC pool 'ConcurrentMarkSweep' had collection(s): count=1 time=1389ms
2014-06-30 19:56:41,477 WARN  [RpcServer.handler=11,port=60020] ipc.RpcServer: RpcServer.respondercallId: 1990 service: ClientService methodName: Multi size: 2.9m connection: 9.1.143.58:50306: output error
2014-06-30 19:56:41,477 WARN  [RpcServer.handler=11,port=60020] ipc.RpcServer: RpcServer.handler=11,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-06-30 19:56:41,479 WARN  [RpcServer.handler=2,port=60020] ipc.RpcServer: RpcServer.respondercallId: 1988 service: ClientService methodName: Multi size: 2.9m connection: 9.1.143.58:50306: output error
2014-06-30 19:56:41,490 WARN  [RpcServer.handler=17,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":16559,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.58:50155","starttimems":1404183384930,"queuetimems":0,"class":"HRegionServer","responsesize":16181,"method":"Multi"}
2014-06-30 19:56:41,490 WARN  [RpcServer.handler=43,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":16568,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.58:50155","starttimems":1404183384921,"queuetimems":1,"class":"HRegionServer","responsesize":16670,"method":"Multi"}
2014-06-30 19:56:41,490 WARN  [RpcServer.handler=17,port=60020] ipc.RpcServer: RpcServer.respondercallId: 1958 service: ClientService methodName: Multi size: 2.8m connection: 9.1.143.58:50155: output error
2014-06-30 19:56:41,490 WARN  [RpcServer.handler=17,port=60020] ipc.RpcServer: RpcServer.handler=17,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-06-30 19:56:41,490 WARN  [RpcServer.handler=43,port=60020] ipc.RpcServer: RpcServer.respondercallId: 1959 service: ClientService methodName: Multi size: 2.9m connection: 9.1.143.58:50155: output error
2014-06-30 19:56:41,490 WARN  [RpcServer.handler=43,port=60020] ipc.RpcServer: RpcServer.handler=43,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-06-30 19:56:41,500 WARN  [RpcServer.handler=2,port=60020] ipc.RpcServer: RpcServer.handler=2,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-06-30 19:56:41,500 WARN  [RpcServer.handler=46,port=60020] ipc.RpcServer: RpcServer.respondercallId: 1987 service: ClientService methodName: Multi size: 2.9m connection: 9.1.143.58:50306: output error
2014-06-30 19:56:41,500 WARN  [RpcServer.handler=46,port=60020] ipc.RpcServer: RpcServer.handler=46,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-06-30 19:56:41,500 WARN  [RpcServer.handler=13,port=60020] ipc.RpcServer: RpcServer.respondercallId: 1989 service: ClientService methodName: Multi size: 2.8m connection: 9.1.143.58:50306: output error
2014-06-30 19:56:41,500 WARN  [RpcServer.handler=13,port=60020] ipc.RpcServer: RpcServer.handler=13,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-06-30 19:56:41,500 WARN  [RpcServer.handler=31,port=60020] ipc.RpcServer: RpcServer.respondercallId: 1986 service: ClientService methodName: Multi size: 2.9m connection: 9.1.143.58:50306: output error
2014-06-30 19:56:41,500 WARN  [RpcServer.handler=31,port=60020] ipc.RpcServer: RpcServer.handler=31,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-06-30 19:56:41,546 WARN  [RpcServer.handler=21,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":16605,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.58:50155","starttimems":1404183384940,"queuetimems":0,"class":"HRegionServer","responsesize":16176,"method":"Multi"}
2014-06-30 19:56:41,547 WARN  [RpcServer.handler=21,port=60020] ipc.RpcServer: RpcServer.respondercallId: 1957 service: ClientService methodName: Multi size: 2.8m connection: 9.1.143.58:50155: output error
2014-06-30 19:56:41,547 WARN  [RpcServer.handler=21,port=60020] ipc.RpcServer: RpcServer.handler=21,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-06-30 19:56:42,007 WARN  [RpcServer.handler=25,port=60020] ipc.RpcServer: RpcServer.respondercallId: 1982 service: ClientService methodName: Multi size: 2.8m connection: 9.1.143.58:50306: output error
2014-06-30 19:56:42,007 WARN  [RpcServer.handler=25,port=60020] ipc.RpcServer: RpcServer.handler=25,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-06-30 19:56:42,008 WARN  [RpcServer.handler=27,port=60020] ipc.RpcServer: RpcServer.respondercallId: 1993 service: ClientService methodName: Multi size: 2.8m connection: 9.1.143.58:50306: output error
2014-06-30 19:56:42,008 WARN  [RpcServer.handler=27,port=60020] ipc.RpcServer: RpcServer.handler=27,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-06-30 19:56:42,008 WARN  [RpcServer.handler=42,port=60020] ipc.RpcServer: RpcServer.respondercallId: 1981 service: ClientService methodName: Multi size: 2.8m connection: 9.1.143.58:50306: output error
2014-06-30 19:56:42,008 WARN  [RpcServer.handler=42,port=60020] ipc.RpcServer: RpcServer.handler=42,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-06-30 19:56:42,009 WARN  [RpcServer.handler=12,port=60020] ipc.RpcServer: RpcServer.respondercallId: 1995 service: ClientService methodName: Multi size: 2.8m connection: 9.1.143.58:50306: output error
2014-06-30 19:56:42,009 WARN  [RpcServer.handler=12,port=60020] ipc.RpcServer: RpcServer.handler=12,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-06-30 19:56:42,009 WARN  [RpcServer.handler=9,port=60020] ipc.RpcServer: RpcServer.respondercallId: 1998 service: ClientService methodName: Multi size: 2.9m connection: 9.1.143.58:50306: output error
2014-06-30 19:56:42,009 WARN  [RpcServer.handler=9,port=60020] ipc.RpcServer: RpcServer.handler=9,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-06-30 19:56:42,014 WARN  [RpcServer.handler=33,port=60020] ipc.RpcServer: RpcServer.respondercallId: 1994 service: ClientService methodName: Multi size: 2.9m connection: 9.1.143.58:50306: output error
2014-06-30 19:56:42,014 WARN  [RpcServer.handler=33,port=60020] ipc.RpcServer: RpcServer.handler=33,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-06-30 19:56:42,014 WARN  [RpcServer.handler=3,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":11796,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.58:50155","starttimems":1404183390218,"queuetimems":0,"class":"HRegionServer","responsesize":16176,"method":"Multi"}
2014-06-30 19:56:42,015 WARN  [RpcServer.handler=3,port=60020] ipc.RpcServer: RpcServer.respondercallId: 1974 service: ClientService methodName: Multi size: 2.8m connection: 9.1.143.58:50155: output error
2014-06-30 19:56:42,015 WARN  [RpcServer.handler=3,port=60020] ipc.RpcServer: RpcServer.handler=3,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-06-30 19:56:42,268 WARN  [RpcServer.handler=45,port=60020] ipc.RpcServer: RpcServer.respondercallId: 1984 service: ClientService methodName: Multi size: 2.9m connection: 9.1.143.58:50306: output error
2014-06-30 19:56:42,268 WARN  [RpcServer.handler=24,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":12060,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.58:50155","starttimems":1404183390207,"queuetimems":0,"class":"HRegionServer","responsesize":16341,"method":"Multi"}
2014-06-30 19:56:42,268 WARN  [RpcServer.handler=26,port=60020] ipc.RpcServer: RpcServer.respondercallId: 1985 service: ClientService methodName: Multi size: 2.8m connection: 9.1.143.58:50306: output error
2014-06-30 19:56:42,268 WARN  [RpcServer.handler=26,port=60020] ipc.RpcServer: RpcServer.handler=26,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-06-30 19:56:42,268 WARN  [RpcServer.handler=45,port=60020] ipc.RpcServer: RpcServer.handler=45,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-06-30 19:56:42,268 WARN  [RpcServer.handler=36,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":12070,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.58:50155","starttimems":1404183390197,"queuetimems":0,"class":"HRegionServer","responsesize":16294,"method":"Multi"}
2014-06-30 19:56:42,268 WARN  [RpcServer.handler=24,port=60020] ipc.RpcServer: RpcServer.respondercallId: 1975 service: ClientService methodName: Multi size: 2.9m connection: 9.1.143.58:50155: output error
2014-06-30 19:56:42,268 WARN  [RpcServer.handler=24,port=60020] ipc.RpcServer: RpcServer.handler=24,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-06-30 19:56:42,268 WARN  [RpcServer.handler=35,port=60020] ipc.RpcServer: RpcServer.respondercallId: 1983 service: ClientService methodName: Multi size: 2.9m connection: 9.1.143.58:50306: output error
2014-06-30 19:56:42,268 WARN  [RpcServer.handler=36,port=60020] ipc.RpcServer: RpcServer.respondercallId: 1969 service: ClientService methodName: Multi size: 2.8m connection: 9.1.143.58:50155: output error
2014-06-30 19:56:42,268 WARN  [RpcServer.handler=36,port=60020] ipc.RpcServer: RpcServer.handler=36,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-06-30 19:56:42,268 WARN  [RpcServer.handler=35,port=60020] ipc.RpcServer: RpcServer.handler=35,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-06-30 19:56:42,277 DEBUG [MemStoreFlusher.1] regionserver.MemStoreFlusher: Flush thread woke up because memory above low water=367.3m
2014-06-30 19:56:42,277 INFO  [MemStoreFlusher.1] regionserver.MemStoreFlusher: Flush of region usertable,user2,1404183160444.5446f1f888398e97c0ec3cc30da5bf97. due to global heap pressure
2014-06-30 19:56:42,277 DEBUG [MemStoreFlusher.1] regionserver.HRegion: Started memstore flush for usertable,user2,1404183160444.5446f1f888398e97c0ec3cc30da5bf97., current region memstore size 106.1m
2014-06-30 19:56:43,737 DEBUG [LruStats #0] hfile.LruBlockCache: Total=407.28 KB, free=386.28 MB, max=386.68 MB, blocks=0, accesses=24623, hits=0, hitRatio=0, cachingAccesses=0, cachingHits=0, cachingHitsRatio=0,evictions=0, evicted=0, evictedPerRun=NaN
2014-06-30 19:56:43,737 INFO  [JvmPauseMonitor] util.JvmPauseMonitor: Detected pause in JVM or host machine (eg GC): pause of approximately 1346ms
GC pool 'ParNew' had collection(s): count=1 time=0ms
GC pool 'ConcurrentMarkSweep' had collection(s): count=2 time=1521ms
2014-06-30 19:56:43,803 INFO  [MemStoreFlusher.0] regionserver.DefaultStoreFlusher: Flushed, sequenceid=525, memsize=65.5m, hasBloomFilter=true, into tmp file hdfs://master:54310/hbase/data/default/usertable/9638edaa7b212ccfbe14e9984bd5a5a3/.tmp/343384998d0c417e92ea49ed09e37ee5
2014-06-30 19:56:43,813 DEBUG [MemStoreFlusher.0] regionserver.HRegionFileSystem: Committing store file hdfs://master:54310/hbase/data/default/usertable/9638edaa7b212ccfbe14e9984bd5a5a3/.tmp/343384998d0c417e92ea49ed09e37ee5 as hdfs://master:54310/hbase/data/default/usertable/9638edaa7b212ccfbe14e9984bd5a5a3/family/343384998d0c417e92ea49ed09e37ee5
2014-06-30 19:56:43,813 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-06-30 19:56:43,827 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 3252 synced till here 3247
2014-06-30 19:56:43,837 DEBUG [MemStoreFlusher.1] util.FSUtils: DFS Client does not support most favored nodes create; using default create
2014-06-30 19:56:43,838 INFO  [MemStoreFlusher.0] regionserver.HStore: Added hdfs://master:54310/hbase/data/default/usertable/9638edaa7b212ccfbe14e9984bd5a5a3/family/343384998d0c417e92ea49ed09e37ee5, entries=238420, sequenceid=525, filesize=35.5m
2014-06-30 19:56:43,839 INFO  [MemStoreFlusher.0] regionserver.HRegion: Finished memstore flush of ~88.1m/92361120, currentsize=0.0/0 for region usertable,user9,1404183160445.9638edaa7b212ccfbe14e9984bd5a5a3. in 4124ms, sequenceid=525, compaction requested=false
2014-06-30 19:56:43,921 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1404182803191/sceplus-vm48.almaden.ibm.com%2C60020%2C1404182803191.1404183399893 with entries=74, filesize=64.4m; new WAL /hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1404182803191/sceplus-vm48.almaden.ibm.com%2C60020%2C1404182803191.1404183403813
2014-06-30 19:56:43,921 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1404182803191/sceplus-vm48.almaden.ibm.com%2C60020%2C1404182803191.1404183286011
2014-06-30 19:56:43,921 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1404182803191/sceplus-vm48.almaden.ibm.com%2C60020%2C1404182803191.1404183289432
2014-06-30 19:56:43,921 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1404182803191/sceplus-vm48.almaden.ibm.com%2C60020%2C1404182803191.1404183292027
2014-06-30 19:56:43,921 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1404182803191/sceplus-vm48.almaden.ibm.com%2C60020%2C1404182803191.1404183294633
2014-06-30 19:56:43,921 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1404182803191/sceplus-vm48.almaden.ibm.com%2C60020%2C1404182803191.1404183304525
2014-06-30 19:56:43,921 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1404182803191/sceplus-vm48.almaden.ibm.com%2C60020%2C1404182803191.1404183319982
2014-06-30 19:56:43,921 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1404182803191/sceplus-vm48.almaden.ibm.com%2C60020%2C1404182803191.1404183349254
2014-06-30 19:56:43,921 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1404182803191/sceplus-vm48.almaden.ibm.com%2C60020%2C1404182803191.1404183356781
2014-06-30 19:56:43,921 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1404182803191/sceplus-vm48.almaden.ibm.com%2C60020%2C1404182803191.1404183359676
2014-06-30 19:56:43,921 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1404182803191/sceplus-vm48.almaden.ibm.com%2C60020%2C1404182803191.1404183363056
2014-06-30 19:56:43,921 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1404182803191/sceplus-vm48.almaden.ibm.com%2C60020%2C1404182803191.1404183366669
2014-06-30 19:56:43,921 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1404182803191/sceplus-vm48.almaden.ibm.com%2C60020%2C1404182803191.1404183368832
2014-06-30 19:56:43,921 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1404182803191/sceplus-vm48.almaden.ibm.com%2C60020%2C1404182803191.1404183371707
2014-06-30 19:56:44,738 DEBUG [MemStoreFlusher.0] regionserver.MemStoreFlusher: Flush thread woke up because memory above low water=367.3m
2014-06-30 19:56:44,738 INFO  [MemStoreFlusher.0] regionserver.MemStoreFlusher: Flush of region usertable,user8,1404183160444.28938d267c1c61f7feb582b85e4c172f. due to global heap pressure
2014-06-30 19:56:44,738 DEBUG [MemStoreFlusher.0] regionserver.HRegion: Started memstore flush for usertable,user8,1404183160444.28938d267c1c61f7feb582b85e4c172f., current region memstore size 113.8m
2014-06-30 19:56:44,769 INFO  [MemStoreFlusher.1] regionserver.DefaultStoreFlusher: Flushed, sequenceid=693, memsize=36.4m, hasBloomFilter=true, into tmp file hdfs://master:54310/hbase/data/default/usertable/5446f1f888398e97c0ec3cc30da5bf97/.tmp/2413196db49c47759aac46e5b35a303c
2014-06-30 19:56:44,778 DEBUG [MemStoreFlusher.1] regionserver.HRegionFileSystem: Committing store file hdfs://master:54310/hbase/data/default/usertable/5446f1f888398e97c0ec3cc30da5bf97/.tmp/2413196db49c47759aac46e5b35a303c as hdfs://master:54310/hbase/data/default/usertable/5446f1f888398e97c0ec3cc30da5bf97/family/2413196db49c47759aac46e5b35a303c
2014-06-30 19:56:44,785 INFO  [MemStoreFlusher.1] regionserver.HStore: Added hdfs://master:54310/hbase/data/default/usertable/5446f1f888398e97c0ec3cc30da5bf97/family/2413196db49c47759aac46e5b35a303c, entries=132610, sequenceid=693, filesize=19.8m
2014-06-30 19:56:44,786 INFO  [MemStoreFlusher.1] regionserver.HRegion: Finished memstore flush of ~106.1m/111205120, currentsize=16.9m/17734480 for region usertable,user2,1404183160444.5446f1f888398e97c0ec3cc30da5bf97. in 2508ms, sequenceid=693, compaction requested=true
2014-06-30 19:56:44,786 DEBUG [MemStoreFlusher.1] regionserver.CompactSplitThread: Small Compaction requested: system; Because: MemStoreFlusher.1; compaction_queue=(0:3), split_queue=0, merge_queue=0
2014-06-30 19:56:44,848 DEBUG [MemStoreFlusher.0] util.FSUtils: DFS Client does not support most favored nodes create; using default create
2014-06-30 19:56:45,450 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-06-30 19:56:45,674 DEBUG [MemStoreFlusher.1] regionserver.HRegion: NOT flushing memstore for region usertable,user8,1404183160444.28938d267c1c61f7feb582b85e4c172f., flushing=true, writesEnabled=true
2014-06-30 19:56:45,675 DEBUG [RpcServer.handler=9,port=60020] regionserver.HRegion: Flush requested on usertable,user8,1404183160444.28938d267c1c61f7feb582b85e4c172f.
2014-06-30 19:56:45,722 INFO  [MemStoreFlusher.0] regionserver.DefaultStoreFlusher: Flushed, sequenceid=701, memsize=36.4m, hasBloomFilter=true, into tmp file hdfs://master:54310/hbase/data/default/usertable/28938d267c1c61f7feb582b85e4c172f/.tmp/f9d8ee587f0a4719927f3ee9b867a605
2014-06-30 19:56:45,750 DEBUG [MemStoreFlusher.0] regionserver.HRegionFileSystem: Committing store file hdfs://master:54310/hbase/data/default/usertable/28938d267c1c61f7feb582b85e4c172f/.tmp/f9d8ee587f0a4719927f3ee9b867a605 as hdfs://master:54310/hbase/data/default/usertable/28938d267c1c61f7feb582b85e4c172f/family/f9d8ee587f0a4719927f3ee9b867a605
2014-06-30 19:56:46,361 INFO  [MemStoreFlusher.0] regionserver.HStore: Added hdfs://master:54310/hbase/data/default/usertable/28938d267c1c61f7feb582b85e4c172f/family/f9d8ee587f0a4719927f3ee9b867a605, entries=132680, sequenceid=701, filesize=19.8m
2014-06-30 19:56:46,362 INFO  [MemStoreFlusher.0] regionserver.HRegion: Finished memstore flush of ~113.8m/119317280, currentsize=24.0m/25198960 for region usertable,user8,1404183160444.28938d267c1c61f7feb582b85e4c172f. in 1623ms, sequenceid=701, compaction requested=false
2014-06-30 19:56:46,802 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 3389 synced till here 3387
2014-06-30 19:56:47,412 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1404182803191/sceplus-vm48.almaden.ibm.com%2C60020%2C1404182803191.1404183403813 with entries=137, filesize=92.5m; new WAL /hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1404182803191/sceplus-vm48.almaden.ibm.com%2C60020%2C1404182803191.1404183405451
2014-06-30 19:56:47,412 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1404182803191/sceplus-vm48.almaden.ibm.com%2C60020%2C1404182803191.1404183374206
2014-06-30 19:56:47,412 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1404182803191/sceplus-vm48.almaden.ibm.com%2C60020%2C1404182803191.1404183380242
2014-06-30 19:56:49,020 DEBUG [regionserver60020-smallCompactions-1404182848217] regionserver.HRegionFileSystem: Committing store file hdfs://master:54310/hbase/data/default/usertable/28938d267c1c61f7feb582b85e4c172f/.tmp/e2d1d10bf5e34ad69f71813430f1bdad as hdfs://master:54310/hbase/data/default/usertable/28938d267c1c61f7feb582b85e4c172f/family/e2d1d10bf5e34ad69f71813430f1bdad
2014-06-30 19:56:49,031 DEBUG [regionserver60020-smallCompactions-1404182848217] regionserver.HStore: Removing store files after compaction...
2014-06-30 19:56:49,038 DEBUG [regionserver60020-smallCompactions-1404182848217] backup.HFileArchiver: Finished archiving from class org.apache.hadoop.hbase.backup.HFileArchiver$FileableStoreFile, file:hdfs://master:54310/hbase/data/default/usertable/28938d267c1c61f7feb582b85e4c172f/family/8c88397781b449829d9e7935422bfa7e, to hdfs://master:54310/hbase/archive/data/default/usertable/28938d267c1c61f7feb582b85e4c172f/family/8c88397781b449829d9e7935422bfa7e
2014-06-30 19:56:49,040 DEBUG [regionserver60020-smallCompactions-1404182848217] backup.HFileArchiver: Finished archiving from class org.apache.hadoop.hbase.backup.HFileArchiver$FileableStoreFile, file:hdfs://master:54310/hbase/data/default/usertable/28938d267c1c61f7feb582b85e4c172f/family/4ee3e63f7e1b44b891e842efc382a974, to hdfs://master:54310/hbase/archive/data/default/usertable/28938d267c1c61f7feb582b85e4c172f/family/4ee3e63f7e1b44b891e842efc382a974
2014-06-30 19:56:49,041 DEBUG [regionserver60020-smallCompactions-1404182848217] backup.HFileArchiver: Finished archiving from class org.apache.hadoop.hbase.backup.HFileArchiver$FileableStoreFile, file:hdfs://master:54310/hbase/data/default/usertable/28938d267c1c61f7feb582b85e4c172f/family/04113e6551ba48c5bfd046d960978314, to hdfs://master:54310/hbase/archive/data/default/usertable/28938d267c1c61f7feb582b85e4c172f/family/04113e6551ba48c5bfd046d960978314
2014-06-30 19:56:49,044 DEBUG [regionserver60020-smallCompactions-1404182848217] backup.HFileArchiver: Finished archiving from class org.apache.hadoop.hbase.backup.HFileArchiver$FileableStoreFile, file:hdfs://master:54310/hbase/data/default/usertable/28938d267c1c61f7feb582b85e4c172f/family/3f8f1c17a86848db9e7be2613b5186a6, to hdfs://master:54310/hbase/archive/data/default/usertable/28938d267c1c61f7feb582b85e4c172f/family/3f8f1c17a86848db9e7be2613b5186a6
2014-06-30 19:56:49,044 INFO  [regionserver60020-smallCompactions-1404182848217] regionserver.HStore: Completed major compaction of 4 file(s) in family of usertable,user8,1404183160444.28938d267c1c61f7feb582b85e4c172f. into e2d1d10bf5e34ad69f71813430f1bdad(size=186.6m), total size for store is 206.4m. This selection was in queue for 0sec, and took 9sec to execute.
2014-06-30 19:56:49,044 INFO  [regionserver60020-smallCompactions-1404182848217] regionserver.CompactSplitThread: Completed compaction: Request = regionName=usertable,user8,1404183160444.28938d267c1c61f7feb582b85e4c172f., storeName=family, fileCount=4, fileSize=260.6m, priority=6, time=3542337801975; duration=9sec
2014-06-30 19:56:49,044 DEBUG [regionserver60020-smallCompactions-1404182848217] regionserver.CompactSplitThread: CompactSplitThread Status: compaction_queue=(0:3), split_queue=0, merge_queue=0
2014-06-30 19:56:49,044 DEBUG [regionserver60020-smallCompactions-1404182848217] compactions.RatioBasedCompactionPolicy: Selecting compaction from 4 store files, 0 compacting, 4 eligible, 10 blocking
2014-06-30 19:56:49,044 DEBUG [regionserver60020-smallCompactions-1404182848217] compactions.ExploringCompactionPolicy: Exploring compaction algorithm has selected 4 files of size 264712470 starting at candidate #0 after considering 3 permutations with 2 in ratio
2014-06-30 19:56:49,045 DEBUG [regionserver60020-smallCompactions-1404182848217] regionserver.HStore: 994b975d2c4cd5bbe532bddfb5e4ac46 - family: Initiating major compaction
2014-06-30 19:56:49,045 INFO  [regionserver60020-smallCompactions-1404182848217] regionserver.HRegion: Starting compaction on family in region usertable,user7,1404183160444.994b975d2c4cd5bbe532bddfb5e4ac46.
2014-06-30 19:56:49,045 INFO  [regionserver60020-smallCompactions-1404182848217] regionserver.HStore: Starting compaction of 4 file(s) in family of usertable,user7,1404183160444.994b975d2c4cd5bbe532bddfb5e4ac46. into tmpdir=hdfs://master:54310/hbase/data/default/usertable/994b975d2c4cd5bbe532bddfb5e4ac46/.tmp, totalSize=252.4m
2014-06-30 19:56:49,045 DEBUG [regionserver60020-smallCompactions-1404182848217] compactions.Compactor: Compacting hdfs://master:54310/hbase/data/default/usertable/994b975d2c4cd5bbe532bddfb5e4ac46/family/204ac70e73ae438abf77926eb42a0572, keycount=91320, bloomtype=ROW, size=135.2m, encoding=NONE, seqNum=442, earliestPutTs=1404183223115
2014-06-30 19:56:49,045 DEBUG [regionserver60020-smallCompactions-1404182848217] compactions.Compactor: Compacting hdfs://master:54310/hbase/data/default/usertable/994b975d2c4cd5bbe532bddfb5e4ac46/family/54b03d51827347ce888d58f01e2cdc1c, keycount=24657, bloomtype=ROW, size=36.8m, encoding=NONE, seqNum=511, earliestPutTs=1404183349262
2014-06-30 19:56:49,045 DEBUG [regionserver60020-smallCompactions-1404182848217] compactions.Compactor: Compacting hdfs://master:54310/hbase/data/default/usertable/994b975d2c4cd5bbe532bddfb5e4ac46/family/e20161fb9a30414facd313203b4b327e, keycount=25124, bloomtype=ROW, size=37.4m, encoding=NONE, seqNum=565, earliestPutTs=1404183360139
2014-06-30 19:56:49,045 DEBUG [regionserver60020-smallCompactions-1404182848217] compactions.Compactor: Compacting hdfs://master:54310/hbase/data/default/usertable/994b975d2c4cd5bbe532bddfb5e4ac46/family/0e2a219931684eadbfeb39bf53323383, keycount=28837, bloomtype=ROW, size=43.0m, encoding=NONE, seqNum=653, earliestPutTs=1404183370339
2014-06-30 19:56:49,063 DEBUG [regionserver60020-smallCompactions-1404182848217] util.FSUtils: DFS Client does not support most favored nodes create; using default create
2014-06-30 19:56:50,253 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-06-30 19:56:50,274 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1404182803191/sceplus-vm48.almaden.ibm.com%2C60020%2C1404182803191.1404183405451 with entries=99, filesize=61.6m; new WAL /hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1404182803191/sceplus-vm48.almaden.ibm.com%2C60020%2C1404182803191.1404183410253
2014-06-30 19:56:50,310 DEBUG [MemStoreFlusher.0] regionserver.MemStoreFlusher: Flush thread woke up because memory above low water=367.3m
2014-06-30 19:56:50,311 INFO  [MemStoreFlusher.0] regionserver.MemStoreFlusher: Flush of region usertable,user7,1404183160444.994b975d2c4cd5bbe532bddfb5e4ac46. due to global heap pressure
2014-06-30 19:56:50,311 DEBUG [MemStoreFlusher.0] regionserver.HRegion: Started memstore flush for usertable,user7,1404183160444.994b975d2c4cd5bbe532bddfb5e4ac46., current region memstore size 124.1m
2014-06-30 19:56:50,565 DEBUG [MemStoreFlusher.0] util.FSUtils: DFS Client does not support most favored nodes create; using default create
2014-06-30 19:56:50,592 INFO  [RpcServer.handler=17,port=60020] regionserver.MemStoreFlusher: Blocking updates on sceplus-vm48.almaden.ibm.com,60020,1404182803191: the global memstore size 389.6m is >= than blocking 386.7m size
2014-06-30 19:56:50,592 INFO  [RpcServer.handler=30,port=60020] regionserver.MemStoreFlusher: Blocking updates on sceplus-vm48.almaden.ibm.com,60020,1404182803191: the global memstore size 389.6m is >= than blocking 386.7m size
2014-06-30 19:56:50,600 INFO  [RpcServer.handler=19,port=60020] regionserver.MemStoreFlusher: Blocking updates on sceplus-vm48.almaden.ibm.com,60020,1404182803191: the global memstore size 391.1m is >= than blocking 386.7m size
2014-06-30 19:56:50,822 INFO  [RpcServer.handler=11,port=60020] regionserver.MemStoreFlusher: Blocking updates on sceplus-vm48.almaden.ibm.com,60020,1404182803191: the global memstore size 391.1m is >= than blocking 386.7m size
2014-06-30 19:56:50,846 INFO  [RpcServer.handler=23,port=60020] regionserver.MemStoreFlusher: Blocking updates on sceplus-vm48.almaden.ibm.com,60020,1404182803191: the global memstore size 391.1m is >= than blocking 386.7m size
2014-06-30 19:56:51,003 INFO  [RpcServer.handler=32,port=60020] regionserver.MemStoreFlusher: Blocking updates on sceplus-vm48.almaden.ibm.com,60020,1404182803191: the global memstore size 391.1m is >= than blocking 386.7m size
2014-06-30 19:56:51,604 INFO  [MemStoreFlusher.0] regionserver.DefaultStoreFlusher: Flushed, sequenceid=732, memsize=85.8m, hasBloomFilter=true, into tmp file hdfs://master:54310/hbase/data/default/usertable/994b975d2c4cd5bbe532bddfb5e4ac46/.tmp/d14db766018349c6b23c4eb5a800efe1
2014-06-30 19:56:51,613 DEBUG [MemStoreFlusher.0] regionserver.HRegionFileSystem: Committing store file hdfs://master:54310/hbase/data/default/usertable/994b975d2c4cd5bbe532bddfb5e4ac46/.tmp/d14db766018349c6b23c4eb5a800efe1 as hdfs://master:54310/hbase/data/default/usertable/994b975d2c4cd5bbe532bddfb5e4ac46/family/d14db766018349c6b23c4eb5a800efe1
2014-06-30 19:56:51,620 INFO  [MemStoreFlusher.0] regionserver.HStore: Added hdfs://master:54310/hbase/data/default/usertable/994b975d2c4cd5bbe532bddfb5e4ac46/family/d14db766018349c6b23c4eb5a800efe1, entries=312240, sequenceid=732, filesize=46.6m
2014-06-30 19:56:51,621 INFO  [MemStoreFlusher.0] regionserver.HRegion: Finished memstore flush of ~124.1m/130100080, currentsize=3.3m/3441600 for region usertable,user7,1404183160444.994b975d2c4cd5bbe532bddfb5e4ac46. in 1310ms, sequenceid=732, compaction requested=false
2014-06-30 19:56:51,621 WARN  [RpcServer.handler=32,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 618ms
2014-06-30 19:56:51,621 INFO  [RpcServer.handler=32,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server sceplus-vm48.almaden.ibm.com,60020,1404182803191
2014-06-30 19:56:51,621 WARN  [RpcServer.handler=23,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 775ms
2014-06-30 19:56:51,621 INFO  [RpcServer.handler=23,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server sceplus-vm48.almaden.ibm.com,60020,1404182803191
2014-06-30 19:56:51,621 WARN  [RpcServer.handler=11,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 799ms
2014-06-30 19:56:51,621 INFO  [RpcServer.handler=11,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server sceplus-vm48.almaden.ibm.com,60020,1404182803191
2014-06-30 19:56:51,622 WARN  [RpcServer.handler=19,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 1022ms
2014-06-30 19:56:51,622 INFO  [RpcServer.handler=19,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server sceplus-vm48.almaden.ibm.com,60020,1404182803191
2014-06-30 19:56:51,622 WARN  [RpcServer.handler=30,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 1030ms
2014-06-30 19:56:51,622 INFO  [RpcServer.handler=30,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server sceplus-vm48.almaden.ibm.com,60020,1404182803191
2014-06-30 19:56:51,623 WARN  [RpcServer.handler=17,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 1030ms
2014-06-30 19:56:51,623 INFO  [RpcServer.handler=17,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server sceplus-vm48.almaden.ibm.com,60020,1404182803191
2014-06-30 19:56:52,801 DEBUG [RpcServer.handler=14,port=60020] regionserver.HRegion: Flush requested on usertable,user5,1404183160444.1d66ac045cc0407c0fa426db565fcbba.
2014-06-30 19:56:52,802 DEBUG [MemStoreFlusher.1] regionserver.HRegion: Started memstore flush for usertable,user5,1404183160444.1d66ac045cc0407c0fa426db565fcbba., current region memstore size 129.3m
2014-06-30 19:56:53,220 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-06-30 19:56:53,335 DEBUG [MemStoreFlusher.1] util.FSUtils: DFS Client does not support most favored nodes create; using default create
2014-06-30 19:56:53,370 DEBUG [MemStoreFlusher.0] regionserver.MemStoreFlusher: Flush thread woke up because memory above low water=367.3m
2014-06-30 19:56:53,371 INFO  [MemStoreFlusher.0] regionserver.MemStoreFlusher: Flush of region usertable,user2,1404183160444.5446f1f888398e97c0ec3cc30da5bf97. due to global heap pressure
2014-06-30 19:56:53,371 DEBUG [MemStoreFlusher.0] regionserver.HRegion: Started memstore flush for usertable,user2,1404183160444.5446f1f888398e97c0ec3cc30da5bf97., current region memstore size 102.2m
2014-06-30 19:56:53,391 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 3610 synced till here 3599
2014-06-30 19:56:54,866 INFO  [JvmPauseMonitor] util.JvmPauseMonitor: Detected pause in JVM or host machine (eg GC): pause of approximately 1124ms
GC pool 'ParNew' had collection(s): count=1 time=0ms
GC pool 'ConcurrentMarkSweep' had collection(s): count=1 time=1406ms
2014-06-30 19:56:54,884 INFO  [RpcServer.handler=15,port=60020] regionserver.MemStoreFlusher: Blocking updates on sceplus-vm48.almaden.ibm.com,60020,1404182803191: the global memstore size 387.1m is >= than blocking 386.7m size
2014-06-30 19:56:54,906 INFO  [RpcServer.handler=30,port=60020] regionserver.MemStoreFlusher: Blocking updates on sceplus-vm48.almaden.ibm.com,60020,1404182803191: the global memstore size 387.1m is >= than blocking 386.7m size
2014-06-30 19:56:54,924 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1404182803191/sceplus-vm48.almaden.ibm.com%2C60020%2C1404182803191.1404183410253 with entries=122, filesize=81.0m; new WAL /hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1404182803191/sceplus-vm48.almaden.ibm.com%2C60020%2C1404182803191.1404183413220
2014-06-30 19:56:54,937 INFO  [RpcServer.handler=17,port=60020] regionserver.MemStoreFlusher: Blocking updates on sceplus-vm48.almaden.ibm.com,60020,1404182803191: the global memstore size 388.0m is >= than blocking 386.7m size
2014-06-30 19:56:54,938 DEBUG [MemStoreFlusher.0] util.FSUtils: DFS Client does not support most favored nodes create; using default create
2014-06-30 19:56:54,954 INFO  [RpcServer.handler=11,port=60020] regionserver.MemStoreFlusher: Blocking updates on sceplus-vm48.almaden.ibm.com,60020,1404182803191: the global memstore size 388.0m is >= than blocking 386.7m size
2014-06-30 19:56:54,962 INFO  [RpcServer.handler=32,port=60020] regionserver.MemStoreFlusher: Blocking updates on sceplus-vm48.almaden.ibm.com,60020,1404182803191: the global memstore size 388.0m is >= than blocking 386.7m size
2014-06-30 19:56:54,973 INFO  [RpcServer.handler=3,port=60020] regionserver.MemStoreFlusher: Blocking updates on sceplus-vm48.almaden.ibm.com,60020,1404182803191: the global memstore size 388.0m is >= than blocking 386.7m size
2014-06-30 19:56:54,989 INFO  [RpcServer.handler=45,port=60020] regionserver.MemStoreFlusher: Blocking updates on sceplus-vm48.almaden.ibm.com,60020,1404182803191: the global memstore size 388.0m is >= than blocking 386.7m size
2014-06-30 19:56:55,103 INFO  [RpcServer.handler=10,port=60020] regionserver.MemStoreFlusher: Blocking updates on sceplus-vm48.almaden.ibm.com,60020,1404182803191: the global memstore size 394.4m is >= than blocking 386.7m size
2014-06-30 19:56:55,103 INFO  [RpcServer.handler=47,port=60020] regionserver.MemStoreFlusher: Blocking updates on sceplus-vm48.almaden.ibm.com,60020,1404182803191: the global memstore size 396.2m is >= than blocking 386.7m size
2014-06-30 19:56:55,104 INFO  [RpcServer.handler=46,port=60020] regionserver.MemStoreFlusher: Blocking updates on sceplus-vm48.almaden.ibm.com,60020,1404182803191: the global memstore size 398.3m is >= than blocking 386.7m size
2014-06-30 19:56:55,106 INFO  [RpcServer.handler=43,port=60020] regionserver.MemStoreFlusher: Blocking updates on sceplus-vm48.almaden.ibm.com,60020,1404182803191: the global memstore size 400.0m is >= than blocking 386.7m size
2014-06-30 19:56:55,106 INFO  [RpcServer.handler=2,port=60020] regionserver.MemStoreFlusher: Blocking updates on sceplus-vm48.almaden.ibm.com,60020,1404182803191: the global memstore size 400.0m is >= than blocking 386.7m size
2014-06-30 19:56:55,108 INFO  [RpcServer.handler=42,port=60020] regionserver.MemStoreFlusher: Blocking updates on sceplus-vm48.almaden.ibm.com,60020,1404182803191: the global memstore size 400.0m is >= than blocking 386.7m size
2014-06-30 19:56:55,131 INFO  [RpcServer.handler=27,port=60020] regionserver.MemStoreFlusher: Blocking updates on sceplus-vm48.almaden.ibm.com,60020,1404182803191: the global memstore size 404.8m is >= than blocking 386.7m size
2014-06-30 19:56:55,131 INFO  [RpcServer.handler=38,port=60020] regionserver.MemStoreFlusher: Blocking updates on sceplus-vm48.almaden.ibm.com,60020,1404182803191: the global memstore size 404.8m is >= than blocking 386.7m size
2014-06-30 19:56:55,135 INFO  [RpcServer.handler=37,port=60020] regionserver.MemStoreFlusher: Blocking updates on sceplus-vm48.almaden.ibm.com,60020,1404182803191: the global memstore size 408.0m is >= than blocking 386.7m size
2014-06-30 19:56:55,137 INFO  [RpcServer.handler=4,port=60020] regionserver.MemStoreFlusher: Blocking updates on sceplus-vm48.almaden.ibm.com,60020,1404182803191: the global memstore size 408.0m is >= than blocking 386.7m size
2014-06-30 19:56:55,151 INFO  [RpcServer.handler=21,port=60020] regionserver.MemStoreFlusher: Blocking updates on sceplus-vm48.almaden.ibm.com,60020,1404182803191: the global memstore size 408.0m is >= than blocking 386.7m size
2014-06-30 19:56:55,169 INFO  [RpcServer.handler=13,port=60020] regionserver.MemStoreFlusher: Blocking updates on sceplus-vm48.almaden.ibm.com,60020,1404182803191: the global memstore size 408.0m is >= than blocking 386.7m size
2014-06-30 19:56:55,474 INFO  [RpcServer.handler=9,port=60020] regionserver.MemStoreFlusher: Blocking updates on sceplus-vm48.almaden.ibm.com,60020,1404182803191: the global memstore size 408.0m is >= than blocking 386.7m size
2014-06-30 19:56:55,486 INFO  [RpcServer.handler=33,port=60020] regionserver.MemStoreFlusher: Blocking updates on sceplus-vm48.almaden.ibm.com,60020,1404182803191: the global memstore size 408.0m is >= than blocking 386.7m size
2014-06-30 19:56:55,497 INFO  [RpcServer.handler=31,port=60020] regionserver.MemStoreFlusher: Blocking updates on sceplus-vm48.almaden.ibm.com,60020,1404182803191: the global memstore size 408.0m is >= than blocking 386.7m size
2014-06-30 19:56:55,508 INFO  [RpcServer.handler=24,port=60020] regionserver.MemStoreFlusher: Blocking updates on sceplus-vm48.almaden.ibm.com,60020,1404182803191: the global memstore size 408.0m is >= than blocking 386.7m size
2014-06-30 19:56:55,510 INFO  [RpcServer.handler=25,port=60020] regionserver.MemStoreFlusher: Blocking updates on sceplus-vm48.almaden.ibm.com,60020,1404182803191: the global memstore size 408.0m is >= than blocking 386.7m size
2014-06-30 19:56:55,520 INFO  [RpcServer.handler=35,port=60020] regionserver.MemStoreFlusher: Blocking updates on sceplus-vm48.almaden.ibm.com,60020,1404182803191: the global memstore size 408.0m is >= than blocking 386.7m size
2014-06-30 19:56:55,520 INFO  [RpcServer.handler=36,port=60020] regionserver.MemStoreFlusher: Blocking updates on sceplus-vm48.almaden.ibm.com,60020,1404182803191: the global memstore size 408.0m is >= than blocking 386.7m size
2014-06-30 19:56:55,531 INFO  [RpcServer.handler=34,port=60020] regionserver.MemStoreFlusher: Blocking updates on sceplus-vm48.almaden.ibm.com,60020,1404182803191: the global memstore size 408.0m is >= than blocking 386.7m size
2014-06-30 19:56:55,544 INFO  [RpcServer.handler=26,port=60020] regionserver.MemStoreFlusher: Blocking updates on sceplus-vm48.almaden.ibm.com,60020,1404182803191: the global memstore size 408.0m is >= than blocking 386.7m size
2014-06-30 19:56:55,551 INFO  [RpcServer.handler=39,port=60020] regionserver.MemStoreFlusher: Blocking updates on sceplus-vm48.almaden.ibm.com,60020,1404182803191: the global memstore size 408.0m is >= than blocking 386.7m size
2014-06-30 19:56:57,859 INFO  [JvmPauseMonitor] util.JvmPauseMonitor: Detected pause in JVM or host machine (eg GC): pause of approximately 1887ms
GC pool 'ParNew' had collection(s): count=1 time=0ms
GC pool 'ConcurrentMarkSweep' had collection(s): count=2 time=2050ms
2014-06-30 19:56:57,874 INFO  [RpcServer.handler=48,port=60020] regionserver.MemStoreFlusher: Blocking updates on sceplus-vm48.almaden.ibm.com,60020,1404182803191: the global memstore size 408.0m is >= than blocking 386.7m size
2014-06-30 19:56:57,888 INFO  [RpcServer.handler=22,port=60020] regionserver.MemStoreFlusher: Blocking updates on sceplus-vm48.almaden.ibm.com,60020,1404182803191: the global memstore size 408.0m is >= than blocking 386.7m size
2014-06-30 19:56:57,900 INFO  [RpcServer.handler=5,port=60020] regionserver.MemStoreFlusher: Blocking updates on sceplus-vm48.almaden.ibm.com,60020,1404182803191: the global memstore size 408.0m is >= than blocking 386.7m size
2014-06-30 19:56:57,911 INFO  [RpcServer.handler=1,port=60020] regionserver.MemStoreFlusher: Blocking updates on sceplus-vm48.almaden.ibm.com,60020,1404182803191: the global memstore size 408.0m is >= than blocking 386.7m size
2014-06-30 19:56:57,924 INFO  [RpcServer.handler=29,port=60020] regionserver.MemStoreFlusher: Blocking updates on sceplus-vm48.almaden.ibm.com,60020,1404182803191: the global memstore size 408.0m is >= than blocking 386.7m size
2014-06-30 19:56:57,935 INFO  [RpcServer.handler=28,port=60020] regionserver.MemStoreFlusher: Blocking updates on sceplus-vm48.almaden.ibm.com,60020,1404182803191: the global memstore size 408.0m is >= than blocking 386.7m size
2014-06-30 19:56:57,947 INFO  [RpcServer.handler=44,port=60020] regionserver.MemStoreFlusher: Blocking updates on sceplus-vm48.almaden.ibm.com,60020,1404182803191: the global memstore size 408.0m is >= than blocking 386.7m size
2014-06-30 19:56:57,963 INFO  [RpcServer.handler=14,port=60020] regionserver.MemStoreFlusher: Blocking updates on sceplus-vm48.almaden.ibm.com,60020,1404182803191: the global memstore size 408.0m is >= than blocking 386.7m size
2014-06-30 19:56:57,974 INFO  [RpcServer.handler=40,port=60020] regionserver.MemStoreFlusher: Blocking updates on sceplus-vm48.almaden.ibm.com,60020,1404182803191: the global memstore size 408.0m is >= than blocking 386.7m size
2014-06-30 19:56:57,975 INFO  [RpcServer.handler=16,port=60020] regionserver.MemStoreFlusher: Blocking updates on sceplus-vm48.almaden.ibm.com,60020,1404182803191: the global memstore size 408.0m is >= than blocking 386.7m size
2014-06-30 19:56:57,977 INFO  [RpcServer.handler=49,port=60020] regionserver.MemStoreFlusher: Blocking updates on sceplus-vm48.almaden.ibm.com,60020,1404182803191: the global memstore size 408.0m is >= than blocking 386.7m size
2014-06-30 19:56:57,990 INFO  [RpcServer.handler=6,port=60020] regionserver.MemStoreFlusher: Blocking updates on sceplus-vm48.almaden.ibm.com,60020,1404182803191: the global memstore size 408.0m is >= than blocking 386.7m size
2014-06-30 19:56:58,001 INFO  [RpcServer.handler=7,port=60020] regionserver.MemStoreFlusher: Blocking updates on sceplus-vm48.almaden.ibm.com,60020,1404182803191: the global memstore size 408.0m is >= than blocking 386.7m size
2014-06-30 19:56:58,014 INFO  [RpcServer.handler=23,port=60020] regionserver.MemStoreFlusher: Blocking updates on sceplus-vm48.almaden.ibm.com,60020,1404182803191: the global memstore size 408.0m is >= than blocking 386.7m size
2014-06-30 19:56:58,027 INFO  [RpcServer.handler=41,port=60020] regionserver.MemStoreFlusher: Blocking updates on sceplus-vm48.almaden.ibm.com,60020,1404182803191: the global memstore size 408.0m is >= than blocking 386.7m size
2014-06-30 19:56:58,039 INFO  [RpcServer.handler=18,port=60020] regionserver.MemStoreFlusher: Blocking updates on sceplus-vm48.almaden.ibm.com,60020,1404182803191: the global memstore size 408.0m is >= than blocking 386.7m size
2014-06-30 19:56:59,719 INFO  [JvmPauseMonitor] util.JvmPauseMonitor: Detected pause in JVM or host machine (eg GC): pause of approximately 1359ms
GC pool 'ParNew' had collection(s): count=1 time=0ms
GC pool 'ConcurrentMarkSweep' had collection(s): count=1 time=1676ms
2014-06-30 19:56:59,779 INFO  [RpcServer.handler=0,port=60020] regionserver.MemStoreFlusher: Blocking updates on sceplus-vm48.almaden.ibm.com,60020,1404182803191: the global memstore size 408.0m is >= than blocking 386.7m size
2014-06-30 19:56:59,792 INFO  [RpcServer.handler=12,port=60020] regionserver.MemStoreFlusher: Blocking updates on sceplus-vm48.almaden.ibm.com,60020,1404182803191: the global memstore size 408.0m is >= than blocking 386.7m size
2014-06-30 19:56:59,804 INFO  [RpcServer.handler=19,port=60020] regionserver.MemStoreFlusher: Blocking updates on sceplus-vm48.almaden.ibm.com,60020,1404182803191: the global memstore size 408.0m is >= than blocking 386.7m size
2014-06-30 19:56:59,815 INFO  [RpcServer.handler=8,port=60020] regionserver.MemStoreFlusher: Blocking updates on sceplus-vm48.almaden.ibm.com,60020,1404182803191: the global memstore size 408.0m is >= than blocking 386.7m size
2014-06-30 19:56:59,825 INFO  [RpcServer.handler=20,port=60020] regionserver.MemStoreFlusher: Blocking updates on sceplus-vm48.almaden.ibm.com,60020,1404182803191: the global memstore size 408.0m is >= than blocking 386.7m size
2014-06-30 19:56:59,884 WARN  [RpcServer.handler=15,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-06-30 19:56:59,906 WARN  [RpcServer.handler=30,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-06-30 19:56:59,937 WARN  [RpcServer.handler=17,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-06-30 19:56:59,954 WARN  [RpcServer.handler=11,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-06-30 19:56:59,962 WARN  [RpcServer.handler=32,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-06-30 19:56:59,973 WARN  [RpcServer.handler=3,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-06-30 19:56:59,989 WARN  [RpcServer.handler=45,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-06-30 19:57:00,098 INFO  [MemStoreFlusher.1] regionserver.DefaultStoreFlusher: Flushed, sequenceid=753, memsize=105.2m, hasBloomFilter=true, into tmp file hdfs://master:54310/hbase/data/default/usertable/1d66ac045cc0407c0fa426db565fcbba/.tmp/b48f46f83b7748339001cde0126e5bd5
2014-06-30 19:57:00,103 WARN  [RpcServer.handler=10,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5001ms
2014-06-30 19:57:00,104 WARN  [RpcServer.handler=47,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5001ms
2014-06-30 19:57:00,104 WARN  [RpcServer.handler=46,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-06-30 19:57:00,106 WARN  [RpcServer.handler=43,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-06-30 19:57:00,106 WARN  [RpcServer.handler=2,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-06-30 19:57:00,109 WARN  [RpcServer.handler=42,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5001ms
2014-06-30 19:57:00,110 DEBUG [MemStoreFlusher.1] regionserver.HRegionFileSystem: Committing store file hdfs://master:54310/hbase/data/default/usertable/1d66ac045cc0407c0fa426db565fcbba/.tmp/b48f46f83b7748339001cde0126e5bd5 as hdfs://master:54310/hbase/data/default/usertable/1d66ac045cc0407c0fa426db565fcbba/family/b48f46f83b7748339001cde0126e5bd5
2014-06-30 19:57:00,131 WARN  [RpcServer.handler=27,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-06-30 19:57:00,132 WARN  [RpcServer.handler=38,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5001ms
2014-06-30 19:57:00,135 WARN  [RpcServer.handler=37,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5001ms
2014-06-30 19:57:00,137 WARN  [RpcServer.handler=4,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-06-30 19:57:00,151 WARN  [RpcServer.handler=21,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-06-30 19:57:00,170 WARN  [RpcServer.handler=13,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5001ms
2014-06-30 19:57:00,428 INFO  [MemStoreFlusher.0] regionserver.DefaultStoreFlusher: Flushed, sequenceid=762, memsize=108.6m, hasBloomFilter=true, into tmp file hdfs://master:54310/hbase/data/default/usertable/5446f1f888398e97c0ec3cc30da5bf97/.tmp/bdf69bd317a745659be2bfe35c59232e
2014-06-30 19:57:00,438 DEBUG [MemStoreFlusher.0] regionserver.HRegionFileSystem: Committing store file hdfs://master:54310/hbase/data/default/usertable/5446f1f888398e97c0ec3cc30da5bf97/.tmp/bdf69bd317a745659be2bfe35c59232e as hdfs://master:54310/hbase/data/default/usertable/5446f1f888398e97c0ec3cc30da5bf97/family/bdf69bd317a745659be2bfe35c59232e
2014-06-30 19:57:00,474 WARN  [RpcServer.handler=9,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-06-30 19:57:00,486 WARN  [RpcServer.handler=33,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-06-30 19:57:00,497 WARN  [RpcServer.handler=31,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-06-30 19:57:00,509 WARN  [RpcServer.handler=24,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5001ms
2014-06-30 19:57:00,510 WARN  [RpcServer.handler=25,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-06-30 19:57:00,520 WARN  [RpcServer.handler=35,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-06-30 19:57:00,520 WARN  [RpcServer.handler=36,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-06-30 19:57:00,532 WARN  [RpcServer.handler=34,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5001ms
2014-06-30 19:57:00,544 WARN  [RpcServer.handler=26,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5001ms
2014-06-30 19:57:00,552 WARN  [RpcServer.handler=39,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5001ms
2014-06-30 19:57:00,882 INFO  [MemStoreFlusher.1] regionserver.HStore: Added hdfs://master:54310/hbase/data/default/usertable/1d66ac045cc0407c0fa426db565fcbba/family/b48f46f83b7748339001cde0126e5bd5, entries=382940, sequenceid=753, filesize=57.1m
2014-06-30 19:57:00,882 INFO  [MemStoreFlusher.1] regionserver.HRegion: Finished memstore flush of ~132.5m/138936480, currentsize=19.4m/20323600 for region usertable,user5,1404183160444.1d66ac045cc0407c0fa426db565fcbba. in 8080ms, sequenceid=753, compaction requested=true
2014-06-30 19:57:00,882 DEBUG [MemStoreFlusher.1] regionserver.CompactSplitThread: Small Compaction requested: system; Because: MemStoreFlusher.1; compaction_queue=(0:3), split_queue=0, merge_queue=0
2014-06-30 19:57:00,882 WARN  [RpcServer.handler=39,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5331ms
2014-06-30 19:57:00,883 INFO  [RpcServer.handler=39,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server sceplus-vm48.almaden.ibm.com,60020,1404182803191
2014-06-30 19:57:00,883 WARN  [RpcServer.handler=26,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5340ms
2014-06-30 19:57:00,883 INFO  [RpcServer.handler=26,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server sceplus-vm48.almaden.ibm.com,60020,1404182803191
2014-06-30 19:57:00,883 WARN  [RpcServer.handler=34,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5352ms
2014-06-30 19:57:00,883 INFO  [RpcServer.handler=34,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server sceplus-vm48.almaden.ibm.com,60020,1404182803191
2014-06-30 19:57:00,883 WARN  [RpcServer.handler=36,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5363ms
2014-06-30 19:57:00,883 INFO  [RpcServer.handler=36,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server sceplus-vm48.almaden.ibm.com,60020,1404182803191
2014-06-30 19:57:00,885 WARN  [RpcServer.handler=35,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5365ms
2014-06-30 19:57:00,886 INFO  [RpcServer.handler=35,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server sceplus-vm48.almaden.ibm.com,60020,1404182803191
2014-06-30 19:57:00,886 WARN  [RpcServer.handler=25,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5376ms
2014-06-30 19:57:00,886 INFO  [RpcServer.handler=25,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server sceplus-vm48.almaden.ibm.com,60020,1404182803191
2014-06-30 19:57:00,886 WARN  [RpcServer.handler=24,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5378ms
2014-06-30 19:57:00,887 INFO  [RpcServer.handler=24,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server sceplus-vm48.almaden.ibm.com,60020,1404182803191
2014-06-30 19:57:00,887 WARN  [RpcServer.handler=31,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5390ms
2014-06-30 19:57:00,887 INFO  [RpcServer.handler=31,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server sceplus-vm48.almaden.ibm.com,60020,1404182803191
2014-06-30 19:57:00,887 WARN  [RpcServer.handler=33,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5401ms
2014-06-30 19:57:00,887 INFO  [RpcServer.handler=33,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server sceplus-vm48.almaden.ibm.com,60020,1404182803191
2014-06-30 19:57:00,887 WARN  [RpcServer.handler=9,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5413ms
2014-06-30 19:57:00,887 INFO  [RpcServer.handler=9,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server sceplus-vm48.almaden.ibm.com,60020,1404182803191
2014-06-30 19:57:00,887 WARN  [RpcServer.handler=13,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5718ms
2014-06-30 19:57:00,887 INFO  [RpcServer.handler=13,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server sceplus-vm48.almaden.ibm.com,60020,1404182803191
2014-06-30 19:57:00,887 WARN  [RpcServer.handler=21,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5736ms
2014-06-30 19:57:00,887 INFO  [RpcServer.handler=21,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server sceplus-vm48.almaden.ibm.com,60020,1404182803191
2014-06-30 19:57:00,888 WARN  [RpcServer.handler=4,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5751ms
2014-06-30 19:57:00,888 INFO  [RpcServer.handler=4,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server sceplus-vm48.almaden.ibm.com,60020,1404182803191
2014-06-30 19:57:00,888 WARN  [RpcServer.handler=37,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5754ms
2014-06-30 19:57:00,888 INFO  [RpcServer.handler=37,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server sceplus-vm48.almaden.ibm.com,60020,1404182803191
2014-06-30 19:57:00,888 WARN  [RpcServer.handler=38,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5757ms
2014-06-30 19:57:00,888 INFO  [RpcServer.handler=38,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server sceplus-vm48.almaden.ibm.com,60020,1404182803191
2014-06-30 19:57:00,888 WARN  [RpcServer.handler=27,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5757ms
2014-06-30 19:57:00,888 INFO  [RpcServer.handler=27,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server sceplus-vm48.almaden.ibm.com,60020,1404182803191
2014-06-30 19:57:00,888 WARN  [RpcServer.handler=42,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5780ms
2014-06-30 19:57:00,888 INFO  [RpcServer.handler=42,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server sceplus-vm48.almaden.ibm.com,60020,1404182803191
2014-06-30 19:57:00,888 WARN  [RpcServer.handler=2,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5782ms
2014-06-30 19:57:00,889 INFO  [RpcServer.handler=2,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server sceplus-vm48.almaden.ibm.com,60020,1404182803191
2014-06-30 19:57:00,889 WARN  [RpcServer.handler=43,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5783ms
2014-06-30 19:57:00,889 INFO  [RpcServer.handler=43,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server sceplus-vm48.almaden.ibm.com,60020,1404182803191
2014-06-30 19:57:00,889 WARN  [RpcServer.handler=46,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5785ms
2014-06-30 19:57:00,889 INFO  [RpcServer.handler=46,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server sceplus-vm48.almaden.ibm.com,60020,1404182803191
2014-06-30 19:57:00,890 WARN  [RpcServer.handler=47,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5786ms
2014-06-30 19:57:01,047 INFO  [RpcServer.handler=47,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server sceplus-vm48.almaden.ibm.com,60020,1404182803191
2014-06-30 19:57:01,047 WARN  [RpcServer.handler=10,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5945ms
2014-06-30 19:57:01,047 INFO  [RpcServer.handler=10,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server sceplus-vm48.almaden.ibm.com,60020,1404182803191
2014-06-30 19:57:01,047 WARN  [RpcServer.handler=45,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 6058ms
2014-06-30 19:57:01,047 INFO  [RpcServer.handler=45,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server sceplus-vm48.almaden.ibm.com,60020,1404182803191
2014-06-30 19:57:01,047 WARN  [RpcServer.handler=3,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 6074ms
2014-06-30 19:57:01,047 INFO  [RpcServer.handler=3,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server sceplus-vm48.almaden.ibm.com,60020,1404182803191
2014-06-30 19:57:01,048 WARN  [RpcServer.handler=32,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 6086ms
2014-06-30 19:57:01,048 INFO  [RpcServer.handler=32,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server sceplus-vm48.almaden.ibm.com,60020,1404182803191
2014-06-30 19:57:01,048 WARN  [RpcServer.handler=11,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 6094ms
2014-06-30 19:57:01,048 INFO  [RpcServer.handler=11,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server sceplus-vm48.almaden.ibm.com,60020,1404182803191
2014-06-30 19:57:01,048 WARN  [RpcServer.handler=17,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 6111ms
2014-06-30 19:57:01,048 INFO  [RpcServer.handler=17,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server sceplus-vm48.almaden.ibm.com,60020,1404182803191
2014-06-30 19:57:01,048 WARN  [RpcServer.handler=30,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 6142ms
2014-06-30 19:57:01,048 INFO  [RpcServer.handler=30,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server sceplus-vm48.almaden.ibm.com,60020,1404182803191
2014-06-30 19:57:01,048 WARN  [RpcServer.handler=15,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 6164ms
2014-06-30 19:57:01,048 INFO  [RpcServer.handler=15,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server sceplus-vm48.almaden.ibm.com,60020,1404182803191
2014-06-30 19:57:01,048 WARN  [RpcServer.handler=20,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 1223ms
2014-06-30 19:57:01,048 INFO  [RpcServer.handler=20,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server sceplus-vm48.almaden.ibm.com,60020,1404182803191
2014-06-30 19:57:01,048 WARN  [RpcServer.handler=8,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 1233ms
2014-06-30 19:57:01,049 INFO  [RpcServer.handler=8,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server sceplus-vm48.almaden.ibm.com,60020,1404182803191
2014-06-30 19:57:01,049 WARN  [RpcServer.handler=19,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 1245ms
2014-06-30 19:57:01,049 INFO  [RpcServer.handler=19,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server sceplus-vm48.almaden.ibm.com,60020,1404182803191
2014-06-30 19:57:01,049 WARN  [RpcServer.handler=12,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 1257ms
2014-06-30 19:57:01,049 INFO  [RpcServer.handler=12,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server sceplus-vm48.almaden.ibm.com,60020,1404182803191
2014-06-30 19:57:01,049 WARN  [RpcServer.handler=0,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 1270ms
2014-06-30 19:57:01,049 INFO  [RpcServer.handler=0,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server sceplus-vm48.almaden.ibm.com,60020,1404182803191
2014-06-30 19:57:01,049 WARN  [RpcServer.handler=18,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 3010ms
2014-06-30 19:57:01,049 INFO  [RpcServer.handler=18,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server sceplus-vm48.almaden.ibm.com,60020,1404182803191
2014-06-30 19:57:01,051 WARN  [RpcServer.handler=41,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 3022ms
2014-06-30 19:57:01,055 INFO  [RpcServer.handler=41,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server sceplus-vm48.almaden.ibm.com,60020,1404182803191
2014-06-30 19:57:01,056 WARN  [RpcServer.handler=23,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 3042ms
2014-06-30 19:57:01,056 INFO  [RpcServer.handler=23,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server sceplus-vm48.almaden.ibm.com,60020,1404182803191
2014-06-30 19:57:01,056 WARN  [RpcServer.handler=7,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 3055ms
2014-06-30 19:57:01,056 INFO  [RpcServer.handler=7,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server sceplus-vm48.almaden.ibm.com,60020,1404182803191
2014-06-30 19:57:01,057 WARN  [RpcServer.handler=6,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 3066ms
2014-06-30 19:57:01,057 INFO  [RpcServer.handler=6,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server sceplus-vm48.almaden.ibm.com,60020,1404182803191
2014-06-30 19:57:01,057 WARN  [RpcServer.handler=49,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 3080ms
2014-06-30 19:57:01,058 INFO  [RpcServer.handler=49,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server sceplus-vm48.almaden.ibm.com,60020,1404182803191
2014-06-30 19:57:01,058 WARN  [RpcServer.handler=16,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 3084ms
2014-06-30 19:57:01,058 INFO  [RpcServer.handler=16,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server sceplus-vm48.almaden.ibm.com,60020,1404182803191
2014-06-30 19:57:01,060 WARN  [RpcServer.handler=40,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 3086ms
2014-06-30 19:57:01,061 INFO  [RpcServer.handler=40,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server sceplus-vm48.almaden.ibm.com,60020,1404182803191
2014-06-30 19:57:01,061 WARN  [RpcServer.handler=14,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 3098ms
2014-06-30 19:57:01,061 INFO  [RpcServer.handler=14,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server sceplus-vm48.almaden.ibm.com,60020,1404182803191
2014-06-30 19:57:01,062 WARN  [RpcServer.handler=44,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 3115ms
2014-06-30 19:57:01,062 INFO  [RpcServer.handler=44,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server sceplus-vm48.almaden.ibm.com,60020,1404182803191
2014-06-30 19:57:01,062 WARN  [RpcServer.handler=28,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 3127ms
2014-06-30 19:57:01,062 INFO  [RpcServer.handler=28,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server sceplus-vm48.almaden.ibm.com,60020,1404182803191
2014-06-30 19:57:01,063 WARN  [RpcServer.handler=29,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 3139ms
2014-06-30 19:57:01,063 INFO  [RpcServer.handler=29,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server sceplus-vm48.almaden.ibm.com,60020,1404182803191
2014-06-30 19:57:01,063 WARN  [RpcServer.handler=1,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 3152ms
2014-06-30 19:57:01,072 INFO  [RpcServer.handler=1,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server sceplus-vm48.almaden.ibm.com,60020,1404182803191
2014-06-30 19:57:01,073 WARN  [RpcServer.handler=5,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 3173ms
2014-06-30 19:57:01,073 INFO  [RpcServer.handler=5,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server sceplus-vm48.almaden.ibm.com,60020,1404182803191
2014-06-30 19:57:01,073 WARN  [RpcServer.handler=22,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 3185ms
2014-06-30 19:57:01,073 INFO  [RpcServer.handler=22,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server sceplus-vm48.almaden.ibm.com,60020,1404182803191
2014-06-30 19:57:01,073 WARN  [RpcServer.handler=48,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 3200ms
2014-06-30 19:57:01,073 INFO  [RpcServer.handler=48,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server sceplus-vm48.almaden.ibm.com,60020,1404182803191
2014-06-30 19:57:01,084 DEBUG [regionserver60020-smallCompactions-1404182848217] regionserver.HRegionFileSystem: Committing store file hdfs://master:54310/hbase/data/default/usertable/994b975d2c4cd5bbe532bddfb5e4ac46/.tmp/5a4bf61fc5e04044a0e1dc17cbd4e184 as hdfs://master:54310/hbase/data/default/usertable/994b975d2c4cd5bbe532bddfb5e4ac46/family/5a4bf61fc5e04044a0e1dc17cbd4e184
2014-06-30 19:57:03,435 INFO  [JvmPauseMonitor] util.JvmPauseMonitor: Detected pause in JVM or host machine (eg GC): pause of approximately 2215ms
GC pool 'ConcurrentMarkSweep' had collection(s): count=2 time=2439ms
2014-06-30 19:57:03,455 INFO  [MemStoreFlusher.0] regionserver.HStore: Added hdfs://master:54310/hbase/data/default/usertable/5446f1f888398e97c0ec3cc30da5bf97/family/bdf69bd317a745659be2bfe35c59232e, entries=395500, sequenceid=762, filesize=59.0m
2014-06-30 19:57:03,465 DEBUG [regionserver60020-smallCompactions-1404182848217] regionserver.HStore: Removing store files after compaction...
2014-06-30 19:57:03,466 INFO  [MemStoreFlusher.0] regionserver.HRegion: Finished memstore flush of ~108.6m/113900880, currentsize=1.6m/1710720 for region usertable,user2,1404183160444.5446f1f888398e97c0ec3cc30da5bf97. in 10095ms, sequenceid=762, compaction requested=true
2014-06-30 19:57:03,466 DEBUG [MemStoreFlusher.0] regionserver.CompactSplitThread: Small Compaction requested: system; Because: MemStoreFlusher.0; compaction_queue=(0:4), split_queue=0, merge_queue=0
2014-06-30 19:57:03,471 DEBUG [regionserver60020-smallCompactions-1404182848217] backup.HFileArchiver: Finished archiving from class org.apache.hadoop.hbase.backup.HFileArchiver$FileableStoreFile, file:hdfs://master:54310/hbase/data/default/usertable/994b975d2c4cd5bbe532bddfb5e4ac46/family/204ac70e73ae438abf77926eb42a0572, to hdfs://master:54310/hbase/archive/data/default/usertable/994b975d2c4cd5bbe532bddfb5e4ac46/family/204ac70e73ae438abf77926eb42a0572
2014-06-30 19:57:03,473 DEBUG [regionserver60020-smallCompactions-1404182848217] backup.HFileArchiver: Finished archiving from class org.apache.hadoop.hbase.backup.HFileArchiver$FileableStoreFile, file:hdfs://master:54310/hbase/data/default/usertable/994b975d2c4cd5bbe532bddfb5e4ac46/family/54b03d51827347ce888d58f01e2cdc1c, to hdfs://master:54310/hbase/archive/data/default/usertable/994b975d2c4cd5bbe532bddfb5e4ac46/family/54b03d51827347ce888d58f01e2cdc1c
2014-06-30 19:57:03,475 DEBUG [regionserver60020-smallCompactions-1404182848217] backup.HFileArchiver: Finished archiving from class org.apache.hadoop.hbase.backup.HFileArchiver$FileableStoreFile, file:hdfs://master:54310/hbase/data/default/usertable/994b975d2c4cd5bbe532bddfb5e4ac46/family/e20161fb9a30414facd313203b4b327e, to hdfs://master:54310/hbase/archive/data/default/usertable/994b975d2c4cd5bbe532bddfb5e4ac46/family/e20161fb9a30414facd313203b4b327e
2014-06-30 19:57:03,477 DEBUG [regionserver60020-smallCompactions-1404182848217] backup.HFileArchiver: Finished archiving from class org.apache.hadoop.hbase.backup.HFileArchiver$FileableStoreFile, file:hdfs://master:54310/hbase/data/default/usertable/994b975d2c4cd5bbe532bddfb5e4ac46/family/0e2a219931684eadbfeb39bf53323383, to hdfs://master:54310/hbase/archive/data/default/usertable/994b975d2c4cd5bbe532bddfb5e4ac46/family/0e2a219931684eadbfeb39bf53323383
2014-06-30 19:57:03,477 INFO  [regionserver60020-smallCompactions-1404182848217] regionserver.HStore: Completed major compaction of 4 file(s) in family of usertable,user7,1404183160444.994b975d2c4cd5bbe532bddfb5e4ac46. into 5a4bf61fc5e04044a0e1dc17cbd4e184(size=186.7m), total size for store is 233.2m. This selection was in queue for 0sec, and took 14sec to execute.
2014-06-30 19:57:03,477 INFO  [regionserver60020-smallCompactions-1404182848217] regionserver.CompactSplitThread: Completed compaction: Request = regionName=usertable,user7,1404183160444.994b975d2c4cd5bbe532bddfb5e4ac46., storeName=family, fileCount=4, fileSize=252.4m, priority=6, time=3551402333369; duration=14sec
2014-06-30 19:57:03,477 DEBUG [regionserver60020-smallCompactions-1404182848217] regionserver.CompactSplitThread: CompactSplitThread Status: compaction_queue=(0:4), split_queue=0, merge_queue=0
2014-06-30 19:57:03,477 DEBUG [regionserver60020-smallCompactions-1404182848217] compactions.RatioBasedCompactionPolicy: Selecting compaction from 5 store files, 0 compacting, 5 eligible, 10 blocking
2014-06-30 19:57:03,478 DEBUG [regionserver60020-smallCompactions-1404182848217] compactions.ExploringCompactionPolicy: Exploring compaction algorithm has selected 5 files of size 328108261 starting at candidate #0 after considering 6 permutations with 5 in ratio
2014-06-30 19:57:03,478 DEBUG [regionserver60020-smallCompactions-1404182848217] regionserver.HStore: 5446f1f888398e97c0ec3cc30da5bf97 - family: Initiating major compaction
2014-06-30 19:57:03,478 INFO  [regionserver60020-smallCompactions-1404182848217] regionserver.HRegion: Starting compaction on family in region usertable,user2,1404183160444.5446f1f888398e97c0ec3cc30da5bf97.
2014-06-30 19:57:03,478 INFO  [regionserver60020-smallCompactions-1404182848217] regionserver.HStore: Starting compaction of 5 file(s) in family of usertable,user2,1404183160444.5446f1f888398e97c0ec3cc30da5bf97. into tmpdir=hdfs://master:54310/hbase/data/default/usertable/5446f1f888398e97c0ec3cc30da5bf97/.tmp, totalSize=312.9m
2014-06-30 19:57:03,478 DEBUG [regionserver60020-smallCompactions-1404182848217] compactions.Compactor: Compacting hdfs://master:54310/hbase/data/default/usertable/5446f1f888398e97c0ec3cc30da5bf97/family/a0169492de6b4f00aec2ed8adddb6971, keycount=91605, bloomtype=ROW, size=135.7m, encoding=NONE, seqNum=472, earliestPutTs=1404183222023
2014-06-30 19:57:03,478 DEBUG [regionserver60020-smallCompactions-1404182848217] compactions.Compactor: Compacting hdfs://master:54310/hbase/data/default/usertable/5446f1f888398e97c0ec3cc30da5bf97/family/d386e992ead34e91b89d6f9e2dda27c1, keycount=34996, bloomtype=ROW, size=52.2m, encoding=NONE, seqNum=555, earliestPutTs=1404183349096
2014-06-30 19:57:03,478 DEBUG [regionserver60020-smallCompactions-1404182848217] compactions.Compactor: Compacting hdfs://master:54310/hbase/data/default/usertable/5446f1f888398e97c0ec3cc30da5bf97/family/896fee37320d41b9a9316a4e242fc409, keycount=31074, bloomtype=ROW, size=46.3m, encoding=NONE, seqNum=626, earliestPutTs=1404183367986
2014-06-30 19:57:03,478 DEBUG [regionserver60020-smallCompactions-1404182848217] compactions.Compactor: Compacting hdfs://master:54310/hbase/data/default/usertable/5446f1f888398e97c0ec3cc30da5bf97/family/2413196db49c47759aac46e5b35a303c, keycount=13261, bloomtype=ROW, size=19.8m, encoding=NONE, seqNum=693, earliestPutTs=1404183380149
2014-06-30 19:57:03,478 DEBUG [regionserver60020-smallCompactions-1404182848217] compactions.Compactor: Compacting hdfs://master:54310/hbase/data/default/usertable/5446f1f888398e97c0ec3cc30da5bf97/family/bdf69bd317a745659be2bfe35c59232e, keycount=39550, bloomtype=ROW, size=59.0m, encoding=NONE, seqNum=762, earliestPutTs=1404183403756
2014-06-30 19:57:03,594 DEBUG [regionserver60020-smallCompactions-1404182848217] util.FSUtils: DFS Client does not support most favored nodes create; using default create
2014-06-30 19:57:03,700 WARN  [RpcServer.handler=37,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":10890,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.58:50316","starttimems":1404183412809,"queuetimems":0,"class":"HRegionServer","responsesize":16181,"method":"Multi"}
2014-06-30 19:57:03,704 WARN  [RpcServer.handler=38,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":10911,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.58:50316","starttimems":1404183412793,"queuetimems":1,"class":"HRegionServer","responsesize":15988,"method":"Multi"}
2014-06-30 19:57:03,728 WARN  [RpcServer.handler=15,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":10945,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.58:50316","starttimems":1404183412782,"queuetimems":1,"class":"HRegionServer","responsesize":16409,"method":"Multi"}
2014-06-30 19:57:05,927 INFO  [JvmPauseMonitor] util.JvmPauseMonitor: Detected pause in JVM or host machine (eg GC): pause of approximately 1992ms
GC pool 'ConcurrentMarkSweep' had collection(s): count=1 time=2093ms
2014-06-30 19:57:05,989 WARN  [RpcServer.handler=4,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":13163,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.58:50316","starttimems":1404183412825,"queuetimems":0,"class":"HRegionServer","responsesize":16176,"method":"Multi"}
2014-06-30 19:57:05,990 WARN  [RpcServer.handler=4,port=60020] ipc.RpcServer: RpcServer.respondercallId: 2204 service: ClientService methodName: Multi size: 2.8m connection: 9.1.143.58:50316: output error
2014-06-30 19:57:05,991 WARN  [RpcServer.handler=4,port=60020] ipc.RpcServer: RpcServer.handler=4,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-06-30 19:57:08,692 INFO  [JvmPauseMonitor] util.JvmPauseMonitor: Detected pause in JVM or host machine (eg GC): pause of approximately 2264ms
GC pool 'ConcurrentMarkSweep' had collection(s): count=1 time=2479ms
2014-06-30 19:57:08,866 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-06-30 19:57:08,868 WARN  [RpcServer.handler=10,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":15656,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.58:50316","starttimems":1404183413211,"queuetimems":0,"class":"HRegionServer","responsesize":16552,"method":"Multi"}
2014-06-30 19:57:08,868 WARN  [RpcServer.handler=43,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":15627,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.58:50316","starttimems":1404183413241,"queuetimems":1,"class":"HRegionServer","responsesize":16406,"method":"Multi"}
2014-06-30 19:57:08,868 WARN  [RpcServer.handler=2,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":15616,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.58:50316","starttimems":1404183413252,"queuetimems":0,"class":"HRegionServer","responsesize":16091,"method":"Multi"}
2014-06-30 19:57:08,868 WARN  [RpcServer.handler=10,port=60020] ipc.RpcServer: RpcServer.respondercallId: 2223 service: ClientService methodName: Multi size: 2.9m connection: 9.1.143.58:50316: output error
2014-06-30 19:57:08,868 WARN  [RpcServer.handler=10,port=60020] ipc.RpcServer: RpcServer.handler=10,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-06-30 19:57:08,868 WARN  [RpcServer.handler=2,port=60020] ipc.RpcServer: RpcServer.respondercallId: 2234 service: ClientService methodName: Multi size: 2.8m connection: 9.1.143.58:50316: output error
2014-06-30 19:57:08,869 WARN  [RpcServer.handler=2,port=60020] ipc.RpcServer: RpcServer.handler=2,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-06-30 19:57:08,869 WARN  [RpcServer.handler=43,port=60020] ipc.RpcServer: RpcServer.respondercallId: 2227 service: ClientService methodName: Multi size: 2.9m connection: 9.1.143.58:50316: output error
2014-06-30 19:57:08,869 WARN  [RpcServer.handler=43,port=60020] ipc.RpcServer: RpcServer.handler=43,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-06-30 19:57:08,885 WARN  [RpcServer.handler=47,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":15658,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.58:50316","starttimems":1404183413226,"queuetimems":1,"class":"HRegionServer","responsesize":16463,"method":"Multi"}
2014-06-30 19:57:08,885 WARN  [RpcServer.handler=47,port=60020] ipc.RpcServer: RpcServer.respondercallId: 2225 service: ClientService methodName: Multi size: 2.9m connection: 9.1.143.58:50316: output error
2014-06-30 19:57:08,886 WARN  [RpcServer.handler=47,port=60020] ipc.RpcServer: RpcServer.handler=47,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-06-30 19:57:08,901 WARN  [RpcServer.handler=42,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":15447,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.58:50316","starttimems":1404183413442,"queuetimems":3,"class":"HRegionServer","responsesize":16388,"method":"Multi"}
2014-06-30 19:57:08,902 WARN  [RpcServer.handler=42,port=60020] ipc.RpcServer: RpcServer.respondercallId: 2240 service: ClientService methodName: Multi size: 2.9m connection: 9.1.143.58:50316: output error
2014-06-30 19:57:08,902 WARN  [RpcServer.handler=42,port=60020] ipc.RpcServer: RpcServer.handler=42,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-06-30 19:57:08,911 WARN  [RpcServer.handler=46,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":15647,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.58:50316","starttimems":1404183413263,"queuetimems":0,"class":"HRegionServer","responsesize":16611,"method":"Multi"}
2014-06-30 19:57:08,911 WARN  [RpcServer.handler=46,port=60020] ipc.RpcServer: RpcServer.respondercallId: 2232 service: ClientService methodName: Multi size: 2.9m connection: 9.1.143.58:50316: output error
2014-06-30 19:57:08,912 WARN  [RpcServer.handler=46,port=60020] ipc.RpcServer: RpcServer.handler=46,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-06-30 19:57:08,940 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 3693 synced till here 3691
2014-06-30 19:57:09,143 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1404182803191/sceplus-vm48.almaden.ibm.com%2C60020%2C1404182803191.1404183413220 with entries=83, filesize=66.3m; new WAL /hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1404182803191/sceplus-vm48.almaden.ibm.com%2C60020%2C1404182803191.1404183428866
2014-06-30 19:57:09,143 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1404182803191/sceplus-vm48.almaden.ibm.com%2C60020%2C1404182803191.1404183390185
2014-06-30 19:57:09,143 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1404182803191/sceplus-vm48.almaden.ibm.com%2C60020%2C1404182803191.1404183399893
2014-06-30 19:57:11,501 INFO  [JvmPauseMonitor] util.JvmPauseMonitor: Detected pause in JVM or host machine (eg GC): pause of approximately 1803ms
GC pool 'ConcurrentMarkSweep' had collection(s): count=1 time=2155ms
2014-06-30 19:57:13,951 INFO  [JvmPauseMonitor] util.JvmPauseMonitor: Detected pause in JVM or host machine (eg GC): pause of approximately 1949ms
GC pool 'ConcurrentMarkSweep' had collection(s): count=1 time=2105ms
2014-06-30 19:57:16,536 INFO  [JvmPauseMonitor] util.JvmPauseMonitor: Detected pause in JVM or host machine (eg GC): pause of approximately 2084ms
GC pool 'ConcurrentMarkSweep' had collection(s): count=1 time=2165ms
2014-06-30 19:57:16,756 WARN  [RpcServer.handler=32,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":21796,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.58:50316","starttimems":1404183414960,"queuetimems":0,"class":"HRegionServer","responsesize":14121,"method":"Multi"}
2014-06-30 19:57:16,757 WARN  [RpcServer.handler=32,port=60020] ipc.RpcServer: RpcServer.respondercallId: 2260 service: ClientService methodName: Multi size: 2.5m connection: 9.1.143.58:50316: output error
2014-06-30 19:57:16,759 WARN  [RpcServer.handler=32,port=60020] ipc.RpcServer: RpcServer.handler=32,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-06-30 19:57:16,869 WARN  [RpcServer.handler=3,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":21898,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.58:50316","starttimems":1404183414970,"queuetimems":0,"class":"HRegionServer","responsesize":14416,"method":"Multi"}
2014-06-30 19:57:16,869 WARN  [RpcServer.handler=3,port=60020] ipc.RpcServer: RpcServer.respondercallId: 2258 service: ClientService methodName: Multi size: 2.5m connection: 9.1.143.58:50316: output error
2014-06-30 19:57:16,869 WARN  [RpcServer.handler=3,port=60020] ipc.RpcServer: RpcServer.handler=3,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-06-30 19:57:19,248 INFO  [JvmPauseMonitor] util.JvmPauseMonitor: Detected pause in JVM or host machine (eg GC): pause of approximately 2211ms
GC pool 'ConcurrentMarkSweep' had collection(s): count=1 time=2455ms
2014-06-30 19:57:21,671 INFO  [JvmPauseMonitor] util.JvmPauseMonitor: Detected pause in JVM or host machine (eg GC): pause of approximately 1922ms
GC pool 'ConcurrentMarkSweep' had collection(s): count=1 time=2119ms
2014-06-30 19:57:21,911 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-06-30 19:57:21,966 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 3803 synced till here 3779
2014-06-30 19:57:24,425 DEBUG [MemStoreFlusher.0] regionserver.MemStoreFlusher: Flush thread woke up because memory above low water=367.3m
2014-06-30 19:57:24,425 INFO  [MemStoreFlusher.0] regionserver.MemStoreFlusher: Flush of region usertable,user8,1404183160444.28938d267c1c61f7feb582b85e4c172f. due to global heap pressure
2014-06-30 19:57:24,425 DEBUG [MemStoreFlusher.0] regionserver.HRegion: Started memstore flush for usertable,user8,1404183160444.28938d267c1c61f7feb582b85e4c172f., current region memstore size 100.9m
2014-06-30 19:57:24,426 INFO  [RpcServer.handler=38,port=60020] regionserver.MemStoreFlusher: Blocking updates on sceplus-vm48.almaden.ibm.com,60020,1404182803191: the global memstore size 388.1m is >= than blocking 386.7m size
2014-06-30 19:57:24,426 DEBUG [MemStoreFlusher.1] regionserver.MemStoreFlusher: Flush thread woke up because memory above low water=367.3m
2014-06-30 19:57:24,427 INFO  [MemStoreFlusher.1] regionserver.MemStoreFlusher: Flush of region usertable,user5,1404183160444.1d66ac045cc0407c0fa426db565fcbba. due to global heap pressure
2014-06-30 19:57:24,427 DEBUG [MemStoreFlusher.1] regionserver.HRegion: Started memstore flush for usertable,user5,1404183160444.1d66ac045cc0407c0fa426db565fcbba., current region memstore size 99.6m
2014-06-30 19:57:24,427 INFO  [RpcServer.handler=6,port=60020] regionserver.MemStoreFlusher: Blocking updates on sceplus-vm48.almaden.ibm.com,60020,1404182803191: the global memstore size 388.1m is >= than blocking 386.7m size
2014-06-30 19:57:24,428 INFO  [RpcServer.handler=44,port=60020] regionserver.MemStoreFlusher: Blocking updates on sceplus-vm48.almaden.ibm.com,60020,1404182803191: the global memstore size 388.1m is >= than blocking 386.7m size
2014-06-30 19:57:24,428 INFO  [RpcServer.handler=16,port=60020] regionserver.MemStoreFlusher: Blocking updates on sceplus-vm48.almaden.ibm.com,60020,1404182803191: the global memstore size 388.1m is >= than blocking 386.7m size
2014-06-30 19:57:24,430 INFO  [RpcServer.handler=45,port=60020] regionserver.MemStoreFlusher: Blocking updates on sceplus-vm48.almaden.ibm.com,60020,1404182803191: the global memstore size 388.1m is >= than blocking 386.7m size
2014-06-30 19:57:24,430 INFO  [RpcServer.handler=11,port=60020] regionserver.MemStoreFlusher: Blocking updates on sceplus-vm48.almaden.ibm.com,60020,1404182803191: the global memstore size 388.1m is >= than blocking 386.7m size
2014-06-30 19:57:24,431 INFO  [RpcServer.handler=24,port=60020] regionserver.MemStoreFlusher: Blocking updates on sceplus-vm48.almaden.ibm.com,60020,1404182803191: the global memstore size 388.1m is >= than blocking 386.7m size
2014-06-30 19:57:24,431 INFO  [RpcServer.handler=3,port=60020] regionserver.MemStoreFlusher: Blocking updates on sceplus-vm48.almaden.ibm.com,60020,1404182803191: the global memstore size 388.1m is >= than blocking 386.7m size
2014-06-30 19:57:24,431 INFO  [RpcServer.handler=22,port=60020] regionserver.MemStoreFlusher: Blocking updates on sceplus-vm48.almaden.ibm.com,60020,1404182803191: the global memstore size 388.1m is >= than blocking 386.7m size
2014-06-30 19:57:24,431 INFO  [JvmPauseMonitor] util.JvmPauseMonitor: Detected pause in JVM or host machine (eg GC): pause of approximately 2259ms
GC pool 'ConcurrentMarkSweep' had collection(s): count=1 time=2557ms
2014-06-30 19:57:24,431 INFO  [RpcServer.handler=32,port=60020] regionserver.MemStoreFlusher: Blocking updates on sceplus-vm48.almaden.ibm.com,60020,1404182803191: the global memstore size 388.1m is >= than blocking 386.7m size
2014-06-30 19:57:24,431 INFO  [RpcServer.handler=35,port=60020] regionserver.MemStoreFlusher: Blocking updates on sceplus-vm48.almaden.ibm.com,60020,1404182803191: the global memstore size 388.1m is >= than blocking 386.7m size
2014-06-30 19:57:24,432 INFO  [RpcServer.handler=42,port=60020] regionserver.MemStoreFlusher: Blocking updates on sceplus-vm48.almaden.ibm.com,60020,1404182803191: the global memstore size 388.1m is >= than blocking 386.7m size
2014-06-30 19:57:24,433 INFO  [RpcServer.handler=4,port=60020] regionserver.MemStoreFlusher: Blocking updates on sceplus-vm48.almaden.ibm.com,60020,1404182803191: the global memstore size 388.1m is >= than blocking 386.7m size
2014-06-30 19:57:24,433 INFO  [RpcServer.handler=5,port=60020] regionserver.MemStoreFlusher: Blocking updates on sceplus-vm48.almaden.ibm.com,60020,1404182803191: the global memstore size 388.1m is >= than blocking 386.7m size
2014-06-30 19:57:24,434 INFO  [RpcServer.handler=34,port=60020] regionserver.MemStoreFlusher: Blocking updates on sceplus-vm48.almaden.ibm.com,60020,1404182803191: the global memstore size 388.1m is >= than blocking 386.7m size
2014-06-30 19:57:24,434 INFO  [RpcServer.handler=47,port=60020] regionserver.MemStoreFlusher: Blocking updates on sceplus-vm48.almaden.ibm.com,60020,1404182803191: the global memstore size 388.1m is >= than blocking 386.7m size
2014-06-30 19:57:24,435 INFO  [RpcServer.handler=23,port=60020] regionserver.MemStoreFlusher: Blocking updates on sceplus-vm48.almaden.ibm.com,60020,1404182803191: the global memstore size 388.1m is >= than blocking 386.7m size
2014-06-30 19:57:24,563 DEBUG [MemStoreFlusher.1] util.FSUtils: DFS Client does not support most favored nodes create; using default create
2014-06-30 19:57:27,184 INFO  [JvmPauseMonitor] util.JvmPauseMonitor: Detected pause in JVM or host machine (eg GC): pause of approximately 2252ms
GC pool 'ConcurrentMarkSweep' had collection(s): count=1 time=2578ms
2014-06-30 19:57:27,194 INFO  [RpcServer.handler=36,port=60020] regionserver.MemStoreFlusher: Blocking updates on sceplus-vm48.almaden.ibm.com,60020,1404182803191: the global memstore size 397.7m is >= than blocking 386.7m size
2014-06-30 19:57:27,196 INFO  [RpcServer.handler=13,port=60020] regionserver.MemStoreFlusher: Blocking updates on sceplus-vm48.almaden.ibm.com,60020,1404182803191: the global memstore size 400.7m is >= than blocking 386.7m size
2014-06-30 19:57:27,197 INFO  [RpcServer.handler=18,port=60020] regionserver.MemStoreFlusher: Blocking updates on sceplus-vm48.almaden.ibm.com,60020,1404182803191: the global memstore size 405.3m is >= than blocking 386.7m size
2014-06-30 19:57:27,197 INFO  [RpcServer.handler=7,port=60020] regionserver.MemStoreFlusher: Blocking updates on sceplus-vm48.almaden.ibm.com,60020,1404182803191: the global memstore size 406.9m is >= than blocking 386.7m size
2014-06-30 19:57:27,197 WARN  [RpcServer.handler=12,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":27408,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.58:50316","starttimems":1404183419789,"queuetimems":1,"class":"HRegionServer","responsesize":16114,"method":"Multi"}
2014-06-30 19:57:27,201 INFO  [RpcServer.handler=20,port=60020] regionserver.MemStoreFlusher: Blocking updates on sceplus-vm48.almaden.ibm.com,60020,1404182803191: the global memstore size 406.9m is >= than blocking 386.7m size
2014-06-30 19:57:27,202 WARN  [RpcServer.handler=12,port=60020] ipc.RpcServer: RpcServer.respondercallId: 2296 service: ClientService methodName: Multi size: 2.8m connection: 9.1.143.58:50316: output error
2014-06-30 19:57:27,202 WARN  [RpcServer.handler=12,port=60020] ipc.RpcServer: RpcServer.handler=12,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-06-30 19:57:27,202 WARN  [RpcServer.handler=31,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":31708,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.58:50316","starttimems":1404183415494,"queuetimems":0,"class":"HRegionServer","responsesize":16441,"method":"Multi"}
2014-06-30 19:57:27,201 WARN  [RpcServer.handler=39,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":31652,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.58:50316","starttimems":1404183415549,"queuetimems":0,"class":"HRegionServer","responsesize":16406,"method":"Multi"}
2014-06-30 19:57:27,201 WARN  [RpcServer.handler=27,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":32073,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.58:50316","starttimems":1404183415128,"queuetimems":0,"class":"HRegionServer","responsesize":15139,"method":"Multi"}
2014-06-30 19:57:27,200 WARN  [RpcServer.handler=30,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":32295,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.58:50316","starttimems":1404183414904,"queuetimems":1,"class":"HRegionServer","responsesize":16319,"method":"Multi"}
2014-06-30 19:57:27,202 WARN  [RpcServer.handler=31,port=60020] ipc.RpcServer: RpcServer.respondercallId: 2271 service: ClientService methodName: Multi size: 2.9m connection: 9.1.143.58:50316: output error
2014-06-30 19:57:27,203 WARN  [RpcServer.handler=31,port=60020] ipc.RpcServer: RpcServer.handler=31,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-06-30 19:57:27,203 WARN  [RpcServer.handler=30,port=60020] ipc.RpcServer: RpcServer.respondercallId: 2244 service: ClientService methodName: Multi size: 2.9m connection: 9.1.143.58:50316: output error
2014-06-30 19:57:27,203 WARN  [RpcServer.handler=30,port=60020] ipc.RpcServer: RpcServer.handler=30,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-06-30 19:57:27,203 WARN  [RpcServer.handler=27,port=60020] ipc.RpcServer: RpcServer.respondercallId: 2255 service: ClientService methodName: Multi size: 2.6m connection: 9.1.143.58:50316: output error
2014-06-30 19:57:27,203 WARN  [RpcServer.handler=27,port=60020] ipc.RpcServer: RpcServer.handler=27,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-06-30 19:57:27,204 WARN  [RpcServer.handler=39,port=60020] ipc.RpcServer: RpcServer.respondercallId: 2274 service: ClientService methodName: Multi size: 2.9m connection: 9.1.143.58:50316: output error
2014-06-30 19:57:27,202 INFO  [RpcServer.handler=48,port=60020] regionserver.MemStoreFlusher: Blocking updates on sceplus-vm48.almaden.ibm.com,60020,1404182803191: the global memstore size 414.8m is >= than blocking 386.7m size
2014-06-30 19:57:27,204 INFO  [RpcServer.handler=15,port=60020] regionserver.MemStoreFlusher: Blocking updates on sceplus-vm48.almaden.ibm.com,60020,1404182803191: the global memstore size 427.1m is >= than blocking 386.7m size
2014-06-30 19:57:27,205 INFO  [RpcServer.handler=37,port=60020] regionserver.MemStoreFlusher: Blocking updates on sceplus-vm48.almaden.ibm.com,60020,1404182803191: the global memstore size 430.3m is >= than blocking 386.7m size
2014-06-30 19:57:27,205 INFO  [RpcServer.handler=0,port=60020] regionserver.MemStoreFlusher: Blocking updates on sceplus-vm48.almaden.ibm.com,60020,1404182803191: the global memstore size 430.3m is >= than blocking 386.7m size
2014-06-30 19:57:27,205 INFO  [RpcServer.handler=19,port=60020] regionserver.MemStoreFlusher: Blocking updates on sceplus-vm48.almaden.ibm.com,60020,1404182803191: the global memstore size 430.3m is >= than blocking 386.7m size
2014-06-30 19:57:27,205 INFO  [RpcServer.handler=8,port=60020] regionserver.MemStoreFlusher: Blocking updates on sceplus-vm48.almaden.ibm.com,60020,1404182803191: the global memstore size 431.9m is >= than blocking 386.7m size
2014-06-30 19:57:27,206 INFO  [RpcServer.handler=28,port=60020] regionserver.MemStoreFlusher: Blocking updates on sceplus-vm48.almaden.ibm.com,60020,1404182803191: the global memstore size 433.1m is >= than blocking 386.7m size
2014-06-30 19:57:27,206 WARN  [RpcServer.handler=39,port=60020] ipc.RpcServer: RpcServer.handler=39,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-06-30 19:57:27,277 INFO  [RpcServer.handler=21,port=60020] regionserver.MemStoreFlusher: Blocking updates on sceplus-vm48.almaden.ibm.com,60020,1404182803191: the global memstore size 434.7m is >= than blocking 386.7m size
2014-06-30 19:57:27,288 DEBUG [MemStoreFlusher.0] util.FSUtils: DFS Client does not support most favored nodes create; using default create
2014-06-30 19:57:27,290 INFO  [RpcServer.handler=9,port=60020] regionserver.MemStoreFlusher: Blocking updates on sceplus-vm48.almaden.ibm.com,60020,1404182803191: the global memstore size 434.7m is >= than blocking 386.7m size
2014-06-30 19:57:27,324 INFO  [RpcServer.handler=26,port=60020] regionserver.MemStoreFlusher: Blocking updates on sceplus-vm48.almaden.ibm.com,60020,1404182803191: the global memstore size 434.7m is >= than blocking 386.7m size
2014-06-30 19:57:27,338 INFO  [RpcServer.handler=33,port=60020] regionserver.MemStoreFlusher: Blocking updates on sceplus-vm48.almaden.ibm.com,60020,1404182803191: the global memstore size 434.7m is >= than blocking 386.7m size
2014-06-30 19:57:27,343 INFO  [RpcServer.handler=46,port=60020] regionserver.MemStoreFlusher: Blocking updates on sceplus-vm48.almaden.ibm.com,60020,1404182803191: the global memstore size 434.7m is >= than blocking 386.7m size
2014-06-30 19:57:27,347 INFO  [RpcServer.handler=41,port=60020] regionserver.MemStoreFlusher: Blocking updates on sceplus-vm48.almaden.ibm.com,60020,1404182803191: the global memstore size 434.7m is >= than blocking 386.7m size
2014-06-30 19:57:27,351 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1404182803191/sceplus-vm48.almaden.ibm.com%2C60020%2C1404182803191.1404183428866 with entries=110, filesize=92.5m; new WAL /hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1404182803191/sceplus-vm48.almaden.ibm.com%2C60020%2C1404182803191.1404183441911
2014-06-30 19:57:27,358 INFO  [RpcServer.handler=29,port=60020] regionserver.MemStoreFlusher: Blocking updates on sceplus-vm48.almaden.ibm.com,60020,1404182803191: the global memstore size 434.7m is >= than blocking 386.7m size
2014-06-30 19:57:27,362 INFO  [RpcServer.handler=1,port=60020] regionserver.MemStoreFlusher: Blocking updates on sceplus-vm48.almaden.ibm.com,60020,1404182803191: the global memstore size 434.7m is >= than blocking 386.7m size
2014-06-30 19:57:27,374 INFO  [RpcServer.handler=25,port=60020] regionserver.MemStoreFlusher: Blocking updates on sceplus-vm48.almaden.ibm.com,60020,1404182803191: the global memstore size 434.7m is >= than blocking 386.7m size
2014-06-30 19:57:29,607 WARN  [RpcServer.handler=23,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5172ms
2014-06-30 19:57:29,607 WARN  [RpcServer.handler=11,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5177ms
2014-06-30 19:57:29,607 WARN  [RpcServer.handler=38,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5181ms
2014-06-30 19:57:29,607 WARN  [RpcServer.handler=6,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5180ms
2014-06-30 19:57:29,607 INFO  [JvmPauseMonitor] util.JvmPauseMonitor: Detected pause in JVM or host machine (eg GC): pause of approximately 1923ms
GC pool 'ConcurrentMarkSweep' had collection(s): count=1 time=2231ms
2014-06-30 19:57:29,608 WARN  [RpcServer.handler=16,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5180ms
2014-06-30 19:57:29,608 WARN  [RpcServer.handler=44,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5180ms
2014-06-30 19:57:29,608 WARN  [RpcServer.handler=47,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5174ms
2014-06-30 19:57:29,608 WARN  [RpcServer.handler=45,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5179ms
2014-06-30 19:57:29,608 WARN  [RpcServer.handler=24,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5177ms
2014-06-30 19:57:29,609 WARN  [RpcServer.handler=3,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5178ms
2014-06-30 19:57:29,610 WARN  [RpcServer.handler=22,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5181ms
2014-06-30 19:57:29,611 WARN  [RpcServer.handler=32,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5181ms
2014-06-30 19:57:29,612 WARN  [RpcServer.handler=35,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5180ms
2014-06-30 19:57:29,612 WARN  [RpcServer.handler=42,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5180ms
2014-06-30 19:57:29,612 WARN  [RpcServer.handler=4,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5179ms
2014-06-30 19:57:29,612 WARN  [RpcServer.handler=5,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5179ms
2014-06-30 19:57:29,612 WARN  [RpcServer.handler=34,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5178ms
2014-06-30 19:57:29,613 INFO  [RpcServer.handler=2,port=60020] regionserver.MemStoreFlusher: Blocking updates on sceplus-vm48.almaden.ibm.com,60020,1404182803191: the global memstore size 434.7m is >= than blocking 386.7m size
2014-06-30 19:57:29,613 INFO  [RpcServer.handler=49,port=60020] regionserver.MemStoreFlusher: Blocking updates on sceplus-vm48.almaden.ibm.com,60020,1404182803191: the global memstore size 434.7m is >= than blocking 386.7m size
2014-06-30 19:57:29,698 INFO  [RpcServer.handler=14,port=60020] regionserver.MemStoreFlusher: Blocking updates on sceplus-vm48.almaden.ibm.com,60020,1404182803191: the global memstore size 434.7m is >= than blocking 386.7m size
2014-06-30 19:57:29,704 INFO  [RpcServer.handler=12,port=60020] regionserver.MemStoreFlusher: Blocking updates on sceplus-vm48.almaden.ibm.com,60020,1404182803191: the global memstore size 434.7m is >= than blocking 386.7m size
2014-06-30 19:57:29,714 INFO  [RpcServer.handler=31,port=60020] regionserver.MemStoreFlusher: Blocking updates on sceplus-vm48.almaden.ibm.com,60020,1404182803191: the global memstore size 434.7m is >= than blocking 386.7m size
2014-06-30 19:57:29,725 INFO  [RpcServer.handler=30,port=60020] regionserver.MemStoreFlusher: Blocking updates on sceplus-vm48.almaden.ibm.com,60020,1404182803191: the global memstore size 434.7m is >= than blocking 386.7m size
2014-06-30 19:57:29,739 INFO  [RpcServer.handler=27,port=60020] regionserver.MemStoreFlusher: Blocking updates on sceplus-vm48.almaden.ibm.com,60020,1404182803191: the global memstore size 434.7m is >= than blocking 386.7m size
2014-06-30 19:57:29,749 INFO  [RpcServer.handler=10,port=60020] regionserver.MemStoreFlusher: Blocking updates on sceplus-vm48.almaden.ibm.com,60020,1404182803191: the global memstore size 440.1m is >= than blocking 386.7m size
2014-06-30 19:57:29,750 INFO  [RpcServer.handler=17,port=60020] regionserver.MemStoreFlusher: Blocking updates on sceplus-vm48.almaden.ibm.com,60020,1404182803191: the global memstore size 440.1m is >= than blocking 386.7m size
2014-06-30 19:57:29,750 INFO  [RpcServer.handler=43,port=60020] regionserver.MemStoreFlusher: Blocking updates on sceplus-vm48.almaden.ibm.com,60020,1404182803191: the global memstore size 440.1m is >= than blocking 386.7m size
2014-06-30 19:57:29,750 INFO  [RpcServer.handler=40,port=60020] regionserver.MemStoreFlusher: Blocking updates on sceplus-vm48.almaden.ibm.com,60020,1404182803191: the global memstore size 440.1m is >= than blocking 386.7m size
2014-06-30 19:57:32,138 INFO  [JvmPauseMonitor] util.JvmPauseMonitor: Detected pause in JVM or host machine (eg GC): pause of approximately 2030ms
GC pool 'ConcurrentMarkSweep' had collection(s): count=1 time=2184ms
2014-06-30 19:57:32,194 WARN  [RpcServer.handler=36,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-06-30 19:57:32,197 WARN  [RpcServer.handler=13,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-06-30 19:57:32,197 WARN  [RpcServer.handler=18,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-06-30 19:57:32,198 WARN  [RpcServer.handler=7,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5001ms
2014-06-30 19:57:32,202 WARN  [RpcServer.handler=20,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5005ms
2014-06-30 19:57:32,204 WARN  [RpcServer.handler=48,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5007ms
2014-06-30 19:57:32,204 WARN  [RpcServer.handler=15,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5003ms
2014-06-30 19:57:32,205 WARN  [RpcServer.handler=37,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-06-30 19:57:32,205 WARN  [RpcServer.handler=0,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5004ms
2014-06-30 19:57:32,206 WARN  [RpcServer.handler=19,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5006ms
2014-06-30 19:57:32,206 WARN  [RpcServer.handler=28,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5007ms
2014-06-30 19:57:32,206 WARN  [RpcServer.handler=8,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5007ms
2014-06-30 19:57:32,277 WARN  [RpcServer.handler=21,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-06-30 19:57:32,291 WARN  [RpcServer.handler=9,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5001ms
2014-06-30 19:57:32,325 WARN  [RpcServer.handler=26,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-06-30 19:57:32,338 WARN  [RpcServer.handler=33,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-06-30 19:57:32,344 WARN  [RpcServer.handler=46,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5001ms
2014-06-30 19:57:32,348 WARN  [RpcServer.handler=41,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5001ms
2014-06-30 19:57:32,466 WARN  [RpcServer.handler=25,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5092ms
2014-06-30 19:57:32,466 WARN  [RpcServer.handler=1,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5104ms
2014-06-30 19:57:32,467 WARN  [RpcServer.handler=29,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5109ms
2014-06-30 19:57:32,469 INFO  [MemStoreFlusher.1] regionserver.DefaultStoreFlusher: Flushed, sequenceid=817, memsize=50.3m, hasBloomFilter=true, into tmp file hdfs://master:54310/hbase/data/default/usertable/1d66ac045cc0407c0fa426db565fcbba/.tmp/3f20a353a3524604b41b751b155aafe9
2014-06-30 19:57:32,477 INFO  [RpcServer.handler=39,port=60020] regionserver.MemStoreFlusher: Blocking updates on sceplus-vm48.almaden.ibm.com,60020,1404182803191: the global memstore size 440.1m is >= than blocking 386.7m size
2014-06-30 19:57:32,487 DEBUG [MemStoreFlusher.1] regionserver.HRegionFileSystem: Committing store file hdfs://master:54310/hbase/data/default/usertable/1d66ac045cc0407c0fa426db565fcbba/.tmp/3f20a353a3524604b41b751b155aafe9 as hdfs://master:54310/hbase/data/default/usertable/1d66ac045cc0407c0fa426db565fcbba/family/3f20a353a3524604b41b751b155aafe9
2014-06-30 19:57:32,494 INFO  [MemStoreFlusher.1] regionserver.HStore: Added hdfs://master:54310/hbase/data/default/usertable/1d66ac045cc0407c0fa426db565fcbba/family/3f20a353a3524604b41b751b155aafe9, entries=183260, sequenceid=817, filesize=27.3m
2014-06-30 19:57:32,494 INFO  [MemStoreFlusher.1] regionserver.HRegion: Finished memstore flush of ~99.6m/104469600, currentsize=0.0/0 for region usertable,user5,1404183160444.1d66ac045cc0407c0fa426db565fcbba. in 8067ms, sequenceid=817, compaction requested=true
2014-06-30 19:57:32,494 DEBUG [MemStoreFlusher.1] regionserver.CompactSplitThread: Small Compaction requested: system; Because: MemStoreFlusher.1; compaction_queue=(0:4), split_queue=0, merge_queue=0
2014-06-30 19:57:32,495 WARN  [RpcServer.handler=39,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 18ms
2014-06-30 19:57:32,495 INFO  [RpcServer.handler=39,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server sceplus-vm48.almaden.ibm.com,60020,1404182803191
2014-06-30 19:57:32,495 WARN  [RpcServer.handler=29,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5137ms
2014-06-30 19:57:32,495 INFO  [RpcServer.handler=29,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server sceplus-vm48.almaden.ibm.com,60020,1404182803191
2014-06-30 19:57:32,495 WARN  [RpcServer.handler=1,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5133ms
2014-06-30 19:57:32,495 INFO  [RpcServer.handler=1,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server sceplus-vm48.almaden.ibm.com,60020,1404182803191
2014-06-30 19:57:32,496 WARN  [RpcServer.handler=25,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5121ms
2014-06-30 19:57:32,496 INFO  [RpcServer.handler=25,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server sceplus-vm48.almaden.ibm.com,60020,1404182803191
2014-06-30 19:57:32,498 WARN  [RpcServer.handler=41,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5151ms
2014-06-30 19:57:32,498 INFO  [RpcServer.handler=41,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server sceplus-vm48.almaden.ibm.com,60020,1404182803191
2014-06-30 19:57:32,499 WARN  [RpcServer.handler=46,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5156ms
2014-06-30 19:57:32,499 INFO  [RpcServer.handler=46,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server sceplus-vm48.almaden.ibm.com,60020,1404182803191
2014-06-30 19:57:32,499 WARN  [RpcServer.handler=33,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5161ms
2014-06-30 19:57:32,499 INFO  [RpcServer.handler=33,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server sceplus-vm48.almaden.ibm.com,60020,1404182803191
2014-06-30 19:57:32,502 WARN  [RpcServer.handler=26,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5178ms
2014-06-30 19:57:32,502 INFO  [RpcServer.handler=26,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server sceplus-vm48.almaden.ibm.com,60020,1404182803191
2014-06-30 19:57:32,502 WARN  [RpcServer.handler=9,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5212ms
2014-06-30 19:57:32,502 INFO  [RpcServer.handler=9,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server sceplus-vm48.almaden.ibm.com,60020,1404182803191
2014-06-30 19:57:32,505 WARN  [RpcServer.handler=21,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5228ms
2014-06-30 19:57:32,505 INFO  [RpcServer.handler=21,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server sceplus-vm48.almaden.ibm.com,60020,1404182803191
2014-06-30 19:57:32,505 WARN  [RpcServer.handler=8,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5306ms
2014-06-30 19:57:32,505 INFO  [RpcServer.handler=8,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server sceplus-vm48.almaden.ibm.com,60020,1404182803191
2014-06-30 19:57:32,506 WARN  [RpcServer.handler=28,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5307ms
2014-06-30 19:57:32,506 INFO  [RpcServer.handler=28,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server sceplus-vm48.almaden.ibm.com,60020,1404182803191
2014-06-30 19:57:32,506 WARN  [RpcServer.handler=19,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5306ms
2014-06-30 19:57:32,506 INFO  [RpcServer.handler=19,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server sceplus-vm48.almaden.ibm.com,60020,1404182803191
2014-06-30 19:57:32,506 WARN  [RpcServer.handler=0,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5305ms
2014-06-30 19:57:32,506 INFO  [RpcServer.handler=0,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server sceplus-vm48.almaden.ibm.com,60020,1404182803191
2014-06-30 19:57:32,506 WARN  [RpcServer.handler=37,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5301ms
2014-06-30 19:57:32,506 INFO  [RpcServer.handler=37,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server sceplus-vm48.almaden.ibm.com,60020,1404182803191
2014-06-30 19:57:32,506 WARN  [RpcServer.handler=15,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5305ms
2014-06-30 19:57:32,506 INFO  [RpcServer.handler=15,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server sceplus-vm48.almaden.ibm.com,60020,1404182803191
2014-06-30 19:57:32,507 WARN  [RpcServer.handler=48,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5310ms
2014-06-30 19:57:32,507 INFO  [RpcServer.handler=48,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server sceplus-vm48.almaden.ibm.com,60020,1404182803191
2014-06-30 19:57:32,508 WARN  [RpcServer.handler=20,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5311ms
2014-06-30 19:57:32,508 INFO  [RpcServer.handler=20,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server sceplus-vm48.almaden.ibm.com,60020,1404182803191
2014-06-30 19:57:32,508 WARN  [RpcServer.handler=7,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5311ms
2014-06-30 19:57:32,508 INFO  [RpcServer.handler=7,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server sceplus-vm48.almaden.ibm.com,60020,1404182803191
2014-06-30 19:57:32,508 WARN  [RpcServer.handler=18,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5311ms
2014-06-30 19:57:32,508 INFO  [RpcServer.handler=18,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server sceplus-vm48.almaden.ibm.com,60020,1404182803191
2014-06-30 19:57:32,509 WARN  [RpcServer.handler=13,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5313ms
2014-06-30 19:57:32,509 INFO  [RpcServer.handler=13,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server sceplus-vm48.almaden.ibm.com,60020,1404182803191
2014-06-30 19:57:32,509 WARN  [RpcServer.handler=36,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5315ms
2014-06-30 19:57:32,509 INFO  [RpcServer.handler=36,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server sceplus-vm48.almaden.ibm.com,60020,1404182803191
2014-06-30 19:57:32,516 WARN  [RpcServer.handler=40,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 2760ms
2014-06-30 19:57:32,518 INFO  [RpcServer.handler=40,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server sceplus-vm48.almaden.ibm.com,60020,1404182803191
2014-06-30 19:57:32,518 WARN  [RpcServer.handler=43,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 2768ms
2014-06-30 19:57:32,519 INFO  [RpcServer.handler=43,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server sceplus-vm48.almaden.ibm.com,60020,1404182803191
2014-06-30 19:57:32,519 WARN  [RpcServer.handler=17,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 2769ms
2014-06-30 19:57:32,519 INFO  [RpcServer.handler=17,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server sceplus-vm48.almaden.ibm.com,60020,1404182803191
2014-06-30 19:57:32,519 WARN  [RpcServer.handler=10,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 2770ms
2014-06-30 19:57:32,519 INFO  [RpcServer.handler=10,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server sceplus-vm48.almaden.ibm.com,60020,1404182803191
2014-06-30 19:57:32,523 WARN  [RpcServer.handler=27,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 2780ms
2014-06-30 19:57:32,528 INFO  [RpcServer.handler=27,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server sceplus-vm48.almaden.ibm.com,60020,1404182803191
2014-06-30 19:57:32,534 WARN  [RpcServer.handler=30,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 2811ms
2014-06-30 19:57:32,537 INFO  [RpcServer.handler=30,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server sceplus-vm48.almaden.ibm.com,60020,1404182803191
2014-06-30 19:57:32,539 WARN  [RpcServer.handler=31,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 2825ms
2014-06-30 19:57:32,543 INFO  [RpcServer.handler=31,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server sceplus-vm48.almaden.ibm.com,60020,1404182803191
2014-06-30 19:57:32,543 WARN  [RpcServer.handler=12,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 2839ms
2014-06-30 19:57:32,543 INFO  [RpcServer.handler=12,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server sceplus-vm48.almaden.ibm.com,60020,1404182803191
2014-06-30 19:57:32,543 WARN  [RpcServer.handler=14,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 2936ms
2014-06-30 19:57:32,544 INFO  [RpcServer.handler=14,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server sceplus-vm48.almaden.ibm.com,60020,1404182803191
2014-06-30 19:57:32,544 WARN  [RpcServer.handler=49,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 2937ms
2014-06-30 19:57:32,566 INFO  [RpcServer.handler=49,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server sceplus-vm48.almaden.ibm.com,60020,1404182803191
2014-06-30 19:57:32,571 WARN  [RpcServer.handler=2,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 2954ms
2014-06-30 19:57:32,571 INFO  [RpcServer.handler=2,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server sceplus-vm48.almaden.ibm.com,60020,1404182803191
2014-06-30 19:57:32,571 WARN  [RpcServer.handler=34,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 8137ms
2014-06-30 19:57:32,572 INFO  [RpcServer.handler=34,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server sceplus-vm48.almaden.ibm.com,60020,1404182803191
2014-06-30 19:57:32,572 WARN  [RpcServer.handler=5,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 8139ms
2014-06-30 19:57:32,572 INFO  [RpcServer.handler=5,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server sceplus-vm48.almaden.ibm.com,60020,1404182803191
2014-06-30 19:57:32,573 WARN  [RpcServer.handler=4,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 8139ms
2014-06-30 19:57:32,573 INFO  [RpcServer.handler=4,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server sceplus-vm48.almaden.ibm.com,60020,1404182803191
2014-06-30 19:57:32,573 WARN  [RpcServer.handler=42,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 8141ms
2014-06-30 19:57:32,573 INFO  [RpcServer.handler=42,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server sceplus-vm48.almaden.ibm.com,60020,1404182803191
2014-06-30 19:57:32,574 WARN  [RpcServer.handler=35,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 8143ms
2014-06-30 19:57:32,574 INFO  [RpcServer.handler=35,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server sceplus-vm48.almaden.ibm.com,60020,1404182803191
2014-06-30 19:57:32,575 WARN  [RpcServer.handler=32,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 8145ms
2014-06-30 19:57:32,575 INFO  [RpcServer.handler=32,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server sceplus-vm48.almaden.ibm.com,60020,1404182803191
2014-06-30 19:57:32,575 WARN  [RpcServer.handler=22,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 8146ms
2014-06-30 19:57:32,575 INFO  [RpcServer.handler=22,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server sceplus-vm48.almaden.ibm.com,60020,1404182803191
2014-06-30 19:57:32,575 WARN  [RpcServer.handler=3,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 8145ms
2014-06-30 19:57:32,575 INFO  [RpcServer.handler=3,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server sceplus-vm48.almaden.ibm.com,60020,1404182803191
2014-06-30 19:57:32,576 WARN  [RpcServer.handler=24,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 8144ms
2014-06-30 19:57:32,576 INFO  [RpcServer.handler=24,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server sceplus-vm48.almaden.ibm.com,60020,1404182803191
2014-06-30 19:57:32,576 WARN  [RpcServer.handler=45,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 8147ms
2014-06-30 19:57:32,577 INFO  [RpcServer.handler=45,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server sceplus-vm48.almaden.ibm.com,60020,1404182803191
2014-06-30 19:57:32,577 WARN  [RpcServer.handler=47,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 8143ms
2014-06-30 19:57:32,577 INFO  [RpcServer.handler=47,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server sceplus-vm48.almaden.ibm.com,60020,1404182803191
2014-06-30 19:57:32,577 WARN  [RpcServer.handler=44,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 8149ms
2014-06-30 19:57:32,595 INFO  [RpcServer.handler=44,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server sceplus-vm48.almaden.ibm.com,60020,1404182803191
2014-06-30 19:57:32,595 WARN  [RpcServer.handler=16,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 8167ms
2014-06-30 19:57:32,604 INFO  [RpcServer.handler=16,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server sceplus-vm48.almaden.ibm.com,60020,1404182803191
2014-06-30 19:57:32,605 WARN  [RpcServer.handler=6,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 8178ms
2014-06-30 19:57:32,607 INFO  [RpcServer.handler=6,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server sceplus-vm48.almaden.ibm.com,60020,1404182803191
2014-06-30 19:57:32,620 WARN  [RpcServer.handler=38,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 8194ms
2014-06-30 19:57:32,620 INFO  [RpcServer.handler=38,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server sceplus-vm48.almaden.ibm.com,60020,1404182803191
2014-06-30 19:57:32,624 WARN  [RpcServer.handler=11,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 8194ms
2014-06-30 19:57:32,624 INFO  [RpcServer.handler=11,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server sceplus-vm48.almaden.ibm.com,60020,1404182803191
2014-06-30 19:57:32,624 WARN  [RpcServer.handler=23,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 8189ms
2014-06-30 19:57:32,625 INFO  [RpcServer.handler=23,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server sceplus-vm48.almaden.ibm.com,60020,1404182803191
2014-06-30 19:57:34,788 INFO  [JvmPauseMonitor] util.JvmPauseMonitor: Detected pause in JVM or host machine (eg GC): pause of approximately 1648ms
GC pool 'ConcurrentMarkSweep' had collection(s): count=1 time=2072ms
2014-06-30 19:57:34,973 WARN  [RpcServer.handler=29,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":37053,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.58:50316","starttimems":1404183417920,"queuetimems":0,"class":"HRegionServer","responsesize":15022,"method":"Multi"}
2014-06-30 19:57:34,973 WARN  [RpcServer.handler=1,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":37065,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.58:50316","starttimems":1404183417908,"queuetimems":1,"class":"HRegionServer","responsesize":16091,"method":"Multi"}
2014-06-30 19:57:34,973 WARN  [RpcServer.handler=41,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":36949,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.58:50316","starttimems":1404183418024,"queuetimems":1,"class":"HRegionServer","responsesize":16319,"method":"Multi"}
2014-06-30 19:57:34,974 WARN  [RpcServer.handler=29,port=60020] ipc.RpcServer: RpcServer.respondercallId: 2290 service: ClientService methodName: Multi size: 2.6m connection: 9.1.143.58:50316: output error
2014-06-30 19:57:34,974 WARN  [RpcServer.handler=25,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":39466,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.58:50316","starttimems":1404183415507,"queuetimems":0,"class":"HRegionServer","responsesize":16409,"method":"Multi"}
2014-06-30 19:57:34,974 WARN  [RpcServer.handler=29,port=60020] ipc.RpcServer: RpcServer.handler=29,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-06-30 19:57:34,974 WARN  [RpcServer.handler=25,port=60020] ipc.RpcServer: RpcServer.respondercallId: 2270 service: ClientService methodName: Multi size: 2.9m connection: 9.1.143.58:50316: output error
2014-06-30 19:57:34,974 WARN  [RpcServer.handler=25,port=60020] ipc.RpcServer: RpcServer.handler=25,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-06-30 19:57:34,975 WARN  [RpcServer.handler=1,port=60020] ipc.RpcServer: RpcServer.respondercallId: 2291 service: ClientService methodName: Multi size: 2.8m connection: 9.1.143.58:50316: output error
2014-06-30 19:57:34,975 WARN  [RpcServer.handler=1,port=60020] ipc.RpcServer: RpcServer.handler=1,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-06-30 19:57:34,983 WARN  [RpcServer.handler=41,port=60020] ipc.RpcServer: RpcServer.respondercallId: 2280 service: ClientService methodName: Multi size: 2.9m connection: 9.1.143.58:50316: output error
2014-06-30 19:57:34,983 WARN  [RpcServer.handler=41,port=60020] ipc.RpcServer: RpcServer.handler=41,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-06-30 19:57:37,400 INFO  [JvmPauseMonitor] util.JvmPauseMonitor: Detected pause in JVM or host machine (eg GC): pause of approximately 2112ms
GC pool 'ConcurrentMarkSweep' had collection(s): count=1 time=2313ms
2014-06-30 19:57:37,445 WARN  [RpcServer.handler=33,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":41961,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.58:50316","starttimems":1404183415483,"queuetimems":0,"class":"HRegionServer","responsesize":16487,"method":"Multi"}
2014-06-30 19:57:37,445 WARN  [RpcServer.handler=33,port=60020] ipc.RpcServer: RpcServer.respondercallId: 2273 service: ClientService methodName: Multi size: 2.9m connection: 9.1.143.58:50316: output error
2014-06-30 19:57:37,445 WARN  [RpcServer.handler=33,port=60020] ipc.RpcServer: RpcServer.handler=33,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-06-30 19:57:37,447 WARN  [RpcServer.handler=46,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":28535,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.58:50417","starttimems":1404183428912,"queuetimems":185,"class":"HRegionServer","responsesize":16552,"method":"Multi"}
2014-06-30 19:57:37,448 WARN  [RpcServer.handler=46,port=60020] ipc.RpcServer: RpcServer.respondercallId: 2368 service: ClientService methodName: Multi size: 2.9m connection: 9.1.143.58:50417: output error
2014-06-30 19:57:37,448 WARN  [RpcServer.handler=46,port=60020] ipc.RpcServer: RpcServer.handler=46,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-06-30 19:57:37,705 INFO  [MemStoreFlusher.0] regionserver.DefaultStoreFlusher: Flushed, sequenceid=771, memsize=105.5m, hasBloomFilter=true, into tmp file hdfs://master:54310/hbase/data/default/usertable/28938d267c1c61f7feb582b85e4c172f/.tmp/a09a880e6ea4499daa2125fe2ae81274
2014-06-30 19:57:37,721 DEBUG [MemStoreFlusher.0] regionserver.HRegionFileSystem: Committing store file hdfs://master:54310/hbase/data/default/usertable/28938d267c1c61f7feb582b85e4c172f/.tmp/a09a880e6ea4499daa2125fe2ae81274 as hdfs://master:54310/hbase/data/default/usertable/28938d267c1c61f7feb582b85e4c172f/family/a09a880e6ea4499daa2125fe2ae81274
2014-06-30 19:57:37,733 INFO  [MemStoreFlusher.0] regionserver.HStore: Added hdfs://master:54310/hbase/data/default/usertable/28938d267c1c61f7feb582b85e4c172f/family/a09a880e6ea4499daa2125fe2ae81274, entries=384130, sequenceid=771, filesize=57.3m
2014-06-30 19:57:37,733 INFO  [MemStoreFlusher.0] regionserver.HRegion: Finished memstore flush of ~108.7m/113949280, currentsize=9.5m/9987600 for region usertable,user8,1404183160444.28938d267c1c61f7feb582b85e4c172f. in 13308ms, sequenceid=771, compaction requested=true
2014-06-30 19:57:37,733 DEBUG [MemStoreFlusher.0] regionserver.CompactSplitThread: Small Compaction requested: system; Because: MemStoreFlusher.0; compaction_queue=(0:5), split_queue=0, merge_queue=0
2014-06-30 19:57:37,769 WARN  [RpcServer.handler=48,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":39898,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.58:50316","starttimems":1404183417870,"queuetimems":0,"class":"HRegionServer","responsesize":14121,"method":"Multi"}
2014-06-30 19:57:37,769 WARN  [RpcServer.handler=7,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":39770,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.58:50316","starttimems":1404183417998,"queuetimems":0,"class":"HRegionServer","responsesize":16589,"method":"Multi"}
2014-06-30 19:57:37,769 WARN  [RpcServer.handler=48,port=60020] ipc.RpcServer: RpcServer.respondercallId: 2279 service: ClientService methodName: Multi size: 2.5m connection: 9.1.143.58:50316: output error
2014-06-30 19:57:37,769 WARN  [RpcServer.handler=48,port=60020] ipc.RpcServer: RpcServer.handler=48,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-06-30 19:57:37,769 WARN  [RpcServer.handler=7,port=60020] ipc.RpcServer: RpcServer.respondercallId: 2282 service: ClientService methodName: Multi size: 2.9m connection: 9.1.143.58:50316: output error
2014-06-30 19:57:37,769 WARN  [RpcServer.handler=7,port=60020] ipc.RpcServer: RpcServer.handler=7,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-06-30 19:57:37,772 WARN  [RpcServer.handler=14,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":39812,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.58:50316","starttimems":1404183417960,"queuetimems":1,"class":"HRegionServer","responsesize":16210,"method":"Multi"}
2014-06-30 19:57:37,772 WARN  [RpcServer.handler=35,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":34167,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.58:50316","starttimems":1404183423605,"queuetimems":3773,"class":"HRegionServer","responsesize":16566,"method":"Multi"}
2014-06-30 19:57:37,773 WARN  [RpcServer.handler=14,port=60020] ipc.RpcServer: RpcServer.respondercallId: 2287 service: ClientService methodName: Multi size: 2.8m connection: 9.1.143.58:50316: output error
2014-06-30 19:57:37,773 WARN  [RpcServer.handler=14,port=60020] ipc.RpcServer: RpcServer.handler=14,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-06-30 19:57:37,773 WARN  [RpcServer.handler=35,port=60020] ipc.RpcServer: RpcServer.respondercallId: 2299 service: ClientService methodName: Multi size: 2.9m connection: 9.1.143.58:50316: output error
2014-06-30 19:57:37,773 WARN  [RpcServer.handler=35,port=60020] ipc.RpcServer: RpcServer.handler=35,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-06-30 19:57:37,774 WARN  [RpcServer.handler=19,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":37973,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.58:50316","starttimems":1404183419800,"queuetimems":0,"class":"HRegionServer","responsesize":15712,"method":"Multi"}
2014-06-30 19:57:37,774 WARN  [RpcServer.handler=13,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":42608,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.58:50316","starttimems":1404183415166,"queuetimems":1,"class":"HRegionServer","responsesize":16533,"method":"Multi"}
2014-06-30 19:57:37,772 WARN  [RpcServer.handler=6,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":39785,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.58:50316","starttimems":1404183417987,"queuetimems":1,"class":"HRegionServer","responsesize":16611,"method":"Multi"}
2014-06-30 19:57:37,774 WARN  [RpcServer.handler=9,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":42302,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.58:50316","starttimems":1404183415472,"queuetimems":291,"class":"HRegionServer","responsesize":15699,"method":"Multi"}
2014-06-30 19:57:37,774 WARN  [RpcServer.handler=19,port=60020] ipc.RpcServer: RpcServer.respondercallId: 2295 service: ClientService methodName: Multi size: 2.7m connection: 9.1.143.58:50316: output error
2014-06-30 19:57:37,774 WARN  [RpcServer.handler=19,port=60020] ipc.RpcServer: RpcServer.handler=19,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-06-30 19:57:37,774 WARN  [RpcServer.handler=6,port=60020] ipc.RpcServer: RpcServer.respondercallId: 2283 service: ClientService methodName: Multi size: 2.9m connection: 9.1.143.58:50316: output error
2014-06-30 19:57:37,774 WARN  [RpcServer.handler=6,port=60020] ipc.RpcServer: RpcServer.handler=6,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-06-30 19:57:37,775 WARN  [RpcServer.handler=15,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":34047,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.58:50316","starttimems":1404183423728,"queuetimems":3776,"class":"HRegionServer","responsesize":16408,"method":"Multi"}
2014-06-30 19:57:37,775 WARN  [RpcServer.handler=15,port=60020] ipc.RpcServer: RpcServer.respondercallId: 2302 service: ClientService methodName: Multi size: 2.9m connection: 9.1.143.58:50316: output error
2014-06-30 19:57:37,775 WARN  [RpcServer.handler=15,port=60020] ipc.RpcServer: RpcServer.handler=15,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-06-30 19:57:37,776 WARN  [RpcServer.handler=13,port=60020] ipc.RpcServer: RpcServer.respondercallId: 2251 service: ClientService methodName: Multi size: 2.9m connection: 9.1.143.58:50316: output error
2014-06-30 19:57:37,776 WARN  [RpcServer.handler=13,port=60020] ipc.RpcServer: RpcServer.handler=13,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-06-30 19:57:37,777 WARN  [RpcServer.handler=28,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":39844,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.58:50316","starttimems":1404183417932,"queuetimems":0,"class":"HRegionServer","responsesize":15139,"method":"Multi"}
2014-06-30 19:57:37,777 WARN  [RpcServer.handler=10,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":28907,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.58:50417","starttimems":1404183428869,"queuetimems":2723,"class":"HRegionServer","responsesize":16388,"method":"Multi"}
2014-06-30 19:57:37,777 WARN  [RpcServer.handler=10,port=60020] ipc.RpcServer: RpcServer.respondercallId: 2366 service: ClientService methodName: Multi size: 2.9m connection: 9.1.143.58:50417: output error
2014-06-30 19:57:37,777 WARN  [RpcServer.handler=18,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":39740,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.58:50316","starttimems":1404183418037,"queuetimems":1,"class":"HRegionServer","responsesize":16666,"method":"Multi"}
2014-06-30 19:57:37,778 WARN  [RpcServer.handler=9,port=60020] ipc.RpcServer: RpcServer.respondercallId: 2250 service: ClientService methodName: Multi size: 2.7m connection: 9.1.143.58:50316: output error
2014-06-30 19:57:37,779 WARN  [RpcServer.handler=9,port=60020] ipc.RpcServer: RpcServer.handler=9,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-06-30 19:57:37,782 WARN  [RpcServer.handler=10,port=60020] ipc.RpcServer: RpcServer.handler=10,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-06-30 19:57:37,783 WARN  [RpcServer.handler=28,port=60020] ipc.RpcServer: RpcServer.respondercallId: 2289 service: ClientService methodName: Multi size: 2.6m connection: 9.1.143.58:50316: output error
2014-06-30 19:57:37,784 WARN  [RpcServer.handler=20,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":37961,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.58:50316","starttimems":1404183419822,"queuetimems":0,"class":"HRegionServer","responsesize":16268,"method":"Multi"}
2014-06-30 19:57:37,787 WARN  [RpcServer.handler=28,port=60020] ipc.RpcServer: RpcServer.handler=28,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-06-30 19:57:37,787 WARN  [RpcServer.handler=18,port=60020] ipc.RpcServer: RpcServer.respondercallId: 2298 service: ClientService methodName: Multi size: 2.9m connection: 9.1.143.58:50316: output error
2014-06-30 19:57:37,787 WARN  [RpcServer.handler=18,port=60020] ipc.RpcServer: RpcServer.handler=18,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-06-30 19:57:37,787 WARN  [RpcServer.handler=20,port=60020] ipc.RpcServer: RpcServer.respondercallId: 2300 service: ClientService methodName: Multi size: 2.8m connection: 9.1.143.58:50316: output error
2014-06-30 19:57:37,787 WARN  [RpcServer.handler=20,port=60020] ipc.RpcServer: RpcServer.handler=20,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-06-30 19:57:37,788 WARN  [RpcServer.handler=36,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":42269,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.58:50316","starttimems":1404183415518,"queuetimems":1,"class":"HRegionServer","responsesize":16344,"method":"Multi"}
2014-06-30 19:57:37,788 WARN  [RpcServer.handler=36,port=60020] ipc.RpcServer: RpcServer.respondercallId: 2265 service: ClientService methodName: Multi size: 2.9m connection: 9.1.143.58:50316: output error
2014-06-30 19:57:37,788 WARN  [RpcServer.handler=21,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":42634,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.58:50316","starttimems":1404183415148,"queuetimems":0,"class":"HRegionServer","responsesize":15853,"method":"Multi"}
2014-06-30 19:57:37,788 WARN  [RpcServer.handler=5,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":39892,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.58:50316","starttimems":1404183417896,"queuetimems":1,"class":"HRegionServer","responsesize":14416,"method":"Multi"}
2014-06-30 19:57:37,788 WARN  [RpcServer.handler=36,port=60020] ipc.RpcServer: RpcServer.handler=36,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-06-30 19:57:37,789 WARN  [RpcServer.handler=37,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":34088,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.58:50316","starttimems":1404183423700,"queuetimems":3777,"class":"HRegionServer","responsesize":16406,"method":"Multi"}
2014-06-30 19:57:37,789 WARN  [RpcServer.handler=21,port=60020] ipc.RpcServer: RpcServer.respondercallId: 2253 service: ClientService methodName: Multi size: 2.8m connection: 9.1.143.58:50316: output error
2014-06-30 19:57:37,794 WARN  [RpcServer.handler=21,port=60020] ipc.RpcServer: RpcServer.handler=21,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-06-30 19:57:37,797 WARN  [RpcServer.handler=37,port=60020] ipc.RpcServer: RpcServer.respondercallId: 2306 service: ClientService methodName: Multi size: 2.9m connection: 9.1.143.58:50316: output error
2014-06-30 19:57:37,797 WARN  [RpcServer.handler=37,port=60020] ipc.RpcServer: RpcServer.handler=37,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-06-30 19:57:37,797 WARN  [RpcServer.handler=5,port=60020] ipc.RpcServer: RpcServer.respondercallId: 2292 service: ClientService methodName: Multi size: 2.5m connection: 9.1.143.58:50316: output error
2014-06-30 19:57:37,797 WARN  [RpcServer.handler=5,port=60020] ipc.RpcServer: RpcServer.handler=5,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-06-30 19:57:39,686 INFO  [JvmPauseMonitor] util.JvmPauseMonitor: Detected pause in JVM or host machine (eg GC): pause of approximately 1785ms
GC pool 'ConcurrentMarkSweep' had collection(s): count=1 time=1980ms
2014-06-30 19:57:39,801 WARN  [RpcServer.handler=17,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":44867,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.58:50316","starttimems":1404183414933,"queuetimems":1,"class":"HRegionServer","responsesize":16589,"method":"Multi"}
2014-06-30 19:57:39,801 WARN  [RpcServer.handler=34,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":44271,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.58:50316","starttimems":1404183415529,"queuetimems":0,"class":"HRegionServer","responsesize":16463,"method":"Multi"}
2014-06-30 19:57:39,801 WARN  [RpcServer.handler=2,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":30931,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.58:50417","starttimems":1404183428869,"queuetimems":2710,"class":"HRegionServer","responsesize":15022,"method":"Multi"}
2014-06-30 19:57:39,802 WARN  [RpcServer.handler=17,port=60020] ipc.RpcServer: RpcServer.respondercallId: 2246 service: ClientService methodName: Multi size: 2.9m connection: 9.1.143.58:50316: output error
2014-06-30 19:57:39,801 WARN  [RpcServer.handler=43,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":30931,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.58:50417","starttimems":1404183428869,"queuetimems":173,"class":"HRegionServer","responsesize":16406,"method":"Multi"}
2014-06-30 19:57:39,802 WARN  [RpcServer.handler=2,port=60020] ipc.RpcServer: RpcServer.respondercallId: 2365 service: ClientService methodName: Multi size: 2.6m connection: 9.1.143.58:50417: output error
2014-06-30 19:57:39,802 WARN  [RpcServer.handler=2,port=60020] ipc.RpcServer: RpcServer.handler=2,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-06-30 19:57:39,802 WARN  [RpcServer.handler=34,port=60020] ipc.RpcServer: RpcServer.respondercallId: 2276 service: ClientService methodName: Multi size: 2.9m connection: 9.1.143.58:50316: output error
2014-06-30 19:57:39,802 WARN  [RpcServer.handler=34,port=60020] ipc.RpcServer: RpcServer.handler=34,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-06-30 19:57:39,802 WARN  [RpcServer.handler=43,port=60020] ipc.RpcServer: RpcServer.respondercallId: 2367 service: ClientService methodName: Multi size: 2.9m connection: 9.1.143.58:50417: output error
2014-06-30 19:57:39,802 WARN  [RpcServer.handler=43,port=60020] ipc.RpcServer: RpcServer.handler=43,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-06-30 19:57:39,802 WARN  [RpcServer.handler=17,port=60020] ipc.RpcServer: RpcServer.handler=17,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-06-30 19:57:39,803 WARN  [RpcServer.handler=8,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":39991,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.58:50316","starttimems":1404183419812,"queuetimems":1,"class":"HRegionServer","responsesize":16409,"method":"Multi"}
2014-06-30 19:57:39,803 WARN  [RpcServer.handler=8,port=60020] ipc.RpcServer: RpcServer.respondercallId: 2294 service: ClientService methodName: Multi size: 2.9m connection: 9.1.143.58:50316: output error
2014-06-30 19:57:39,803 WARN  [RpcServer.handler=8,port=60020] ipc.RpcServer: RpcServer.handler=8,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-06-30 19:57:39,811 WARN  [RpcServer.handler=24,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":36206,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.58:50316","starttimems":1404183423605,"queuetimems":3762,"class":"HRegionServer","responsesize":16552,"method":"Multi"}
2014-06-30 19:57:39,811 WARN  [RpcServer.handler=45,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":44826,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.58:50316","starttimems":1404183414985,"queuetimems":0,"class":"HRegionServer","responsesize":15022,"method":"Multi"}
2014-06-30 19:57:39,812 WARN  [RpcServer.handler=49,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":36116,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.58:50316","starttimems":1404183423695,"queuetimems":3799,"class":"HRegionServer","responsesize":16463,"method":"Multi"}
2014-06-30 19:57:39,812 WARN  [RpcServer.handler=24,port=60020] ipc.RpcServer: RpcServer.respondercallId: 2309 service: ClientService methodName: Multi size: 2.9m connection: 9.1.143.58:50316: output error
2014-06-30 19:57:39,812 WARN  [RpcServer.handler=24,port=60020] ipc.RpcServer: RpcServer.handler=24,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-06-30 19:57:39,812 WARN  [RpcServer.handler=49,port=60020] ipc.RpcServer: RpcServer.respondercallId: 2307 service: ClientService methodName: Multi size: 2.9m connection: 9.1.143.58:50316: output error
2014-06-30 19:57:39,812 WARN  [RpcServer.handler=49,port=60020] ipc.RpcServer: RpcServer.handler=49,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-06-30 19:57:39,812 WARN  [RpcServer.handler=45,port=60020] ipc.RpcServer: RpcServer.respondercallId: 2256 service: ClientService methodName: Multi size: 2.6m connection: 9.1.143.58:50316: output error
2014-06-30 19:57:39,812 WARN  [RpcServer.handler=45,port=60020] ipc.RpcServer: RpcServer.handler=45,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-06-30 19:57:39,817 WARN  [RpcServer.handler=42,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":30915,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.58:50417","starttimems":1404183428902,"queuetimems":185,"class":"HRegionServer","responsesize":16463,"method":"Multi"}
2014-06-30 19:57:39,817 WARN  [RpcServer.handler=42,port=60020] ipc.RpcServer: RpcServer.respondercallId: 2369 service: ClientService methodName: Multi size: 2.9m connection: 9.1.143.58:50417: output error
2014-06-30 19:57:39,817 WARN  [RpcServer.handler=42,port=60020] ipc.RpcServer: RpcServer.handler=42,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-06-30 19:57:39,823 WARN  [RpcServer.handler=4,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":33689,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.58:50417","starttimems":1404183426134,"queuetimems":0,"class":"HRegionServer","responsesize":16487,"method":"Multi"}
2014-06-30 19:57:39,824 WARN  [RpcServer.handler=4,port=60020] ipc.RpcServer: RpcServer.respondercallId: 2364 service: ClientService methodName: Multi size: 2.9m connection: 9.1.143.58:50417: output error
2014-06-30 19:57:39,824 WARN  [RpcServer.handler=4,port=60020] ipc.RpcServer: RpcServer.handler=4,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-06-30 19:57:39,824 WARN  [RpcServer.handler=26,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":44284,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.58:50316","starttimems":1404183415540,"queuetimems":1,"class":"HRegionServer","responsesize":16552,"method":"Multi"}
2014-06-30 19:57:39,825 WARN  [RpcServer.handler=26,port=60020] ipc.RpcServer: RpcServer.respondercallId: 2275 service: ClientService methodName: Multi size: 2.9m connection: 9.1.143.58:50316: output error
2014-06-30 19:57:39,825 WARN  [RpcServer.handler=26,port=60020] ipc.RpcServer: RpcServer.handler=26,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-06-30 19:57:39,825 WARN  [RpcServer.handler=23,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":41814,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.58:50316","starttimems":1404183418011,"queuetimems":0,"class":"HRegionServer","responsesize":16388,"method":"Multi"}
2014-06-30 19:57:39,825 WARN  [RpcServer.handler=23,port=60020] ipc.RpcServer: RpcServer.respondercallId: 2281 service: ClientService methodName: Multi size: 2.9m connection: 9.1.143.58:50316: output error
2014-06-30 19:57:39,825 WARN  [RpcServer.handler=23,port=60020] ipc.RpcServer: RpcServer.handler=23,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-06-30 19:57:39,826 WARN  [RpcServer.handler=11,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":44875,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.58:50316","starttimems":1404183414951,"queuetimems":2,"class":"HRegionServer","responsesize":16210,"method":"Multi"}
2014-06-30 19:57:39,828 WARN  [RpcServer.handler=44,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":41883,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.58:50316","starttimems":1404183417944,"queuetimems":0,"class":"HRegionServer","responsesize":15699,"method":"Multi"}
2014-06-30 19:57:39,828 WARN  [RpcServer.handler=22,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":41943,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.58:50316","starttimems":1404183417884,"queuetimems":0,"class":"HRegionServer","responsesize":15853,"method":"Multi"}
2014-06-30 19:57:39,828 WARN  [RpcServer.handler=44,port=60020] ipc.RpcServer: RpcServer.respondercallId: 2288 service: ClientService methodName: Multi size: 2.7m connection: 9.1.143.58:50316: output error
2014-06-30 19:57:39,828 WARN  [RpcServer.handler=44,port=60020] ipc.RpcServer: RpcServer.handler=44,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-06-30 19:57:39,828 WARN  [RpcServer.handler=22,port=60020] ipc.RpcServer: RpcServer.respondercallId: 2293 service: ClientService methodName: Multi size: 2.8m connection: 9.1.143.58:50316: output error
2014-06-30 19:57:39,828 WARN  [RpcServer.handler=22,port=60020] ipc.RpcServer: RpcServer.handler=22,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-06-30 19:57:39,829 WARN  [RpcServer.handler=40,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":41857,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.58:50316","starttimems":1404183417972,"queuetimems":0,"class":"HRegionServer","responsesize":16344,"method":"Multi"}
2014-06-30 19:57:39,829 WARN  [RpcServer.handler=40,port=60020] ipc.RpcServer: RpcServer.respondercallId: 2286 service: ClientService methodName: Multi size: 2.9m connection: 9.1.143.58:50316: output error
2014-06-30 19:57:39,829 WARN  [RpcServer.handler=40,port=60020] ipc.RpcServer: RpcServer.handler=40,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-06-30 19:57:39,830 WARN  [RpcServer.handler=11,port=60020] ipc.RpcServer: RpcServer.respondercallId: 2248 service: ClientService methodName: Multi size: 2.8m connection: 9.1.143.58:50316: output error
2014-06-30 19:57:39,830 WARN  [RpcServer.handler=11,port=60020] ipc.RpcServer: RpcServer.handler=11,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-06-30 19:57:41,752 INFO  [JvmPauseMonitor] util.JvmPauseMonitor: Detected pause in JVM or host machine (eg GC): pause of approximately 1565ms
GC pool 'ParNew' had collection(s): count=1 time=0ms
GC pool 'ConcurrentMarkSweep' had collection(s): count=1 time=1894ms
2014-06-30 19:57:41,803 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-06-30 19:57:41,804 WARN  [RpcServer.handler=0,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":42027,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.58:50316","starttimems":1404183419777,"queuetimems":0,"class":"HRegionServer","responsesize":16791,"method":"Multi"}
2014-06-30 19:57:41,804 DEBUG [RpcServer.handler=39,port=60020] regionserver.HRegion: Flush requested on usertable,user7,1404183160444.994b975d2c4cd5bbe532bddfb5e4ac46.
2014-06-30 19:57:41,805 WARN  [RpcServer.handler=0,port=60020] ipc.RpcServer: RpcServer.respondercallId: 2297 service: ClientService methodName: Multi size: 2.9m connection: 9.1.143.58:50316: output error
2014-06-30 19:57:41,805 WARN  [RpcServer.handler=0,port=60020] ipc.RpcServer: RpcServer.handler=0,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-06-30 19:57:41,805 DEBUG [MemStoreFlusher.0] regionserver.HRegion: Started memstore flush for usertable,user7,1404183160444.994b975d2c4cd5bbe532bddfb5e4ac46., current region memstore size 128.6m
2014-06-30 19:57:41,806 WARN  [RpcServer.handler=47,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":32920,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.58:50417","starttimems":1404183428886,"queuetimems":180,"class":"HRegionServer","responsesize":16409,"method":"Multi"}
2014-06-30 19:57:41,806 WARN  [RpcServer.handler=38,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":38101,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.58:50316","starttimems":1404183423705,"queuetimems":3771,"class":"HRegionServer","responsesize":16487,"method":"Multi"}
2014-06-30 19:57:41,806 WARN  [RpcServer.handler=47,port=60020] ipc.RpcServer: RpcServer.respondercallId: 2370 service: ClientService methodName: Multi size: 2.9m connection: 9.1.143.58:50417: output error
2014-06-30 19:57:41,806 WARN  [RpcServer.handler=47,port=60020] ipc.RpcServer: RpcServer.handler=47,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-06-30 19:57:41,806 WARN  [RpcServer.handler=38,port=60020] ipc.RpcServer: RpcServer.respondercallId: 2305 service: ClientService methodName: Multi size: 2.9m connection: 9.1.143.58:50316: output error
2014-06-30 19:57:41,806 WARN  [RpcServer.handler=16,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":38189,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.58:50316","starttimems":1404183423617,"queuetimems":3764,"class":"HRegionServer","responsesize":16441,"method":"Multi"}
2014-06-30 19:57:41,806 WARN  [RpcServer.handler=38,port=60020] ipc.RpcServer: RpcServer.handler=38,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-06-30 19:57:41,807 WARN  [RpcServer.handler=16,port=60020] ipc.RpcServer: RpcServer.respondercallId: 2308 service: ClientService methodName: Multi size: 2.9m connection: 9.1.143.58:50316: output error
2014-06-30 19:57:41,807 WARN  [RpcServer.handler=16,port=60020] ipc.RpcServer: RpcServer.handler=16,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-06-30 19:57:41,825 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 3900 synced till here 3881
2014-06-30 19:57:41,908 WARN  [RpcServer.handler=32,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":25144,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.58:50424","starttimems":1404183436763,"queuetimems":89,"class":"HRegionServer","responsesize":16406,"method":"Multi"}
2014-06-30 19:57:41,908 WARN  [RpcServer.handler=32,port=60020] ipc.RpcServer: RpcServer.respondercallId: 2439 service: ClientService methodName: Multi size: 2.9m connection: 9.1.143.58:50424: output error
2014-06-30 19:57:41,908 WARN  [RpcServer.handler=3,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":25039,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.58:50424","starttimems":1404183436869,"queuetimems":167,"class":"HRegionServer","responsesize":16670,"method":"Multi"}
2014-06-30 19:57:41,908 WARN  [RpcServer.handler=32,port=60020] ipc.RpcServer: RpcServer.handler=32,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-06-30 19:57:41,909 WARN  [RpcServer.handler=3,port=60020] ipc.RpcServer: RpcServer.respondercallId: 2440 service: ClientService methodName: Multi size: 2.9m connection: 9.1.143.58:50424: output error
2014-06-30 19:57:41,910 WARN  [RpcServer.handler=3,port=60020] ipc.RpcServer: RpcServer.handler=3,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-06-30 19:57:41,915 DEBUG [MemStoreFlusher.1] regionserver.MemStoreFlusher: Flush thread woke up because memory above low water=367.3m
2014-06-30 19:57:41,915 INFO  [MemStoreFlusher.1] regionserver.MemStoreFlusher: Flush of region usertable,user2,1404183160444.5446f1f888398e97c0ec3cc30da5bf97. due to global heap pressure
2014-06-30 19:57:41,915 DEBUG [MemStoreFlusher.1] regionserver.HRegion: Started memstore flush for usertable,user2,1404183160444.5446f1f888398e97c0ec3cc30da5bf97., current region memstore size 115.6m
2014-06-30 19:57:41,952 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1404182803191/sceplus-vm48.almaden.ibm.com%2C60020%2C1404182803191.1404183441911 with entries=97, filesize=82.3m; new WAL /hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1404182803191/sceplus-vm48.almaden.ibm.com%2C60020%2C1404182803191.1404183461803
2014-06-30 19:57:42,253 DEBUG [MemStoreFlusher.0] util.FSUtils: DFS Client does not support most favored nodes create; using default create
2014-06-30 19:57:42,292 INFO  [RpcServer.handler=31,port=60020] regionserver.MemStoreFlusher: Blocking updates on sceplus-vm48.almaden.ibm.com,60020,1404182803191: the global memstore size 388.7m is >= than blocking 386.7m size
2014-06-30 19:57:42,294 INFO  [RpcServer.handler=12,port=60020] regionserver.MemStoreFlusher: Blocking updates on sceplus-vm48.almaden.ibm.com,60020,1404182803191: the global memstore size 396.3m is >= than blocking 386.7m size
2014-06-30 19:57:42,295 INFO  [RpcServer.handler=46,port=60020] regionserver.MemStoreFlusher: Blocking updates on sceplus-vm48.almaden.ibm.com,60020,1404182803191: the global memstore size 396.3m is >= than blocking 386.7m size
2014-06-30 19:57:42,296 INFO  [RpcServer.handler=36,port=60020] regionserver.MemStoreFlusher: Blocking updates on sceplus-vm48.almaden.ibm.com,60020,1404182803191: the global memstore size 396.3m is >= than blocking 386.7m size
2014-06-30 19:57:42,296 INFO  [RpcServer.handler=14,port=60020] regionserver.MemStoreFlusher: Blocking updates on sceplus-vm48.almaden.ibm.com,60020,1404182803191: the global memstore size 396.3m is >= than blocking 386.7m size
2014-06-30 19:57:42,297 WARN  [RpcServer.handler=30,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":12575,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.58:50434","starttimems":1404183449721,"queuetimems":0,"class":"HRegionServer","responsesize":15114,"method":"Multi"}
2014-06-30 19:57:42,297 WARN  [RpcServer.handler=30,port=60020] ipc.RpcServer: RpcServer.respondercallId: 2492 service: ClientService methodName: Multi size: 2.6m connection: 9.1.143.58:50434: output error
2014-06-30 19:57:42,297 WARN  [RpcServer.handler=30,port=60020] ipc.RpcServer: RpcServer.handler=30,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-06-30 19:57:42,298 INFO  [RpcServer.handler=27,port=60020] regionserver.MemStoreFlusher: Blocking updates on sceplus-vm48.almaden.ibm.com,60020,1404182803191: the global memstore size 404.3m is >= than blocking 386.7m size
2014-06-30 19:57:42,299 INFO  [RpcServer.handler=35,port=60020] regionserver.MemStoreFlusher: Blocking updates on sceplus-vm48.almaden.ibm.com,60020,1404182803191: the global memstore size 412.2m is >= than blocking 386.7m size
2014-06-30 19:57:42,300 INFO  [RpcServer.handler=20,port=60020] regionserver.MemStoreFlusher: Blocking updates on sceplus-vm48.almaden.ibm.com,60020,1404182803191: the global memstore size 412.2m is >= than blocking 386.7m size
2014-06-30 19:57:42,302 INFO  [RpcServer.handler=19,port=60020] regionserver.MemStoreFlusher: Blocking updates on sceplus-vm48.almaden.ibm.com,60020,1404182803191: the global memstore size 412.2m is >= than blocking 386.7m size
2014-06-30 19:57:42,303 INFO  [RpcServer.handler=13,port=60020] regionserver.MemStoreFlusher: Blocking updates on sceplus-vm48.almaden.ibm.com,60020,1404182803191: the global memstore size 412.2m is >= than blocking 386.7m size
2014-06-30 19:57:42,303 INFO  [RpcServer.handler=10,port=60020] regionserver.MemStoreFlusher: Blocking updates on sceplus-vm48.almaden.ibm.com,60020,1404182803191: the global memstore size 412.2m is >= than blocking 386.7m size
2014-06-30 19:57:42,303 INFO  [RpcServer.handler=18,port=60020] regionserver.MemStoreFlusher: Blocking updates on sceplus-vm48.almaden.ibm.com,60020,1404182803191: the global memstore size 412.2m is >= than blocking 386.7m size
2014-06-30 19:57:42,304 INFO  [RpcServer.handler=6,port=60020] regionserver.MemStoreFlusher: Blocking updates on sceplus-vm48.almaden.ibm.com,60020,1404182803191: the global memstore size 412.2m is >= than blocking 386.7m size
2014-06-30 19:57:42,305 INFO  [RpcServer.handler=48,port=60020] regionserver.MemStoreFlusher: Blocking updates on sceplus-vm48.almaden.ibm.com,60020,1404182803191: the global memstore size 412.2m is >= than blocking 386.7m size
2014-06-30 19:57:42,305 INFO  [RpcServer.handler=15,port=60020] regionserver.MemStoreFlusher: Blocking updates on sceplus-vm48.almaden.ibm.com,60020,1404182803191: the global memstore size 412.2m is >= than blocking 386.7m size
2014-06-30 19:57:42,305 INFO  [RpcServer.handler=7,port=60020] regionserver.MemStoreFlusher: Blocking updates on sceplus-vm48.almaden.ibm.com,60020,1404182803191: the global memstore size 412.2m is >= than blocking 386.7m size
2014-06-30 19:57:42,329 INFO  [RpcServer.handler=5,port=60020] regionserver.MemStoreFlusher: Blocking updates on sceplus-vm48.almaden.ibm.com,60020,1404182803191: the global memstore size 420.1m is >= than blocking 386.7m size
2014-06-30 19:57:42,407 DEBUG [MemStoreFlusher.1] util.FSUtils: DFS Client does not support most favored nodes create; using default create
2014-06-30 19:57:44,286 INFO  [JvmPauseMonitor] util.JvmPauseMonitor: Detected pause in JVM or host machine (eg GC): pause of approximately 1032ms
GC pool 'ParNew' had collection(s): count=1 time=0ms
GC pool 'ConcurrentMarkSweep' had collection(s): count=2 time=1418ms
2014-06-30 19:57:44,303 INFO  [RpcServer.handler=2,port=60020] regionserver.MemStoreFlusher: Blocking updates on sceplus-vm48.almaden.ibm.com,60020,1404182803191: the global memstore size 420.1m is >= than blocking 386.7m size
2014-06-30 19:57:44,464 INFO  [RpcServer.handler=43,port=60020] regionserver.MemStoreFlusher: Blocking updates on sceplus-vm48.almaden.ibm.com,60020,1404182803191: the global memstore size 420.1m is >= than blocking 386.7m size
2014-06-30 19:57:44,465 INFO  [RpcServer.handler=34,port=60020] regionserver.MemStoreFlusher: Blocking updates on sceplus-vm48.almaden.ibm.com,60020,1404182803191: the global memstore size 420.1m is >= than blocking 386.7m size
2014-06-30 19:57:44,509 INFO  [RpcServer.handler=17,port=60020] regionserver.MemStoreFlusher: Blocking updates on sceplus-vm48.almaden.ibm.com,60020,1404182803191: the global memstore size 420.1m is >= than blocking 386.7m size
2014-06-30 19:57:44,522 INFO  [RpcServer.handler=8,port=60020] regionserver.MemStoreFlusher: Blocking updates on sceplus-vm48.almaden.ibm.com,60020,1404182803191: the global memstore size 420.1m is >= than blocking 386.7m size
2014-06-30 19:57:44,531 INFO  [MemStoreFlusher.1] regionserver.DefaultStoreFlusher: Flushed, sequenceid=841, memsize=50.9m, hasBloomFilter=true, into tmp file hdfs://master:54310/hbase/data/default/usertable/5446f1f888398e97c0ec3cc30da5bf97/.tmp/766c1e753d09441e8a3d2ee485477048
2014-06-30 19:57:44,541 DEBUG [MemStoreFlusher.1] regionserver.HRegionFileSystem: Committing store file hdfs://master:54310/hbase/data/default/usertable/5446f1f888398e97c0ec3cc30da5bf97/.tmp/766c1e753d09441e8a3d2ee485477048 as hdfs://master:54310/hbase/data/default/usertable/5446f1f888398e97c0ec3cc30da5bf97/family/766c1e753d09441e8a3d2ee485477048
2014-06-30 19:57:44,541 INFO  [RpcServer.handler=24,port=60020] regionserver.MemStoreFlusher: Blocking updates on sceplus-vm48.almaden.ibm.com,60020,1404182803191: the global memstore size 420.1m is >= than blocking 386.7m size
2014-06-30 19:57:44,549 INFO  [MemStoreFlusher.1] regionserver.HStore: Added hdfs://master:54310/hbase/data/default/usertable/5446f1f888398e97c0ec3cc30da5bf97/family/766c1e753d09441e8a3d2ee485477048, entries=185340, sequenceid=841, filesize=27.6m
2014-06-30 19:57:44,549 INFO  [MemStoreFlusher.1] regionserver.HRegion: Finished memstore flush of ~120.3m/126150320, currentsize=0.0/0 for region usertable,user2,1404183160444.5446f1f888398e97c0ec3cc30da5bf97. in 2634ms, sequenceid=841, compaction requested=false
2014-06-30 19:57:44,549 WARN  [RpcServer.handler=24,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 8ms
2014-06-30 19:57:44,550 INFO  [RpcServer.handler=24,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server sceplus-vm48.almaden.ibm.com,60020,1404182803191
2014-06-30 19:57:44,550 WARN  [RpcServer.handler=8,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 28ms
2014-06-30 19:57:44,550 INFO  [RpcServer.handler=8,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server sceplus-vm48.almaden.ibm.com,60020,1404182803191
2014-06-30 19:57:44,550 WARN  [RpcServer.handler=17,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 41ms
2014-06-30 19:57:44,550 INFO  [RpcServer.handler=17,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server sceplus-vm48.almaden.ibm.com,60020,1404182803191
2014-06-30 19:57:44,550 WARN  [RpcServer.handler=34,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 85ms
2014-06-30 19:57:44,550 INFO  [RpcServer.handler=34,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server sceplus-vm48.almaden.ibm.com,60020,1404182803191
2014-06-30 19:57:44,550 WARN  [RpcServer.handler=43,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 86ms
2014-06-30 19:57:44,550 INFO  [RpcServer.handler=43,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server sceplus-vm48.almaden.ibm.com,60020,1404182803191
2014-06-30 19:57:44,550 WARN  [RpcServer.handler=2,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 248ms
2014-06-30 19:57:44,550 INFO  [RpcServer.handler=2,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server sceplus-vm48.almaden.ibm.com,60020,1404182803191
2014-06-30 19:57:44,550 WARN  [RpcServer.handler=5,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 2221ms
2014-06-30 19:57:44,550 INFO  [RpcServer.handler=5,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server sceplus-vm48.almaden.ibm.com,60020,1404182803191
2014-06-30 19:57:44,553 WARN  [RpcServer.handler=7,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 2247ms
2014-06-30 19:57:44,553 INFO  [RpcServer.handler=7,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server sceplus-vm48.almaden.ibm.com,60020,1404182803191
2014-06-30 19:57:44,553 WARN  [RpcServer.handler=15,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 2248ms
2014-06-30 19:57:44,553 INFO  [RpcServer.handler=15,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server sceplus-vm48.almaden.ibm.com,60020,1404182803191
2014-06-30 19:57:44,553 WARN  [RpcServer.handler=48,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 2248ms
2014-06-30 19:57:44,553 INFO  [RpcServer.handler=48,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server sceplus-vm48.almaden.ibm.com,60020,1404182803191
2014-06-30 19:57:44,553 WARN  [RpcServer.handler=6,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 2250ms
2014-06-30 19:57:44,553 INFO  [RpcServer.handler=6,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server sceplus-vm48.almaden.ibm.com,60020,1404182803191
2014-06-30 19:57:44,554 WARN  [RpcServer.handler=18,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 2250ms
2014-06-30 19:57:44,554 INFO  [RpcServer.handler=18,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server sceplus-vm48.almaden.ibm.com,60020,1404182803191
2014-06-30 19:57:44,554 WARN  [RpcServer.handler=10,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 2251ms
2014-06-30 19:57:44,555 INFO  [RpcServer.handler=10,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server sceplus-vm48.almaden.ibm.com,60020,1404182803191
2014-06-30 19:57:44,555 WARN  [RpcServer.handler=13,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 2252ms
2014-06-30 19:57:44,555 INFO  [RpcServer.handler=13,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server sceplus-vm48.almaden.ibm.com,60020,1404182803191
2014-06-30 19:57:44,555 WARN  [RpcServer.handler=19,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 2253ms
2014-06-30 19:57:44,555 INFO  [RpcServer.handler=19,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server sceplus-vm48.almaden.ibm.com,60020,1404182803191
2014-06-30 19:57:44,555 WARN  [RpcServer.handler=20,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 2255ms
2014-06-30 19:57:44,555 INFO  [RpcServer.handler=20,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server sceplus-vm48.almaden.ibm.com,60020,1404182803191
2014-06-30 19:57:44,555 WARN  [RpcServer.handler=35,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 2256ms
2014-06-30 19:57:44,555 INFO  [RpcServer.handler=35,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server sceplus-vm48.almaden.ibm.com,60020,1404182803191
2014-06-30 19:57:44,556 WARN  [RpcServer.handler=27,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 2263ms
2014-06-30 19:57:44,556 INFO  [RpcServer.handler=27,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server sceplus-vm48.almaden.ibm.com,60020,1404182803191
2014-06-30 19:57:44,562 WARN  [RpcServer.handler=14,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 2266ms
2014-06-30 19:57:44,562 INFO  [RpcServer.handler=14,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server sceplus-vm48.almaden.ibm.com,60020,1404182803191
2014-06-30 19:57:44,562 WARN  [RpcServer.handler=36,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 2266ms
2014-06-30 19:57:44,562 INFO  [RpcServer.handler=36,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server sceplus-vm48.almaden.ibm.com,60020,1404182803191
2014-06-30 19:57:44,562 WARN  [RpcServer.handler=46,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 2267ms
2014-06-30 19:57:44,562 INFO  [RpcServer.handler=46,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server sceplus-vm48.almaden.ibm.com,60020,1404182803191
2014-06-30 19:57:44,562 WARN  [RpcServer.handler=12,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 2268ms
2014-06-30 19:57:44,562 INFO  [RpcServer.handler=12,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server sceplus-vm48.almaden.ibm.com,60020,1404182803191
2014-06-30 19:57:44,563 WARN  [RpcServer.handler=31,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 2270ms
2014-06-30 19:57:44,563 INFO  [RpcServer.handler=31,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server sceplus-vm48.almaden.ibm.com,60020,1404182803191
2014-06-30 19:57:44,590 WARN  [RpcServer.handler=27,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":14852,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.58:50434","starttimems":1404183449737,"queuetimems":1,"class":"HRegionServer","responsesize":17167,"method":"Multi"}
2014-06-30 19:57:44,591 WARN  [RpcServer.handler=12,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":14888,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.58:50434","starttimems":1404183449702,"queuetimems":2,"class":"HRegionServer","responsesize":17169,"method":"Multi"}
2014-06-30 19:57:44,591 WARN  [RpcServer.handler=27,port=60020] ipc.RpcServer: RpcServer.respondercallId: 2491 service: ClientService methodName: Multi size: 3.0m connection: 9.1.143.58:50434: output error
2014-06-30 19:57:44,591 WARN  [RpcServer.handler=27,port=60020] ipc.RpcServer: RpcServer.handler=27,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-06-30 19:57:44,591 WARN  [RpcServer.handler=31,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":14880,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.58:50434","starttimems":1404183449711,"queuetimems":0,"class":"HRegionServer","responsesize":16760,"method":"Multi"}
2014-06-30 19:57:44,591 WARN  [RpcServer.handler=12,port=60020] ipc.RpcServer: RpcServer.respondercallId: 2488 service: ClientService methodName: Multi size: 3.0m connection: 9.1.143.58:50434: output error
2014-06-30 19:57:44,594 WARN  [RpcServer.handler=12,port=60020] ipc.RpcServer: RpcServer.handler=12,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-06-30 19:57:44,594 WARN  [RpcServer.handler=31,port=60020] ipc.RpcServer: RpcServer.respondercallId: 2489 service: ClientService methodName: Multi size: 2.9m connection: 9.1.143.58:50434: output error
2014-06-30 19:57:44,594 WARN  [RpcServer.handler=31,port=60020] ipc.RpcServer: RpcServer.handler=31,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-06-30 19:57:44,598 INFO  [MemStoreFlusher.0] regionserver.DefaultStoreFlusher: Flushed, sequenceid=822, memsize=79.3m, hasBloomFilter=true, into tmp file hdfs://master:54310/hbase/data/default/usertable/994b975d2c4cd5bbe532bddfb5e4ac46/.tmp/28f1cc4bb2c049c6b53ce07c6091748a
2014-06-30 19:57:44,610 DEBUG [MemStoreFlusher.0] regionserver.HRegionFileSystem: Committing store file hdfs://master:54310/hbase/data/default/usertable/994b975d2c4cd5bbe532bddfb5e4ac46/.tmp/28f1cc4bb2c049c6b53ce07c6091748a as hdfs://master:54310/hbase/data/default/usertable/994b975d2c4cd5bbe532bddfb5e4ac46/family/28f1cc4bb2c049c6b53ce07c6091748a
2014-06-30 19:57:44,844 INFO  [MemStoreFlusher.0] regionserver.HStore: Added hdfs://master:54310/hbase/data/default/usertable/994b975d2c4cd5bbe532bddfb5e4ac46/family/28f1cc4bb2c049c6b53ce07c6091748a, entries=288790, sequenceid=822, filesize=43.1m
2014-06-30 19:57:44,845 INFO  [MemStoreFlusher.0] regionserver.HRegion: Finished memstore flush of ~133.1m/139616240, currentsize=9.4m/9846480 for region usertable,user7,1404183160444.994b975d2c4cd5bbe532bddfb5e4ac46. in 3040ms, sequenceid=822, compaction requested=true
2014-06-30 19:57:44,845 DEBUG [MemStoreFlusher.0] regionserver.CompactSplitThread: Small Compaction requested: system; Because: MemStoreFlusher.0; compaction_queue=(0:6), split_queue=0, merge_queue=0
2014-06-30 19:57:46,491 INFO  [JvmPauseMonitor] util.JvmPauseMonitor: Detected pause in JVM or host machine (eg GC): pause of approximately 1185ms
GC pool 'ParNew' had collection(s): count=1 time=0ms
GC pool 'ConcurrentMarkSweep' had collection(s): count=1 time=1412ms
2014-06-30 19:57:46,579 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-06-30 19:57:46,592 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 3994 synced till here 3984
2014-06-30 19:57:46,654 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1404182803191/sceplus-vm48.almaden.ibm.com%2C60020%2C1404182803191.1404183461803 with entries=94, filesize=76.0m; new WAL /hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1404182803191/sceplus-vm48.almaden.ibm.com%2C60020%2C1404182803191.1404183466579
2014-06-30 19:57:46,655 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1404182803191/sceplus-vm48.almaden.ibm.com%2C60020%2C1404182803191.1404183428866
2014-06-30 19:57:47,089 DEBUG [RpcServer.handler=22,port=60020] regionserver.HRegion: Flush requested on usertable,user8,1404183160444.28938d267c1c61f7feb582b85e4c172f.
2014-06-30 19:57:47,090 DEBUG [MemStoreFlusher.1] regionserver.HRegion: Started memstore flush for usertable,user8,1404183160444.28938d267c1c61f7feb582b85e4c172f., current region memstore size 129.6m
2014-06-30 19:57:47,214 DEBUG [MemStoreFlusher.1] util.FSUtils: DFS Client does not support most favored nodes create; using default create
2014-06-30 19:57:47,904 INFO  [MemStoreFlusher.1] regionserver.DefaultStoreFlusher: Flushed, sequenceid=854, memsize=52.1m, hasBloomFilter=true, into tmp file hdfs://master:54310/hbase/data/default/usertable/28938d267c1c61f7feb582b85e4c172f/.tmp/ab1cdeebebb947f4a7ee24699bda4963
2014-06-30 19:57:47,913 DEBUG [MemStoreFlusher.1] regionserver.HRegionFileSystem: Committing store file hdfs://master:54310/hbase/data/default/usertable/28938d267c1c61f7feb582b85e4c172f/.tmp/ab1cdeebebb947f4a7ee24699bda4963 as hdfs://master:54310/hbase/data/default/usertable/28938d267c1c61f7feb582b85e4c172f/family/ab1cdeebebb947f4a7ee24699bda4963
2014-06-30 19:57:47,920 INFO  [MemStoreFlusher.1] regionserver.HStore: Added hdfs://master:54310/hbase/data/default/usertable/28938d267c1c61f7feb582b85e4c172f/family/ab1cdeebebb947f4a7ee24699bda4963, entries=189590, sequenceid=854, filesize=28.3m
2014-06-30 19:57:47,920 INFO  [MemStoreFlusher.1] regionserver.HRegion: Finished memstore flush of ~129.6m/135947520, currentsize=4.7m/4956480 for region usertable,user8,1404183160444.28938d267c1c61f7feb582b85e4c172f. in 830ms, sequenceid=854, compaction requested=true
2014-06-30 19:57:47,920 DEBUG [MemStoreFlusher.1] regionserver.CompactSplitThread: Small Compaction requested: system; Because: MemStoreFlusher.1; compaction_queue=(0:7), split_queue=0, merge_queue=0
2014-06-30 19:57:48,597 DEBUG [regionserver60020-smallCompactions-1404182848217] regionserver.HRegionFileSystem: Committing store file hdfs://master:54310/hbase/data/default/usertable/5446f1f888398e97c0ec3cc30da5bf97/.tmp/660c0dc2216e4474b31f69142339fcab as hdfs://master:54310/hbase/data/default/usertable/5446f1f888398e97c0ec3cc30da5bf97/family/660c0dc2216e4474b31f69142339fcab
2014-06-30 19:57:48,608 DEBUG [regionserver60020-smallCompactions-1404182848217] regionserver.HStore: Removing store files after compaction...
2014-06-30 19:57:48,613 DEBUG [regionserver60020-smallCompactions-1404182848217] backup.HFileArchiver: Finished archiving from class org.apache.hadoop.hbase.backup.HFileArchiver$FileableStoreFile, file:hdfs://master:54310/hbase/data/default/usertable/5446f1f888398e97c0ec3cc30da5bf97/family/a0169492de6b4f00aec2ed8adddb6971, to hdfs://master:54310/hbase/archive/data/default/usertable/5446f1f888398e97c0ec3cc30da5bf97/family/a0169492de6b4f00aec2ed8adddb6971
2014-06-30 19:57:48,615 DEBUG [regionserver60020-smallCompactions-1404182848217] backup.HFileArchiver: Finished archiving from class org.apache.hadoop.hbase.backup.HFileArchiver$FileableStoreFile, file:hdfs://master:54310/hbase/data/default/usertable/5446f1f888398e97c0ec3cc30da5bf97/family/d386e992ead34e91b89d6f9e2dda27c1, to hdfs://master:54310/hbase/archive/data/default/usertable/5446f1f888398e97c0ec3cc30da5bf97/family/d386e992ead34e91b89d6f9e2dda27c1
2014-06-30 19:57:48,616 DEBUG [regionserver60020-smallCompactions-1404182848217] backup.HFileArchiver: Finished archiving from class org.apache.hadoop.hbase.backup.HFileArchiver$FileableStoreFile, file:hdfs://master:54310/hbase/data/default/usertable/5446f1f888398e97c0ec3cc30da5bf97/family/896fee37320d41b9a9316a4e242fc409, to hdfs://master:54310/hbase/archive/data/default/usertable/5446f1f888398e97c0ec3cc30da5bf97/family/896fee37320d41b9a9316a4e242fc409
2014-06-30 19:57:48,618 DEBUG [regionserver60020-smallCompactions-1404182848217] backup.HFileArchiver: Finished archiving from class org.apache.hadoop.hbase.backup.HFileArchiver$FileableStoreFile, file:hdfs://master:54310/hbase/data/default/usertable/5446f1f888398e97c0ec3cc30da5bf97/family/2413196db49c47759aac46e5b35a303c, to hdfs://master:54310/hbase/archive/data/default/usertable/5446f1f888398e97c0ec3cc30da5bf97/family/2413196db49c47759aac46e5b35a303c
2014-06-30 19:57:48,620 DEBUG [regionserver60020-smallCompactions-1404182848217] backup.HFileArchiver: Finished archiving from class org.apache.hadoop.hbase.backup.HFileArchiver$FileableStoreFile, file:hdfs://master:54310/hbase/data/default/usertable/5446f1f888398e97c0ec3cc30da5bf97/family/bdf69bd317a745659be2bfe35c59232e, to hdfs://master:54310/hbase/archive/data/default/usertable/5446f1f888398e97c0ec3cc30da5bf97/family/bdf69bd317a745659be2bfe35c59232e
2014-06-30 19:57:48,620 INFO  [regionserver60020-smallCompactions-1404182848217] regionserver.HStore: Completed major compaction of 5 file(s) in family of usertable,user2,1404183160444.5446f1f888398e97c0ec3cc30da5bf97. into 660c0dc2216e4474b31f69142339fcab(size=230.7m), total size for store is 258.3m. This selection was in queue for 0sec, and took 45sec to execute.
2014-06-30 19:57:48,620 INFO  [regionserver60020-smallCompactions-1404182848217] regionserver.CompactSplitThread: Completed compaction: Request = regionName=usertable,user2,1404183160444.5446f1f888398e97c0ec3cc30da5bf97., storeName=family, fileCount=5, fileSize=312.9m, priority=5, time=3565835412463; duration=45sec
2014-06-30 19:57:48,620 DEBUG [regionserver60020-smallCompactions-1404182848217] regionserver.CompactSplitThread: CompactSplitThread Status: compaction_queue=(0:7), split_queue=0, merge_queue=0
2014-06-30 19:57:48,620 DEBUG [regionserver60020-smallCompactions-1404182848217] compactions.RatioBasedCompactionPolicy: Selecting compaction from 4 store files, 0 compacting, 4 eligible, 10 blocking
2014-06-30 19:57:48,621 DEBUG [regionserver60020-smallCompactions-1404182848217] compactions.ExploringCompactionPolicy: Exploring compaction algorithm has selected 3 files of size 112552258 starting at candidate #1 after considering 3 permutations with 1 in ratio
2014-06-30 19:57:48,621 DEBUG [regionserver60020-smallCompactions-1404182848217] regionserver.HStore: 1d66ac045cc0407c0fa426db565fcbba - family: Initiating minor compaction
2014-06-30 19:57:48,621 INFO  [regionserver60020-smallCompactions-1404182848217] regionserver.HRegion: Starting compaction on family in region usertable,user5,1404183160444.1d66ac045cc0407c0fa426db565fcbba.
2014-06-30 19:57:48,621 INFO  [regionserver60020-smallCompactions-1404182848217] regionserver.HStore: Starting compaction of 3 file(s) in family of usertable,user5,1404183160444.1d66ac045cc0407c0fa426db565fcbba. into tmpdir=hdfs://master:54310/hbase/data/default/usertable/1d66ac045cc0407c0fa426db565fcbba/.tmp, totalSize=107.3m
2014-06-30 19:57:48,621 DEBUG [regionserver60020-smallCompactions-1404182848217] compactions.Compactor: Compacting hdfs://master:54310/hbase/data/default/usertable/1d66ac045cc0407c0fa426db565fcbba/family/2aa952bdfe9a4c1da775b8b46dd807f3, keycount=15373, bloomtype=ROW, size=22.9m, encoding=NONE, seqNum=665
2014-06-30 19:57:48,621 DEBUG [regionserver60020-smallCompactions-1404182848217] compactions.Compactor: Compacting hdfs://master:54310/hbase/data/default/usertable/1d66ac045cc0407c0fa426db565fcbba/family/b48f46f83b7748339001cde0126e5bd5, keycount=38294, bloomtype=ROW, size=57.1m, encoding=NONE, seqNum=753
2014-06-30 19:57:48,621 DEBUG [regionserver60020-smallCompactions-1404182848217] compactions.Compactor: Compacting hdfs://master:54310/hbase/data/default/usertable/1d66ac045cc0407c0fa426db565fcbba/family/3f20a353a3524604b41b751b155aafe9, keycount=18326, bloomtype=ROW, size=27.3m, encoding=NONE, seqNum=817
2014-06-30 19:57:48,628 DEBUG [regionserver60020-smallCompactions-1404182848217] util.FSUtils: DFS Client does not support most favored nodes create; using default create
2014-06-30 19:57:48,775 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-06-30 19:57:48,793 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1404182803191/sceplus-vm48.almaden.ibm.com%2C60020%2C1404182803191.1404183466579 with entries=93, filesize=61.5m; new WAL /hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1404182803191/sceplus-vm48.almaden.ibm.com%2C60020%2C1404182803191.1404183468776
2014-06-30 19:57:51,588 INFO  [JvmPauseMonitor] util.JvmPauseMonitor: Detected pause in JVM or host machine (eg GC): pause of approximately 1020ms
GC pool 'ParNew' had collection(s): count=1 time=0ms
GC pool 'ConcurrentMarkSweep' had collection(s): count=1 time=1418ms
2014-06-30 19:57:51,671 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-06-30 19:57:51,690 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 4185 synced till here 4172
2014-06-30 19:57:51,761 DEBUG [MemStoreFlusher.0] regionserver.MemStoreFlusher: Flush thread woke up because memory above low water=367.3m
2014-06-30 19:57:51,761 INFO  [MemStoreFlusher.0] regionserver.MemStoreFlusher: Flush of region usertable,user5,1404183160444.1d66ac045cc0407c0fa426db565fcbba. due to global heap pressure
2014-06-30 19:57:51,761 DEBUG [MemStoreFlusher.0] regionserver.HRegion: Started memstore flush for usertable,user5,1404183160444.1d66ac045cc0407c0fa426db565fcbba., current region memstore size 109.2m
2014-06-30 19:57:51,763 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1404182803191/sceplus-vm48.almaden.ibm.com%2C60020%2C1404182803191.1404183468776 with entries=98, filesize=71.7m; new WAL /hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1404182803191/sceplus-vm48.almaden.ibm.com%2C60020%2C1404182803191.1404183471672
2014-06-30 19:57:52,144 INFO  [RpcServer.handler=48,port=60020] regionserver.MemStoreFlusher: Blocking updates on sceplus-vm48.almaden.ibm.com,60020,1404182803191: the global memstore size 391.0m is >= than blocking 386.7m size
2014-06-30 19:57:52,147 INFO  [RpcServer.handler=15,port=60020] regionserver.MemStoreFlusher: Blocking updates on sceplus-vm48.almaden.ibm.com,60020,1404182803191: the global memstore size 391.5m is >= than blocking 386.7m size
2014-06-30 19:57:52,147 INFO  [RpcServer.handler=6,port=60020] regionserver.MemStoreFlusher: Blocking updates on sceplus-vm48.almaden.ibm.com,60020,1404182803191: the global memstore size 391.5m is >= than blocking 386.7m size
2014-06-30 19:57:52,156 INFO  [RpcServer.handler=49,port=60020] regionserver.MemStoreFlusher: Blocking updates on sceplus-vm48.almaden.ibm.com,60020,1404182803191: the global memstore size 391.5m is >= than blocking 386.7m size
2014-06-30 19:57:52,173 INFO  [RpcServer.handler=12,port=60020] regionserver.MemStoreFlusher: Blocking updates on sceplus-vm48.almaden.ibm.com,60020,1404182803191: the global memstore size 396.3m is >= than blocking 386.7m size
2014-06-30 19:57:52,176 INFO  [RpcServer.handler=18,port=60020] regionserver.MemStoreFlusher: Blocking updates on sceplus-vm48.almaden.ibm.com,60020,1404182803191: the global memstore size 399.5m is >= than blocking 386.7m size
2014-06-30 19:57:52,182 INFO  [RpcServer.handler=42,port=60020] regionserver.MemStoreFlusher: Blocking updates on sceplus-vm48.almaden.ibm.com,60020,1404182803191: the global memstore size 399.5m is >= than blocking 386.7m size
2014-06-30 19:57:52,191 INFO  [RpcServer.handler=26,port=60020] regionserver.MemStoreFlusher: Blocking updates on sceplus-vm48.almaden.ibm.com,60020,1404182803191: the global memstore size 399.5m is >= than blocking 386.7m size
2014-06-30 19:57:52,199 INFO  [RpcServer.handler=31,port=60020] regionserver.MemStoreFlusher: Blocking updates on sceplus-vm48.almaden.ibm.com,60020,1404182803191: the global memstore size 404.3m is >= than blocking 386.7m size
2014-06-30 19:57:52,199 INFO  [RpcServer.handler=19,port=60020] regionserver.MemStoreFlusher: Blocking updates on sceplus-vm48.almaden.ibm.com,60020,1404182803191: the global memstore size 404.3m is >= than blocking 386.7m size
2014-06-30 19:57:52,200 INFO  [RpcServer.handler=24,port=60020] regionserver.MemStoreFlusher: Blocking updates on sceplus-vm48.almaden.ibm.com,60020,1404182803191: the global memstore size 404.3m is >= than blocking 386.7m size
2014-06-30 19:57:52,202 INFO  [RpcServer.handler=2,port=60020] regionserver.MemStoreFlusher: Blocking updates on sceplus-vm48.almaden.ibm.com,60020,1404182803191: the global memstore size 407.4m is >= than blocking 386.7m size
2014-06-30 19:57:52,204 INFO  [RpcServer.handler=14,port=60020] regionserver.MemStoreFlusher: Blocking updates on sceplus-vm48.almaden.ibm.com,60020,1404182803191: the global memstore size 407.4m is >= than blocking 386.7m size
2014-06-30 19:57:52,205 INFO  [RpcServer.handler=36,port=60020] regionserver.MemStoreFlusher: Blocking updates on sceplus-vm48.almaden.ibm.com,60020,1404182803191: the global memstore size 407.4m is >= than blocking 386.7m size
2014-06-30 19:57:52,212 INFO  [RpcServer.handler=17,port=60020] regionserver.MemStoreFlusher: Blocking updates on sceplus-vm48.almaden.ibm.com,60020,1404182803191: the global memstore size 407.4m is >= than blocking 386.7m size
2014-06-30 19:57:52,218 INFO  [RpcServer.handler=7,port=60020] regionserver.MemStoreFlusher: Blocking updates on sceplus-vm48.almaden.ibm.com,60020,1404182803191: the global memstore size 412.0m is >= than blocking 386.7m size
2014-06-30 19:57:52,630 INFO  [RpcServer.handler=45,port=60020] regionserver.MemStoreFlusher: Blocking updates on sceplus-vm48.almaden.ibm.com,60020,1404182803191: the global memstore size 412.0m is >= than blocking 386.7m size
2014-06-30 19:57:52,630 INFO  [RpcServer.handler=34,port=60020] regionserver.MemStoreFlusher: Blocking updates on sceplus-vm48.almaden.ibm.com,60020,1404182803191: the global memstore size 412.0m is >= than blocking 386.7m size
2014-06-30 19:57:52,637 DEBUG [MemStoreFlusher.0] util.FSUtils: DFS Client does not support most favored nodes create; using default create
2014-06-30 19:57:52,641 INFO  [RpcServer.handler=11,port=60020] regionserver.MemStoreFlusher: Blocking updates on sceplus-vm48.almaden.ibm.com,60020,1404182803191: the global memstore size 412.0m is >= than blocking 386.7m size
2014-06-30 19:57:52,641 INFO  [RpcServer.handler=23,port=60020] regionserver.MemStoreFlusher: Blocking updates on sceplus-vm48.almaden.ibm.com,60020,1404182803191: the global memstore size 412.0m is >= than blocking 386.7m size
2014-06-30 19:57:52,652 INFO  [RpcServer.handler=22,port=60020] regionserver.MemStoreFlusher: Blocking updates on sceplus-vm48.almaden.ibm.com,60020,1404182803191: the global memstore size 412.0m is >= than blocking 386.7m size
2014-06-30 19:57:52,661 INFO  [RpcServer.handler=40,port=60020] regionserver.MemStoreFlusher: Blocking updates on sceplus-vm48.almaden.ibm.com,60020,1404182803191: the global memstore size 412.0m is >= than blocking 386.7m size
2014-06-30 19:57:52,662 INFO  [RpcServer.handler=44,port=60020] regionserver.MemStoreFlusher: Blocking updates on sceplus-vm48.almaden.ibm.com,60020,1404182803191: the global memstore size 412.0m is >= than blocking 386.7m size
2014-06-30 19:57:52,662 INFO  [RpcServer.handler=47,port=60020] regionserver.MemStoreFlusher: Blocking updates on sceplus-vm48.almaden.ibm.com,60020,1404182803191: the global memstore size 412.0m is >= than blocking 386.7m size
2014-06-30 19:57:52,663 INFO  [RpcServer.handler=38,port=60020] regionserver.MemStoreFlusher: Blocking updates on sceplus-vm48.almaden.ibm.com,60020,1404182803191: the global memstore size 412.0m is >= than blocking 386.7m size
2014-06-30 19:57:52,673 INFO  [RpcServer.handler=16,port=60020] regionserver.MemStoreFlusher: Blocking updates on sceplus-vm48.almaden.ibm.com,60020,1404182803191: the global memstore size 412.0m is >= than blocking 386.7m size
2014-06-30 19:57:52,682 INFO  [RpcServer.handler=39,port=60020] regionserver.MemStoreFlusher: Blocking updates on sceplus-vm48.almaden.ibm.com,60020,1404182803191: the global memstore size 412.0m is >= than blocking 386.7m size
2014-06-30 19:57:53,130 DEBUG [regionserver60020-smallCompactions-1404182848217] regionserver.HRegionFileSystem: Committing store file hdfs://master:54310/hbase/data/default/usertable/1d66ac045cc0407c0fa426db565fcbba/.tmp/5a1803b009754e75b5e9ee05d0fdea9a as hdfs://master:54310/hbase/data/default/usertable/1d66ac045cc0407c0fa426db565fcbba/family/5a1803b009754e75b5e9ee05d0fdea9a
2014-06-30 19:57:53,144 DEBUG [regionserver60020-smallCompactions-1404182848217] regionserver.HStore: Removing store files after compaction...
2014-06-30 19:57:53,150 DEBUG [regionserver60020-smallCompactions-1404182848217] backup.HFileArchiver: Finished archiving from class org.apache.hadoop.hbase.backup.HFileArchiver$FileableStoreFile, file:hdfs://master:54310/hbase/data/default/usertable/1d66ac045cc0407c0fa426db565fcbba/family/2aa952bdfe9a4c1da775b8b46dd807f3, to hdfs://master:54310/hbase/archive/data/default/usertable/1d66ac045cc0407c0fa426db565fcbba/family/2aa952bdfe9a4c1da775b8b46dd807f3
2014-06-30 19:57:53,152 DEBUG [regionserver60020-smallCompactions-1404182848217] backup.HFileArchiver: Finished archiving from class org.apache.hadoop.hbase.backup.HFileArchiver$FileableStoreFile, file:hdfs://master:54310/hbase/data/default/usertable/1d66ac045cc0407c0fa426db565fcbba/family/b48f46f83b7748339001cde0126e5bd5, to hdfs://master:54310/hbase/archive/data/default/usertable/1d66ac045cc0407c0fa426db565fcbba/family/b48f46f83b7748339001cde0126e5bd5
2014-06-30 19:57:53,154 DEBUG [regionserver60020-smallCompactions-1404182848217] backup.HFileArchiver: Finished archiving from class org.apache.hadoop.hbase.backup.HFileArchiver$FileableStoreFile, file:hdfs://master:54310/hbase/data/default/usertable/1d66ac045cc0407c0fa426db565fcbba/family/3f20a353a3524604b41b751b155aafe9, to hdfs://master:54310/hbase/archive/data/default/usertable/1d66ac045cc0407c0fa426db565fcbba/family/3f20a353a3524604b41b751b155aafe9
2014-06-30 19:57:53,154 INFO  [regionserver60020-smallCompactions-1404182848217] regionserver.HStore: Completed compaction of 3 file(s) in family of usertable,user5,1404183160444.1d66ac045cc0407c0fa426db565fcbba. into 5a1803b009754e75b5e9ee05d0fdea9a(size=76.2m), total size for store is 257.9m. This selection was in queue for 0sec, and took 4sec to execute.
2014-06-30 19:57:53,154 INFO  [regionserver60020-smallCompactions-1404182848217] regionserver.CompactSplitThread: Completed compaction: Request = regionName=usertable,user5,1404183160444.1d66ac045cc0407c0fa426db565fcbba., storeName=family, fileCount=3, fileSize=107.3m, priority=6, time=3610978454591; duration=4sec
2014-06-30 19:57:53,154 DEBUG [regionserver60020-smallCompactions-1404182848217] regionserver.CompactSplitThread: CompactSplitThread Status: compaction_queue=(0:6), split_queue=0, merge_queue=0
2014-06-30 19:57:53,155 DEBUG [regionserver60020-smallCompactions-1404182848217] compactions.RatioBasedCompactionPolicy: Selecting compaction from 4 store files, 0 compacting, 4 eligible, 10 blocking
2014-06-30 19:57:53,155 DEBUG [regionserver60020-smallCompactions-1404182848217] compactions.ExploringCompactionPolicy: Exploring compaction algorithm has selected 3 files of size 110422513 starting at candidate #1 after considering 3 permutations with 1 in ratio
2014-06-30 19:57:53,155 DEBUG [regionserver60020-smallCompactions-1404182848217] regionserver.HStore: 28938d267c1c61f7feb582b85e4c172f - family: Initiating minor compaction
2014-06-30 19:57:53,155 INFO  [regionserver60020-smallCompactions-1404182848217] regionserver.HRegion: Starting compaction on family in region usertable,user8,1404183160444.28938d267c1c61f7feb582b85e4c172f.
2014-06-30 19:57:53,155 INFO  [regionserver60020-smallCompactions-1404182848217] regionserver.HStore: Starting compaction of 3 file(s) in family of usertable,user8,1404183160444.28938d267c1c61f7feb582b85e4c172f. into tmpdir=hdfs://master:54310/hbase/data/default/usertable/28938d267c1c61f7feb582b85e4c172f/.tmp, totalSize=105.3m
2014-06-30 19:57:53,155 DEBUG [regionserver60020-smallCompactions-1404182848217] compactions.Compactor: Compacting hdfs://master:54310/hbase/data/default/usertable/28938d267c1c61f7feb582b85e4c172f/family/f9d8ee587f0a4719927f3ee9b867a605, keycount=13268, bloomtype=ROW, size=19.8m, encoding=NONE, seqNum=701
2014-06-30 19:57:53,155 DEBUG [regionserver60020-smallCompactions-1404182848217] compactions.Compactor: Compacting hdfs://master:54310/hbase/data/default/usertable/28938d267c1c61f7feb582b85e4c172f/family/a09a880e6ea4499daa2125fe2ae81274, keycount=38413, bloomtype=ROW, size=57.3m, encoding=NONE, seqNum=771
2014-06-30 19:57:53,155 DEBUG [regionserver60020-smallCompactions-1404182848217] compactions.Compactor: Compacting hdfs://master:54310/hbase/data/default/usertable/28938d267c1c61f7feb582b85e4c172f/family/ab1cdeebebb947f4a7ee24699bda4963, keycount=18959, bloomtype=ROW, size=28.3m, encoding=NONE, seqNum=854
2014-06-30 19:57:53,161 DEBUG [regionserver60020-smallCompactions-1404182848217] util.FSUtils: DFS Client does not support most favored nodes create; using default create
2014-06-30 19:57:54,903 INFO  [JvmPauseMonitor] util.JvmPauseMonitor: Detected pause in JVM or host machine (eg GC): pause of approximately 1273ms
GC pool 'ParNew' had collection(s): count=1 time=0ms
GC pool 'ConcurrentMarkSweep' had collection(s): count=2 time=1608ms
2014-06-30 19:57:54,904 INFO  [RpcServer.handler=1,port=60020] regionserver.MemStoreFlusher: Blocking updates on sceplus-vm48.almaden.ibm.com,60020,1404182803191: the global memstore size 412.0m is >= than blocking 386.7m size
2014-06-30 19:57:54,906 INFO  [RpcServer.handler=0,port=60020] regionserver.MemStoreFlusher: Blocking updates on sceplus-vm48.almaden.ibm.com,60020,1404182803191: the global memstore size 412.0m is >= than blocking 386.7m size
2014-06-30 19:57:54,919 INFO  [RpcServer.handler=28,port=60020] regionserver.MemStoreFlusher: Blocking updates on sceplus-vm48.almaden.ibm.com,60020,1404182803191: the global memstore size 412.0m is >= than blocking 386.7m size
2014-06-30 19:57:54,932 INFO  [RpcServer.handler=32,port=60020] regionserver.MemStoreFlusher: Blocking updates on sceplus-vm48.almaden.ibm.com,60020,1404182803191: the global memstore size 412.0m is >= than blocking 386.7m size
2014-06-30 19:57:54,946 INFO  [RpcServer.handler=43,port=60020] regionserver.MemStoreFlusher: Blocking updates on sceplus-vm48.almaden.ibm.com,60020,1404182803191: the global memstore size 412.0m is >= than blocking 386.7m size
2014-06-30 19:57:54,960 INFO  [RpcServer.handler=5,port=60020] regionserver.MemStoreFlusher: Blocking updates on sceplus-vm48.almaden.ibm.com,60020,1404182803191: the global memstore size 412.0m is >= than blocking 386.7m size
2014-06-30 19:57:54,968 INFO  [RpcServer.handler=3,port=60020] regionserver.MemStoreFlusher: Blocking updates on sceplus-vm48.almaden.ibm.com,60020,1404182803191: the global memstore size 412.0m is >= than blocking 386.7m size
2014-06-30 19:57:54,969 INFO  [RpcServer.handler=33,port=60020] regionserver.MemStoreFlusher: Blocking updates on sceplus-vm48.almaden.ibm.com,60020,1404182803191: the global memstore size 412.0m is >= than blocking 386.7m size
2014-06-30 19:57:55,050 INFO  [MemStoreFlusher.0] regionserver.DefaultStoreFlusher: Flushed, sequenceid=888, memsize=91.6m, hasBloomFilter=true, into tmp file hdfs://master:54310/hbase/data/default/usertable/1d66ac045cc0407c0fa426db565fcbba/.tmp/fcd9321ae5844b83b530712863210dc1
2014-06-30 19:57:55,061 DEBUG [MemStoreFlusher.0] regionserver.HRegionFileSystem: Committing store file hdfs://master:54310/hbase/data/default/usertable/1d66ac045cc0407c0fa426db565fcbba/.tmp/fcd9321ae5844b83b530712863210dc1 as hdfs://master:54310/hbase/data/default/usertable/1d66ac045cc0407c0fa426db565fcbba/family/fcd9321ae5844b83b530712863210dc1
2014-06-30 19:57:55,069 INFO  [MemStoreFlusher.0] regionserver.HStore: Added hdfs://master:54310/hbase/data/default/usertable/1d66ac045cc0407c0fa426db565fcbba/family/fcd9321ae5844b83b530712863210dc1, entries=333450, sequenceid=888, filesize=49.7m
2014-06-30 19:57:55,069 INFO  [MemStoreFlusher.0] regionserver.HRegion: Finished memstore flush of ~110.8m/116210400, currentsize=9.4m/9811920 for region usertable,user5,1404183160444.1d66ac045cc0407c0fa426db565fcbba. in 3308ms, sequenceid=888, compaction requested=true
2014-06-30 19:57:55,070 DEBUG [MemStoreFlusher.0] regionserver.CompactSplitThread: Small Compaction requested: system; Because: MemStoreFlusher.0; compaction_queue=(0:6), split_queue=0, merge_queue=0
2014-06-30 19:57:55,070 WARN  [RpcServer.handler=33,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 101ms
2014-06-30 19:57:55,070 INFO  [RpcServer.handler=33,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server sceplus-vm48.almaden.ibm.com,60020,1404182803191
2014-06-30 19:57:55,070 WARN  [RpcServer.handler=3,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 102ms
2014-06-30 19:57:55,070 INFO  [RpcServer.handler=3,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server sceplus-vm48.almaden.ibm.com,60020,1404182803191
2014-06-30 19:57:55,070 WARN  [RpcServer.handler=5,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 110ms
2014-06-30 19:57:55,070 INFO  [RpcServer.handler=5,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server sceplus-vm48.almaden.ibm.com,60020,1404182803191
2014-06-30 19:57:55,070 WARN  [RpcServer.handler=43,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 124ms
2014-06-30 19:57:55,070 INFO  [RpcServer.handler=43,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server sceplus-vm48.almaden.ibm.com,60020,1404182803191
2014-06-30 19:57:55,071 WARN  [RpcServer.handler=32,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 138ms
2014-06-30 19:57:55,071 INFO  [RpcServer.handler=32,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server sceplus-vm48.almaden.ibm.com,60020,1404182803191
2014-06-30 19:57:55,071 WARN  [RpcServer.handler=28,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 152ms
2014-06-30 19:57:55,071 INFO  [RpcServer.handler=28,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server sceplus-vm48.almaden.ibm.com,60020,1404182803191
2014-06-30 19:57:55,071 WARN  [RpcServer.handler=0,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 165ms
2014-06-30 19:57:55,071 INFO  [RpcServer.handler=0,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server sceplus-vm48.almaden.ibm.com,60020,1404182803191
2014-06-30 19:57:55,071 WARN  [RpcServer.handler=1,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 167ms
2014-06-30 19:57:55,071 INFO  [RpcServer.handler=1,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server sceplus-vm48.almaden.ibm.com,60020,1404182803191
2014-06-30 19:57:55,082 WARN  [RpcServer.handler=39,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 2400ms
2014-06-30 19:57:55,082 INFO  [RpcServer.handler=39,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server sceplus-vm48.almaden.ibm.com,60020,1404182803191
2014-06-30 19:57:55,082 WARN  [RpcServer.handler=16,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 2409ms
2014-06-30 19:57:55,082 INFO  [RpcServer.handler=16,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server sceplus-vm48.almaden.ibm.com,60020,1404182803191
2014-06-30 19:57:55,090 WARN  [RpcServer.handler=38,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 2427ms
2014-06-30 19:57:55,090 INFO  [RpcServer.handler=38,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server sceplus-vm48.almaden.ibm.com,60020,1404182803191
2014-06-30 19:57:55,090 WARN  [RpcServer.handler=47,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 2428ms
2014-06-30 19:57:55,090 INFO  [RpcServer.handler=47,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server sceplus-vm48.almaden.ibm.com,60020,1404182803191
2014-06-30 19:57:55,091 WARN  [RpcServer.handler=44,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 2429ms
2014-06-30 19:57:55,091 INFO  [RpcServer.handler=44,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server sceplus-vm48.almaden.ibm.com,60020,1404182803191
2014-06-30 19:57:55,091 WARN  [RpcServer.handler=40,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 2430ms
2014-06-30 19:57:55,091 INFO  [RpcServer.handler=40,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server sceplus-vm48.almaden.ibm.com,60020,1404182803191
2014-06-30 19:57:55,094 WARN  [RpcServer.handler=22,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 2442ms
2014-06-30 19:57:55,094 INFO  [RpcServer.handler=22,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server sceplus-vm48.almaden.ibm.com,60020,1404182803191
2014-06-30 19:57:55,094 WARN  [RpcServer.handler=23,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 2453ms
2014-06-30 19:57:55,094 INFO  [RpcServer.handler=23,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server sceplus-vm48.almaden.ibm.com,60020,1404182803191
2014-06-30 19:57:55,095 WARN  [RpcServer.handler=11,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 2454ms
2014-06-30 19:57:55,095 INFO  [RpcServer.handler=11,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server sceplus-vm48.almaden.ibm.com,60020,1404182803191
2014-06-30 19:57:55,096 WARN  [RpcServer.handler=34,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 2466ms
2014-06-30 19:57:55,096 INFO  [RpcServer.handler=34,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server sceplus-vm48.almaden.ibm.com,60020,1404182803191
2014-06-30 19:57:55,097 WARN  [RpcServer.handler=45,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 2467ms
2014-06-30 19:57:55,097 INFO  [RpcServer.handler=45,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server sceplus-vm48.almaden.ibm.com,60020,1404182803191
2014-06-30 19:57:55,097 WARN  [RpcServer.handler=7,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 2879ms
2014-06-30 19:57:55,097 INFO  [RpcServer.handler=7,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server sceplus-vm48.almaden.ibm.com,60020,1404182803191
2014-06-30 19:57:55,097 WARN  [RpcServer.handler=17,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 2885ms
2014-06-30 19:57:55,097 INFO  [RpcServer.handler=17,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server sceplus-vm48.almaden.ibm.com,60020,1404182803191
2014-06-30 19:57:55,098 WARN  [RpcServer.handler=36,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 2892ms
2014-06-30 19:57:55,098 INFO  [RpcServer.handler=36,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server sceplus-vm48.almaden.ibm.com,60020,1404182803191
2014-06-30 19:57:55,098 WARN  [RpcServer.handler=14,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 2894ms
2014-06-30 19:57:55,098 INFO  [RpcServer.handler=14,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server sceplus-vm48.almaden.ibm.com,60020,1404182803191
2014-06-30 19:57:55,099 WARN  [RpcServer.handler=2,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 2896ms
2014-06-30 19:57:55,099 INFO  [RpcServer.handler=2,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server sceplus-vm48.almaden.ibm.com,60020,1404182803191
2014-06-30 19:57:55,101 WARN  [RpcServer.handler=24,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 2902ms
2014-06-30 19:57:55,101 INFO  [RpcServer.handler=24,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server sceplus-vm48.almaden.ibm.com,60020,1404182803191
2014-06-30 19:57:55,101 WARN  [RpcServer.handler=19,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 2902ms
2014-06-30 19:57:55,101 INFO  [RpcServer.handler=19,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server sceplus-vm48.almaden.ibm.com,60020,1404182803191
2014-06-30 19:57:55,101 WARN  [RpcServer.handler=31,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 2902ms
2014-06-30 19:57:55,101 INFO  [RpcServer.handler=31,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server sceplus-vm48.almaden.ibm.com,60020,1404182803191
2014-06-30 19:57:55,101 WARN  [RpcServer.handler=26,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 2911ms
2014-06-30 19:57:55,101 INFO  [RpcServer.handler=26,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server sceplus-vm48.almaden.ibm.com,60020,1404182803191
2014-06-30 19:57:55,102 WARN  [RpcServer.handler=42,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 2919ms
2014-06-30 19:57:55,102 INFO  [RpcServer.handler=42,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server sceplus-vm48.almaden.ibm.com,60020,1404182803191
2014-06-30 19:57:55,102 WARN  [RpcServer.handler=18,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 2927ms
2014-06-30 19:57:55,102 INFO  [RpcServer.handler=18,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server sceplus-vm48.almaden.ibm.com,60020,1404182803191
2014-06-30 19:57:55,110 WARN  [RpcServer.handler=12,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 2937ms
2014-06-30 19:57:55,110 INFO  [RpcServer.handler=12,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server sceplus-vm48.almaden.ibm.com,60020,1404182803191
2014-06-30 19:57:55,111 WARN  [RpcServer.handler=49,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 2954ms
2014-06-30 19:57:55,111 INFO  [RpcServer.handler=49,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server sceplus-vm48.almaden.ibm.com,60020,1404182803191
2014-06-30 19:57:55,111 WARN  [RpcServer.handler=6,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 2966ms
2014-06-30 19:57:55,111 INFO  [RpcServer.handler=6,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server sceplus-vm48.almaden.ibm.com,60020,1404182803191
2014-06-30 19:57:55,111 WARN  [RpcServer.handler=15,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 2966ms
2014-06-30 19:57:55,112 INFO  [RpcServer.handler=15,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server sceplus-vm48.almaden.ibm.com,60020,1404182803191
2014-06-30 19:57:55,115 WARN  [RpcServer.handler=48,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 2971ms
2014-06-30 19:57:55,115 INFO  [RpcServer.handler=48,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server sceplus-vm48.almaden.ibm.com,60020,1404182803191
2014-06-30 19:57:57,017 INFO  [JvmPauseMonitor] util.JvmPauseMonitor: Detected pause in JVM or host machine (eg GC): pause of approximately 1614ms
GC pool 'ParNew' had collection(s): count=1 time=0ms
GC pool 'ConcurrentMarkSweep' had collection(s): count=1 time=1781ms
2014-06-30 19:57:59,576 INFO  [JvmPauseMonitor] util.JvmPauseMonitor: Detected pause in JVM or host machine (eg GC): pause of approximately 2058ms
GC pool 'ParNew' had collection(s): count=1 time=558ms
GC pool 'ConcurrentMarkSweep' had collection(s): count=1 time=1814ms
2014-06-30 19:57:59,714 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-06-30 19:57:59,729 DEBUG [MemStoreFlusher.0] regionserver.MemStoreFlusher: Flush thread woke up because memory above low water=367.3m
2014-06-30 19:57:59,730 INFO  [MemStoreFlusher.0] regionserver.MemStoreFlusher: Flush of region usertable,user2,1404183160444.5446f1f888398e97c0ec3cc30da5bf97. due to global heap pressure
2014-06-30 19:57:59,730 DEBUG [MemStoreFlusher.0] regionserver.HRegion: Started memstore flush for usertable,user2,1404183160444.5446f1f888398e97c0ec3cc30da5bf97., current region memstore size 122.4m
2014-06-30 19:57:59,736 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 4299 synced till here 4280
2014-06-30 19:58:01,894 INFO  [JvmPauseMonitor] util.JvmPauseMonitor: Detected pause in JVM or host machine (eg GC): pause of approximately 1817ms
GC pool 'ParNew' had collection(s): count=1 time=0ms
GC pool 'ConcurrentMarkSweep' had collection(s): count=1 time=2065ms
2014-06-30 19:58:01,896 DEBUG [RpcServer.handler=25,port=60020] regionserver.HRegion: Flush requested on usertable,user2,1404183160444.5446f1f888398e97c0ec3cc30da5bf97.
2014-06-30 19:58:01,898 INFO  [RpcServer.handler=28,port=60020] regionserver.MemStoreFlusher: Blocking updates on sceplus-vm48.almaden.ibm.com,60020,1404182803191: the global memstore size 411.0m is >= than blocking 386.7m size
2014-06-30 19:58:01,899 INFO  [RpcServer.handler=9,port=60020] regionserver.MemStoreFlusher: Blocking updates on sceplus-vm48.almaden.ibm.com,60020,1404182803191: the global memstore size 411.0m is >= than blocking 386.7m size
2014-06-30 19:58:01,901 INFO  [RpcServer.handler=19,port=60020] regionserver.MemStoreFlusher: Blocking updates on sceplus-vm48.almaden.ibm.com,60020,1404182803191: the global memstore size 411.0m is >= than blocking 386.7m size
2014-06-30 19:58:01,901 INFO  [RpcServer.handler=20,port=60020] regionserver.MemStoreFlusher: Blocking updates on sceplus-vm48.almaden.ibm.com,60020,1404182803191: the global memstore size 411.0m is >= than blocking 386.7m size
2014-06-30 19:58:01,902 INFO  [RpcServer.handler=11,port=60020] regionserver.MemStoreFlusher: Blocking updates on sceplus-vm48.almaden.ibm.com,60020,1404182803191: the global memstore size 412.5m is >= than blocking 386.7m size
2014-06-30 19:58:01,902 DEBUG [MemStoreFlusher.1] regionserver.HRegion: NOT flushing memstore for region usertable,user2,1404183160444.5446f1f888398e97c0ec3cc30da5bf97., flushing=true, writesEnabled=true
2014-06-30 19:58:01,902 INFO  [RpcServer.handler=25,port=60020] regionserver.MemStoreFlusher: Blocking updates on sceplus-vm48.almaden.ibm.com,60020,1404182803191: the global memstore size 412.5m is >= than blocking 386.7m size
2014-06-30 19:58:01,903 INFO  [RpcServer.handler=22,port=60020] regionserver.MemStoreFlusher: Blocking updates on sceplus-vm48.almaden.ibm.com,60020,1404182803191: the global memstore size 412.5m is >= than blocking 386.7m size
2014-06-30 19:58:01,904 INFO  [RpcServer.handler=17,port=60020] regionserver.MemStoreFlusher: Blocking updates on sceplus-vm48.almaden.ibm.com,60020,1404182803191: the global memstore size 412.5m is >= than blocking 386.7m size
2014-06-30 19:58:01,905 INFO  [RpcServer.handler=26,port=60020] regionserver.MemStoreFlusher: Blocking updates on sceplus-vm48.almaden.ibm.com,60020,1404182803191: the global memstore size 412.5m is >= than blocking 386.7m size
2014-06-30 19:58:01,905 INFO  [RpcServer.handler=32,port=60020] regionserver.MemStoreFlusher: Blocking updates on sceplus-vm48.almaden.ibm.com,60020,1404182803191: the global memstore size 412.5m is >= than blocking 386.7m size
2014-06-30 19:58:01,906 INFO  [RpcServer.handler=5,port=60020] regionserver.MemStoreFlusher: Blocking updates on sceplus-vm48.almaden.ibm.com,60020,1404182803191: the global memstore size 414.1m is >= than blocking 386.7m size
2014-06-30 19:58:01,907 INFO  [RpcServer.handler=49,port=60020] regionserver.MemStoreFlusher: Blocking updates on sceplus-vm48.almaden.ibm.com,60020,1404182803191: the global memstore size 414.1m is >= than blocking 386.7m size
2014-06-30 19:58:01,908 INFO  [RpcServer.handler=16,port=60020] regionserver.MemStoreFlusher: Blocking updates on sceplus-vm48.almaden.ibm.com,60020,1404182803191: the global memstore size 414.1m is >= than blocking 386.7m size
2014-06-30 19:58:01,908 INFO  [RpcServer.handler=2,port=60020] regionserver.MemStoreFlusher: Blocking updates on sceplus-vm48.almaden.ibm.com,60020,1404182803191: the global memstore size 414.1m is >= than blocking 386.7m size
2014-06-30 19:58:01,909 INFO  [RpcServer.handler=35,port=60020] regionserver.MemStoreFlusher: Blocking updates on sceplus-vm48.almaden.ibm.com,60020,1404182803191: the global memstore size 414.1m is >= than blocking 386.7m size
2014-06-30 19:58:01,910 INFO  [RpcServer.handler=43,port=60020] regionserver.MemStoreFlusher: Blocking updates on sceplus-vm48.almaden.ibm.com,60020,1404182803191: the global memstore size 414.1m is >= than blocking 386.7m size
2014-06-30 19:58:01,910 INFO  [RpcServer.handler=38,port=60020] regionserver.MemStoreFlusher: Blocking updates on sceplus-vm48.almaden.ibm.com,60020,1404182803191: the global memstore size 414.1m is >= than blocking 386.7m size
2014-06-30 19:58:01,911 INFO  [RpcServer.handler=13,port=60020] regionserver.MemStoreFlusher: Blocking updates on sceplus-vm48.almaden.ibm.com,60020,1404182803191: the global memstore size 415.7m is >= than blocking 386.7m size
2014-06-30 19:58:01,911 INFO  [RpcServer.handler=4,port=60020] regionserver.MemStoreFlusher: Blocking updates on sceplus-vm48.almaden.ibm.com,60020,1404182803191: the global memstore size 415.7m is >= than blocking 386.7m size
2014-06-30 19:58:01,911 INFO  [RpcServer.handler=42,port=60020] regionserver.MemStoreFlusher: Blocking updates on sceplus-vm48.almaden.ibm.com,60020,1404182803191: the global memstore size 415.7m is >= than blocking 386.7m size
2014-06-30 19:58:01,911 INFO  [RpcServer.handler=8,port=60020] regionserver.MemStoreFlusher: Blocking updates on sceplus-vm48.almaden.ibm.com,60020,1404182803191: the global memstore size 415.7m is >= than blocking 386.7m size
2014-06-30 19:58:01,912 DEBUG [MemStoreFlusher.1] regionserver.MemStoreFlusher: Flush thread woke up because memory above low water=367.3m
2014-06-30 19:58:01,912 WARN  [RpcServer.handler=42,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 3ms
2014-06-30 19:58:01,912 INFO  [MemStoreFlusher.1] regionserver.MemStoreFlusher: Flush of region usertable,user7,1404183160444.994b975d2c4cd5bbe532bddfb5e4ac46. due to global heap pressure
2014-06-30 19:58:01,912 DEBUG [MemStoreFlusher.1] regionserver.HRegion: Started memstore flush for usertable,user7,1404183160444.994b975d2c4cd5bbe532bddfb5e4ac46., current region memstore size 122.3m
2014-06-30 19:58:01,912 WARN  [RpcServer.handler=4,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 1ms
2014-06-30 19:58:01,912 WARN  [RpcServer.handler=13,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 11ms
2014-06-30 19:58:01,912 WARN  [RpcServer.handler=38,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 2ms
2014-06-30 19:58:01,912 WARN  [RpcServer.handler=43,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 2ms
2014-06-30 19:58:01,912 INFO  [RpcServer.handler=46,port=60020] regionserver.MemStoreFlusher: Blocking updates on sceplus-vm48.almaden.ibm.com,60020,1404182803191: the global memstore size 415.7m is >= than blocking 386.7m size
2014-06-30 19:58:01,913 WARN  [RpcServer.handler=35,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 4ms
2014-06-30 19:58:01,913 WARN  [RpcServer.handler=2,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5ms
2014-06-30 19:58:01,913 INFO  [RpcServer.handler=3,port=60020] regionserver.MemStoreFlusher: Blocking updates on sceplus-vm48.almaden.ibm.com,60020,1404182803191: the global memstore size 415.7m is >= than blocking 386.7m size
2014-06-30 19:58:01,913 WARN  [RpcServer.handler=16,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5ms
2014-06-30 19:58:01,913 WARN  [RpcServer.handler=49,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 6ms
2014-06-30 19:58:01,913 WARN  [RpcServer.handler=5,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 7ms
2014-06-30 19:58:01,914 WARN  [RpcServer.handler=32,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 9ms
2014-06-30 19:58:01,914 WARN  [RpcServer.handler=26,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 9ms
2014-06-30 19:58:01,914 WARN  [RpcServer.handler=17,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10ms
2014-06-30 19:58:01,914 WARN  [RpcServer.handler=22,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 11ms
2014-06-30 19:58:01,914 WARN  [RpcServer.handler=25,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 12ms
2014-06-30 19:58:01,914 WARN  [RpcServer.handler=11,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 12ms
2014-06-30 19:58:01,914 WARN  [RpcServer.handler=20,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 13ms
2014-06-30 19:58:01,915 WARN  [RpcServer.handler=19,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 14ms
2014-06-30 19:58:01,915 WARN  [RpcServer.handler=9,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 16ms
2014-06-30 19:58:01,915 WARN  [RpcServer.handler=28,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 17ms
2014-06-30 19:58:01,915 INFO  [RpcServer.handler=30,port=60020] regionserver.MemStoreFlusher: Blocking updates on sceplus-vm48.almaden.ibm.com,60020,1404182803191: the global memstore size 415.7m is >= than blocking 386.7m size
2014-06-30 19:58:01,987 INFO  [RpcServer.handler=29,port=60020] regionserver.MemStoreFlusher: Blocking updates on sceplus-vm48.almaden.ibm.com,60020,1404182803191: the global memstore size 420.4m is >= than blocking 386.7m size
2014-06-30 19:58:01,988 INFO  [RpcServer.handler=21,port=60020] regionserver.MemStoreFlusher: Blocking updates on sceplus-vm48.almaden.ibm.com,60020,1404182803191: the global memstore size 420.4m is >= than blocking 386.7m size
2014-06-30 19:58:01,988 INFO  [RpcServer.handler=41,port=60020] regionserver.MemStoreFlusher: Blocking updates on sceplus-vm48.almaden.ibm.com,60020,1404182803191: the global memstore size 420.4m is >= than blocking 386.7m size
2014-06-30 19:58:01,992 INFO  [RpcServer.handler=0,port=60020] regionserver.MemStoreFlusher: Blocking updates on sceplus-vm48.almaden.ibm.com,60020,1404182803191: the global memstore size 420.4m is >= than blocking 386.7m size
2014-06-30 19:58:01,992 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1404182803191/sceplus-vm48.almaden.ibm.com%2C60020%2C1404182803191.1404183471672 with entries=114, filesize=88.0m; new WAL /hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1404182803191/sceplus-vm48.almaden.ibm.com%2C60020%2C1404182803191.1404183479714
2014-06-30 19:58:02,008 INFO  [RpcServer.handler=1,port=60020] regionserver.MemStoreFlusher: Blocking updates on sceplus-vm48.almaden.ibm.com,60020,1404182803191: the global memstore size 420.4m is >= than blocking 386.7m size
2014-06-30 19:58:02,020 INFO  [RpcServer.handler=47,port=60020] regionserver.MemStoreFlusher: Blocking updates on sceplus-vm48.almaden.ibm.com,60020,1404182803191: the global memstore size 420.4m is >= than blocking 386.7m size
2014-06-30 19:58:02,029 INFO  [RpcServer.handler=40,port=60020] regionserver.MemStoreFlusher: Blocking updates on sceplus-vm48.almaden.ibm.com,60020,1404182803191: the global memstore size 420.4m is >= than blocking 386.7m size
2014-06-30 19:58:02,055 INFO  [RpcServer.handler=27,port=60020] regionserver.MemStoreFlusher: Blocking updates on sceplus-vm48.almaden.ibm.com,60020,1404182803191: the global memstore size 433.3m is >= than blocking 386.7m size
2014-06-30 19:58:02,056 INFO  [RpcServer.handler=39,port=60020] regionserver.MemStoreFlusher: Blocking updates on sceplus-vm48.almaden.ibm.com,60020,1404182803191: the global memstore size 433.3m is >= than blocking 386.7m size
2014-06-30 19:58:02,128 DEBUG [MemStoreFlusher.0] util.FSUtils: DFS Client does not support most favored nodes create; using default create
2014-06-30 19:58:02,165 DEBUG [MemStoreFlusher.1] util.FSUtils: DFS Client does not support most favored nodes create; using default create
2014-06-30 19:58:05,052 INFO  [JvmPauseMonitor] util.JvmPauseMonitor: Detected pause in JVM or host machine (eg GC): pause of approximately 2157ms
GC pool 'ConcurrentMarkSweep' had collection(s): count=1 time=2613ms
2014-06-30 19:58:05,057 INFO  [RpcServer.handler=44,port=60020] regionserver.MemStoreFlusher: Blocking updates on sceplus-vm48.almaden.ibm.com,60020,1404182803191: the global memstore size 433.3m is >= than blocking 386.7m size
2014-06-30 19:58:05,069 INFO  [RpcServer.handler=23,port=60020] regionserver.MemStoreFlusher: Blocking updates on sceplus-vm48.almaden.ibm.com,60020,1404182803191: the global memstore size 433.3m is >= than blocking 386.7m size
2014-06-30 19:58:05,080 INFO  [RpcServer.handler=37,port=60020] regionserver.MemStoreFlusher: Blocking updates on sceplus-vm48.almaden.ibm.com,60020,1404182803191: the global memstore size 433.3m is >= than blocking 386.7m size
2014-06-30 19:58:05,089 INFO  [RpcServer.handler=7,port=60020] regionserver.MemStoreFlusher: Blocking updates on sceplus-vm48.almaden.ibm.com,60020,1404182803191: the global memstore size 433.3m is >= than blocking 386.7m size
2014-06-30 19:58:07,319 WARN  [RpcServer.handler=39,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5264ms
2014-06-30 19:58:07,320 WARN  [RpcServer.handler=41,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5332ms
2014-06-30 19:58:07,320 WARN  [RpcServer.handler=29,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5333ms
2014-06-30 19:58:07,320 WARN  [RpcServer.handler=20,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5419ms
2014-06-30 19:58:07,320 WARN  [RpcServer.handler=17,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5416ms
2014-06-30 19:58:07,320 WARN  [RpcServer.handler=0,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5329ms
2014-06-30 19:58:07,320 WARN  [RpcServer.handler=16,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5412ms
2014-06-30 19:58:07,321 WARN  [RpcServer.handler=3,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5408ms
2014-06-30 19:58:07,321 WARN  [RpcServer.handler=2,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5413ms
2014-06-30 19:58:07,321 WARN  [RpcServer.handler=35,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5412ms
2014-06-30 19:58:07,321 WARN  [RpcServer.handler=1,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5313ms
2014-06-30 19:58:07,321 WARN  [RpcServer.handler=38,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5411ms
2014-06-30 19:58:07,321 WARN  [RpcServer.handler=42,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5412ms
2014-06-30 19:58:07,321 WARN  [RpcServer.handler=40,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5293ms
2014-06-30 19:58:07,322 WARN  [RpcServer.handler=47,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5303ms
2014-06-30 19:58:07,322 WARN  [RpcServer.handler=27,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5267ms
2014-06-30 19:58:07,322 INFO  [JvmPauseMonitor] util.JvmPauseMonitor: Detected pause in JVM or host machine (eg GC): pause of approximately 1269ms
GC pool 'ConcurrentMarkSweep' had collection(s): count=1 time=1690ms
2014-06-30 19:58:07,322 WARN  [RpcServer.handler=8,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5421ms
2014-06-30 19:58:07,322 WARN  [RpcServer.handler=4,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5411ms
2014-06-30 19:58:07,322 WARN  [RpcServer.handler=13,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5421ms
2014-06-30 19:58:07,323 WARN  [RpcServer.handler=43,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5413ms
2014-06-30 19:58:07,323 WARN  [RpcServer.handler=46,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5411ms
2014-06-30 19:58:07,323 WARN  [RpcServer.handler=49,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5416ms
2014-06-30 19:58:07,323 WARN  [RpcServer.handler=5,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5417ms
2014-06-30 19:58:07,323 WARN  [RpcServer.handler=32,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5418ms
2014-06-30 19:58:07,323 WARN  [RpcServer.handler=26,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5418ms
2014-06-30 19:58:07,324 WARN  [RpcServer.handler=22,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5421ms
2014-06-30 19:58:07,324 WARN  [RpcServer.handler=25,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5422ms
2014-06-30 19:58:07,324 WARN  [RpcServer.handler=11,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5422ms
2014-06-30 19:58:07,324 WARN  [RpcServer.handler=19,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5423ms
2014-06-30 19:58:07,325 WARN  [RpcServer.handler=9,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5426ms
2014-06-30 19:58:07,325 WARN  [RpcServer.handler=28,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5427ms
2014-06-30 19:58:07,325 WARN  [RpcServer.handler=30,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5411ms
2014-06-30 19:58:07,325 WARN  [RpcServer.handler=21,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5338ms
2014-06-30 19:58:07,335 INFO  [RpcServer.handler=12,port=60020] regionserver.MemStoreFlusher: Blocking updates on sceplus-vm48.almaden.ibm.com,60020,1404182803191: the global memstore size 433.3m is >= than blocking 386.7m size
2014-06-30 19:58:07,344 INFO  [RpcServer.handler=10,port=60020] regionserver.MemStoreFlusher: Blocking updates on sceplus-vm48.almaden.ibm.com,60020,1404182803191: the global memstore size 433.3m is >= than blocking 386.7m size
2014-06-30 19:58:07,354 INFO  [RpcServer.handler=31,port=60020] regionserver.MemStoreFlusher: Blocking updates on sceplus-vm48.almaden.ibm.com,60020,1404182803191: the global memstore size 433.3m is >= than blocking 386.7m size
2014-06-30 19:58:07,439 INFO  [RpcServer.handler=15,port=60020] regionserver.MemStoreFlusher: Blocking updates on sceplus-vm48.almaden.ibm.com,60020,1404182803191: the global memstore size 433.3m is >= than blocking 386.7m size
2014-06-30 19:58:07,440 DEBUG [regionserver60020-smallCompactions-1404182848217] regionserver.HRegionFileSystem: Committing store file hdfs://master:54310/hbase/data/default/usertable/28938d267c1c61f7feb582b85e4c172f/.tmp/fe2d4f4f0d9044e791165a744e6e4b44 as hdfs://master:54310/hbase/data/default/usertable/28938d267c1c61f7feb582b85e4c172f/family/fe2d4f4f0d9044e791165a744e6e4b44
2014-06-30 19:58:07,450 INFO  [RpcServer.handler=6,port=60020] regionserver.MemStoreFlusher: Blocking updates on sceplus-vm48.almaden.ibm.com,60020,1404182803191: the global memstore size 433.3m is >= than blocking 386.7m size
2014-06-30 19:58:07,453 DEBUG [regionserver60020-smallCompactions-1404182848217] regionserver.HStore: Removing store files after compaction...
2014-06-30 19:58:07,458 DEBUG [regionserver60020-smallCompactions-1404182848217] backup.HFileArchiver: Finished archiving from class org.apache.hadoop.hbase.backup.HFileArchiver$FileableStoreFile, file:hdfs://master:54310/hbase/data/default/usertable/28938d267c1c61f7feb582b85e4c172f/family/f9d8ee587f0a4719927f3ee9b867a605, to hdfs://master:54310/hbase/archive/data/default/usertable/28938d267c1c61f7feb582b85e4c172f/family/f9d8ee587f0a4719927f3ee9b867a605
2014-06-30 19:58:07,458 INFO  [RpcServer.handler=48,port=60020] regionserver.MemStoreFlusher: Blocking updates on sceplus-vm48.almaden.ibm.com,60020,1404182803191: the global memstore size 433.3m is >= than blocking 386.7m size
2014-06-30 19:58:07,460 DEBUG [regionserver60020-smallCompactions-1404182848217] backup.HFileArchiver: Finished archiving from class org.apache.hadoop.hbase.backup.HFileArchiver$FileableStoreFile, file:hdfs://master:54310/hbase/data/default/usertable/28938d267c1c61f7feb582b85e4c172f/family/a09a880e6ea4499daa2125fe2ae81274, to hdfs://master:54310/hbase/archive/data/default/usertable/28938d267c1c61f7feb582b85e4c172f/family/a09a880e6ea4499daa2125fe2ae81274
2014-06-30 19:58:07,462 DEBUG [regionserver60020-smallCompactions-1404182848217] backup.HFileArchiver: Finished archiving from class org.apache.hadoop.hbase.backup.HFileArchiver$FileableStoreFile, file:hdfs://master:54310/hbase/data/default/usertable/28938d267c1c61f7feb582b85e4c172f/family/ab1cdeebebb947f4a7ee24699bda4963, to hdfs://master:54310/hbase/archive/data/default/usertable/28938d267c1c61f7feb582b85e4c172f/family/ab1cdeebebb947f4a7ee24699bda4963
2014-06-30 19:58:07,462 INFO  [regionserver60020-smallCompactions-1404182848217] regionserver.HStore: Completed compaction of 3 file(s) in family of usertable,user8,1404183160444.28938d267c1c61f7feb582b85e4c172f. into fe2d4f4f0d9044e791165a744e6e4b44(size=76.5m), total size for store is 263.0m. This selection was in queue for 0sec, and took 14sec to execute.
2014-06-30 19:58:07,462 INFO  [regionserver60020-smallCompactions-1404182848217] regionserver.CompactSplitThread: Completed compaction: Request = regionName=usertable,user8,1404183160444.28938d267c1c61f7feb582b85e4c172f., storeName=family, fileCount=3, fileSize=105.3m, priority=6, time=3615512585511; duration=14sec
2014-06-30 19:58:07,462 DEBUG [regionserver60020-smallCompactions-1404182848217] regionserver.CompactSplitThread: CompactSplitThread Status: compaction_queue=(0:6), split_queue=0, merge_queue=0
2014-06-30 19:58:07,463 DEBUG [regionserver60020-smallCompactions-1404182848217] compactions.RatioBasedCompactionPolicy: Selecting compaction from 3 store files, 0 compacting, 3 eligible, 10 blocking
2014-06-30 19:58:07,463 DEBUG [regionserver60020-smallCompactions-1404182848217] compactions.ExploringCompactionPolicy: Exploring compaction algorithm has selected 0 files of size 0 starting at candidate #-1 after considering 1 permutations with 0 in ratio
2014-06-30 19:58:07,463 DEBUG [regionserver60020-smallCompactions-1404182848217] compactions.RatioBasedCompactionPolicy: Not compacting files because we only have 0 files ready for compaction. Need 3 to initiate.
2014-06-30 19:58:07,463 DEBUG [regionserver60020-smallCompactions-1404182848217] regionserver.CompactSplitThread: Not compacting usertable,user5,1404183160444.1d66ac045cc0407c0fa426db565fcbba. because compaction request was cancelled
2014-06-30 19:58:07,463 DEBUG [regionserver60020-smallCompactions-1404182848217] compactions.RatioBasedCompactionPolicy: Selecting compaction from 3 store files, 0 compacting, 3 eligible, 10 blocking
2014-06-30 19:58:07,463 DEBUG [regionserver60020-smallCompactions-1404182848217] compactions.ExploringCompactionPolicy: Exploring compaction algorithm has selected 0 files of size 0 starting at candidate #-1 after considering 1 permutations with 0 in ratio
2014-06-30 19:58:07,463 DEBUG [regionserver60020-smallCompactions-1404182848217] compactions.RatioBasedCompactionPolicy: Not compacting files because we only have 0 files ready for compaction. Need 3 to initiate.
2014-06-30 19:58:07,463 DEBUG [regionserver60020-smallCompactions-1404182848217] regionserver.CompactSplitThread: Not compacting usertable,user5,1404183160444.1d66ac045cc0407c0fa426db565fcbba. because compaction request was cancelled
2014-06-30 19:58:07,463 DEBUG [regionserver60020-smallCompactions-1404182848217] compactions.RatioBasedCompactionPolicy: Selecting compaction from 3 store files, 0 compacting, 3 eligible, 10 blocking
2014-06-30 19:58:07,463 DEBUG [regionserver60020-smallCompactions-1404182848217] compactions.ExploringCompactionPolicy: Exploring compaction algorithm has selected 0 files of size 0 starting at candidate #-1 after considering 1 permutations with 0 in ratio
2014-06-30 19:58:07,463 DEBUG [regionserver60020-smallCompactions-1404182848217] compactions.RatioBasedCompactionPolicy: Not compacting files because we only have 0 files ready for compaction. Need 3 to initiate.
2014-06-30 19:58:07,463 DEBUG [regionserver60020-smallCompactions-1404182848217] regionserver.CompactSplitThread: Not compacting usertable,user7,1404183160444.994b975d2c4cd5bbe532bddfb5e4ac46. because compaction request was cancelled
2014-06-30 19:58:07,463 DEBUG [regionserver60020-smallCompactions-1404182848217] compactions.RatioBasedCompactionPolicy: Selecting compaction from 2 store files, 0 compacting, 2 eligible, 10 blocking
2014-06-30 19:58:07,463 DEBUG [regionserver60020-smallCompactions-1404182848217] compactions.ExploringCompactionPolicy: Exploring compaction algorithm has selected 0 files of size 0 starting at candidate #-1 after considering 0 permutations with 0 in ratio
2014-06-30 19:58:07,463 DEBUG [regionserver60020-smallCompactions-1404182848217] compactions.RatioBasedCompactionPolicy: Not compacting files because we only have 0 files ready for compaction. Need 3 to initiate.
2014-06-30 19:58:07,463 DEBUG [regionserver60020-smallCompactions-1404182848217] regionserver.CompactSplitThread: Not compacting usertable,user2,1404183160444.5446f1f888398e97c0ec3cc30da5bf97. because compaction request was cancelled
2014-06-30 19:58:07,464 DEBUG [regionserver60020-smallCompactions-1404182848217] compactions.RatioBasedCompactionPolicy: Selecting compaction from 2 store files, 0 compacting, 2 eligible, 10 blocking
2014-06-30 19:58:07,464 DEBUG [regionserver60020-smallCompactions-1404182848217] compactions.ExploringCompactionPolicy: Exploring compaction algorithm has selected 0 files of size 0 starting at candidate #-1 after considering 0 permutations with 0 in ratio
2014-06-30 19:58:07,464 DEBUG [regionserver60020-smallCompactions-1404182848217] compactions.RatioBasedCompactionPolicy: Not compacting files because we only have 0 files ready for compaction. Need 3 to initiate.
2014-06-30 19:58:07,464 DEBUG [regionserver60020-smallCompactions-1404182848217] regionserver.CompactSplitThread: Not compacting usertable,user8,1404183160444.28938d267c1c61f7feb582b85e4c172f. because compaction request was cancelled
2014-06-30 19:58:07,464 DEBUG [regionserver60020-smallCompactions-1404182848217] compactions.RatioBasedCompactionPolicy: Selecting compaction from 2 store files, 0 compacting, 2 eligible, 10 blocking
2014-06-30 19:58:07,464 DEBUG [regionserver60020-smallCompactions-1404182848217] compactions.ExploringCompactionPolicy: Exploring compaction algorithm has selected 0 files of size 0 starting at candidate #-1 after considering 0 permutations with 0 in ratio
2014-06-30 19:58:07,464 DEBUG [regionserver60020-smallCompactions-1404182848217] compactions.RatioBasedCompactionPolicy: Not compacting files because we only have 0 files ready for compaction. Need 3 to initiate.
2014-06-30 19:58:07,464 DEBUG [regionserver60020-smallCompactions-1404182848217] regionserver.CompactSplitThread: Not compacting usertable,user2,1404183160444.5446f1f888398e97c0ec3cc30da5bf97. because compaction request was cancelled
2014-06-30 19:58:07,470 INFO  [RpcServer.handler=24,port=60020] regionserver.MemStoreFlusher: Blocking updates on sceplus-vm48.almaden.ibm.com,60020,1404182803191: the global memstore size 433.3m is >= than blocking 386.7m size
2014-06-30 19:58:07,481 INFO  [RpcServer.handler=34,port=60020] regionserver.MemStoreFlusher: Blocking updates on sceplus-vm48.almaden.ibm.com,60020,1404182803191: the global memstore size 433.3m is >= than blocking 386.7m size
2014-06-30 19:58:07,498 INFO  [RpcServer.handler=45,port=60020] regionserver.MemStoreFlusher: Blocking updates on sceplus-vm48.almaden.ibm.com,60020,1404182803191: the global memstore size 433.3m is >= than blocking 386.7m size
2014-06-30 19:58:07,507 INFO  [MemStoreFlusher.0] regionserver.DefaultStoreFlusher: Flushed, sequenceid=929, memsize=100.2m, hasBloomFilter=true, into tmp file hdfs://master:54310/hbase/data/default/usertable/5446f1f888398e97c0ec3cc30da5bf97/.tmp/1f64dba0373248cabb758206c4a243dc
2014-06-30 19:58:07,516 DEBUG [MemStoreFlusher.0] regionserver.HRegionFileSystem: Committing store file hdfs://master:54310/hbase/data/default/usertable/5446f1f888398e97c0ec3cc30da5bf97/.tmp/1f64dba0373248cabb758206c4a243dc as hdfs://master:54310/hbase/data/default/usertable/5446f1f888398e97c0ec3cc30da5bf97/family/1f64dba0373248cabb758206c4a243dc
2014-06-30 19:58:07,516 INFO  [RpcServer.handler=18,port=60020] regionserver.MemStoreFlusher: Blocking updates on sceplus-vm48.almaden.ibm.com,60020,1404182803191: the global memstore size 433.3m is >= than blocking 386.7m size
2014-06-30 19:58:07,524 INFO  [MemStoreFlusher.0] regionserver.HStore: Added hdfs://master:54310/hbase/data/default/usertable/5446f1f888398e97c0ec3cc30da5bf97/family/1f64dba0373248cabb758206c4a243dc, entries=364940, sequenceid=929, filesize=54.4m
2014-06-30 19:58:07,524 INFO  [MemStoreFlusher.0] regionserver.HRegion: Finished memstore flush of ~136.7m/143391280, currentsize=0.0/0 for region usertable,user2,1404183160444.5446f1f888398e97c0ec3cc30da5bf97. in 7794ms, sequenceid=929, compaction requested=true
2014-06-30 19:58:07,525 DEBUG [MemStoreFlusher.0] regionserver.CompactSplitThread: Small Compaction requested: system; Because: MemStoreFlusher.0; compaction_queue=(0:0), split_queue=0, merge_queue=0
2014-06-30 19:58:07,525 DEBUG [regionserver60020-smallCompactions-1404182848217] compactions.RatioBasedCompactionPolicy: Selecting compaction from 3 store files, 0 compacting, 3 eligible, 10 blocking
2014-06-30 19:58:07,525 WARN  [RpcServer.handler=18,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 9ms
2014-06-30 19:58:07,525 DEBUG [regionserver60020-smallCompactions-1404182848217] compactions.ExploringCompactionPolicy: Exploring compaction algorithm has selected 0 files of size 0 starting at candidate #-1 after considering 1 permutations with 0 in ratio
2014-06-30 19:58:07,525 DEBUG [regionserver60020-smallCompactions-1404182848217] compactions.RatioBasedCompactionPolicy: Not compacting files because we only have 0 files ready for compaction. Need 3 to initiate.
2014-06-30 19:58:07,525 INFO  [RpcServer.handler=18,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server sceplus-vm48.almaden.ibm.com,60020,1404182803191
2014-06-30 19:58:07,525 DEBUG [regionserver60020-smallCompactions-1404182848217] regionserver.CompactSplitThread: Not compacting usertable,user2,1404183160444.5446f1f888398e97c0ec3cc30da5bf97. because compaction request was cancelled
2014-06-30 19:58:07,525 WARN  [RpcServer.handler=45,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 27ms
2014-06-30 19:58:07,525 INFO  [RpcServer.handler=45,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server sceplus-vm48.almaden.ibm.com,60020,1404182803191
2014-06-30 19:58:07,525 WARN  [RpcServer.handler=34,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 44ms
2014-06-30 19:58:07,525 INFO  [RpcServer.handler=34,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server sceplus-vm48.almaden.ibm.com,60020,1404182803191
2014-06-30 19:58:07,526 WARN  [RpcServer.handler=24,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 56ms
2014-06-30 19:58:07,526 INFO  [RpcServer.handler=24,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server sceplus-vm48.almaden.ibm.com,60020,1404182803191
2014-06-30 19:58:07,528 WARN  [RpcServer.handler=48,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 69ms
2014-06-30 19:58:07,528 INFO  [RpcServer.handler=48,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server sceplus-vm48.almaden.ibm.com,60020,1404182803191
2014-06-30 19:58:07,529 WARN  [RpcServer.handler=6,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 78ms
2014-06-30 19:58:07,529 INFO  [RpcServer.handler=6,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server sceplus-vm48.almaden.ibm.com,60020,1404182803191
2014-06-30 19:58:07,529 WARN  [RpcServer.handler=15,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 90ms
2014-06-30 19:58:07,529 INFO  [RpcServer.handler=15,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server sceplus-vm48.almaden.ibm.com,60020,1404182803191
2014-06-30 19:58:07,529 WARN  [RpcServer.handler=31,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 175ms
2014-06-30 19:58:07,529 INFO  [RpcServer.handler=31,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server sceplus-vm48.almaden.ibm.com,60020,1404182803191
2014-06-30 19:58:07,529 WARN  [RpcServer.handler=10,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 185ms
2014-06-30 19:58:07,529 INFO  [RpcServer.handler=10,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server sceplus-vm48.almaden.ibm.com,60020,1404182803191
2014-06-30 19:58:07,529 WARN  [RpcServer.handler=12,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 194ms
2014-06-30 19:58:07,529 INFO  [RpcServer.handler=12,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server sceplus-vm48.almaden.ibm.com,60020,1404182803191
2014-06-30 19:58:07,529 WARN  [RpcServer.handler=21,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5542ms
2014-06-30 19:58:07,529 INFO  [RpcServer.handler=21,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server sceplus-vm48.almaden.ibm.com,60020,1404182803191
2014-06-30 19:58:07,529 WARN  [RpcServer.handler=30,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5615ms
2014-06-30 19:58:07,530 INFO  [RpcServer.handler=30,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server sceplus-vm48.almaden.ibm.com,60020,1404182803191
2014-06-30 19:58:07,540 WARN  [RpcServer.handler=28,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5642ms
2014-06-30 19:58:07,540 INFO  [RpcServer.handler=28,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server sceplus-vm48.almaden.ibm.com,60020,1404182803191
2014-06-30 19:58:07,542 WARN  [RpcServer.handler=9,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5643ms
2014-06-30 19:58:07,542 INFO  [RpcServer.handler=9,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server sceplus-vm48.almaden.ibm.com,60020,1404182803191
2014-06-30 19:58:07,542 WARN  [RpcServer.handler=19,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5641ms
2014-06-30 19:58:07,542 INFO  [RpcServer.handler=19,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server sceplus-vm48.almaden.ibm.com,60020,1404182803191
2014-06-30 19:58:07,544 WARN  [RpcServer.handler=11,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5642ms
2014-06-30 19:58:07,544 INFO  [RpcServer.handler=11,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server sceplus-vm48.almaden.ibm.com,60020,1404182803191
2014-06-30 19:58:07,546 WARN  [RpcServer.handler=25,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5644ms
2014-06-30 19:58:07,546 INFO  [RpcServer.handler=25,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server sceplus-vm48.almaden.ibm.com,60020,1404182803191
2014-06-30 19:58:07,546 WARN  [RpcServer.handler=22,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5643ms
2014-06-30 19:58:07,546 INFO  [RpcServer.handler=22,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server sceplus-vm48.almaden.ibm.com,60020,1404182803191
2014-06-30 19:58:07,550 WARN  [RpcServer.handler=26,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5645ms
2014-06-30 19:58:07,550 INFO  [RpcServer.handler=26,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server sceplus-vm48.almaden.ibm.com,60020,1404182803191
2014-06-30 19:58:07,550 WARN  [RpcServer.handler=32,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5645ms
2014-06-30 19:58:07,550 INFO  [RpcServer.handler=32,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server sceplus-vm48.almaden.ibm.com,60020,1404182803191
2014-06-30 19:58:07,552 WARN  [RpcServer.handler=5,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5646ms
2014-06-30 19:58:07,552 INFO  [RpcServer.handler=5,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server sceplus-vm48.almaden.ibm.com,60020,1404182803191
2014-06-30 19:58:07,552 WARN  [RpcServer.handler=49,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5645ms
2014-06-30 19:58:07,553 INFO  [RpcServer.handler=49,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server sceplus-vm48.almaden.ibm.com,60020,1404182803191
2014-06-30 19:58:07,553 WARN  [RpcServer.handler=46,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5641ms
2014-06-30 19:58:07,553 INFO  [RpcServer.handler=46,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server sceplus-vm48.almaden.ibm.com,60020,1404182803191
2014-06-30 19:58:07,553 WARN  [RpcServer.handler=43,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5643ms
2014-06-30 19:58:07,553 INFO  [RpcServer.handler=43,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server sceplus-vm48.almaden.ibm.com,60020,1404182803191
2014-06-30 19:58:07,554 WARN  [RpcServer.handler=13,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5653ms
2014-06-30 19:58:07,554 INFO  [RpcServer.handler=13,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server sceplus-vm48.almaden.ibm.com,60020,1404182803191
2014-06-30 19:58:07,554 WARN  [RpcServer.handler=4,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5643ms
2014-06-30 19:58:07,554 INFO  [RpcServer.handler=4,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server sceplus-vm48.almaden.ibm.com,60020,1404182803191
2014-06-30 19:58:07,554 WARN  [RpcServer.handler=8,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5653ms
2014-06-30 19:58:07,555 INFO  [RpcServer.handler=8,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server sceplus-vm48.almaden.ibm.com,60020,1404182803191
2014-06-30 19:58:07,555 WARN  [RpcServer.handler=27,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5500ms
2014-06-30 19:58:07,555 INFO  [RpcServer.handler=27,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server sceplus-vm48.almaden.ibm.com,60020,1404182803191
2014-06-30 19:58:07,555 WARN  [RpcServer.handler=47,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5536ms
2014-06-30 19:58:07,555 INFO  [RpcServer.handler=47,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server sceplus-vm48.almaden.ibm.com,60020,1404182803191
2014-06-30 19:58:07,556 WARN  [RpcServer.handler=40,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5528ms
2014-06-30 19:58:07,556 INFO  [RpcServer.handler=40,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server sceplus-vm48.almaden.ibm.com,60020,1404182803191
2014-06-30 19:58:07,557 WARN  [RpcServer.handler=42,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5648ms
2014-06-30 19:58:07,557 INFO  [RpcServer.handler=42,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server sceplus-vm48.almaden.ibm.com,60020,1404182803191
2014-06-30 19:58:07,558 WARN  [RpcServer.handler=38,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5648ms
2014-06-30 19:58:07,558 INFO  [RpcServer.handler=38,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server sceplus-vm48.almaden.ibm.com,60020,1404182803191
2014-06-30 19:58:07,558 WARN  [RpcServer.handler=1,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5550ms
2014-06-30 19:58:07,558 INFO  [RpcServer.handler=1,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server sceplus-vm48.almaden.ibm.com,60020,1404182803191
2014-06-30 19:58:07,558 WARN  [RpcServer.handler=35,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5649ms
2014-06-30 19:58:07,562 INFO  [RpcServer.handler=35,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server sceplus-vm48.almaden.ibm.com,60020,1404182803191
2014-06-30 19:58:07,562 WARN  [RpcServer.handler=2,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5654ms
2014-06-30 19:58:07,562 INFO  [RpcServer.handler=2,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server sceplus-vm48.almaden.ibm.com,60020,1404182803191
2014-06-30 19:58:07,562 WARN  [RpcServer.handler=3,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5649ms
2014-06-30 19:58:07,562 INFO  [RpcServer.handler=3,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server sceplus-vm48.almaden.ibm.com,60020,1404182803191
2014-06-30 19:58:07,562 WARN  [RpcServer.handler=16,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5654ms
2014-06-30 19:58:07,562 INFO  [RpcServer.handler=16,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server sceplus-vm48.almaden.ibm.com,60020,1404182803191
2014-06-30 19:58:07,562 WARN  [RpcServer.handler=0,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5571ms
2014-06-30 19:58:07,562 INFO  [RpcServer.handler=0,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server sceplus-vm48.almaden.ibm.com,60020,1404182803191
2014-06-30 19:58:07,562 WARN  [RpcServer.handler=17,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5658ms
2014-06-30 19:58:07,562 INFO  [RpcServer.handler=17,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server sceplus-vm48.almaden.ibm.com,60020,1404182803191
2014-06-30 19:58:07,562 WARN  [RpcServer.handler=20,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5661ms
2014-06-30 19:58:07,563 INFO  [RpcServer.handler=20,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server sceplus-vm48.almaden.ibm.com,60020,1404182803191
2014-06-30 19:58:07,563 WARN  [RpcServer.handler=29,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5576ms
2014-06-30 19:58:07,563 INFO  [RpcServer.handler=29,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server sceplus-vm48.almaden.ibm.com,60020,1404182803191
2014-06-30 19:58:07,563 WARN  [RpcServer.handler=41,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5575ms
2014-06-30 19:58:07,563 INFO  [RpcServer.handler=41,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server sceplus-vm48.almaden.ibm.com,60020,1404182803191
2014-06-30 19:58:07,563 WARN  [RpcServer.handler=39,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5508ms
2014-06-30 19:58:07,563 INFO  [RpcServer.handler=39,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server sceplus-vm48.almaden.ibm.com,60020,1404182803191
2014-06-30 19:58:07,563 WARN  [RpcServer.handler=7,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 2474ms
2014-06-30 19:58:07,563 INFO  [RpcServer.handler=7,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server sceplus-vm48.almaden.ibm.com,60020,1404182803191
2014-06-30 19:58:07,563 WARN  [RpcServer.handler=37,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 2483ms
2014-06-30 19:58:07,628 INFO  [RpcServer.handler=37,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server sceplus-vm48.almaden.ibm.com,60020,1404182803191
2014-06-30 19:58:07,628 WARN  [RpcServer.handler=23,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 2559ms
2014-06-30 19:58:07,635 INFO  [RpcServer.handler=23,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server sceplus-vm48.almaden.ibm.com,60020,1404182803191
2014-06-30 19:58:07,636 WARN  [RpcServer.handler=44,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 2579ms
2014-06-30 19:58:07,673 INFO  [RpcServer.handler=44,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server sceplus-vm48.almaden.ibm.com,60020,1404182803191
2014-06-30 19:58:11,310 INFO  [JvmPauseMonitor] util.JvmPauseMonitor: Detected pause in JVM or host machine (eg GC): pause of approximately 3488ms
GC pool 'ConcurrentMarkSweep' had collection(s): count=1 time=3605ms
2014-06-30 19:58:11,487 INFO  [MemStoreFlusher.1] regionserver.DefaultStoreFlusher: Flushed, sequenceid=901, memsize=100.3m, hasBloomFilter=true, into tmp file hdfs://master:54310/hbase/data/default/usertable/994b975d2c4cd5bbe532bddfb5e4ac46/.tmp/38d3100371d54bba8c65f96afccfef94
2014-06-30 19:58:11,500 DEBUG [MemStoreFlusher.1] regionserver.HRegionFileSystem: Committing store file hdfs://master:54310/hbase/data/default/usertable/994b975d2c4cd5bbe532bddfb5e4ac46/.tmp/38d3100371d54bba8c65f96afccfef94 as hdfs://master:54310/hbase/data/default/usertable/994b975d2c4cd5bbe532bddfb5e4ac46/family/38d3100371d54bba8c65f96afccfef94
2014-06-30 19:58:11,510 INFO  [MemStoreFlusher.1] regionserver.HStore: Added hdfs://master:54310/hbase/data/default/usertable/994b975d2c4cd5bbe532bddfb5e4ac46/family/38d3100371d54bba8c65f96afccfef94, entries=365100, sequenceid=901, filesize=54.4m
2014-06-30 19:58:11,510 INFO  [MemStoreFlusher.1] regionserver.HRegion: Finished memstore flush of ~124.0m/129971440, currentsize=0.0/0 for region usertable,user7,1404183160444.994b975d2c4cd5bbe532bddfb5e4ac46. in 9598ms, sequenceid=901, compaction requested=true
2014-06-30 19:58:11,511 DEBUG [MemStoreFlusher.1] regionserver.CompactSplitThread: Small Compaction requested: system; Because: MemStoreFlusher.1; compaction_queue=(0:0), split_queue=0, merge_queue=0
2014-06-30 19:58:11,511 DEBUG [regionserver60020-smallCompactions-1404182848217] compactions.RatioBasedCompactionPolicy: Selecting compaction from 4 store files, 0 compacting, 4 eligible, 10 blocking
2014-06-30 19:58:11,511 DEBUG [regionserver60020-smallCompactions-1404182848217] compactions.ExploringCompactionPolicy: Exploring compaction algorithm has selected 3 files of size 151062530 starting at candidate #1 after considering 3 permutations with 1 in ratio
2014-06-30 19:58:11,511 DEBUG [regionserver60020-smallCompactions-1404182848217] regionserver.HStore: 994b975d2c4cd5bbe532bddfb5e4ac46 - family: Initiating minor compaction
2014-06-30 19:58:11,512 INFO  [regionserver60020-smallCompactions-1404182848217] regionserver.HRegion: Starting compaction on family in region usertable,user7,1404183160444.994b975d2c4cd5bbe532bddfb5e4ac46.
2014-06-30 19:58:11,512 INFO  [regionserver60020-smallCompactions-1404182848217] regionserver.HStore: Starting compaction of 3 file(s) in family of usertable,user7,1404183160444.994b975d2c4cd5bbe532bddfb5e4ac46. into tmpdir=hdfs://master:54310/hbase/data/default/usertable/994b975d2c4cd5bbe532bddfb5e4ac46/.tmp, totalSize=144.1m
2014-06-30 19:58:11,512 DEBUG [regionserver60020-smallCompactions-1404182848217] compactions.Compactor: Compacting hdfs://master:54310/hbase/data/default/usertable/994b975d2c4cd5bbe532bddfb5e4ac46/family/d14db766018349c6b23c4eb5a800efe1, keycount=31224, bloomtype=ROW, size=46.6m, encoding=NONE, seqNum=732
2014-06-30 19:58:11,512 DEBUG [regionserver60020-smallCompactions-1404182848217] compactions.Compactor: Compacting hdfs://master:54310/hbase/data/default/usertable/994b975d2c4cd5bbe532bddfb5e4ac46/family/28f1cc4bb2c049c6b53ce07c6091748a, keycount=28879, bloomtype=ROW, size=43.1m, encoding=NONE, seqNum=822
2014-06-30 19:58:11,512 DEBUG [regionserver60020-smallCompactions-1404182848217] compactions.Compactor: Compacting hdfs://master:54310/hbase/data/default/usertable/994b975d2c4cd5bbe532bddfb5e4ac46/family/38d3100371d54bba8c65f96afccfef94, keycount=36510, bloomtype=ROW, size=54.4m, encoding=NONE, seqNum=901
2014-06-30 19:58:11,521 WARN  [RpcServer.handler=39,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":18841,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.58:50439","starttimems":1404183472680,"queuetimems":0,"class":"HRegionServer","responsesize":16155,"method":"Multi"}
2014-06-30 19:58:11,522 WARN  [RpcServer.handler=39,port=60020] ipc.RpcServer: RpcServer.respondercallId: 2701 service: ClientService methodName: Multi size: 2.8m connection: 9.1.143.58:50439: output error
2014-06-30 19:58:11,522 WARN  [RpcServer.handler=39,port=60020] ipc.RpcServer: RpcServer.handler=39,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-06-30 19:58:11,522 DEBUG [regionserver60020-smallCompactions-1404182848217] util.FSUtils: DFS Client does not support most favored nodes create; using default create
2014-06-30 19:58:13,677 INFO  [JvmPauseMonitor] util.JvmPauseMonitor: Detected pause in JVM or host machine (eg GC): pause of approximately 1865ms
GC pool 'ConcurrentMarkSweep' had collection(s): count=1 time=1993ms
2014-06-30 19:58:16,332 INFO  [JvmPauseMonitor] util.JvmPauseMonitor: Detected pause in JVM or host machine (eg GC): pause of approximately 1649ms
GC pool 'ConcurrentMarkSweep' had collection(s): count=1 time=2093ms
2014-06-30 19:58:16,513 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-06-30 19:58:16,547 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 4402 synced till here 4371
2014-06-30 19:58:16,689 WARN  [RpcServer.handler=11,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":24050,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.58:50439","starttimems":1404183472638,"queuetimems":0,"class":"HRegionServer","responsesize":16319,"method":"Multi"}
2014-06-30 19:58:16,690 WARN  [RpcServer.handler=30,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":19646,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.58:50439","starttimems":1404183477043,"queuetimems":1,"class":"HRegionServer","responsesize":16155,"method":"Multi"}
2014-06-30 19:58:16,690 WARN  [RpcServer.handler=11,port=60020] ipc.RpcServer: RpcServer.respondercallId: 2683 service: ClientService methodName: Multi size: 2.9m connection: 9.1.143.58:50439: output error
2014-06-30 19:58:16,690 WARN  [RpcServer.handler=11,port=60020] ipc.RpcServer: RpcServer.handler=11,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-06-30 19:58:16,690 WARN  [RpcServer.handler=30,port=60020] ipc.RpcServer: RpcServer.respondercallId: 2719 service: ClientService methodName: Multi size: 2.8m connection: 9.1.143.58:50439: output error
2014-06-30 19:58:16,692 WARN  [RpcServer.handler=30,port=60020] ipc.RpcServer: RpcServer.handler=30,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-06-30 19:58:19,029 INFO  [JvmPauseMonitor] util.JvmPauseMonitor: Detected pause in JVM or host machine (eg GC): pause of approximately 2196ms
GC pool 'ConcurrentMarkSweep' had collection(s): count=1 time=2363ms
2014-06-30 19:58:19,125 WARN  [RpcServer.handler=9,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":22091,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.58:50439","starttimems":1404183477033,"queuetimems":0,"class":"HRegionServer","responsesize":16424,"method":"Multi"}
2014-06-30 19:58:19,125 WARN  [RpcServer.handler=42,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":26946,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.58:50439","starttimems":1404183472178,"queuetimems":0,"class":"HRegionServer","responsesize":16091,"method":"Multi"}
2014-06-30 19:58:19,125 WARN  [RpcServer.handler=5,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":24168,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.58:50439","starttimems":1404183474957,"queuetimems":0,"class":"HRegionServer","responsesize":16599,"method":"Multi"}
2014-06-30 19:58:19,125 WARN  [RpcServer.handler=22,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":26475,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.58:50439","starttimems":1404183472650,"queuetimems":0,"class":"HRegionServer","responsesize":16272,"method":"Multi"}
2014-06-30 19:58:19,125 WARN  [RpcServer.handler=2,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":26926,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.58:50439","starttimems":1404183472199,"queuetimems":0,"class":"HRegionServer","responsesize":15139,"method":"Multi"}
2014-06-30 19:58:19,125 WARN  [RpcServer.handler=9,port=60020] ipc.RpcServer: RpcServer.respondercallId: 2716 service: ClientService methodName: Multi size: 2.9m connection: 9.1.143.58:50439: output error
2014-06-30 19:58:19,126 WARN  [RpcServer.handler=9,port=60020] ipc.RpcServer: RpcServer.handler=9,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-06-30 19:58:19,126 WARN  [RpcServer.handler=22,port=60020] ipc.RpcServer: RpcServer.respondercallId: 2700 service: ClientService methodName: Multi size: 2.8m connection: 9.1.143.58:50439: output error
2014-06-30 19:58:19,126 WARN  [RpcServer.handler=22,port=60020] ipc.RpcServer: RpcServer.handler=22,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-06-30 19:58:19,126 WARN  [RpcServer.handler=2,port=60020] ipc.RpcServer: RpcServer.respondercallId: 2685 service: ClientService methodName: Multi size: 2.6m connection: 9.1.143.58:50439: output error
2014-06-30 19:58:19,126 WARN  [RpcServer.handler=2,port=60020] ipc.RpcServer: RpcServer.handler=2,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-06-30 19:58:19,126 WARN  [RpcServer.handler=5,port=60020] ipc.RpcServer: RpcServer.respondercallId: 2710 service: ClientService methodName: Multi size: 2.9m connection: 9.1.143.58:50439: output error
2014-06-30 19:58:19,126 WARN  [RpcServer.handler=5,port=60020] ipc.RpcServer: RpcServer.handler=5,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-06-30 19:58:19,126 WARN  [RpcServer.handler=42,port=60020] ipc.RpcServer: RpcServer.respondercallId: 2687 service: ClientService methodName: Multi size: 2.8m connection: 9.1.143.58:50439: output error
2014-06-30 19:58:19,126 WARN  [RpcServer.handler=42,port=60020] ipc.RpcServer: RpcServer.handler=42,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-06-30 19:58:19,267 WARN  [RpcServer.handler=43,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":24323,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.58:50439","starttimems":1404183474944,"queuetimems":0,"class":"HRegionServer","responsesize":16451,"method":"Multi"}
2014-06-30 19:58:19,267 WARN  [RpcServer.handler=43,port=60020] ipc.RpcServer: RpcServer.respondercallId: 2711 service: ClientService methodName: Multi size: 2.9m connection: 9.1.143.58:50439: output error
2014-06-30 19:58:19,267 WARN  [RpcServer.handler=43,port=60020] ipc.RpcServer: RpcServer.handler=43,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-06-30 19:58:19,275 WARN  [RpcServer.handler=32,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":24343,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.58:50439","starttimems":1404183474931,"queuetimems":1,"class":"HRegionServer","responsesize":16163,"method":"Multi"}
2014-06-30 19:58:19,276 WARN  [RpcServer.handler=32,port=60020] ipc.RpcServer: RpcServer.respondercallId: 2712 service: ClientService methodName: Multi size: 2.8m connection: 9.1.143.58:50439: output error
2014-06-30 19:58:19,276 WARN  [RpcServer.handler=49,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":27121,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.58:50439","starttimems":1404183472154,"queuetimems":1,"class":"HRegionServer","responsesize":16344,"method":"Multi"}
2014-06-30 19:58:19,276 WARN  [RpcServer.handler=26,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":27087,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.58:50439","starttimems":1404183472188,"queuetimems":0,"class":"HRegionServer","responsesize":16210,"method":"Multi"}
2014-06-30 19:58:19,276 WARN  [RpcServer.handler=49,port=60020] ipc.RpcServer: RpcServer.respondercallId: 2688 service: ClientService methodName: Multi size: 2.9m connection: 9.1.143.58:50439: output error
2014-06-30 19:58:19,276 WARN  [RpcServer.handler=49,port=60020] ipc.RpcServer: RpcServer.handler=49,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-06-30 19:58:19,276 WARN  [RpcServer.handler=26,port=60020] ipc.RpcServer: RpcServer.respondercallId: 2686 service: ClientService methodName: Multi size: 2.8m connection: 9.1.143.58:50439: output error
2014-06-30 19:58:19,276 WARN  [RpcServer.handler=26,port=60020] ipc.RpcServer: RpcServer.handler=26,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-06-30 19:58:19,283 WARN  [RpcServer.handler=28,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":24367,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.58:50439","starttimems":1404183474916,"queuetimems":0,"class":"HRegionServer","responsesize":16349,"method":"Multi"}
2014-06-30 19:58:19,284 WARN  [RpcServer.handler=28,port=60020] ipc.RpcServer: RpcServer.respondercallId: 2707 service: ClientService methodName: Multi size: 2.9m connection: 9.1.143.58:50439: output error
2014-06-30 19:58:19,284 WARN  [RpcServer.handler=28,port=60020] ipc.RpcServer: RpcServer.handler=28,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-06-30 19:58:19,275 WARN  [RpcServer.handler=35,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":22198,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.58:50439","starttimems":1404183477076,"queuetimems":0,"class":"HRegionServer","responsesize":16406,"method":"Multi"}
2014-06-30 19:58:19,291 WARN  [RpcServer.handler=35,port=60020] ipc.RpcServer: RpcServer.respondercallId: 2720 service: ClientService methodName: Multi size: 2.9m connection: 9.1.143.58:50439: output error
2014-06-30 19:58:19,291 WARN  [RpcServer.handler=35,port=60020] ipc.RpcServer: RpcServer.handler=35,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-06-30 19:58:19,276 WARN  [RpcServer.handler=32,port=60020] ipc.RpcServer: RpcServer.handler=32,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-06-30 19:58:21,897 INFO  [JvmPauseMonitor] util.JvmPauseMonitor: Detected pause in JVM or host machine (eg GC): pause of approximately 2366ms
GC pool 'ConcurrentMarkSweep' had collection(s): count=1 time=2547ms
2014-06-30 19:58:21,901 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1404182803191/sceplus-vm48.almaden.ibm.com%2C60020%2C1404182803191.1404183479714 with entries=103, filesize=89.1m; new WAL /hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1404182803191/sceplus-vm48.almaden.ibm.com%2C60020%2C1404182803191.1404183496514
2014-06-30 19:58:22,138 WARN  [RpcServer.handler=4,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":25075,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.58:50439","starttimems":1404183477062,"queuetimems":0,"class":"HRegionServer","responsesize":17178,"method":"Multi"}
2014-06-30 19:58:22,138 WARN  [RpcServer.handler=17,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":29928,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.58:50439","starttimems":1404183472210,"queuetimems":0,"class":"HRegionServer","responsesize":15853,"method":"Multi"}
2014-06-30 19:58:22,138 WARN  [RpcServer.handler=21,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":22476,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.58:50439","starttimems":1404183479662,"queuetimems":0,"class":"HRegionServer","responsesize":16155,"method":"Multi"}
2014-06-30 19:58:22,138 WARN  [RpcServer.handler=17,port=60020] ipc.RpcServer: RpcServer.respondercallId: 2684 service: ClientService methodName: Multi size: 2.8m connection: 9.1.143.58:50439: output error
2014-06-30 19:58:22,138 WARN  [RpcServer.handler=17,port=60020] ipc.RpcServer: RpcServer.handler=17,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-06-30 19:58:22,139 WARN  [RpcServer.handler=25,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":22498,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.58:50439","starttimems":1404183479640,"queuetimems":0,"class":"HRegionServer","responsesize":16651,"method":"Multi"}
2014-06-30 19:58:22,139 WARN  [RpcServer.handler=25,port=60020] ipc.RpcServer: RpcServer.respondercallId: 2724 service: ClientService methodName: Multi size: 2.9m connection: 9.1.143.58:50439: output error
2014-06-30 19:58:22,139 WARN  [RpcServer.handler=25,port=60020] ipc.RpcServer: RpcServer.handler=25,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-06-30 19:58:22,139 WARN  [RpcServer.handler=4,port=60020] ipc.RpcServer: RpcServer.respondercallId: 2717 service: ClientService methodName: Multi size: 3.0m connection: 9.1.143.58:50439: output error
2014-06-30 19:58:22,139 WARN  [RpcServer.handler=4,port=60020] ipc.RpcServer: RpcServer.handler=4,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-06-30 19:58:22,154 WARN  [RpcServer.handler=21,port=60020] ipc.RpcServer: RpcServer.respondercallId: 2732 service: ClientService methodName: Multi size: 2.8m connection: 9.1.143.58:50439: output error
2014-06-30 19:58:22,154 WARN  [RpcServer.handler=21,port=60020] ipc.RpcServer: RpcServer.handler=21,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-06-30 19:58:25,431 INFO  [JvmPauseMonitor] util.JvmPauseMonitor: Detected pause in JVM or host machine (eg GC): pause of approximately 3034ms
GC pool 'ConcurrentMarkSweep' had collection(s): count=1 time=3156ms
2014-06-30 19:58:25,542 WARN  [RpcServer.handler=19,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":25943,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.58:50439","starttimems":1404183479599,"queuetimems":1,"class":"HRegionServer","responsesize":16163,"method":"Multi"}
2014-06-30 19:58:25,542 WARN  [RpcServer.handler=16,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":32871,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.58:50439","starttimems":1404183472671,"queuetimems":0,"class":"HRegionServer","responsesize":16424,"method":"Multi"}
2014-06-30 19:58:25,542 WARN  [RpcServer.handler=46,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":28490,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.58:50439","starttimems":1404183477052,"queuetimems":0,"class":"HRegionServer","responsesize":16272,"method":"Multi"}
2014-06-30 19:58:25,543 WARN  [RpcServer.handler=19,port=60020] ipc.RpcServer: RpcServer.respondercallId: 2727 service: ClientService methodName: Multi size: 2.8m connection: 9.1.143.58:50439: output error
2014-06-30 19:58:25,543 WARN  [RpcServer.handler=19,port=60020] ipc.RpcServer: RpcServer.handler=19,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-06-30 19:58:25,543 WARN  [RpcServer.handler=46,port=60020] ipc.RpcServer: RpcServer.respondercallId: 2718 service: ClientService methodName: Multi size: 2.8m connection: 9.1.143.58:50439: output error
2014-06-30 19:58:25,543 WARN  [RpcServer.handler=46,port=60020] ipc.RpcServer: RpcServer.handler=46,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-06-30 19:58:25,543 WARN  [RpcServer.handler=16,port=60020] ipc.RpcServer: RpcServer.respondercallId: 2702 service: ClientService methodName: Multi size: 2.9m connection: 9.1.143.58:50439: output error
2014-06-30 19:58:25,543 WARN  [RpcServer.handler=16,port=60020] ipc.RpcServer: RpcServer.handler=16,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-06-30 19:58:25,558 WARN  [RpcServer.handler=13,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":25965,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.58:50439","starttimems":1404183479588,"queuetimems":1,"class":"HRegionServer","responsesize":16349,"method":"Multi"}
2014-06-30 19:58:25,558 WARN  [RpcServer.handler=13,port=60020] ipc.RpcServer: RpcServer.respondercallId: 2723 service: ClientService methodName: Multi size: 2.9m connection: 9.1.143.58:50439: output error
2014-06-30 19:58:25,559 WARN  [RpcServer.handler=13,port=60020] ipc.RpcServer: RpcServer.handler=13,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-06-30 19:58:25,588 WARN  [RpcServer.handler=20,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":25974,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.58:50439","starttimems":1404183479613,"queuetimems":0,"class":"HRegionServer","responsesize":16451,"method":"Multi"}
2014-06-30 19:58:25,588 WARN  [RpcServer.handler=20,port=60020] ipc.RpcServer: RpcServer.respondercallId: 2726 service: ClientService methodName: Multi size: 2.9m connection: 9.1.143.58:50439: output error
2014-06-30 19:58:25,589 WARN  [RpcServer.handler=20,port=60020] ipc.RpcServer: RpcServer.handler=20,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-06-30 19:58:25,591 INFO  [RpcServer.handler=38,port=60020] regionserver.MemStoreFlusher: Blocking updates on sceplus-vm48.almaden.ibm.com,60020,1404182803191: the global memstore size 394.0m is >= than blocking 386.7m size
2014-06-30 19:58:25,594 DEBUG [MemStoreFlusher.0] regionserver.MemStoreFlusher: Flush thread woke up because memory above low water=367.3m
2014-06-30 19:58:25,594 INFO  [MemStoreFlusher.0] regionserver.MemStoreFlusher: Flush of region usertable,user8,1404183160444.28938d267c1c61f7feb582b85e4c172f. due to global heap pressure
2014-06-30 19:58:25,601 DEBUG [MemStoreFlusher.0] regionserver.HRegion: Started memstore flush for usertable,user8,1404183160444.28938d267c1c61f7feb582b85e4c172f., current region memstore size 109.4m
2014-06-30 19:58:25,672 INFO  [RpcServer.handler=36,port=60020] regionserver.MemStoreFlusher: Blocking updates on sceplus-vm48.almaden.ibm.com,60020,1404182803191: the global memstore size 394.0m is >= than blocking 386.7m size
2014-06-30 19:58:27,839 INFO  [JvmPauseMonitor] util.JvmPauseMonitor: Detected pause in JVM or host machine (eg GC): pause of approximately 1907ms
GC pool 'ConcurrentMarkSweep' had collection(s): count=1 time=2043ms
2014-06-30 19:58:27,840 INFO  [RpcServer.handler=3,port=60020] regionserver.MemStoreFlusher: Blocking updates on sceplus-vm48.almaden.ibm.com,60020,1404182803191: the global memstore size 394.0m is >= than blocking 386.7m size
2014-06-30 19:58:27,840 INFO  [RpcServer.handler=31,port=60020] regionserver.MemStoreFlusher: Blocking updates on sceplus-vm48.almaden.ibm.com,60020,1404182803191: the global memstore size 394.0m is >= than blocking 386.7m size
2014-06-30 19:58:27,841 INFO  [RpcServer.handler=42,port=60020] regionserver.MemStoreFlusher: Blocking updates on sceplus-vm48.almaden.ibm.com,60020,1404182803191: the global memstore size 394.0m is >= than blocking 386.7m size
2014-06-30 19:58:27,842 INFO  [RpcServer.handler=6,port=60020] regionserver.MemStoreFlusher: Blocking updates on sceplus-vm48.almaden.ibm.com,60020,1404182803191: the global memstore size 394.0m is >= than blocking 386.7m size
2014-06-30 19:58:27,842 INFO  [RpcServer.handler=43,port=60020] regionserver.MemStoreFlusher: Blocking updates on sceplus-vm48.almaden.ibm.com,60020,1404182803191: the global memstore size 394.0m is >= than blocking 386.7m size
2014-06-30 19:58:27,843 INFO  [RpcServer.handler=35,port=60020] regionserver.MemStoreFlusher: Blocking updates on sceplus-vm48.almaden.ibm.com,60020,1404182803191: the global memstore size 394.0m is >= than blocking 386.7m size
2014-06-30 19:58:27,844 INFO  [RpcServer.handler=19,port=60020] regionserver.MemStoreFlusher: Blocking updates on sceplus-vm48.almaden.ibm.com,60020,1404182803191: the global memstore size 394.0m is >= than blocking 386.7m size
2014-06-30 19:58:27,844 INFO  [RpcServer.handler=10,port=60020] regionserver.MemStoreFlusher: Blocking updates on sceplus-vm48.almaden.ibm.com,60020,1404182803191: the global memstore size 394.0m is >= than blocking 386.7m size
2014-06-30 19:58:27,844 INFO  [RpcServer.handler=2,port=60020] regionserver.MemStoreFlusher: Blocking updates on sceplus-vm48.almaden.ibm.com,60020,1404182803191: the global memstore size 394.0m is >= than blocking 386.7m size
2014-06-30 19:58:27,846 INFO  [RpcServer.handler=46,port=60020] regionserver.MemStoreFlusher: Blocking updates on sceplus-vm48.almaden.ibm.com,60020,1404182803191: the global memstore size 394.0m is >= than blocking 386.7m size
2014-06-30 19:58:27,846 INFO  [RpcServer.handler=0,port=60020] regionserver.MemStoreFlusher: Blocking updates on sceplus-vm48.almaden.ibm.com,60020,1404182803191: the global memstore size 394.0m is >= than blocking 386.7m size
2014-06-30 19:58:27,847 INFO  [RpcServer.handler=26,port=60020] regionserver.MemStoreFlusher: Blocking updates on sceplus-vm48.almaden.ibm.com,60020,1404182803191: the global memstore size 394.0m is >= than blocking 386.7m size
2014-06-30 19:58:27,847 INFO  [RpcServer.handler=47,port=60020] regionserver.MemStoreFlusher: Blocking updates on sceplus-vm48.almaden.ibm.com,60020,1404182803191: the global memstore size 394.0m is >= than blocking 386.7m size
2014-06-30 19:58:27,848 INFO  [RpcServer.handler=45,port=60020] regionserver.MemStoreFlusher: Blocking updates on sceplus-vm48.almaden.ibm.com,60020,1404182803191: the global memstore size 394.0m is >= than blocking 386.7m size
2014-06-30 19:58:27,848 INFO  [RpcServer.handler=1,port=60020] regionserver.MemStoreFlusher: Blocking updates on sceplus-vm48.almaden.ibm.com,60020,1404182803191: the global memstore size 394.0m is >= than blocking 386.7m size
2014-06-30 19:58:27,849 INFO  [RpcServer.handler=49,port=60020] regionserver.MemStoreFlusher: Blocking updates on sceplus-vm48.almaden.ibm.com,60020,1404182803191: the global memstore size 394.0m is >= than blocking 386.7m size
2014-06-30 19:58:27,849 INFO  [RpcServer.handler=14,port=60020] regionserver.MemStoreFlusher: Blocking updates on sceplus-vm48.almaden.ibm.com,60020,1404182803191: the global memstore size 394.0m is >= than blocking 386.7m size
2014-06-30 19:58:27,850 INFO  [RpcServer.handler=27,port=60020] regionserver.MemStoreFlusher: Blocking updates on sceplus-vm48.almaden.ibm.com,60020,1404182803191: the global memstore size 394.0m is >= than blocking 386.7m size
2014-06-30 19:58:27,963 INFO  [RpcServer.handler=13,port=60020] regionserver.MemStoreFlusher: Blocking updates on sceplus-vm48.almaden.ibm.com,60020,1404182803191: the global memstore size 394.0m is >= than blocking 386.7m size
2014-06-30 19:58:27,963 INFO  [RpcServer.handler=39,port=60020] regionserver.MemStoreFlusher: Blocking updates on sceplus-vm48.almaden.ibm.com,60020,1404182803191: the global memstore size 394.0m is >= than blocking 386.7m size
2014-06-30 19:58:27,963 INFO  [RpcServer.handler=32,port=60020] regionserver.MemStoreFlusher: Blocking updates on sceplus-vm48.almaden.ibm.com,60020,1404182803191: the global memstore size 394.0m is >= than blocking 386.7m size
2014-06-30 19:58:27,964 INFO  [RpcServer.handler=24,port=60020] regionserver.MemStoreFlusher: Blocking updates on sceplus-vm48.almaden.ibm.com,60020,1404182803191: the global memstore size 394.0m is >= than blocking 386.7m size
2014-06-30 19:58:27,964 INFO  [RpcServer.handler=37,port=60020] regionserver.MemStoreFlusher: Blocking updates on sceplus-vm48.almaden.ibm.com,60020,1404182803191: the global memstore size 394.0m is >= than blocking 386.7m size
2014-06-30 19:58:27,964 INFO  [RpcServer.handler=40,port=60020] regionserver.MemStoreFlusher: Blocking updates on sceplus-vm48.almaden.ibm.com,60020,1404182803191: the global memstore size 394.0m is >= than blocking 386.7m size
2014-06-30 19:58:27,964 INFO  [RpcServer.handler=20,port=60020] regionserver.MemStoreFlusher: Blocking updates on sceplus-vm48.almaden.ibm.com,60020,1404182803191: the global memstore size 394.0m is >= than blocking 386.7m size
2014-06-30 19:58:27,965 INFO  [RpcServer.handler=28,port=60020] regionserver.MemStoreFlusher: Blocking updates on sceplus-vm48.almaden.ibm.com,60020,1404182803191: the global memstore size 394.0m is >= than blocking 386.7m size
2014-06-30 19:58:27,965 INFO  [RpcServer.handler=23,port=60020] regionserver.MemStoreFlusher: Blocking updates on sceplus-vm48.almaden.ibm.com,60020,1404182803191: the global memstore size 394.0m is >= than blocking 386.7m size
2014-06-30 19:58:27,965 INFO  [RpcServer.handler=12,port=60020] regionserver.MemStoreFlusher: Blocking updates on sceplus-vm48.almaden.ibm.com,60020,1404182803191: the global memstore size 394.0m is >= than blocking 386.7m size
2014-06-30 19:58:27,965 INFO  [RpcServer.handler=34,port=60020] regionserver.MemStoreFlusher: Blocking updates on sceplus-vm48.almaden.ibm.com,60020,1404182803191: the global memstore size 394.0m is >= than blocking 386.7m size
2014-06-30 19:58:27,966 INFO  [RpcServer.handler=9,port=60020] regionserver.MemStoreFlusher: Blocking updates on sceplus-vm48.almaden.ibm.com,60020,1404182803191: the global memstore size 394.0m is >= than blocking 386.7m size
2014-06-30 19:58:27,966 INFO  [RpcServer.handler=15,port=60020] regionserver.MemStoreFlusher: Blocking updates on sceplus-vm48.almaden.ibm.com,60020,1404182803191: the global memstore size 394.0m is >= than blocking 386.7m size
2014-06-30 19:58:27,966 INFO  [RpcServer.handler=7,port=60020] regionserver.MemStoreFlusher: Blocking updates on sceplus-vm48.almaden.ibm.com,60020,1404182803191: the global memstore size 394.0m is >= than blocking 386.7m size
2014-06-30 19:58:27,966 INFO  [RpcServer.handler=44,port=60020] regionserver.MemStoreFlusher: Blocking updates on sceplus-vm48.almaden.ibm.com,60020,1404182803191: the global memstore size 394.0m is >= than blocking 386.7m size
2014-06-30 19:58:27,967 INFO  [RpcServer.handler=22,port=60020] regionserver.MemStoreFlusher: Blocking updates on sceplus-vm48.almaden.ibm.com,60020,1404182803191: the global memstore size 394.0m is >= than blocking 386.7m size
2014-06-30 19:58:27,967 INFO  [RpcServer.handler=48,port=60020] regionserver.MemStoreFlusher: Blocking updates on sceplus-vm48.almaden.ibm.com,60020,1404182803191: the global memstore size 394.0m is >= than blocking 386.7m size
2014-06-30 19:58:27,967 INFO  [RpcServer.handler=16,port=60020] regionserver.MemStoreFlusher: Blocking updates on sceplus-vm48.almaden.ibm.com,60020,1404182803191: the global memstore size 394.0m is >= than blocking 386.7m size
2014-06-30 19:58:27,967 INFO  [RpcServer.handler=5,port=60020] regionserver.MemStoreFlusher: Blocking updates on sceplus-vm48.almaden.ibm.com,60020,1404182803191: the global memstore size 394.0m is >= than blocking 386.7m size
2014-06-30 19:58:28,018 WARN  [RpcServer.handler=41,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":28365,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.58:50439","starttimems":1404183479652,"queuetimems":1,"class":"HRegionServer","responsesize":16424,"method":"Multi"}
2014-06-30 19:58:28,018 WARN  [RpcServer.handler=8,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":28389,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.58:50439","starttimems":1404183479629,"queuetimems":1,"class":"HRegionServer","responsesize":16599,"method":"Multi"}
2014-06-30 19:58:28,018 WARN  [RpcServer.handler=41,port=60020] ipc.RpcServer: RpcServer.respondercallId: 2733 service: ClientService methodName: Multi size: 2.9m connection: 9.1.143.58:50439: output error
2014-06-30 19:58:28,018 WARN  [RpcServer.handler=41,port=60020] ipc.RpcServer: RpcServer.handler=41,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-06-30 19:58:28,018 WARN  [RpcServer.handler=8,port=60020] ipc.RpcServer: RpcServer.respondercallId: 2725 service: ClientService methodName: Multi size: 2.9m connection: 9.1.143.58:50439: output error
2014-06-30 19:58:28,018 WARN  [RpcServer.handler=8,port=60020] ipc.RpcServer: RpcServer.handler=8,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-06-30 19:58:28,019 WARN  [RpcServer.handler=18,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":20504,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.58:50522","starttimems":1404183487514,"queuetimems":0,"class":"HRegionServer","responsesize":16566,"method":"Multi"}
2014-06-30 19:58:28,019 WARN  [RpcServer.handler=18,port=60020] ipc.RpcServer: RpcServer.respondercallId: 2785 service: ClientService methodName: Multi size: 2.9m connection: 9.1.143.58:50522: output error
2014-06-30 19:58:28,019 WARN  [RpcServer.handler=18,port=60020] ipc.RpcServer: RpcServer.handler=18,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-06-30 19:58:28,021 INFO  [RpcServer.handler=25,port=60020] regionserver.MemStoreFlusher: Blocking updates on sceplus-vm48.almaden.ibm.com,60020,1404182803191: the global memstore size 403.6m is >= than blocking 386.7m size
2014-06-30 19:58:28,021 INFO  [RpcServer.handler=30,port=60020] regionserver.MemStoreFlusher: Blocking updates on sceplus-vm48.almaden.ibm.com,60020,1404182803191: the global memstore size 406.5m is >= than blocking 386.7m size
2014-06-30 19:58:28,023 INFO  [RpcServer.handler=41,port=60020] regionserver.MemStoreFlusher: Blocking updates on sceplus-vm48.almaden.ibm.com,60020,1404182803191: the global memstore size 408.1m is >= than blocking 386.7m size
2014-06-30 19:58:28,024 INFO  [RpcServer.handler=8,port=60020] regionserver.MemStoreFlusher: Blocking updates on sceplus-vm48.almaden.ibm.com,60020,1404182803191: the global memstore size 409.7m is >= than blocking 386.7m size
2014-06-30 19:58:28,024 WARN  [RpcServer.handler=29,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":28347,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.58:50439","starttimems":1404183479677,"queuetimems":0,"class":"HRegionServer","responsesize":16272,"method":"Multi"}
2014-06-30 19:58:28,024 WARN  [RpcServer.handler=29,port=60020] ipc.RpcServer: RpcServer.respondercallId: 2731 service: ClientService methodName: Multi size: 2.8m connection: 9.1.143.58:50439: output error
2014-06-30 19:58:28,024 WARN  [RpcServer.handler=29,port=60020] ipc.RpcServer: RpcServer.handler=29,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-06-30 19:58:28,025 INFO  [RpcServer.handler=21,port=60020] regionserver.MemStoreFlusher: Blocking updates on sceplus-vm48.almaden.ibm.com,60020,1404182803191: the global memstore size 409.7m is >= than blocking 386.7m size
2014-06-30 19:58:28,025 INFO  [RpcServer.handler=17,port=60020] regionserver.MemStoreFlusher: Blocking updates on sceplus-vm48.almaden.ibm.com,60020,1404182803191: the global memstore size 409.7m is >= than blocking 386.7m size
2014-06-30 19:58:28,025 INFO  [RpcServer.handler=18,port=60020] regionserver.MemStoreFlusher: Blocking updates on sceplus-vm48.almaden.ibm.com,60020,1404182803191: the global memstore size 411.4m is >= than blocking 386.7m size
2014-06-30 19:58:28,026 INFO  [RpcServer.handler=11,port=60020] regionserver.MemStoreFlusher: Blocking updates on sceplus-vm48.almaden.ibm.com,60020,1404182803191: the global memstore size 411.4m is >= than blocking 386.7m size
2014-06-30 19:58:28,027 INFO  [RpcServer.handler=29,port=60020] regionserver.MemStoreFlusher: Blocking updates on sceplus-vm48.almaden.ibm.com,60020,1404182803191: the global memstore size 411.4m is >= than blocking 386.7m size
2014-06-30 19:58:28,028 INFO  [RpcServer.handler=4,port=60020] regionserver.MemStoreFlusher: Blocking updates on sceplus-vm48.almaden.ibm.com,60020,1404182803191: the global memstore size 411.4m is >= than blocking 386.7m size
2014-06-30 19:58:28,029 INFO  [RpcServer.handler=33,port=60020] regionserver.MemStoreFlusher: Blocking updates on sceplus-vm48.almaden.ibm.com,60020,1404182803191: the global memstore size 411.4m is >= than blocking 386.7m size
2014-06-30 19:58:28,119 DEBUG [MemStoreFlusher.0] util.FSUtils: DFS Client does not support most favored nodes create; using default create
2014-06-30 19:58:30,483 INFO  [JvmPauseMonitor] util.JvmPauseMonitor: Detected pause in JVM or host machine (eg GC): pause of approximately 2143ms
GC pool 'ConcurrentMarkSweep' had collection(s): count=1 time=2351ms
2014-06-30 19:58:30,591 WARN  [RpcServer.handler=38,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5001ms
2014-06-30 19:58:30,673 WARN  [RpcServer.handler=36,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5002ms
2014-06-30 19:58:33,003 DEBUG [MemStoreFlusher.1] regionserver.MemStoreFlusher: Flush thread woke up because memory above low water=367.3m
2014-06-30 19:58:33,004 INFO  [JvmPauseMonitor] util.JvmPauseMonitor: Detected pause in JVM or host machine (eg GC): pause of approximately 2019ms
GC pool 'ConcurrentMarkSweep' had collection(s): count=1 time=2300ms
2014-06-30 19:58:33,004 INFO  [MemStoreFlusher.1] regionserver.MemStoreFlusher: Flush of region usertable,user5,1404183160444.1d66ac045cc0407c0fa426db565fcbba. due to global heap pressure
2014-06-30 19:58:33,003 WARN  [RpcServer.handler=5,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5040ms
2014-06-30 19:58:33,004 WARN  [RpcServer.handler=40,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5043ms
2014-06-30 19:58:33,004 DEBUG [MemStoreFlusher.1] regionserver.HRegion: Started memstore flush for usertable,user5,1404183160444.1d66ac045cc0407c0fa426db565fcbba., current region memstore size 104.8m
2014-06-30 19:58:33,004 WARN  [RpcServer.handler=13,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5154ms
2014-06-30 19:58:33,005 WARN  [RpcServer.handler=20,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5044ms
2014-06-30 19:58:33,005 WARN  [RpcServer.handler=47,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5158ms
2014-06-30 19:58:33,005 WARN  [RpcServer.handler=46,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5160ms
2014-06-30 19:58:33,005 WARN  [RpcServer.handler=28,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5045ms
2014-06-30 19:58:33,005 WARN  [RpcServer.handler=23,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5045ms
2014-06-30 19:58:33,005 WARN  [RpcServer.handler=12,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5046ms
2014-06-30 19:58:33,006 WARN  [RpcServer.handler=34,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5047ms
2014-06-30 19:58:33,006 WARN  [RpcServer.handler=9,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5048ms
2014-06-30 19:58:33,007 WARN  [RpcServer.handler=15,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5048ms
2014-06-30 19:58:33,007 WARN  [RpcServer.handler=7,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5049ms
2014-06-30 19:58:33,007 WARN  [RpcServer.handler=35,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5165ms
2014-06-30 19:58:33,009 WARN  [RpcServer.handler=3,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5168ms
2014-06-30 19:58:33,009 WARN  [RpcServer.handler=44,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5052ms
2014-06-30 19:58:33,009 WARN  [RpcServer.handler=22,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5159ms
2014-06-30 19:58:33,009 WARN  [RpcServer.handler=48,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5159ms
2014-06-30 19:58:33,009 WARN  [RpcServer.handler=16,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5159ms
2014-06-30 19:58:33,009 WARN  [RpcServer.handler=31,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5169ms
2014-06-30 19:58:33,010 WARN  [RpcServer.handler=6,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5168ms
2014-06-30 19:58:33,010 WARN  [RpcServer.handler=42,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5169ms
2014-06-30 19:58:33,011 WARN  [RpcServer.handler=43,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5169ms
2014-06-30 19:58:33,011 WARN  [RpcServer.handler=19,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5167ms
2014-06-30 19:58:33,011 WARN  [RpcServer.handler=10,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5167ms
2014-06-30 19:58:33,012 WARN  [RpcServer.handler=2,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5167ms
2014-06-30 19:58:33,012 WARN  [RpcServer.handler=0,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5166ms
2014-06-30 19:58:33,013 WARN  [RpcServer.handler=26,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5165ms
2014-06-30 19:58:33,013 WARN  [RpcServer.handler=45,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5165ms
2014-06-30 19:58:33,013 WARN  [RpcServer.handler=1,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5165ms
2014-06-30 19:58:33,013 WARN  [RpcServer.handler=49,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5165ms
2014-06-30 19:58:33,014 WARN  [RpcServer.handler=14,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5165ms
2014-06-30 19:58:33,014 WARN  [RpcServer.handler=27,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5164ms
2014-06-30 19:58:33,014 WARN  [RpcServer.handler=39,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5051ms
2014-06-30 19:58:33,014 WARN  [RpcServer.handler=32,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5053ms
2014-06-30 19:58:33,014 WARN  [RpcServer.handler=24,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5053ms
2014-06-30 19:58:33,014 WARN  [RpcServer.handler=37,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5053ms
2014-06-30 19:58:33,021 WARN  [RpcServer.handler=25,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-06-30 19:58:33,022 WARN  [RpcServer.handler=30,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5001ms
2014-06-30 19:58:33,024 WARN  [RpcServer.handler=41,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-06-30 19:58:33,024 WARN  [RpcServer.handler=8,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-06-30 19:58:33,025 WARN  [RpcServer.handler=21,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5001ms
2014-06-30 19:58:33,025 WARN  [RpcServer.handler=17,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5004ms
2014-06-30 19:58:33,026 WARN  [RpcServer.handler=18,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5001ms
2014-06-30 19:58:33,026 WARN  [RpcServer.handler=11,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-06-30 19:58:33,027 WARN  [RpcServer.handler=29,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-06-30 19:58:33,029 WARN  [RpcServer.handler=4,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-06-30 19:58:33,029 WARN  [RpcServer.handler=33,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-06-30 19:58:33,108 DEBUG [MemStoreFlusher.1] util.FSUtils: DFS Client does not support most favored nodes create; using default create
2014-06-30 19:58:35,638 WARN  [RpcServer.handler=38,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10048ms
2014-06-30 19:58:35,638 INFO  [JvmPauseMonitor] util.JvmPauseMonitor: Detected pause in JVM or host machine (eg GC): pause of approximately 2134ms
GC pool 'ConcurrentMarkSweep' had collection(s): count=1 time=2494ms
2014-06-30 19:58:35,674 WARN  [RpcServer.handler=36,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10003ms
2014-06-30 19:58:38,029 WARN  [RpcServer.handler=29,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10001ms
2014-06-30 19:58:38,227 INFO  [JvmPauseMonitor] util.JvmPauseMonitor: Detected pause in JVM or host machine (eg GC): pause of approximately 2088ms
GC pool 'ConcurrentMarkSweep' had collection(s): count=1 time=2459ms
2014-06-30 19:58:38,228 WARN  [RpcServer.handler=4,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10200ms
2014-06-30 19:58:38,229 WARN  [RpcServer.handler=33,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10199ms
2014-06-30 19:58:38,229 WARN  [RpcServer.handler=5,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10266ms
2014-06-30 19:58:38,229 WARN  [RpcServer.handler=40,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10268ms
2014-06-30 19:58:38,230 WARN  [RpcServer.handler=13,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10380ms
2014-06-30 19:58:38,230 WARN  [RpcServer.handler=20,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10270ms
2014-06-30 19:58:38,230 WARN  [RpcServer.handler=47,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10383ms
2014-06-30 19:58:38,231 WARN  [RpcServer.handler=46,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10385ms
2014-06-30 19:58:38,232 WARN  [RpcServer.handler=28,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10272ms
2014-06-30 19:58:38,232 WARN  [RpcServer.handler=23,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10272ms
2014-06-30 19:58:38,232 WARN  [RpcServer.handler=12,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10273ms
2014-06-30 19:58:38,232 WARN  [RpcServer.handler=34,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10273ms
2014-06-30 19:58:38,234 WARN  [RpcServer.handler=9,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10275ms
2014-06-30 19:58:38,235 WARN  [RpcServer.handler=15,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10276ms
2014-06-30 19:58:38,237 WARN  [RpcServer.handler=7,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10278ms
2014-06-30 19:58:38,238 WARN  [RpcServer.handler=35,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10395ms
2014-06-30 19:58:38,239 WARN  [RpcServer.handler=3,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10399ms
2014-06-30 19:58:38,240 WARN  [RpcServer.handler=44,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10282ms
2014-06-30 19:58:38,240 WARN  [RpcServer.handler=22,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10390ms
2014-06-30 19:58:38,241 WARN  [RpcServer.handler=48,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10390ms
2014-06-30 19:58:38,241 WARN  [RpcServer.handler=16,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10391ms
2014-06-30 19:58:38,242 WARN  [RpcServer.handler=31,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10401ms
2014-06-30 19:58:38,242 WARN  [RpcServer.handler=6,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10400ms
2014-06-30 19:58:38,243 WARN  [RpcServer.handler=42,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10401ms
2014-06-30 19:58:38,243 WARN  [RpcServer.handler=43,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10401ms
2014-06-30 19:58:38,243 WARN  [RpcServer.handler=19,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10399ms
2014-06-30 19:58:38,244 WARN  [RpcServer.handler=10,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10399ms
2014-06-30 19:58:38,244 WARN  [RpcServer.handler=2,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10400ms
2014-06-30 19:58:38,244 WARN  [RpcServer.handler=0,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10398ms
2014-06-30 19:58:38,244 WARN  [RpcServer.handler=26,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10397ms
2014-06-30 19:58:38,245 WARN  [RpcServer.handler=45,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10396ms
2014-06-30 19:58:38,245 WARN  [RpcServer.handler=1,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10397ms
2014-06-30 19:58:38,245 WARN  [RpcServer.handler=49,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10397ms
2014-06-30 19:58:38,245 WARN  [RpcServer.handler=14,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10396ms
2014-06-30 19:58:38,246 WARN  [RpcServer.handler=27,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10395ms
2014-06-30 19:58:38,246 WARN  [RpcServer.handler=39,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10283ms
2014-06-30 19:58:38,246 WARN  [RpcServer.handler=32,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10285ms
2014-06-30 19:58:38,247 WARN  [RpcServer.handler=24,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10286ms
2014-06-30 19:58:38,248 WARN  [RpcServer.handler=37,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10286ms
2014-06-30 19:58:38,248 WARN  [RpcServer.handler=25,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10227ms
2014-06-30 19:58:38,248 WARN  [RpcServer.handler=30,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10227ms
2014-06-30 19:58:38,248 WARN  [RpcServer.handler=41,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10225ms
2014-06-30 19:58:38,249 WARN  [RpcServer.handler=18,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10223ms
2014-06-30 19:58:38,249 WARN  [RpcServer.handler=17,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10228ms
2014-06-30 19:58:38,249 WARN  [RpcServer.handler=8,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10225ms
2014-06-30 19:58:38,249 WARN  [RpcServer.handler=21,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10225ms
2014-06-30 19:58:38,249 WARN  [RpcServer.handler=11,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 10223ms
2014-06-30 19:58:40,824 WARN  [RpcServer.handler=36,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 15153ms
2014-06-30 19:58:40,824 INFO  [JvmPauseMonitor] util.JvmPauseMonitor: Detected pause in JVM or host machine (eg GC): pause of approximately 2096ms
GC pool 'ConcurrentMarkSweep' had collection(s): count=1 time=2448ms
2014-06-30 19:58:40,824 WARN  [RpcServer.handler=38,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 15234ms
2014-06-30 19:58:43,309 WARN  [RpcServer.handler=11,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 15283ms
2014-06-30 19:58:43,309 WARN  [RpcServer.handler=24,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 15348ms
2014-06-30 19:58:43,517 INFO  [JvmPauseMonitor] util.JvmPauseMonitor: Detected pause in JVM or host machine (eg GC): pause of approximately 2192ms
GC pool 'ConcurrentMarkSweep' had collection(s): count=1 time=2554ms
2014-06-30 19:58:43,519 WARN  [RpcServer.handler=37,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 15558ms
2014-06-30 19:58:43,520 WARN  [RpcServer.handler=25,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 15498ms
2014-06-30 19:58:43,520 WARN  [RpcServer.handler=30,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 15499ms
2014-06-30 19:58:43,520 WARN  [RpcServer.handler=41,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 15497ms
2014-06-30 19:58:43,520 WARN  [RpcServer.handler=18,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 15495ms
2014-06-30 19:58:43,520 WARN  [RpcServer.handler=17,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 15499ms
2014-06-30 19:58:43,520 WARN  [RpcServer.handler=8,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 15496ms
2014-06-30 19:58:43,520 WARN  [RpcServer.handler=21,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 15496ms
2014-06-30 19:58:43,521 WARN  [RpcServer.handler=29,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 15494ms
2014-06-30 19:58:43,521 WARN  [RpcServer.handler=4,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 15493ms
2014-06-30 19:58:43,521 WARN  [RpcServer.handler=33,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 15492ms
2014-06-30 19:58:43,521 WARN  [RpcServer.handler=5,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 15558ms
2014-06-30 19:58:43,521 WARN  [RpcServer.handler=40,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 15560ms
2014-06-30 19:58:43,521 WARN  [RpcServer.handler=13,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 15671ms
2014-06-30 19:58:43,522 WARN  [RpcServer.handler=20,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 15562ms
2014-06-30 19:58:43,522 WARN  [RpcServer.handler=47,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 15675ms
2014-06-30 19:58:43,522 WARN  [RpcServer.handler=46,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 15677ms
2014-06-30 19:58:43,522 WARN  [RpcServer.handler=28,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 15562ms
2014-06-30 19:58:43,522 WARN  [RpcServer.handler=23,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 15562ms
2014-06-30 19:58:43,526 WARN  [RpcServer.handler=12,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 15567ms
2014-06-30 19:58:43,527 WARN  [RpcServer.handler=34,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 15567ms
2014-06-30 19:58:43,527 WARN  [RpcServer.handler=9,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 15569ms
2014-06-30 19:58:43,527 WARN  [RpcServer.handler=15,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 15569ms
2014-06-30 19:58:43,527 WARN  [RpcServer.handler=7,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 15569ms
2014-06-30 19:58:43,527 WARN  [RpcServer.handler=35,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 15685ms
2014-06-30 19:58:43,528 WARN  [RpcServer.handler=3,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 15688ms
2014-06-30 19:58:43,528 WARN  [RpcServer.handler=44,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 15571ms
2014-06-30 19:58:43,528 WARN  [RpcServer.handler=22,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 15678ms
2014-06-30 19:58:43,529 WARN  [RpcServer.handler=48,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 15679ms
2014-06-30 19:58:43,529 WARN  [RpcServer.handler=16,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 15679ms
2014-06-30 19:58:43,529 WARN  [RpcServer.handler=31,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 15689ms
2014-06-30 19:58:43,529 WARN  [RpcServer.handler=6,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 15687ms
2014-06-30 19:58:43,530 WARN  [RpcServer.handler=42,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 15689ms
2014-06-30 19:58:43,530 WARN  [RpcServer.handler=43,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 15688ms
2014-06-30 19:58:43,530 WARN  [RpcServer.handler=19,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 15686ms
2014-06-30 19:58:43,531 WARN  [RpcServer.handler=10,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 15687ms
2014-06-30 19:58:43,532 WARN  [RpcServer.handler=2,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 15688ms
2014-06-30 19:58:43,532 WARN  [RpcServer.handler=0,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 15686ms
2014-06-30 19:58:43,532 WARN  [RpcServer.handler=26,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 15685ms
2014-06-30 19:58:43,533 WARN  [RpcServer.handler=45,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 15685ms
2014-06-30 19:58:43,533 WARN  [RpcServer.handler=1,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 15685ms
2014-06-30 19:58:43,534 WARN  [RpcServer.handler=49,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 15685ms
2014-06-30 19:58:43,534 WARN  [RpcServer.handler=14,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 15685ms
2014-06-30 19:58:43,534 WARN  [RpcServer.handler=27,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 15684ms
2014-06-30 19:58:43,534 WARN  [RpcServer.handler=39,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 15571ms
2014-06-30 19:58:43,535 WARN  [RpcServer.handler=32,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 15574ms
2014-06-30 19:58:46,137 WARN  [RpcServer.handler=38,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 20547ms
2014-06-30 19:58:46,138 INFO  [JvmPauseMonitor] util.JvmPauseMonitor: Detected pause in JVM or host machine (eg GC): pause of approximately 2118ms
GC pool 'ConcurrentMarkSweep' had collection(s): count=1 time=2532ms
2014-06-30 19:58:46,372 WARN  [RpcServer.handler=36,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 20701ms
2014-06-30 19:58:48,969 INFO  [JvmPauseMonitor] util.JvmPauseMonitor: Detected pause in JVM or host machine (eg GC): pause of approximately 2095ms
GC pool 'ConcurrentMarkSweep' had collection(s): count=1 time=2724ms
2014-06-30 19:58:48,969 WARN  [RpcServer.handler=32,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 21006ms
2014-06-30 19:58:48,969 WARN  [RpcServer.handler=11,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 20943ms
2014-06-30 19:58:48,970 WARN  [RpcServer.handler=24,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 21009ms
2014-06-30 19:58:48,970 WARN  [RpcServer.handler=37,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 21009ms
2014-06-30 19:58:48,970 WARN  [RpcServer.handler=25,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 20949ms
2014-06-30 19:58:48,970 WARN  [RpcServer.handler=30,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 20949ms
2014-06-30 19:58:48,970 WARN  [RpcServer.handler=41,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 20947ms
2014-06-30 19:58:48,970 WARN  [RpcServer.handler=18,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 20945ms
2014-06-30 19:58:48,971 WARN  [RpcServer.handler=17,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 20949ms
2014-06-30 19:58:48,971 WARN  [RpcServer.handler=8,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 20947ms
2014-06-30 19:58:48,971 WARN  [RpcServer.handler=21,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 20947ms
2014-06-30 19:58:48,971 WARN  [RpcServer.handler=29,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 20944ms
2014-06-30 19:58:48,971 WARN  [RpcServer.handler=4,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 20943ms
2014-06-30 19:58:48,971 WARN  [RpcServer.handler=33,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 20942ms
2014-06-30 19:58:48,972 WARN  [RpcServer.handler=5,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 21008ms
2014-06-30 19:58:48,972 WARN  [RpcServer.handler=40,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 21011ms
2014-06-30 19:58:48,972 WARN  [RpcServer.handler=13,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 21122ms
2014-06-30 19:58:48,972 WARN  [RpcServer.handler=20,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 21012ms
2014-06-30 19:58:48,972 WARN  [RpcServer.handler=47,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 21125ms
2014-06-30 19:58:48,972 WARN  [RpcServer.handler=46,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 21127ms
2014-06-30 19:58:48,972 WARN  [RpcServer.handler=28,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 21012ms
2014-06-30 19:58:48,973 WARN  [RpcServer.handler=23,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 21013ms
2014-06-30 19:58:48,973 WARN  [RpcServer.handler=12,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 21014ms
2014-06-30 19:58:48,973 WARN  [RpcServer.handler=34,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 21014ms
2014-06-30 19:58:48,973 WARN  [RpcServer.handler=9,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 21015ms
2014-06-30 19:58:48,973 WARN  [RpcServer.handler=15,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 21015ms
2014-06-30 19:58:48,974 WARN  [RpcServer.handler=7,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 21015ms
2014-06-30 19:58:48,974 WARN  [RpcServer.handler=35,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 21132ms
2014-06-30 19:58:48,974 WARN  [RpcServer.handler=3,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 21134ms
2014-06-30 19:58:48,974 WARN  [RpcServer.handler=44,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 21017ms
2014-06-30 19:58:48,974 WARN  [RpcServer.handler=22,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 21124ms
2014-06-30 19:58:48,975 WARN  [RpcServer.handler=48,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 21125ms
2014-06-30 19:58:48,975 WARN  [RpcServer.handler=16,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 21125ms
2014-06-30 19:58:48,975 WARN  [RpcServer.handler=31,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 21135ms
2014-06-30 19:58:48,975 WARN  [RpcServer.handler=6,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 21133ms
2014-06-30 19:58:48,975 WARN  [RpcServer.handler=42,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 21134ms
2014-06-30 19:58:48,976 WARN  [RpcServer.handler=43,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 21134ms
2014-06-30 19:58:48,976 WARN  [RpcServer.handler=19,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 21132ms
2014-06-30 19:58:48,976 WARN  [RpcServer.handler=10,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 21132ms
2014-06-30 19:58:48,977 WARN  [RpcServer.handler=2,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 21133ms
2014-06-30 19:58:48,977 WARN  [RpcServer.handler=0,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 21131ms
2014-06-30 19:58:48,978 WARN  [RpcServer.handler=26,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 21130ms
2014-06-30 19:58:48,978 WARN  [RpcServer.handler=45,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 21130ms
2014-06-30 19:58:48,978 WARN  [RpcServer.handler=1,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 21130ms
2014-06-30 19:58:48,979 WARN  [RpcServer.handler=49,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 21130ms
2014-06-30 19:58:48,979 WARN  [RpcServer.handler=14,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 21130ms
2014-06-30 19:58:48,979 WARN  [RpcServer.handler=27,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 21129ms
2014-06-30 19:58:48,979 WARN  [RpcServer.handler=39,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 21016ms
2014-06-30 19:58:51,549 WARN  [RpcServer.handler=36,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 25878ms
2014-06-30 19:58:51,550 WARN  [RpcServer.handler=38,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 25959ms
2014-06-30 19:58:51,558 INFO  [JvmPauseMonitor] util.JvmPauseMonitor: Detected pause in JVM or host machine (eg GC): pause of approximately 1859ms
GC pool 'ConcurrentMarkSweep' had collection(s): count=1 time=2524ms
2014-06-30 19:58:54,154 WARN  [RpcServer.handler=39,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 26191ms
2014-06-30 19:58:54,154 WARN  [RpcServer.handler=10,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 26310ms
2014-06-30 19:58:54,154 WARN  [RpcServer.handler=2,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 26310ms
2014-06-30 19:58:54,154 WARN  [RpcServer.handler=6,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 26312ms
2014-06-30 19:58:54,155 WARN  [RpcServer.handler=0,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 26309ms
2014-06-30 19:58:54,155 WARN  [RpcServer.handler=44,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 26198ms
2014-06-30 19:58:54,155 WARN  [RpcServer.handler=26,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 26308ms
2014-06-30 19:58:54,155 WARN  [RpcServer.handler=45,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 26307ms
2014-06-30 19:58:54,155 WARN  [RpcServer.handler=15,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 26197ms
2014-06-30 19:58:54,155 WARN  [RpcServer.handler=1,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 26307ms
2014-06-30 19:58:54,155 WARN  [RpcServer.handler=28,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 26195ms
2014-06-30 19:58:54,155 WARN  [RpcServer.handler=49,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 26307ms
2014-06-30 19:58:54,155 WARN  [RpcServer.handler=27,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 26305ms
2014-06-30 19:58:54,155 WARN  [RpcServer.handler=14,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 26306ms
2014-06-30 19:58:54,156 WARN  [RpcServer.handler=46,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 26311ms
2014-06-30 19:58:54,156 WARN  [RpcServer.handler=23,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 26196ms
2014-06-30 19:58:54,156 WARN  [RpcServer.handler=12,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 26197ms
2014-06-30 19:58:54,156 WARN  [RpcServer.handler=34,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 26197ms
2014-06-30 19:58:54,156 WARN  [RpcServer.handler=9,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 26198ms
2014-06-30 19:58:54,156 WARN  [RpcServer.handler=7,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 26198ms
2014-06-30 19:58:54,156 WARN  [RpcServer.handler=35,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 26314ms
2014-06-30 19:58:54,156 WARN  [RpcServer.handler=3,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 26316ms
2014-06-30 19:58:54,156 WARN  [RpcServer.handler=22,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 26306ms
2014-06-30 19:58:54,157 WARN  [RpcServer.handler=48,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 26307ms
2014-06-30 19:58:54,157 WARN  [RpcServer.handler=16,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 26307ms
2014-06-30 19:58:54,157 WARN  [RpcServer.handler=31,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 26317ms
2014-06-30 19:58:54,157 WARN  [RpcServer.handler=42,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 26316ms
2014-06-30 19:58:54,157 WARN  [RpcServer.handler=43,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 26315ms
2014-06-30 19:58:54,157 WARN  [RpcServer.handler=19,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 26313ms
2014-06-30 19:58:54,158 WARN  [RpcServer.handler=47,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 26311ms
2014-06-30 19:58:54,158 WARN  [RpcServer.handler=40,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 26197ms
2014-06-30 19:58:54,159 WARN  [RpcServer.handler=32,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 26198ms
2014-06-30 19:58:54,160 INFO  [JvmPauseMonitor] util.JvmPauseMonitor: Detected pause in JVM or host machine (eg GC): pause of approximately 2101ms
GC pool 'ConcurrentMarkSweep' had collection(s): count=1 time=2550ms
2014-06-30 19:58:54,161 WARN  [RpcServer.handler=11,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 26134ms
2014-06-30 19:58:54,162 WARN  [RpcServer.handler=24,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 26201ms
2014-06-30 19:58:54,163 WARN  [RpcServer.handler=37,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 26202ms
2014-06-30 19:58:54,163 WARN  [RpcServer.handler=25,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 26142ms
2014-06-30 19:58:54,164 WARN  [RpcServer.handler=30,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 26143ms
2014-06-30 19:58:54,164 WARN  [RpcServer.handler=41,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 26141ms
2014-06-30 19:58:54,165 WARN  [RpcServer.handler=18,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 26139ms
2014-06-30 19:58:54,165 WARN  [RpcServer.handler=17,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 26144ms
2014-06-30 19:58:54,166 WARN  [RpcServer.handler=8,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 26142ms
2014-06-30 19:58:54,167 WARN  [RpcServer.handler=21,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 26143ms
2014-06-30 19:58:54,168 WARN  [RpcServer.handler=29,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 26141ms
2014-06-30 19:58:54,170 WARN  [RpcServer.handler=4,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 26142ms
2014-06-30 19:58:54,170 WARN  [RpcServer.handler=33,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 26141ms
2014-06-30 19:58:54,170 WARN  [RpcServer.handler=5,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 26207ms
2014-06-30 19:58:54,171 WARN  [RpcServer.handler=13,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 26321ms
2014-06-30 19:58:54,171 WARN  [RpcServer.handler=20,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 26211ms
2014-06-30 19:58:56,814 WARN  [RpcServer.handler=38,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 30972ms
2014-06-30 19:58:56,814 WARN  [RpcServer.handler=36,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 31143ms
2014-06-30 19:58:56,815 INFO  [JvmPauseMonitor] util.JvmPauseMonitor: Detected pause in JVM or host machine (eg GC): pause of approximately 1901ms
GC pool 'ConcurrentMarkSweep' had collection(s): count=1 time=2374ms
2014-06-30 19:58:59,638 WARN  [RpcServer.handler=20,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 31678ms
2014-06-30 19:58:59,638 WARN  [RpcServer.handler=25,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 31617ms
2014-06-30 19:58:59,638 WARN  [RpcServer.handler=30,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 31617ms
2014-06-30 19:58:59,638 WARN  [RpcServer.handler=41,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 31615ms
2014-06-30 19:58:59,638 WARN  [RpcServer.handler=34,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 31679ms
2014-06-30 19:58:59,639 WARN  [RpcServer.handler=18,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 31613ms
2014-06-30 19:58:59,639 WARN  [RpcServer.handler=27,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 31789ms
2014-06-30 19:58:59,639 WARN  [RpcServer.handler=17,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 31618ms
2014-06-30 19:58:59,639 WARN  [RpcServer.handler=45,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 31791ms
2014-06-30 19:58:59,639 WARN  [RpcServer.handler=6,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 31797ms
2014-06-30 19:58:59,639 WARN  [RpcServer.handler=8,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 31615ms
2014-06-30 19:58:59,639 WARN  [RpcServer.handler=21,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 31615ms
2014-06-30 19:58:59,639 INFO  [JvmPauseMonitor] util.JvmPauseMonitor: Detected pause in JVM or host machine (eg GC): pause of approximately 2324ms
GC pool 'ConcurrentMarkSweep' had collection(s): count=1 time=3061ms
2014-06-30 19:58:59,639 WARN  [RpcServer.handler=29,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 31612ms
2014-06-30 19:58:59,640 WARN  [RpcServer.handler=4,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 31611ms
2014-06-30 19:59:02,201 WARN  [RpcServer.handler=33,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 34172ms
2014-06-30 19:59:02,202 WARN  [RpcServer.handler=5,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 34239ms
2014-06-30 19:59:02,202 WARN  [RpcServer.handler=13,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 34352ms
2014-06-30 19:59:02,202 WARN  [RpcServer.handler=38,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 36612ms
2014-06-30 19:59:02,202 WARN  [RpcServer.handler=36,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 36531ms
2014-06-30 19:59:02,202 WARN  [RpcServer.handler=39,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 34239ms
2014-06-30 19:59:02,203 WARN  [RpcServer.handler=10,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 34358ms
2014-06-30 19:59:02,203 WARN  [RpcServer.handler=2,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 34359ms
2014-06-30 19:59:02,203 WARN  [RpcServer.handler=0,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 34357ms
2014-06-30 19:59:02,203 INFO  [JvmPauseMonitor] util.JvmPauseMonitor: Detected pause in JVM or host machine (eg GC): pause of approximately 2060ms
GC pool 'ConcurrentMarkSweep' had collection(s): count=1 time=2558ms
2014-06-30 19:59:07,192 WARN  [RpcServer.handler=44,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 34246ms
2014-06-30 19:59:07,192 INFO  [JvmPauseMonitor] util.JvmPauseMonitor: Detected pause in JVM or host machine (eg GC): pause of approximately 4488ms
GC pool 'ConcurrentMarkSweep' had collection(s): count=2 time=4986ms
2014-06-30 19:59:07,192 WARN  [RpcServer.handler=26,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 39345ms
2014-06-30 19:59:07,192 WARN  [RpcServer.handler=15,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 39234ms
2014-06-30 19:59:12,526 WARN  [RpcServer.handler=1,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 39571ms
2014-06-30 19:59:12,526 INFO  [JvmPauseMonitor] util.JvmPauseMonitor: Detected pause in JVM or host machine (eg GC): pause of approximately 4833ms
GC pool 'ConcurrentMarkSweep' had collection(s): count=2 time=5329ms
2014-06-30 19:59:14,848 WARN  [RpcServer.handler=28,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 46888ms
2014-06-30 19:59:14,849 WARN  [RpcServer.handler=49,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 47001ms
2014-06-30 19:59:20,403 INFO  [main] zookeeper.ZooKeeper: Client environment:zookeeper.version=3.4.6-1569965, built on 02/20/2014 09:09 GMT
2014-06-30 19:59:20,404 INFO  [main] zookeeper.ZooKeeper: Client environment:host.name=sceplus-vm48.almaden.ibm.com
2014-06-30 19:59:20,404 INFO  [main] zookeeper.ZooKeeper: Client environment:java.version=1.6.0_31
2014-06-30 19:59:20,404 INFO  [main] zookeeper.ZooKeeper: Client environment:java.vendor=Sun Microsystems Inc.
2014-06-30 19:59:20,404 INFO  [main] zookeeper.ZooKeeper: Client environment:java.home=/usr/lib/jvm/java-6-openjdk-amd64/jre
2014-06-30 19:59:20,404 INFO  [main] zookeeper.ZooKeeper: Client environment:java.class.path=/home/hadoop/hbase/bin/../conf:/usr/lib/jvm/java-1.6.0-openjdk-amd64//lib/tools.jar:/home/hadoop/hbase/bin/..:/home/hadoop/hbase/bin/../lib/activation-1.1.jar:/home/hadoop/hbase/bin/../lib/asm-3.1.jar:/home/hadoop/hbase/bin/../lib/commons-beanutils-1.7.0.jar:/home/hadoop/hbase/bin/../lib/commons-beanutils-core-1.8.0.jar:/home/hadoop/hbase/bin/../lib/commons-cli-1.2.jar:/home/hadoop/hbase/bin/../lib/commons-codec-1.7.jar:/home/hadoop/hbase/bin/../lib/commons-collections-3.2.1.jar:/home/hadoop/hbase/bin/../lib/commons-configuration-1.6.jar:/home/hadoop/hbase/bin/../lib/commons-digester-1.8.jar:/home/hadoop/hbase/bin/../lib/commons-el-1.0.jar:/home/hadoop/hbase/bin/../lib/commons-httpclient-3.1.jar:/home/hadoop/hbase/bin/../lib/commons-io-2.4.jar:/home/hadoop/hbase/bin/../lib/commons-lang-2.6.jar:/home/hadoop/hbase/bin/../lib/commons-logging-1.1.1.jar:/home/hadoop/hbase/bin/../lib/commons-math-2.1.jar:/home/hadoop/hbase/bin/../lib/commons-net-1.4.1.jar:/home/hadoop/hbase/bin/../lib/findbugs-annotations-1.3.9-1.jar:/home/hadoop/hbase/bin/../lib/guava-12.0.1.jar:/home/hadoop/hbase/bin/../lib/hadoop-core-1.2.1.jar:/home/hadoop/hbase/bin/../lib/hamcrest-core-1.3.jar:/home/hadoop/hbase/bin/../lib/hbase-client-0.98.3-hadoop1.jar:/home/hadoop/hbase/bin/../lib/hbase-common-0.98.3-hadoop1.jar:/home/hadoop/hbase/bin/../lib/hbase-common-0.98.3-hadoop1-tests.jar:/home/hadoop/hbase/bin/../lib/hbase-examples-0.98.3-hadoop1.jar:/home/hadoop/hbase/bin/../lib/hbase-hadoop1-compat-0.98.3-hadoop1.jar:/home/hadoop/hbase/bin/../lib/hbase-hadoop-compat-0.98.3-hadoop1.jar:/home/hadoop/hbase/bin/../lib/hbase-it-0.98.3-hadoop1.jar:/home/hadoop/hbase/bin/../lib/hbase-it-0.98.3-hadoop1-tests.jar:/home/hadoop/hbase/bin/../lib/hbase-prefix-tree-0.98.3-hadoop1.jar:/home/hadoop/hbase/bin/../lib/hbase-protocol-0.98.3-hadoop1.jar:/home/hadoop/hbase/bin/../lib/hbase-server-0.98.3-hadoop1.jar:/home/hadoop/hbase/bin/../lib/hbase-server-0.98.3-hadoop1-tests.jar:/home/hadoop/hbase/bin/../lib/hbase-shell-0.98.3-hadoop1.jar:/home/hadoop/hbase/bin/../lib/hbase-testing-util-0.98.3-hadoop1.jar:/home/hadoop/hbase/bin/../lib/hbase-thrift-0.98.3-hadoop1.jar:/home/hadoop/hbase/bin/../lib/high-scale-lib-1.1.1.jar:/home/hadoop/hbase/bin/../lib/htrace-core-2.04.jar:/home/hadoop/hbase/bin/../lib/httpclient-4.1.3.jar:/home/hadoop/hbase/bin/../lib/httpcore-4.1.3.jar:/home/hadoop/hbase/bin/../lib/jackson-core-asl-1.8.8.jar:/home/hadoop/hbase/bin/../lib/jackson-jaxrs-1.8.8.jar:/home/hadoop/hbase/bin/../lib/jackson-mapper-asl-1.8.8.jar:/home/hadoop/hbase/bin/../lib/jackson-xc-1.8.8.jar:/home/hadoop/hbase/bin/../lib/jamon-runtime-2.3.1.jar:/home/hadoop/hbase/bin/../lib/jasper-compiler-5.5.23.jar:/home/hadoop/hbase/bin/../lib/jasper-runtime-5.5.23.jar:/home/hadoop/hbase/bin/../lib/jaxb-api-2.2.2.jar:/home/hadoop/hbase/bin/../lib/jaxb-impl-2.2.3-1.jar:/home/hadoop/hbase/bin/../lib/jersey-core-1.8.jar:/home/hadoop/hbase/bin/../lib/jersey-json-1.8.jar:/home/hadoop/hbase/bin/../lib/jersey-server-1.8.jar:/home/hadoop/hbase/bin/../lib/jettison-1.3.1.jar:/home/hadoop/hbase/bin/../lib/jetty-6.1.26.jar:/home/hadoop/hbase/bin/../lib/jetty-sslengine-6.1.26.jar:/home/hadoop/hbase/bin/../lib/jetty-util-6.1.26.jar:/home/hadoop/hbase/bin/../lib/jruby-complete-1.6.8.jar:/home/hadoop/hbase/bin/../lib/jsp-2.1-6.1.14.jar:/home/hadoop/hbase/bin/../lib/jsp-api-2.1-6.1.14.jar:/home/hadoop/hbase/bin/../lib/jsr305-1.3.9.jar:/home/hadoop/hbase/bin/../lib/junit-4.11.jar:/home/hadoop/hbase/bin/../lib/libthrift-0.9.0.jar:/home/hadoop/hbase/bin/../lib/log4j-1.2.17.jar:/home/hadoop/hbase/bin/../lib/metrics-core-2.1.2.jar:/home/hadoop/hbase/bin/../lib/netty-3.6.6.Final.jar:/home/hadoop/hbase/bin/../lib/protobuf-java-2.5.0.jar:/home/hadoop/hbase/bin/../lib/servlet-api-2.5-6.1.14.jar:/home/hadoop/hbase/bin/../lib/slf4j-api-1.6.4.jar:/home/hadoop/hbase/bin/../lib/slf4j-log4j12-1.6.4.jar:/home/hadoop/hbase/bin/../lib/xmlenc-0.52.jar:/home/hadoop/hbase/bin/../lib/zookeeper-3.4.6.jar:
2014-06-30 19:59:20,404 INFO  [main] zookeeper.ZooKeeper: Client environment:java.library.path=/home/hadoop/hbase/bin/../lib/native/Linux-amd64-64
2014-06-30 19:59:20,404 INFO  [main] zookeeper.ZooKeeper: Client environment:java.io.tmpdir=/tmp
2014-06-30 19:59:20,404 INFO  [main] zookeeper.ZooKeeper: Client environment:java.compiler=<NA>
2014-06-30 19:59:20,404 INFO  [main] zookeeper.ZooKeeper: Client environment:os.name=Linux
2014-06-30 19:59:20,404 INFO  [main] zookeeper.ZooKeeper: Client environment:os.arch=amd64
2014-06-30 19:59:20,404 INFO  [main] zookeeper.ZooKeeper: Client environment:os.version=3.13.0-24-generic
2014-06-30 19:59:20,404 INFO  [main] zookeeper.ZooKeeper: Client environment:user.name=hadoop
2014-06-30 19:59:20,404 INFO  [main] zookeeper.ZooKeeper: Client environment:user.home=/home/hadoop
2014-06-30 19:59:20,404 INFO  [main] zookeeper.ZooKeeper: Client environment:user.dir=/home/hadoop/hbase-0.98.3-hadoop1
2014-06-30 19:59:20,406 INFO  [main] zookeeper.ZooKeeper: Initiating client connection, connectString=sceplus-vm49.almaden.ibm.com:2181,sceplus-vm48.almaden.ibm.com:2181 sessionTimeout=30000 watcher=org.apache.zookeeper.ZooKeeperMain$MyWatcher@20e8ac93
2014-06-30 19:59:20,454 INFO  [main-SendThread(slave1:2181)] zookeeper.ClientCnxn: Opening socket connection to server slave1/9.1.143.59:2181. Will not attempt to authenticate using SASL (unknown error)
2014-06-30 19:59:20,463 INFO  [main-SendThread(slave1:2181)] zookeeper.ClientCnxn: Socket connection established to slave1/9.1.143.59:2181, initiating session
2014-06-30 19:59:20,474 INFO  [main-SendThread(slave1:2181)] zookeeper.ClientCnxn: Session establishment complete on server slave1/9.1.143.59:2181, sessionid = 0x146efd1264e000d, negotiated timeout = 30000
