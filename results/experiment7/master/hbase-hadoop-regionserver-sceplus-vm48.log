Tue Jul  1 15:53:46 PDT 2014 Starting regionserver on sceplus-vm48
core file size          (blocks, -c) 0
data seg size           (kbytes, -d) unlimited
scheduling priority             (-e) 0
file size               (blocks, -f) unlimited
pending signals                 (-i) 128203
max locked memory       (kbytes, -l) 64
max memory size         (kbytes, -m) unlimited
open files                      (-n) 32768
pipe size            (512 bytes, -p) 8
POSIX message queues     (bytes, -q) 819200
real-time priority              (-r) 0
stack size              (kbytes, -s) 8192
cpu time               (seconds, -t) unlimited
max user processes              (-u) 128203
virtual memory          (kbytes, -v) unlimited
file locks                      (-x) unlimited
2014-07-01 15:53:46,828 INFO  [main] util.VersionInfo: HBase 0.98.3-hadoop1
2014-07-01 15:53:46,829 INFO  [main] util.VersionInfo: Subversion git://acer/usr/src/Hadoop/hbase -r d5e65a9144e315bb0a964e7730871af32f5018d5
2014-07-01 15:53:46,829 INFO  [main] util.VersionInfo: Compiled by apurtell on Sat May 31 19:34:57 PDT 2014
2014-07-01 15:53:47,057 INFO  [main] util.ServerCommandLine: env:JAVA_HOME=/usr/lib/jvm/java-1.6.0-openjdk-amd64/
2014-07-01 15:53:47,057 INFO  [main] util.ServerCommandLine: env:SHLVL=3
2014-07-01 15:53:47,057 INFO  [main] util.ServerCommandLine: env:HBASE_LOG_DIR=/home/hadoop/hbase/bin/../logs
2014-07-01 15:53:47,057 INFO  [main] util.ServerCommandLine: env:HBASE_HOME=/home/hadoop/hbase/bin/..
2014-07-01 15:53:47,057 INFO  [main] util.ServerCommandLine: env:HBASE_OPTS=-XX:+UseConcMarkSweepGC   -Dhbase.log.dir=/home/hadoop/hbase/bin/../logs -Dhbase.log.file=hbase-hadoop-regionserver-sceplus-vm48.log -Dhbase.home.dir=/home/hadoop/hbase/bin/.. -Dhbase.id.str=hadoop -Dhbase.root.logger=INFO,RFA -Djava.library.path=/home/hadoop/hbase/bin/../lib/native/Linux-amd64-64 -Dhbase.security.logger=INFO,RFAS
2014-07-01 15:53:47,057 INFO  [main] util.ServerCommandLine: env:HBASE_ENV_INIT=true
2014-07-01 15:53:47,058 INFO  [main] util.ServerCommandLine: env:SSH_CLIENT=9.1.143.58 44981 22
2014-07-01 15:53:47,058 INFO  [main] util.ServerCommandLine: env:HBASE_HEAPSIZE=10240
2014-07-01 15:53:47,058 INFO  [main] util.ServerCommandLine: env:MAIL=/var/mail/hadoop
2014-07-01 15:53:47,058 INFO  [main] util.ServerCommandLine: env:HBASE_ZNODE_FILE=/var/hadoop/pids/hbase-hadoop-regionserver.znode
2014-07-01 15:53:47,058 INFO  [main] util.ServerCommandLine: env:PWD=/home/hadoop/hbase
2014-07-01 15:53:47,058 INFO  [main] util.ServerCommandLine: env:LOGNAME=hadoop
2014-07-01 15:53:47,058 INFO  [main] util.ServerCommandLine: env:HBASE_MANAGES_ZK=true
2014-07-01 15:53:47,058 INFO  [main] util.ServerCommandLine: env:HBASE_NICENESS=0
2014-07-01 15:53:47,058 INFO  [main] util.ServerCommandLine: env:HBASE_REST_OPTS=
2014-07-01 15:53:47,059 INFO  [main] util.ServerCommandLine: env:LD_LIBRARY_PATH=/usr/lib/jvm/java-6-openjdk-amd64/jre/lib/amd64/server:/usr/lib/jvm/java-6-openjdk-amd64/jre/lib/amd64:/usr/lib/jvm/java-6-openjdk-amd64/jre/../lib/amd64::/home/hadoop/hbase/bin/../lib/native/Linux-amd64-64
2014-07-01 15:53:47,059 INFO  [main] util.ServerCommandLine: env:MALLOC_ARENA_MAX=4
2014-07-01 15:53:47,059 INFO  [main] util.ServerCommandLine: env:SSH_CONNECTION=9.1.143.58 44981 9.1.143.58 22
2014-07-01 15:53:47,059 INFO  [main] util.ServerCommandLine: env:SHELL=/bin/bash
2014-07-01 15:53:47,059 INFO  [main] util.ServerCommandLine: env:HBASE_PID_DIR=/var/hadoop/pids
2014-07-01 15:53:47,059 INFO  [main] util.ServerCommandLine: env:HBASE_ROOT_LOGGER=INFO,RFA
2014-07-01 15:53:47,061 INFO  [main] util.ServerCommandLine: env:CLASSPATH=/home/hadoop/hbase/conf:/usr/lib/jvm/java-1.6.0-openjdk-amd64//lib/tools.jar:/home/hadoop/hbase/bin/..:/home/hadoop/hbase/bin/../lib/activation-1.1.jar:/home/hadoop/hbase/bin/../lib/asm-3.1.jar:/home/hadoop/hbase/bin/../lib/commons-beanutils-1.7.0.jar:/home/hadoop/hbase/bin/../lib/commons-beanutils-core-1.8.0.jar:/home/hadoop/hbase/bin/../lib/commons-cli-1.2.jar:/home/hadoop/hbase/bin/../lib/commons-codec-1.7.jar:/home/hadoop/hbase/bin/../lib/commons-collections-3.2.1.jar:/home/hadoop/hbase/bin/../lib/commons-configuration-1.6.jar:/home/hadoop/hbase/bin/../lib/commons-digester-1.8.jar:/home/hadoop/hbase/bin/../lib/commons-el-1.0.jar:/home/hadoop/hbase/bin/../lib/commons-httpclient-3.1.jar:/home/hadoop/hbase/bin/../lib/commons-io-2.4.jar:/home/hadoop/hbase/bin/../lib/commons-lang-2.6.jar:/home/hadoop/hbase/bin/../lib/commons-logging-1.1.1.jar:/home/hadoop/hbase/bin/../lib/commons-math-2.1.jar:/home/hadoop/hbase/bin/../lib/commons-net-1.4.1.jar:/home/hadoop/hbase/bin/../lib/findbugs-annotations-1.3.9-1.jar:/home/hadoop/hbase/bin/../lib/guava-12.0.1.jar:/home/hadoop/hbase/bin/../lib/hadoop-core-1.2.1.jar:/home/hadoop/hbase/bin/../lib/hamcrest-core-1.3.jar:/home/hadoop/hbase/bin/../lib/hbase-client-0.98.3-hadoop1.jar:/home/hadoop/hbase/bin/../lib/hbase-common-0.98.3-hadoop1.jar:/home/hadoop/hbase/bin/../lib/hbase-common-0.98.3-hadoop1-tests.jar:/home/hadoop/hbase/bin/../lib/hbase-examples-0.98.3-hadoop1.jar:/home/hadoop/hbase/bin/../lib/hbase-hadoop1-compat-0.98.3-hadoop1.jar:/home/hadoop/hbase/bin/../lib/hbase-hadoop-compat-0.98.3-hadoop1.jar:/home/hadoop/hbase/bin/../lib/hbase-it-0.98.3-hadoop1.jar:/home/hadoop/hbase/bin/../lib/hbase-it-0.98.3-hadoop1-tests.jar:/home/hadoop/hbase/bin/../lib/hbase-prefix-tree-0.98.3-hadoop1.jar:/home/hadoop/hbase/bin/../lib/hbase-protocol-0.98.3-hadoop1.jar:/home/hadoop/hbase/bin/../lib/hbase-server-0.98.3-hadoop1.jar:/home/hadoop/hbase/bin/../lib/hbase-server-0.98.3-hadoop1-tests.jar:/home/hadoop/hbase/bin/../lib/hbase-shell-0.98.3-hadoop1.jar:/home/hadoop/hbase/bin/../lib/hbase-testing-util-0.98.3-hadoop1.jar:/home/hadoop/hbase/bin/../lib/hbase-thrift-0.98.3-hadoop1.jar:/home/hadoop/hbase/bin/../lib/high-scale-lib-1.1.1.jar:/home/hadoop/hbase/bin/../lib/htrace-core-2.04.jar:/home/hadoop/hbase/bin/../lib/httpclient-4.1.3.jar:/home/hadoop/hbase/bin/../lib/httpcore-4.1.3.jar:/home/hadoop/hbase/bin/../lib/jackson-core-asl-1.8.8.jar:/home/hadoop/hbase/bin/../lib/jackson-jaxrs-1.8.8.jar:/home/hadoop/hbase/bin/../lib/jackson-mapper-asl-1.8.8.jar:/home/hadoop/hbase/bin/../lib/jackson-xc-1.8.8.jar:/home/hadoop/hbase/bin/../lib/jamon-runtime-2.3.1.jar:/home/hadoop/hbase/bin/../lib/jasper-compiler-5.5.23.jar:/home/hadoop/hbase/bin/../lib/jasper-runtime-5.5.23.jar:/home/hadoop/hbase/bin/../lib/jaxb-api-2.2.2.jar:/home/hadoop/hbase/bin/../lib/jaxb-impl-2.2.3-1.jar:/home/hadoop/hbase/bin/../lib/jersey-core-1.8.jar:/home/hadoop/hbase/bin/../lib/jersey-json-1.8.jar:/home/hadoop/hbase/bin/../lib/jersey-server-1.8.jar:/home/hadoop/hbase/bin/../lib/jettison-1.3.1.jar:/home/hadoop/hbase/bin/../lib/jetty-6.1.26.jar:/home/hadoop/hbase/bin/../lib/jetty-sslengine-6.1.26.jar:/home/hadoop/hbase/bin/../lib/jetty-util-6.1.26.jar:/home/hadoop/hbase/bin/../lib/jruby-complete-1.6.8.jar:/home/hadoop/hbase/bin/../lib/jsp-2.1-6.1.14.jar:/home/hadoop/hbase/bin/../lib/jsp-api-2.1-6.1.14.jar:/home/hadoop/hbase/bin/../lib/jsr305-1.3.9.jar:/home/hadoop/hbase/bin/../lib/junit-4.11.jar:/home/hadoop/hbase/bin/../lib/libthrift-0.9.0.jar:/home/hadoop/hbase/bin/../lib/log4j-1.2.17.jar:/home/hadoop/hbase/bin/../lib/metrics-core-2.1.2.jar:/home/hadoop/hbase/bin/../lib/netty-3.6.6.Final.jar:/home/hadoop/hbase/bin/../lib/protobuf-java-2.5.0.jar:/home/hadoop/hbase/bin/../lib/servlet-api-2.5-6.1.14.jar:/home/hadoop/hbase/bin/../lib/slf4j-api-1.6.4.jar:/home/hadoop/hbase/bin/../lib/slf4j-log4j12-1.6.4.jar:/home/hadoop/hbase/bin/../lib/xmlenc-0.52.jar:/home/hadoop/hbase/bin/../lib/zookeeper-3.4.6.jar:
2014-07-01 15:53:47,061 INFO  [main] util.ServerCommandLine: env:HBASE_THRIFT_OPTS=
2014-07-01 15:53:47,061 INFO  [main] util.ServerCommandLine: env:PATH=/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/usr/games:/usr/local/games
2014-07-01 15:53:47,062 INFO  [main] util.ServerCommandLine: env:USER=hadoop
2014-07-01 15:53:47,062 INFO  [main] util.ServerCommandLine: env:HBASE_SECURITY_LOGGER=INFO,RFAS
2014-07-01 15:53:47,062 INFO  [main] util.ServerCommandLine: env:HOME=/home/hadoop
2014-07-01 15:53:47,062 INFO  [main] util.ServerCommandLine: env:HBASE_LIBRARY_PATH=/home/hadoop/hbase/lib/native/Linux-amd64-64
2014-07-01 15:53:47,062 INFO  [main] util.ServerCommandLine: env:HBASE_START_FILE=/var/hadoop/pids/hbase-hadoop-regionserver.autorestart
2014-07-01 15:53:47,062 INFO  [main] util.ServerCommandLine: env:XDG_SESSION_ID=44
2014-07-01 15:53:47,062 INFO  [main] util.ServerCommandLine: env:HBASE_LOGFILE=hbase-hadoop-regionserver-sceplus-vm48.log
2014-07-01 15:53:47,062 INFO  [main] util.ServerCommandLine: env:XDG_RUNTIME_DIR=/run/user/1001
2014-07-01 15:53:47,062 INFO  [main] util.ServerCommandLine: env:LANG=en_US.UTF-8
2014-07-01 15:53:47,062 INFO  [main] util.ServerCommandLine: env:HBASE_LOG_PREFIX=hbase-hadoop-regionserver-sceplus-vm48
2014-07-01 15:53:47,063 INFO  [main] util.ServerCommandLine: env:HBASE_IDENT_STRING=hadoop
2014-07-01 15:53:47,065 INFO  [main] util.ServerCommandLine: vmName=OpenJDK 64-Bit Server VM, vmVendor=Sun Microsystems Inc., vmVersion=23.25-b01
2014-07-01 15:53:47,065 INFO  [main] util.ServerCommandLine: vmInputArguments=[-Dproc_regionserver, -XX:OnOutOfMemoryError=kill -9 %p, -Xmx10240m, -XX:+UseConcMarkSweepGC, -Dhbase.log.dir=/home/hadoop/hbase/bin/../logs, -Dhbase.log.file=hbase-hadoop-regionserver-sceplus-vm48.log, -Dhbase.home.dir=/home/hadoop/hbase/bin/.., -Dhbase.id.str=hadoop, -Dhbase.root.logger=INFO,RFA, -Djava.library.path=/home/hadoop/hbase/bin/../lib/native/Linux-amd64-64, -Dhbase.security.logger=INFO,RFAS]
2014-07-01 15:53:47,286 DEBUG [main] regionserver.HRegionServer: regionserver/sceplus-vm48.almaden.ibm.com/9.1.143.58:60020 HConnection server-to-server retries=350
2014-07-01 15:53:47,664 INFO  [main] ipc.RpcServer: regionserver/sceplus-vm48.almaden.ibm.com/9.1.143.58:60020: started 10 reader(s).
2014-07-01 15:53:47,744 INFO  [main] impl.MetricsConfig: loaded properties from hadoop-metrics2-hbase.properties
2014-07-01 15:53:47,756 INFO  [main] impl.MetricsSinkAdapter: Sink file-all started
2014-07-01 15:53:47,819 INFO  [main] impl.MetricsSourceAdapter: MBean for source MetricsSystem,sub=Stats registered.
2014-07-01 15:53:47,821 INFO  [main] impl.MetricsSystemImpl: Scheduled snapshot period at 10 second(s).
2014-07-01 15:53:47,821 INFO  [main] impl.MetricsSystemImpl: HBase metrics system started
2014-07-01 15:53:47,825 INFO  [main] impl.MetricsSourceAdapter: MBean for source jvm registered.
2014-07-01 15:53:47,830 INFO  [main] impl.MetricsSourceAdapter: MBean for source IPC,sub=IPC registered.
2014-07-01 15:53:47,913 INFO  [main] impl.MetricsSourceAdapter: MBean for source ugi registered.
2014-07-01 15:53:47,913 WARN  [main] impl.MetricsSystemImpl: Source name ugi already exists!
2014-07-01 15:53:47,918 DEBUG [main] util.DirectMemoryUtils: Failed to retrieve nio.BufferPool direct MemoryUsed attribute.
javax.management.InstanceNotFoundException: java.nio:type=BufferPool,name=direct
	at com.sun.jmx.interceptor.DefaultMBeanServerInterceptor.getMBean(DefaultMBeanServerInterceptor.java:1117)
	at com.sun.jmx.interceptor.DefaultMBeanServerInterceptor.getAttribute(DefaultMBeanServerInterceptor.java:678)
	at com.sun.jmx.mbeanserver.JmxMBeanServer.getAttribute(JmxMBeanServer.java:682)
	at org.apache.hadoop.hbase.util.DirectMemoryUtils.<clinit>(DirectMemoryUtils.java:72)
	at org.apache.hadoop.hbase.io.hfile.CacheConfig.instantiateBlockCache(CacheConfig.java:396)
	at org.apache.hadoop.hbase.io.hfile.CacheConfig.<init>(CacheConfig.java:179)
	at org.apache.hadoop.hbase.regionserver.HRegionServer.<init>(HRegionServer.java:621)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:57)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:534)
	at org.apache.hadoop.hbase.regionserver.HRegionServer.constructRegionServer(HRegionServer.java:2393)
	at org.apache.hadoop.hbase.regionserver.HRegionServerCommandLine.start(HRegionServerCommandLine.java:61)
	at org.apache.hadoop.hbase.regionserver.HRegionServerCommandLine.run(HRegionServerCommandLine.java:85)
	at org.apache.hadoop.util.ToolRunner.run(ToolRunner.java:65)
	at org.apache.hadoop.hbase.util.ServerCommandLine.doMain(ServerCommandLine.java:126)
	at org.apache.hadoop.hbase.regionserver.HRegionServer.main(HRegionServer.java:2410)
2014-07-01 15:53:47,920 INFO  [main] hfile.CacheConfig: Allocating LruBlockCache with maximum size 4.0g
2014-07-01 15:53:47,986 INFO  [main] mortbay.log: Logging to org.slf4j.impl.Log4jLoggerAdapter(org.mortbay.log) via org.mortbay.log.Slf4jLog
2014-07-01 15:53:48,042 INFO  [main] http.HttpServer: Added global filtersafety (class=org.apache.hadoop.http.HttpServer$QuotingInputFilter)
2014-07-01 15:53:48,051 INFO  [main] http.HttpServer: Port returned by webServer.getConnectors()[0].getLocalPort() before open() is -1. Opening the listener on 60030
2014-07-01 15:53:48,053 INFO  [main] http.HttpServer: listener.getLocalPort() returned 60030 webServer.getConnectors()[0].getLocalPort() returned 60030
2014-07-01 15:53:48,053 INFO  [main] http.HttpServer: Jetty bound to port 60030
2014-07-01 15:53:48,053 INFO  [main] mortbay.log: jetty-6.1.26
2014-07-01 15:53:48,384 INFO  [main] mortbay.log: Started SelectChannelConnector@0.0.0.0:60030
2014-07-01 15:53:48,443 INFO  [main] regionserver.ShutdownHook: Installed shutdown hook thread: Shutdownhook:regionserver60020
2014-07-01 15:53:48,443 INFO  [regionserver60020] zookeeper.ZooKeeper: Client environment:zookeeper.version=3.4.6-1569965, built on 02/20/2014 09:09 GMT
2014-07-01 15:53:48,443 INFO  [regionserver60020] zookeeper.ZooKeeper: Client environment:host.name=sceplus-vm48.almaden.ibm.com
2014-07-01 15:53:48,443 INFO  [regionserver60020] zookeeper.ZooKeeper: Client environment:java.version=1.6.0_31
2014-07-01 15:53:48,444 INFO  [regionserver60020] zookeeper.ZooKeeper: Client environment:java.vendor=Sun Microsystems Inc.
2014-07-01 15:53:48,444 INFO  [regionserver60020] zookeeper.ZooKeeper: Client environment:java.home=/usr/lib/jvm/java-6-openjdk-amd64/jre
2014-07-01 15:53:48,444 INFO  [regionserver60020] zookeeper.ZooKeeper: Client environment:java.class.path=/home/hadoop/hbase/conf:/usr/lib/jvm/java-1.6.0-openjdk-amd64//lib/tools.jar:/home/hadoop/hbase/bin/..:/home/hadoop/hbase/bin/../lib/activation-1.1.jar:/home/hadoop/hbase/bin/../lib/asm-3.1.jar:/home/hadoop/hbase/bin/../lib/commons-beanutils-1.7.0.jar:/home/hadoop/hbase/bin/../lib/commons-beanutils-core-1.8.0.jar:/home/hadoop/hbase/bin/../lib/commons-cli-1.2.jar:/home/hadoop/hbase/bin/../lib/commons-codec-1.7.jar:/home/hadoop/hbase/bin/../lib/commons-collections-3.2.1.jar:/home/hadoop/hbase/bin/../lib/commons-configuration-1.6.jar:/home/hadoop/hbase/bin/../lib/commons-digester-1.8.jar:/home/hadoop/hbase/bin/../lib/commons-el-1.0.jar:/home/hadoop/hbase/bin/../lib/commons-httpclient-3.1.jar:/home/hadoop/hbase/bin/../lib/commons-io-2.4.jar:/home/hadoop/hbase/bin/../lib/commons-lang-2.6.jar:/home/hadoop/hbase/bin/../lib/commons-logging-1.1.1.jar:/home/hadoop/hbase/bin/../lib/commons-math-2.1.jar:/home/hadoop/hbase/bin/../lib/commons-net-1.4.1.jar:/home/hadoop/hbase/bin/../lib/findbugs-annotations-1.3.9-1.jar:/home/hadoop/hbase/bin/../lib/guava-12.0.1.jar:/home/hadoop/hbase/bin/../lib/hadoop-core-1.2.1.jar:/home/hadoop/hbase/bin/../lib/hamcrest-core-1.3.jar:/home/hadoop/hbase/bin/../lib/hbase-client-0.98.3-hadoop1.jar:/home/hadoop/hbase/bin/../lib/hbase-common-0.98.3-hadoop1.jar:/home/hadoop/hbase/bin/../lib/hbase-common-0.98.3-hadoop1-tests.jar:/home/hadoop/hbase/bin/../lib/hbase-examples-0.98.3-hadoop1.jar:/home/hadoop/hbase/bin/../lib/hbase-hadoop1-compat-0.98.3-hadoop1.jar:/home/hadoop/hbase/bin/../lib/hbase-hadoop-compat-0.98.3-hadoop1.jar:/home/hadoop/hbase/bin/../lib/hbase-it-0.98.3-hadoop1.jar:/home/hadoop/hbase/bin/../lib/hbase-it-0.98.3-hadoop1-tests.jar:/home/hadoop/hbase/bin/../lib/hbase-prefix-tree-0.98.3-hadoop1.jar:/home/hadoop/hbase/bin/../lib/hbase-protocol-0.98.3-hadoop1.jar:/home/hadoop/hbase/bin/../lib/hbase-server-0.98.3-hadoop1.jar:/home/hadoop/hbase/bin/../lib/hbase-server-0.98.3-hadoop1-tests.jar:/home/hadoop/hbase/bin/../lib/hbase-shell-0.98.3-hadoop1.jar:/home/hadoop/hbase/bin/../lib/hbase-testing-util-0.98.3-hadoop1.jar:/home/hadoop/hbase/bin/../lib/hbase-thrift-0.98.3-hadoop1.jar:/home/hadoop/hbase/bin/../lib/high-scale-lib-1.1.1.jar:/home/hadoop/hbase/bin/../lib/htrace-core-2.04.jar:/home/hadoop/hbase/bin/../lib/httpclient-4.1.3.jar:/home/hadoop/hbase/bin/../lib/httpcore-4.1.3.jar:/home/hadoop/hbase/bin/../lib/jackson-core-asl-1.8.8.jar:/home/hadoop/hbase/bin/../lib/jackson-jaxrs-1.8.8.jar:/home/hadoop/hbase/bin/../lib/jackson-mapper-asl-1.8.8.jar:/home/hadoop/hbase/bin/../lib/jackson-xc-1.8.8.jar:/home/hadoop/hbase/bin/../lib/jamon-runtime-2.3.1.jar:/home/hadoop/hbase/bin/../lib/jasper-compiler-5.5.23.jar:/home/hadoop/hbase/bin/../lib/jasper-runtime-5.5.23.jar:/home/hadoop/hbase/bin/../lib/jaxb-api-2.2.2.jar:/home/hadoop/hbase/bin/../lib/jaxb-impl-2.2.3-1.jar:/home/hadoop/hbase/bin/../lib/jersey-core-1.8.jar:/home/hadoop/hbase/bin/../lib/jersey-json-1.8.jar:/home/hadoop/hbase/bin/../lib/jersey-server-1.8.jar:/home/hadoop/hbase/bin/../lib/jettison-1.3.1.jar:/home/hadoop/hbase/bin/../lib/jetty-6.1.26.jar:/home/hadoop/hbase/bin/../lib/jetty-sslengine-6.1.26.jar:/home/hadoop/hbase/bin/../lib/jetty-util-6.1.26.jar:/home/hadoop/hbase/bin/../lib/jruby-complete-1.6.8.jar:/home/hadoop/hbase/bin/../lib/jsp-2.1-6.1.14.jar:/home/hadoop/hbase/bin/../lib/jsp-api-2.1-6.1.14.jar:/home/hadoop/hbase/bin/../lib/jsr305-1.3.9.jar:/home/hadoop/hbase/bin/../lib/junit-4.11.jar:/home/hadoop/hbase/bin/../lib/libthrift-0.9.0.jar:/home/hadoop/hbase/bin/../lib/log4j-1.2.17.jar:/home/hadoop/hbase/bin/../lib/metrics-core-2.1.2.jar:/home/hadoop/hbase/bin/../lib/netty-3.6.6.Final.jar:/home/hadoop/hbase/bin/../lib/protobuf-java-2.5.0.jar:/home/hadoop/hbase/bin/../lib/servlet-api-2.5-6.1.14.jar:/home/hadoop/hbase/bin/../lib/slf4j-api-1.6.4.jar:/home/hadoop/hbase/bin/../lib/slf4j-log4j12-1.6.4.jar:/home/hadoop/hbase/bin/../lib/xmlenc-0.52.jar:/home/hadoop/hbase/bin/../lib/zookeeper-3.4.6.jar:
2014-07-01 15:53:48,444 INFO  [regionserver60020] zookeeper.ZooKeeper: Client environment:java.library.path=/home/hadoop/hbase/bin/../lib/native/Linux-amd64-64
2014-07-01 15:53:48,444 INFO  [regionserver60020] zookeeper.ZooKeeper: Client environment:java.io.tmpdir=/tmp
2014-07-01 15:53:48,444 INFO  [regionserver60020] zookeeper.ZooKeeper: Client environment:java.compiler=<NA>
2014-07-01 15:53:48,444 INFO  [regionserver60020] zookeeper.ZooKeeper: Client environment:os.name=Linux
2014-07-01 15:53:48,444 INFO  [regionserver60020] zookeeper.ZooKeeper: Client environment:os.arch=amd64
2014-07-01 15:53:48,444 INFO  [regionserver60020] zookeeper.ZooKeeper: Client environment:os.version=3.13.0-24-generic
2014-07-01 15:53:48,444 INFO  [regionserver60020] zookeeper.ZooKeeper: Client environment:user.name=hadoop
2014-07-01 15:53:48,444 INFO  [regionserver60020] zookeeper.ZooKeeper: Client environment:user.home=/home/hadoop
2014-07-01 15:53:48,444 INFO  [regionserver60020] zookeeper.ZooKeeper: Client environment:user.dir=/home/hadoop/hbase-0.98.3-hadoop1
2014-07-01 15:53:48,445 INFO  [regionserver60020] zookeeper.ZooKeeper: Initiating client connection, connectString=sceplus-vm49.almaden.ibm.com:2181,sceplus-vm48.almaden.ibm.com:2181 sessionTimeout=90000 watcher=regionserver:60020, quorum=sceplus-vm49.almaden.ibm.com:2181,sceplus-vm48.almaden.ibm.com:2181, baseZNode=/hbase
2014-07-01 15:53:48,470 INFO  [regionserver60020] zookeeper.RecoverableZooKeeper: Process identifier=regionserver:60020 connecting to ZooKeeper ensemble=sceplus-vm49.almaden.ibm.com:2181,sceplus-vm48.almaden.ibm.com:2181
2014-07-01 15:53:48,474 INFO  [regionserver60020-SendThread(sceplus-vm48.almaden.ibm.com:2181)] zookeeper.ClientCnxn: Opening socket connection to server sceplus-vm48.almaden.ibm.com/9.1.143.58:2181. Will not attempt to authenticate using SASL (unknown error)
2014-07-01 15:53:48,478 INFO  [regionserver60020-SendThread(sceplus-vm48.almaden.ibm.com:2181)] zookeeper.ClientCnxn: Socket connection established to sceplus-vm48.almaden.ibm.com/9.1.143.58:2181, initiating session
2014-07-01 15:53:48,489 INFO  [regionserver60020-SendThread(sceplus-vm48.almaden.ibm.com:2181)] zookeeper.ClientCnxn: Session establishment complete on server sceplus-vm48.almaden.ibm.com/9.1.143.58:2181, sessionid = 0x46f4224a300001, negotiated timeout = 90000
2014-07-01 15:54:18,211 INFO  [regionserver60020] zookeeper.ZooKeeper: Initiating client connection, connectString=sceplus-vm49.almaden.ibm.com:2181,sceplus-vm48.almaden.ibm.com:2181 sessionTimeout=90000 watcher=hconnection-0x6d2acc34, quorum=sceplus-vm49.almaden.ibm.com:2181,sceplus-vm48.almaden.ibm.com:2181, baseZNode=/hbase
2014-07-01 15:54:18,212 INFO  [regionserver60020] zookeeper.RecoverableZooKeeper: Process identifier=hconnection-0x6d2acc34 connecting to ZooKeeper ensemble=sceplus-vm49.almaden.ibm.com:2181,sceplus-vm48.almaden.ibm.com:2181
2014-07-01 15:54:18,213 INFO  [regionserver60020-SendThread(sceplus-vm48.almaden.ibm.com:2181)] zookeeper.ClientCnxn: Opening socket connection to server sceplus-vm48.almaden.ibm.com/9.1.143.58:2181. Will not attempt to authenticate using SASL (unknown error)
2014-07-01 15:54:18,213 INFO  [regionserver60020-SendThread(sceplus-vm48.almaden.ibm.com:2181)] zookeeper.ClientCnxn: Socket connection established to sceplus-vm48.almaden.ibm.com/9.1.143.58:2181, initiating session
2014-07-01 15:54:18,218 INFO  [regionserver60020-SendThread(sceplus-vm48.almaden.ibm.com:2181)] zookeeper.ClientCnxn: Session establishment complete on server sceplus-vm48.almaden.ibm.com/9.1.143.58:2181, sessionid = 0x46f4224a300004, negotiated timeout = 90000
2014-07-01 15:54:18,466 DEBUG [regionserver60020] catalog.CatalogTracker: Starting catalog tracker org.apache.hadoop.hbase.catalog.CatalogTracker@759b6377
2014-07-01 15:54:18,470 INFO  [regionserver60020] regionserver.HRegionServer: ClusterId : 3b61b992-e8ee-43f8-b0c6-14cd23a8afbe
2014-07-01 15:54:18,476 INFO  [regionserver60020] procedure.RegionServerProcedureManagerHost: Procedure online-snapshot is initializing
2014-07-01 15:54:18,491 INFO  [regionserver60020] zookeeper.RecoverableZooKeeper: Node /hbase/online-snapshot/acquired already exists and this is not a retry
2014-07-01 15:54:18,520 INFO  [regionserver60020] procedure.RegionServerProcedureManagerHost: Procedure online-snapshot is initialized
2014-07-01 15:54:18,525 INFO  [regionserver60020] regionserver.MemStoreFlusher: globalMemStoreLimit=4.0g, globalMemStoreLimitLowMark=3.8g, maxHeap=9.9g
2014-07-01 15:54:18,529 INFO  [regionserver60020] regionserver.HRegionServer: CompactionChecker runs every 10sec
2014-07-01 15:54:18,549 INFO  [regionserver60020] regionserver.HRegionServer: reportForDuty to master=sceplus-vm48.almaden.ibm.com,60000,1404255226396 with port=60020, startcode=1404255227840
2014-07-01 15:54:18,915 DEBUG [regionserver60020] regionserver.HRegionServer: Config from master: hbase.rootdir=hdfs://master:54310/hbase
2014-07-01 15:54:18,915 DEBUG [regionserver60020] regionserver.HRegionServer: Config from master: fs.default.name=hdfs://master:54310
2014-07-01 15:54:18,945 INFO  [regionserver60020] fs.HFileSystem: Added intercepting call to namenode#getBlockLocations so can do block reordering using class class org.apache.hadoop.hbase.fs.HFileSystem$ReorderWALBlocks
2014-07-01 15:54:18,953 DEBUG [regionserver60020] regionserver.HRegionServer: logdir=hdfs://master:54310/hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1404255227840
2014-07-01 15:54:18,989 DEBUG [regionserver60020] regionserver.Replication: ReplicationStatisticsThread 300
2014-07-01 15:54:19,000 INFO  [regionserver60020] wal.FSHLog: WAL/HLog configuration: blocksize=64 MB, rollsize=60.8 MB, enabled=true
2014-07-01 15:54:19,108 INFO  [regionserver60020] wal.FSHLog: New WAL /hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1404255227840/sceplus-vm48.almaden.ibm.com%2C60020%2C1404255227840.1404255259008
2014-07-01 15:54:19,125 INFO  [regionserver60020] impl.MetricsSourceAdapter: MBean for source RegionServer,sub=WAL registered.
2014-07-01 15:54:19,130 INFO  [regionserver60020] regionserver.MetricsRegionServerWrapperImpl: Computing regionserver metrics every 5000 milliseconds
2014-07-01 15:54:19,134 INFO  [regionserver60020] impl.MetricsSourceAdapter: MBean for source RegionServer,sub=Server registered.
2014-07-01 15:54:19,138 INFO  [regionserver60020] trace.SpanReceiverHost: SpanReceiver org.cloudera.htrace.impl.LocalFileSpanReceiver was loaded successfully.
2014-07-01 15:54:19,141 DEBUG [regionserver60020] executor.ExecutorService: Starting executor service name=RS_OPEN_REGION-sceplus-vm48:60020, corePoolSize=3, maxPoolSize=3
2014-07-01 15:54:19,141 DEBUG [regionserver60020] executor.ExecutorService: Starting executor service name=RS_OPEN_META-sceplus-vm48:60020, corePoolSize=1, maxPoolSize=1
2014-07-01 15:54:19,141 DEBUG [regionserver60020] executor.ExecutorService: Starting executor service name=RS_CLOSE_REGION-sceplus-vm48:60020, corePoolSize=3, maxPoolSize=3
2014-07-01 15:54:19,142 DEBUG [regionserver60020] executor.ExecutorService: Starting executor service name=RS_CLOSE_META-sceplus-vm48:60020, corePoolSize=1, maxPoolSize=1
2014-07-01 15:54:19,142 DEBUG [regionserver60020] executor.ExecutorService: Starting executor service name=RS_LOG_REPLAY_OPS-sceplus-vm48:60020, corePoolSize=2, maxPoolSize=2
2014-07-01 15:54:19,152 INFO  [regionserver60020] regionserver.ReplicationSourceManager: Current list of replicators: [slave1,60020,1404255226026, sceplus-vm48.almaden.ibm.com,60020,1404255227840] other RSs: [slave1,60020,1404255226026, sceplus-vm48.almaden.ibm.com,60020,1404255227840]
2014-07-01 15:54:19,178 INFO  [regionserver60020] impl.MetricsSourceAdapter: MBean for source RegionServer,sub=Replication registered.
2014-07-01 15:54:19,183 INFO  [regionserver60020] zookeeper.ZooKeeper: Initiating client connection, connectString=sceplus-vm49.almaden.ibm.com:2181,sceplus-vm48.almaden.ibm.com:2181 sessionTimeout=90000 watcher=hconnection-0x46ef5f19, quorum=sceplus-vm49.almaden.ibm.com:2181,sceplus-vm48.almaden.ibm.com:2181, baseZNode=/hbase
2014-07-01 15:54:19,184 INFO  [regionserver60020] zookeeper.RecoverableZooKeeper: Process identifier=hconnection-0x46ef5f19 connecting to ZooKeeper ensemble=sceplus-vm49.almaden.ibm.com:2181,sceplus-vm48.almaden.ibm.com:2181
2014-07-01 15:54:19,185 INFO  [regionserver60020-SendThread(slave1:2181)] zookeeper.ClientCnxn: Opening socket connection to server slave1/9.1.143.59:2181. Will not attempt to authenticate using SASL (unknown error)
2014-07-01 15:54:19,186 INFO  [regionserver60020-SendThread(slave1:2181)] zookeeper.ClientCnxn: Socket connection established to slave1/9.1.143.59:2181, initiating session
2014-07-01 15:54:19,190 INFO  [regionserver60020-SendThread(slave1:2181)] zookeeper.ClientCnxn: Session establishment complete on server slave1/9.1.143.59:2181, sessionid = 0x146f42242d40001, negotiated timeout = 90000
2014-07-01 15:54:19,200 INFO  [RpcServer.responder] ipc.RpcServer: RpcServer.responder: starting
2014-07-01 15:54:19,201 INFO  [RpcServer.listener,port=60020] ipc.RpcServer: RpcServer.listener,port=60020: starting
2014-07-01 15:54:19,247 INFO  [regionserver60020] regionserver.HRegionServer: Serving as sceplus-vm48.almaden.ibm.com,60020,1404255227840, RpcServer on sceplus-vm48.almaden.ibm.com/9.1.143.58:60020, sessionid=0x46f4224a300001
2014-07-01 15:54:19,248 INFO  [SplitLogWorker-sceplus-vm48.almaden.ibm.com,60020,1404255227840] regionserver.SplitLogWorker: SplitLogWorker sceplus-vm48.almaden.ibm.com,60020,1404255227840 starting
2014-07-01 15:54:19,248 INFO  [regionserver60020] procedure.RegionServerProcedureManagerHost: Procedure online-snapshot is starting
2014-07-01 15:54:19,248 DEBUG [regionserver60020] snapshot.RegionServerSnapshotManager: Start Snapshot Manager sceplus-vm48.almaden.ibm.com,60020,1404255227840
2014-07-01 15:54:19,248 DEBUG [regionserver60020] procedure.ZKProcedureMemberRpcs: Starting procedure member 'sceplus-vm48.almaden.ibm.com,60020,1404255227840'
2014-07-01 15:54:19,248 DEBUG [regionserver60020] procedure.ZKProcedureMemberRpcs: Checking for aborted procedures on node: '/hbase/online-snapshot/abort'
2014-07-01 15:54:19,249 DEBUG [regionserver60020] procedure.ZKProcedureMemberRpcs: Looking for new procedures under znode:'/hbase/online-snapshot/acquired'
2014-07-01 15:54:19,250 INFO  [regionserver60020] procedure.RegionServerProcedureManagerHost: Procedure online-snapshot is started
2014-07-01 15:54:22,874 DEBUG [regionserver60020-EventThread] regionserver.SplitLogWorker: tasks arrived or departed
2014-07-01 15:54:27,208 DEBUG [regionserver60020-EventThread] regionserver.SplitLogWorker: tasks arrived or departed
2014-07-01 15:54:28,357 DEBUG [regionserver60020-EventThread] regionserver.SplitLogWorker: tasks arrived or departed
2014-07-01 15:54:28,362 DEBUG [regionserver60020-EventThread] regionserver.SplitLogWorker: tasks arrived or departed
2014-07-01 15:54:28,374 INFO  [SplitLogWorker-sceplus-vm48.almaden.ibm.com,60020,1404255227840] regionserver.SplitLogWorker: worker sceplus-vm48.almaden.ibm.com,60020,1404255227840 acquired task /hbase/splitWAL/WALs%2Fslave1%2C60020%2C1404253868846-splitting%2Fslave1%252C60020%252C1404253868846.1404254470846
2014-07-01 15:54:28,410 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:60020-0] wal.HLogSplitter: Splitting hlog: hdfs://master:54310/hbase/WALs/slave1,60020,1404253868846-splitting/slave1%2C60020%2C1404253868846.1404254470846, length=67610438
2014-07-01 15:54:28,410 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:60020-0] wal.HLogSplitter: DistributedLogReplay = false
2014-07-01 15:54:28,418 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:60020-0] util.FSHDFSUtils: Recovering lease on dfs file hdfs://master:54310/hbase/WALs/slave1,60020,1404253868846-splitting/slave1%2C60020%2C1404253868846.1404254470846
2014-07-01 15:54:28,421 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:60020-0] util.FSHDFSUtils: recoverLease=true, attempt=0 on file=hdfs://master:54310/hbase/WALs/slave1,60020,1404253868846-splitting/slave1%2C60020%2C1404253868846.1404254470846 after 2ms
2014-07-01 15:54:28,471 DEBUG [RS_LOG_REPLAY_OPS-sceplus-vm48:60020-0-Writer-0] wal.HLogSplitter: Writer thread Thread[RS_LOG_REPLAY_OPS-sceplus-vm48:60020-0-Writer-0,5,main]: starting
2014-07-01 15:54:28,471 DEBUG [RS_LOG_REPLAY_OPS-sceplus-vm48:60020-0-Writer-1] wal.HLogSplitter: Writer thread Thread[RS_LOG_REPLAY_OPS-sceplus-vm48:60020-0-Writer-1,5,main]: starting
2014-07-01 15:54:28,471 DEBUG [RS_LOG_REPLAY_OPS-sceplus-vm48:60020-0-Writer-2] wal.HLogSplitter: Writer thread Thread[RS_LOG_REPLAY_OPS-sceplus-vm48:60020-0-Writer-2,5,main]: starting
2014-07-01 15:54:28,687 DEBUG [RS_LOG_REPLAY_OPS-sceplus-vm48:60020-0-Writer-2] wal.HLogSplitter: Creating writer path=hdfs://master:54310/hbase/data/default/usertable/667c1f2c34fe37d48ba25a789d8aba98/recovered.edits/0000000000000004518.temp region=667c1f2c34fe37d48ba25a789d8aba98
2014-07-01 15:54:28,728 DEBUG [RS_LOG_REPLAY_OPS-sceplus-vm48:60020-0-Writer-1] wal.HLogSplitter: Creating writer path=hdfs://master:54310/hbase/data/default/usertable/122e92eabfab38cb15a9965b21db25c5/recovered.edits/0000000000000004555.temp region=122e92eabfab38cb15a9965b21db25c5
2014-07-01 15:54:28,750 DEBUG [RS_LOG_REPLAY_OPS-sceplus-vm48:60020-0-Writer-0] wal.HLogSplitter: Creating writer path=hdfs://master:54310/hbase/data/default/usertable/9cef9e7d34f96f1d9b111be50040a208/recovered.edits/0000000000000005234.temp region=9cef9e7d34f96f1d9b111be50040a208
2014-07-01 15:54:28,862 DEBUG [RS_LOG_REPLAY_OPS-sceplus-vm48:60020-0-Writer-0] wal.HLogSplitter: Creating writer path=hdfs://master:54310/hbase/data/default/usertable/bb79ae773f71289e0745068f68a0abd9/recovered.edits/0000000000000004508.temp region=bb79ae773f71289e0745068f68a0abd9
2014-07-01 15:54:28,915 DEBUG [RS_LOG_REPLAY_OPS-sceplus-vm48:60020-0-Writer-0] wal.HLogSplitter: Creating writer path=hdfs://master:54310/hbase/data/default/usertable/93253e37c284f3174309fede3f32339f/recovered.edits/0000000000000004563.temp region=93253e37c284f3174309fede3f32339f
2014-07-01 15:54:29,056 INFO  [SplitLogWorker-sceplus-vm48.almaden.ibm.com,60020,1404255227840] regionserver.SplitLogWorker: worker sceplus-vm48.almaden.ibm.com,60020,1404255227840 acquired task /hbase/splitWAL/WALs%2Fslave1%2C60020%2C1404253868846-splitting%2Fslave1%252C60020%252C1404253868846.1404254461018
2014-07-01 15:54:29,129 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:60020-1] wal.HLogSplitter: Splitting hlog: hdfs://master:54310/hbase/WALs/slave1,60020,1404253868846-splitting/slave1%2C60020%2C1404253868846.1404254461018, length=66085014
2014-07-01 15:54:29,129 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:60020-1] wal.HLogSplitter: DistributedLogReplay = false
2014-07-01 15:54:29,133 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:60020-1] util.FSHDFSUtils: Recovering lease on dfs file hdfs://master:54310/hbase/WALs/slave1,60020,1404253868846-splitting/slave1%2C60020%2C1404253868846.1404254461018
2014-07-01 15:54:29,136 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:60020-1] util.FSHDFSUtils: recoverLease=true, attempt=0 on file=hdfs://master:54310/hbase/WALs/slave1,60020,1404253868846-splitting/slave1%2C60020%2C1404253868846.1404254461018 after 1ms
2014-07-01 15:54:29,175 DEBUG [RS_LOG_REPLAY_OPS-sceplus-vm48:60020-1-Writer-0] wal.HLogSplitter: Writer thread Thread[RS_LOG_REPLAY_OPS-sceplus-vm48:60020-1-Writer-0,5,main]: starting
2014-07-01 15:54:29,175 DEBUG [RS_LOG_REPLAY_OPS-sceplus-vm48:60020-1-Writer-1] wal.HLogSplitter: Writer thread Thread[RS_LOG_REPLAY_OPS-sceplus-vm48:60020-1-Writer-1,5,main]: starting
2014-07-01 15:54:29,176 DEBUG [RS_LOG_REPLAY_OPS-sceplus-vm48:60020-1-Writer-2] wal.HLogSplitter: Writer thread Thread[RS_LOG_REPLAY_OPS-sceplus-vm48:60020-1-Writer-2,5,main]: starting
2014-07-01 15:54:29,200 DEBUG [RS_LOG_REPLAY_OPS-sceplus-vm48:60020-1-Writer-2] wal.HLogSplitter: Creating writer path=hdfs://master:54310/hbase/data/default/usertable/93253e37c284f3174309fede3f32339f/recovered.edits/0000000000000004496.temp region=93253e37c284f3174309fede3f32339f
2014-07-01 15:54:29,226 DEBUG [RS_LOG_REPLAY_OPS-sceplus-vm48:60020-1-Writer-0] wal.HLogSplitter: Creating writer path=hdfs://master:54310/hbase/data/default/usertable/9cef9e7d34f96f1d9b111be50040a208/recovered.edits/0000000000000005025.temp region=9cef9e7d34f96f1d9b111be50040a208
2014-07-01 15:54:29,268 DEBUG [RS_LOG_REPLAY_OPS-sceplus-vm48:60020-1-Writer-1] wal.HLogSplitter: Creating writer path=hdfs://master:54310/hbase/data/default/usertable/122e92eabfab38cb15a9965b21db25c5/recovered.edits/0000000000000004489.temp region=122e92eabfab38cb15a9965b21db25c5
2014-07-01 15:54:29,312 DEBUG [RS_LOG_REPLAY_OPS-sceplus-vm48:60020-1-Writer-0] wal.HLogSplitter: Creating writer path=hdfs://master:54310/hbase/data/default/usertable/667c1f2c34fe37d48ba25a789d8aba98/recovered.edits/0000000000000004455.temp region=667c1f2c34fe37d48ba25a789d8aba98
2014-07-01 15:54:29,339 DEBUG [RS_LOG_REPLAY_OPS-sceplus-vm48:60020-1-Writer-1] wal.HLogSplitter: Creating writer path=hdfs://master:54310/hbase/data/default/usertable/bb79ae773f71289e0745068f68a0abd9/recovered.edits/0000000000000004445.temp region=bb79ae773f71289e0745068f68a0abd9
2014-07-01 15:54:29,477 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:60020-0] wal.HLogSplitter: Finishing writing output logs and closing down.
2014-07-01 15:54:29,477 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:60020-0] wal.HLogSplitter: Waiting for split writer threads to finish
2014-07-01 15:54:29,485 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:60020-0] wal.HLogSplitter: Split writers finished
2014-07-01 15:54:29,487 DEBUG [RS_LOG_REPLAY_OPS-sceplus-vm48:60020-0] wal.HLogSplitter: Submitting close of hdfs://master:54310/hbase/data/default/usertable/122e92eabfab38cb15a9965b21db25c5/recovered.edits/0000000000000004555.temp
2014-07-01 15:54:29,488 DEBUG [RS_LOG_REPLAY_OPS-sceplus-vm48:60020-0] wal.HLogSplitter: Submitting close of hdfs://master:54310/hbase/data/default/usertable/667c1f2c34fe37d48ba25a789d8aba98/recovered.edits/0000000000000004518.temp
2014-07-01 15:54:29,488 DEBUG [RS_LOG_REPLAY_OPS-sceplus-vm48:60020-0] wal.HLogSplitter: Submitting close of hdfs://master:54310/hbase/data/default/usertable/93253e37c284f3174309fede3f32339f/recovered.edits/0000000000000004563.temp
2014-07-01 15:54:29,488 DEBUG [split-log-closeStream-1] wal.HLogSplitter: Closing hdfs://master:54310/hbase/data/default/usertable/122e92eabfab38cb15a9965b21db25c5/recovered.edits/0000000000000004555.temp
2014-07-01 15:54:29,488 DEBUG [split-log-closeStream-2] wal.HLogSplitter: Closing hdfs://master:54310/hbase/data/default/usertable/667c1f2c34fe37d48ba25a789d8aba98/recovered.edits/0000000000000004518.temp
2014-07-01 15:54:29,488 DEBUG [RS_LOG_REPLAY_OPS-sceplus-vm48:60020-0] wal.HLogSplitter: Submitting close of hdfs://master:54310/hbase/data/default/usertable/9cef9e7d34f96f1d9b111be50040a208/recovered.edits/0000000000000005234.temp
2014-07-01 15:54:29,488 DEBUG [split-log-closeStream-3] wal.HLogSplitter: Closing hdfs://master:54310/hbase/data/default/usertable/93253e37c284f3174309fede3f32339f/recovered.edits/0000000000000004563.temp
2014-07-01 15:54:29,488 DEBUG [RS_LOG_REPLAY_OPS-sceplus-vm48:60020-0] wal.HLogSplitter: Submitting close of hdfs://master:54310/hbase/data/default/usertable/bb79ae773f71289e0745068f68a0abd9/recovered.edits/0000000000000004508.temp
2014-07-01 15:54:29,501 INFO  [split-log-closeStream-1] wal.HLogSplitter: Closed wap hdfs://master:54310/hbase/data/default/usertable/122e92eabfab38cb15a9965b21db25c5/recovered.edits/0000000000000004555.temp (wrote 17 edits in 242ms)
2014-07-01 15:54:29,506 INFO  [split-log-closeStream-3] wal.HLogSplitter: Closed wap hdfs://master:54310/hbase/data/default/usertable/93253e37c284f3174309fede3f32339f/recovered.edits/0000000000000004563.temp (wrote 15 edits in 172ms)
2014-07-01 15:54:29,506 INFO  [split-log-closeStream-2] wal.HLogSplitter: Closed wap hdfs://master:54310/hbase/data/default/usertable/667c1f2c34fe37d48ba25a789d8aba98/recovered.edits/0000000000000004518.temp (wrote 14 edits in 201ms)
2014-07-01 15:54:29,513 DEBUG [split-log-closeStream-1] wal.HLogSplitter: Rename hdfs://master:54310/hbase/data/default/usertable/122e92eabfab38cb15a9965b21db25c5/recovered.edits/0000000000000004555.temp to hdfs://master:54310/hbase/data/default/usertable/122e92eabfab38cb15a9965b21db25c5/recovered.edits/0000000000000004571
2014-07-01 15:54:29,515 DEBUG [split-log-closeStream-1] wal.HLogSplitter: Closing hdfs://master:54310/hbase/data/default/usertable/9cef9e7d34f96f1d9b111be50040a208/recovered.edits/0000000000000005234.temp
2014-07-01 15:54:29,516 DEBUG [split-log-closeStream-3] wal.HLogSplitter: Rename hdfs://master:54310/hbase/data/default/usertable/93253e37c284f3174309fede3f32339f/recovered.edits/0000000000000004563.temp to hdfs://master:54310/hbase/data/default/usertable/93253e37c284f3174309fede3f32339f/recovered.edits/0000000000000004577
2014-07-01 15:54:29,517 DEBUG [split-log-closeStream-3] wal.HLogSplitter: Closing hdfs://master:54310/hbase/data/default/usertable/bb79ae773f71289e0745068f68a0abd9/recovered.edits/0000000000000004508.temp
2014-07-01 15:54:29,525 DEBUG [split-log-closeStream-2] wal.HLogSplitter: Rename hdfs://master:54310/hbase/data/default/usertable/667c1f2c34fe37d48ba25a789d8aba98/recovered.edits/0000000000000004518.temp to hdfs://master:54310/hbase/data/default/usertable/667c1f2c34fe37d48ba25a789d8aba98/recovered.edits/0000000000000004532
2014-07-01 15:54:29,527 INFO  [split-log-closeStream-1] wal.HLogSplitter: Closed wap hdfs://master:54310/hbase/data/default/usertable/9cef9e7d34f96f1d9b111be50040a208/recovered.edits/0000000000000005234.temp (wrote 52 edits in 304ms)
2014-07-01 15:54:29,529 INFO  [split-log-closeStream-3] wal.HLogSplitter: Closed wap hdfs://master:54310/hbase/data/default/usertable/bb79ae773f71289e0745068f68a0abd9/recovered.edits/0000000000000004508.temp (wrote 14 edits in 65ms)
2014-07-01 15:54:29,533 DEBUG [split-log-closeStream-1] wal.HLogSplitter: Rename hdfs://master:54310/hbase/data/default/usertable/9cef9e7d34f96f1d9b111be50040a208/recovered.edits/0000000000000005234.temp to hdfs://master:54310/hbase/data/default/usertable/9cef9e7d34f96f1d9b111be50040a208/recovered.edits/0000000000000005285
2014-07-01 15:54:29,571 DEBUG [split-log-closeStream-3] wal.HLogSplitter: Rename hdfs://master:54310/hbase/data/default/usertable/bb79ae773f71289e0745068f68a0abd9/recovered.edits/0000000000000004508.temp to hdfs://master:54310/hbase/data/default/usertable/bb79ae773f71289e0745068f68a0abd9/recovered.edits/0000000000000004521
2014-07-01 15:54:29,573 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:60020-0] wal.HLogSplitter: Processed 112 edits across 5 regions; log file=hdfs://master:54310/hbase/WALs/slave1,60020,1404253868846-splitting/slave1%2C60020%2C1404253868846.1404254470846 is corrupted = false progress failed = false
2014-07-01 15:54:29,584 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:60020-0] handler.HLogSplitterHandler: successfully transitioned task /hbase/splitWAL/WALs%2Fslave1%2C60020%2C1404253868846-splitting%2Fslave1%252C60020%252C1404253868846.1404254470846 to final state DONE sceplus-vm48.almaden.ibm.com,60020,1404255227840
2014-07-01 15:54:29,584 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:60020-0] handler.HLogSplitterHandler: worker sceplus-vm48.almaden.ibm.com,60020,1404255227840 done with task /hbase/splitWAL/WALs%2Fslave1%2C60020%2C1404253868846-splitting%2Fslave1%252C60020%252C1404253868846.1404254470846 in 1202ms
2014-07-01 15:54:29,739 INFO  [SplitLogWorker-sceplus-vm48.almaden.ibm.com,60020,1404255227840] regionserver.SplitLogWorker: worker sceplus-vm48.almaden.ibm.com,60020,1404255227840 acquired task /hbase/splitWAL/WALs%2Fslave1%2C60020%2C1404253868846-splitting%2Fslave1%252C60020%252C1404253868846.1404254529807
2014-07-01 15:54:29,775 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:60020-0] wal.HLogSplitter: Splitting hlog: hdfs://master:54310/hbase/WALs/slave1,60020,1404253868846-splitting/slave1%2C60020%2C1404253868846.1404254529807, length=66823784
2014-07-01 15:54:29,775 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:60020-0] wal.HLogSplitter: DistributedLogReplay = false
2014-07-01 15:54:29,780 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:60020-0] util.FSHDFSUtils: Recovering lease on dfs file hdfs://master:54310/hbase/WALs/slave1,60020,1404253868846-splitting/slave1%2C60020%2C1404253868846.1404254529807
2014-07-01 15:54:29,781 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:60020-0] util.FSHDFSUtils: recoverLease=true, attempt=0 on file=hdfs://master:54310/hbase/WALs/slave1,60020,1404253868846-splitting/slave1%2C60020%2C1404253868846.1404254529807 after 1ms
2014-07-01 15:54:29,830 DEBUG [RS_LOG_REPLAY_OPS-sceplus-vm48:60020-0-Writer-0] wal.HLogSplitter: Writer thread Thread[RS_LOG_REPLAY_OPS-sceplus-vm48:60020-0-Writer-0,5,main]: starting
2014-07-01 15:54:29,830 DEBUG [RS_LOG_REPLAY_OPS-sceplus-vm48:60020-0-Writer-1] wal.HLogSplitter: Writer thread Thread[RS_LOG_REPLAY_OPS-sceplus-vm48:60020-0-Writer-1,5,main]: starting
2014-07-01 15:54:29,830 DEBUG [RS_LOG_REPLAY_OPS-sceplus-vm48:60020-0-Writer-2] wal.HLogSplitter: Writer thread Thread[RS_LOG_REPLAY_OPS-sceplus-vm48:60020-0-Writer-2,5,main]: starting
2014-07-01 15:54:29,862 DEBUG [RS_LOG_REPLAY_OPS-sceplus-vm48:60020-0-Writer-1] wal.HLogSplitter: Creating writer path=hdfs://master:54310/hbase/data/default/usertable/122e92eabfab38cb15a9965b21db25c5/recovered.edits/0000000000000004635.temp region=122e92eabfab38cb15a9965b21db25c5
2014-07-01 15:54:29,877 DEBUG [RS_LOG_REPLAY_OPS-sceplus-vm48:60020-0-Writer-0] wal.HLogSplitter: Creating writer path=hdfs://master:54310/hbase/data/default/usertable/93253e37c284f3174309fede3f32339f/recovered.edits/0000000000000004643.temp region=93253e37c284f3174309fede3f32339f
2014-07-01 15:54:29,901 DEBUG [RS_LOG_REPLAY_OPS-sceplus-vm48:60020-0-Writer-2] wal.HLogSplitter: Creating writer path=hdfs://master:54310/hbase/data/default/usertable/667c1f2c34fe37d48ba25a789d8aba98/recovered.edits/0000000000000004600.temp region=667c1f2c34fe37d48ba25a789d8aba98
2014-07-01 15:54:29,908 DEBUG [RS_LOG_REPLAY_OPS-sceplus-vm48:60020-0-Writer-1] wal.HLogSplitter: Creating writer path=hdfs://master:54310/hbase/data/default/usertable/9cef9e7d34f96f1d9b111be50040a208/recovered.edits/0000000000000005497.temp region=9cef9e7d34f96f1d9b111be50040a208
2014-07-01 15:54:29,910 DEBUG [RS_LOG_REPLAY_OPS-sceplus-vm48:60020-0-Writer-0] wal.HLogSplitter: Creating writer path=hdfs://master:54310/hbase/data/default/usertable/bb79ae773f71289e0745068f68a0abd9/recovered.edits/0000000000000004586.temp region=bb79ae773f71289e0745068f68a0abd9
2014-07-01 15:54:30,142 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:60020-1] wal.HLogSplitter: Finishing writing output logs and closing down.
2014-07-01 15:54:30,142 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:60020-1] wal.HLogSplitter: Waiting for split writer threads to finish
2014-07-01 15:54:30,150 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:60020-1] wal.HLogSplitter: Split writers finished
2014-07-01 15:54:30,150 DEBUG [RS_LOG_REPLAY_OPS-sceplus-vm48:60020-1] wal.HLogSplitter: Submitting close of hdfs://master:54310/hbase/data/default/usertable/122e92eabfab38cb15a9965b21db25c5/recovered.edits/0000000000000004489.temp
2014-07-01 15:54:30,150 DEBUG [RS_LOG_REPLAY_OPS-sceplus-vm48:60020-1] wal.HLogSplitter: Submitting close of hdfs://master:54310/hbase/data/default/usertable/667c1f2c34fe37d48ba25a789d8aba98/recovered.edits/0000000000000004455.temp
2014-07-01 15:54:30,150 DEBUG [split-log-closeStream-1] wal.HLogSplitter: Closing hdfs://master:54310/hbase/data/default/usertable/122e92eabfab38cb15a9965b21db25c5/recovered.edits/0000000000000004489.temp
2014-07-01 15:54:30,150 DEBUG [RS_LOG_REPLAY_OPS-sceplus-vm48:60020-1] wal.HLogSplitter: Submitting close of hdfs://master:54310/hbase/data/default/usertable/93253e37c284f3174309fede3f32339f/recovered.edits/0000000000000004496.temp
2014-07-01 15:54:30,151 DEBUG [split-log-closeStream-2] wal.HLogSplitter: Closing hdfs://master:54310/hbase/data/default/usertable/667c1f2c34fe37d48ba25a789d8aba98/recovered.edits/0000000000000004455.temp
2014-07-01 15:54:30,151 DEBUG [RS_LOG_REPLAY_OPS-sceplus-vm48:60020-1] wal.HLogSplitter: Submitting close of hdfs://master:54310/hbase/data/default/usertable/9cef9e7d34f96f1d9b111be50040a208/recovered.edits/0000000000000005025.temp
2014-07-01 15:54:30,151 DEBUG [RS_LOG_REPLAY_OPS-sceplus-vm48:60020-1] wal.HLogSplitter: Submitting close of hdfs://master:54310/hbase/data/default/usertable/bb79ae773f71289e0745068f68a0abd9/recovered.edits/0000000000000004445.temp
2014-07-01 15:54:30,151 DEBUG [split-log-closeStream-3] wal.HLogSplitter: Closing hdfs://master:54310/hbase/data/default/usertable/93253e37c284f3174309fede3f32339f/recovered.edits/0000000000000004496.temp
2014-07-01 15:54:30,161 INFO  [split-log-closeStream-3] wal.HLogSplitter: Closed wap hdfs://master:54310/hbase/data/default/usertable/93253e37c284f3174309fede3f32339f/recovered.edits/0000000000000004496.temp (wrote 16 edits in 172ms)
2014-07-01 15:54:30,163 INFO  [split-log-closeStream-1] wal.HLogSplitter: Closed wap hdfs://master:54310/hbase/data/default/usertable/122e92eabfab38cb15a9965b21db25c5/recovered.edits/0000000000000004489.temp (wrote 16 edits in 170ms)
2014-07-01 15:54:30,164 INFO  [split-log-closeStream-2] wal.HLogSplitter: Closed wap hdfs://master:54310/hbase/data/default/usertable/667c1f2c34fe37d48ba25a789d8aba98/recovered.edits/0000000000000004455.temp (wrote 14 edits in 187ms)
2014-07-01 15:54:30,167 DEBUG [split-log-closeStream-3] wal.HLogSplitter: Rename hdfs://master:54310/hbase/data/default/usertable/93253e37c284f3174309fede3f32339f/recovered.edits/0000000000000004496.temp to hdfs://master:54310/hbase/data/default/usertable/93253e37c284f3174309fede3f32339f/recovered.edits/0000000000000004511
2014-07-01 15:54:30,167 DEBUG [split-log-closeStream-3] wal.HLogSplitter: Closing hdfs://master:54310/hbase/data/default/usertable/9cef9e7d34f96f1d9b111be50040a208/recovered.edits/0000000000000005025.temp
2014-07-01 15:54:30,168 DEBUG [split-log-closeStream-1] wal.HLogSplitter: Rename hdfs://master:54310/hbase/data/default/usertable/122e92eabfab38cb15a9965b21db25c5/recovered.edits/0000000000000004489.temp to hdfs://master:54310/hbase/data/default/usertable/122e92eabfab38cb15a9965b21db25c5/recovered.edits/0000000000000004504
2014-07-01 15:54:30,168 DEBUG [split-log-closeStream-1] wal.HLogSplitter: Closing hdfs://master:54310/hbase/data/default/usertable/bb79ae773f71289e0745068f68a0abd9/recovered.edits/0000000000000004445.temp
2014-07-01 15:54:30,184 DEBUG [split-log-closeStream-2] wal.HLogSplitter: Rename hdfs://master:54310/hbase/data/default/usertable/667c1f2c34fe37d48ba25a789d8aba98/recovered.edits/0000000000000004455.temp to hdfs://master:54310/hbase/data/default/usertable/667c1f2c34fe37d48ba25a789d8aba98/recovered.edits/0000000000000004468
2014-07-01 15:54:30,186 INFO  [split-log-closeStream-3] wal.HLogSplitter: Closed wap hdfs://master:54310/hbase/data/default/usertable/9cef9e7d34f96f1d9b111be50040a208/recovered.edits/0000000000000005025.temp (wrote 59 edits in 220ms)
2014-07-01 15:54:30,188 INFO  [split-log-closeStream-1] wal.HLogSplitter: Closed wap hdfs://master:54310/hbase/data/default/usertable/bb79ae773f71289e0745068f68a0abd9/recovered.edits/0000000000000004445.temp (wrote 13 edits in 61ms)
2014-07-01 15:54:30,192 DEBUG [split-log-closeStream-3] wal.HLogSplitter: Rename hdfs://master:54310/hbase/data/default/usertable/9cef9e7d34f96f1d9b111be50040a208/recovered.edits/0000000000000005025.temp to hdfs://master:54310/hbase/data/default/usertable/9cef9e7d34f96f1d9b111be50040a208/recovered.edits/0000000000000005083
2014-07-01 15:54:30,231 DEBUG [split-log-closeStream-1] wal.HLogSplitter: Rename hdfs://master:54310/hbase/data/default/usertable/bb79ae773f71289e0745068f68a0abd9/recovered.edits/0000000000000004445.temp to hdfs://master:54310/hbase/data/default/usertable/bb79ae773f71289e0745068f68a0abd9/recovered.edits/0000000000000004457
2014-07-01 15:54:30,231 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:60020-1] wal.HLogSplitter: Processed 118 edits across 5 regions; log file=hdfs://master:54310/hbase/WALs/slave1,60020,1404253868846-splitting/slave1%2C60020%2C1404253868846.1404254461018 is corrupted = false progress failed = false
2014-07-01 15:54:30,237 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:60020-1] handler.HLogSplitterHandler: successfully transitioned task /hbase/splitWAL/WALs%2Fslave1%2C60020%2C1404253868846-splitting%2Fslave1%252C60020%252C1404253868846.1404254461018 to final state DONE sceplus-vm48.almaden.ibm.com,60020,1404255227840
2014-07-01 15:54:30,237 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:60020-1] handler.HLogSplitterHandler: worker sceplus-vm48.almaden.ibm.com,60020,1404255227840 done with task /hbase/splitWAL/WALs%2Fslave1%2C60020%2C1404253868846-splitting%2Fslave1%252C60020%252C1404253868846.1404254461018 in 1179ms
2014-07-01 15:54:30,493 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:60020-0] wal.HLogSplitter: Finishing writing output logs and closing down.
2014-07-01 15:54:30,493 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:60020-0] wal.HLogSplitter: Waiting for split writer threads to finish
2014-07-01 15:54:30,500 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:60020-0] wal.HLogSplitter: Split writers finished
2014-07-01 15:54:30,500 DEBUG [RS_LOG_REPLAY_OPS-sceplus-vm48:60020-0] wal.HLogSplitter: Submitting close of hdfs://master:54310/hbase/data/default/usertable/122e92eabfab38cb15a9965b21db25c5/recovered.edits/0000000000000004635.temp
2014-07-01 15:54:30,500 DEBUG [RS_LOG_REPLAY_OPS-sceplus-vm48:60020-0] wal.HLogSplitter: Submitting close of hdfs://master:54310/hbase/data/default/usertable/667c1f2c34fe37d48ba25a789d8aba98/recovered.edits/0000000000000004600.temp
2014-07-01 15:54:30,500 DEBUG [split-log-closeStream-1] wal.HLogSplitter: Closing hdfs://master:54310/hbase/data/default/usertable/122e92eabfab38cb15a9965b21db25c5/recovered.edits/0000000000000004635.temp
2014-07-01 15:54:30,500 DEBUG [RS_LOG_REPLAY_OPS-sceplus-vm48:60020-0] wal.HLogSplitter: Submitting close of hdfs://master:54310/hbase/data/default/usertable/93253e37c284f3174309fede3f32339f/recovered.edits/0000000000000004643.temp
2014-07-01 15:54:30,501 DEBUG [split-log-closeStream-2] wal.HLogSplitter: Closing hdfs://master:54310/hbase/data/default/usertable/667c1f2c34fe37d48ba25a789d8aba98/recovered.edits/0000000000000004600.temp
2014-07-01 15:54:30,501 DEBUG [RS_LOG_REPLAY_OPS-sceplus-vm48:60020-0] wal.HLogSplitter: Submitting close of hdfs://master:54310/hbase/data/default/usertable/9cef9e7d34f96f1d9b111be50040a208/recovered.edits/0000000000000005497.temp
2014-07-01 15:54:30,501 DEBUG [split-log-closeStream-3] wal.HLogSplitter: Closing hdfs://master:54310/hbase/data/default/usertable/93253e37c284f3174309fede3f32339f/recovered.edits/0000000000000004643.temp
2014-07-01 15:54:30,501 DEBUG [RS_LOG_REPLAY_OPS-sceplus-vm48:60020-0] wal.HLogSplitter: Submitting close of hdfs://master:54310/hbase/data/default/usertable/bb79ae773f71289e0745068f68a0abd9/recovered.edits/0000000000000004586.temp
2014-07-01 15:54:30,508 INFO  [split-log-closeStream-3] wal.HLogSplitter: Closed wap hdfs://master:54310/hbase/data/default/usertable/93253e37c284f3174309fede3f32339f/recovered.edits/0000000000000004643.temp (wrote 17 edits in 169ms)
2014-07-01 15:54:30,511 INFO  [split-log-closeStream-2] wal.HLogSplitter: Closed wap hdfs://master:54310/hbase/data/default/usertable/667c1f2c34fe37d48ba25a789d8aba98/recovered.edits/0000000000000004600.temp (wrote 17 edits in 208ms)
2014-07-01 15:54:30,513 INFO  [split-log-closeStream-1] wal.HLogSplitter: Closed wap hdfs://master:54310/hbase/data/default/usertable/122e92eabfab38cb15a9965b21db25c5/recovered.edits/0000000000000004635.temp (wrote 17 edits in 165ms)
2014-07-01 15:54:30,515 DEBUG [split-log-closeStream-3] wal.HLogSplitter: Rename hdfs://master:54310/hbase/data/default/usertable/93253e37c284f3174309fede3f32339f/recovered.edits/0000000000000004643.temp to hdfs://master:54310/hbase/data/default/usertable/93253e37c284f3174309fede3f32339f/recovered.edits/0000000000000004659
2014-07-01 15:54:30,515 DEBUG [split-log-closeStream-3] wal.HLogSplitter: Closing hdfs://master:54310/hbase/data/default/usertable/9cef9e7d34f96f1d9b111be50040a208/recovered.edits/0000000000000005497.temp
2014-07-01 15:54:30,516 DEBUG [split-log-closeStream-2] wal.HLogSplitter: Rename hdfs://master:54310/hbase/data/default/usertable/667c1f2c34fe37d48ba25a789d8aba98/recovered.edits/0000000000000004600.temp to hdfs://master:54310/hbase/data/default/usertable/667c1f2c34fe37d48ba25a789d8aba98/recovered.edits/0000000000000004616
2014-07-01 15:54:30,516 DEBUG [split-log-closeStream-2] wal.HLogSplitter: Closing hdfs://master:54310/hbase/data/default/usertable/bb79ae773f71289e0745068f68a0abd9/recovered.edits/0000000000000004586.temp
2014-07-01 15:54:30,519 DEBUG [split-log-closeStream-1] wal.HLogSplitter: Rename hdfs://master:54310/hbase/data/default/usertable/122e92eabfab38cb15a9965b21db25c5/recovered.edits/0000000000000004635.temp to hdfs://master:54310/hbase/data/default/usertable/122e92eabfab38cb15a9965b21db25c5/recovered.edits/0000000000000004651
2014-07-01 15:54:30,525 INFO  [split-log-closeStream-3] wal.HLogSplitter: Closed wap hdfs://master:54310/hbase/data/default/usertable/9cef9e7d34f96f1d9b111be50040a208/recovered.edits/0000000000000005497.temp (wrote 42 edits in 155ms)
2014-07-01 15:54:30,526 INFO  [split-log-closeStream-2] wal.HLogSplitter: Closed wap hdfs://master:54310/hbase/data/default/usertable/bb79ae773f71289e0745068f68a0abd9/recovered.edits/0000000000000004586.temp (wrote 20 edits in 112ms)
2014-07-01 15:54:30,529 DEBUG [split-log-closeStream-3] wal.HLogSplitter: Rename hdfs://master:54310/hbase/data/default/usertable/9cef9e7d34f96f1d9b111be50040a208/recovered.edits/0000000000000005497.temp to hdfs://master:54310/hbase/data/default/usertable/9cef9e7d34f96f1d9b111be50040a208/recovered.edits/0000000000000005538
2014-07-01 15:54:30,539 INFO  [SplitLogWorker-sceplus-vm48.almaden.ibm.com,60020,1404255227840] regionserver.SplitLogWorker: worker sceplus-vm48.almaden.ibm.com,60020,1404255227840 acquired task /hbase/splitWAL/WALs%2Fslave1%2C60020%2C1404253868846-splitting%2Fslave1%252C60020%252C1404253868846.1404254480320
2014-07-01 15:54:30,540 DEBUG [split-log-closeStream-2] wal.HLogSplitter: Rename hdfs://master:54310/hbase/data/default/usertable/bb79ae773f71289e0745068f68a0abd9/recovered.edits/0000000000000004586.temp to hdfs://master:54310/hbase/data/default/usertable/bb79ae773f71289e0745068f68a0abd9/recovered.edits/0000000000000004605
2014-07-01 15:54:30,540 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:60020-0] wal.HLogSplitter: Processed 113 edits across 5 regions; log file=hdfs://master:54310/hbase/WALs/slave1,60020,1404253868846-splitting/slave1%2C60020%2C1404253868846.1404254529807 is corrupted = false progress failed = false
2014-07-01 15:54:30,554 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:60020-0] handler.HLogSplitterHandler: successfully transitioned task /hbase/splitWAL/WALs%2Fslave1%2C60020%2C1404253868846-splitting%2Fslave1%252C60020%252C1404253868846.1404254529807 to final state DONE sceplus-vm48.almaden.ibm.com,60020,1404255227840
2014-07-01 15:54:30,554 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:60020-0] handler.HLogSplitterHandler: worker sceplus-vm48.almaden.ibm.com,60020,1404255227840 done with task /hbase/splitWAL/WALs%2Fslave1%2C60020%2C1404253868846-splitting%2Fslave1%252C60020%252C1404253868846.1404254529807 in 815ms
2014-07-01 15:54:30,595 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:60020-1] wal.HLogSplitter: Splitting hlog: hdfs://master:54310/hbase/WALs/slave1,60020,1404253868846-splitting/slave1%2C60020%2C1404253868846.1404254480320, length=64962856
2014-07-01 15:54:30,595 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:60020-1] wal.HLogSplitter: DistributedLogReplay = false
2014-07-01 15:54:30,602 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:60020-1] util.FSHDFSUtils: Recovering lease on dfs file hdfs://master:54310/hbase/WALs/slave1,60020,1404253868846-splitting/slave1%2C60020%2C1404253868846.1404254480320
2014-07-01 15:54:30,603 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:60020-1] util.FSHDFSUtils: recoverLease=true, attempt=0 on file=hdfs://master:54310/hbase/WALs/slave1,60020,1404253868846-splitting/slave1%2C60020%2C1404253868846.1404254480320 after 1ms
2014-07-01 15:54:30,618 DEBUG [RS_LOG_REPLAY_OPS-sceplus-vm48:60020-1-Writer-0] wal.HLogSplitter: Writer thread Thread[RS_LOG_REPLAY_OPS-sceplus-vm48:60020-1-Writer-0,5,main]: starting
2014-07-01 15:54:30,618 DEBUG [RS_LOG_REPLAY_OPS-sceplus-vm48:60020-1-Writer-1] wal.HLogSplitter: Writer thread Thread[RS_LOG_REPLAY_OPS-sceplus-vm48:60020-1-Writer-1,5,main]: starting
2014-07-01 15:54:30,618 DEBUG [RS_LOG_REPLAY_OPS-sceplus-vm48:60020-1-Writer-2] wal.HLogSplitter: Writer thread Thread[RS_LOG_REPLAY_OPS-sceplus-vm48:60020-1-Writer-2,5,main]: starting
2014-07-01 15:54:30,629 DEBUG [RS_LOG_REPLAY_OPS-sceplus-vm48:60020-1-Writer-2] wal.HLogSplitter: Creating writer path=hdfs://master:54310/hbase/data/default/usertable/9cef9e7d34f96f1d9b111be50040a208/recovered.edits/0000000000000005392.temp region=9cef9e7d34f96f1d9b111be50040a208
2014-07-01 15:54:30,656 DEBUG [RS_LOG_REPLAY_OPS-sceplus-vm48:60020-1-Writer-1] wal.HLogSplitter: Creating writer path=hdfs://master:54310/hbase/data/default/usertable/122e92eabfab38cb15a9965b21db25c5/recovered.edits/0000000000000004602.temp region=122e92eabfab38cb15a9965b21db25c5
2014-07-01 15:54:30,669 DEBUG [RS_LOG_REPLAY_OPS-sceplus-vm48:60020-1-Writer-2] wal.HLogSplitter: Creating writer path=hdfs://master:54310/hbase/data/default/usertable/667c1f2c34fe37d48ba25a789d8aba98/recovered.edits/0000000000000004567.temp region=667c1f2c34fe37d48ba25a789d8aba98
2014-07-01 15:54:30,674 DEBUG [RS_LOG_REPLAY_OPS-sceplus-vm48:60020-1-Writer-0] wal.HLogSplitter: Creating writer path=hdfs://master:54310/hbase/data/default/usertable/bb79ae773f71289e0745068f68a0abd9/recovered.edits/0000000000000004556.temp region=bb79ae773f71289e0745068f68a0abd9
2014-07-01 15:54:30,687 DEBUG [RS_LOG_REPLAY_OPS-sceplus-vm48:60020-1-Writer-1] wal.HLogSplitter: Creating writer path=hdfs://master:54310/hbase/data/default/usertable/93253e37c284f3174309fede3f32339f/recovered.edits/0000000000000004609.temp region=93253e37c284f3174309fede3f32339f
2014-07-01 15:54:31,298 INFO  [SplitLogWorker-sceplus-vm48.almaden.ibm.com,60020,1404255227840] regionserver.SplitLogWorker: worker sceplus-vm48.almaden.ibm.com,60020,1404255227840 acquired task /hbase/splitWAL/WALs%2Fslave1%2C60020%2C1404253868846-splitting%2Fslave1%252C60020%252C1404253868846.1404254467933
2014-07-01 15:54:31,321 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:60020-0] wal.HLogSplitter: Splitting hlog: hdfs://master:54310/hbase/WALs/slave1,60020,1404253868846-splitting/slave1%2C60020%2C1404253868846.1404254467933, length=75102391
2014-07-01 15:54:31,321 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:60020-0] wal.HLogSplitter: DistributedLogReplay = false
2014-07-01 15:54:31,324 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:60020-0] util.FSHDFSUtils: Recovering lease on dfs file hdfs://master:54310/hbase/WALs/slave1,60020,1404253868846-splitting/slave1%2C60020%2C1404253868846.1404254467933
2014-07-01 15:54:31,325 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:60020-0] util.FSHDFSUtils: recoverLease=true, attempt=0 on file=hdfs://master:54310/hbase/WALs/slave1,60020,1404253868846-splitting/slave1%2C60020%2C1404253868846.1404254467933 after 1ms
2014-07-01 15:54:31,359 DEBUG [RS_LOG_REPLAY_OPS-sceplus-vm48:60020-0-Writer-0] wal.HLogSplitter: Writer thread Thread[RS_LOG_REPLAY_OPS-sceplus-vm48:60020-0-Writer-0,5,main]: starting
2014-07-01 15:54:31,359 DEBUG [RS_LOG_REPLAY_OPS-sceplus-vm48:60020-0-Writer-1] wal.HLogSplitter: Writer thread Thread[RS_LOG_REPLAY_OPS-sceplus-vm48:60020-0-Writer-1,5,main]: starting
2014-07-01 15:54:31,360 DEBUG [RS_LOG_REPLAY_OPS-sceplus-vm48:60020-0-Writer-2] wal.HLogSplitter: Writer thread Thread[RS_LOG_REPLAY_OPS-sceplus-vm48:60020-0-Writer-2,5,main]: starting
2014-07-01 15:54:31,373 DEBUG [RS_LOG_REPLAY_OPS-sceplus-vm48:60020-0-Writer-2] wal.HLogSplitter: Creating writer path=hdfs://master:54310/hbase/data/default/usertable/bb79ae773f71289e0745068f68a0abd9/recovered.edits/0000000000000004488.temp region=bb79ae773f71289e0745068f68a0abd9
2014-07-01 15:54:31,387 DEBUG [RS_LOG_REPLAY_OPS-sceplus-vm48:60020-0-Writer-0] wal.HLogSplitter: Creating writer path=hdfs://master:54310/hbase/data/default/usertable/9cef9e7d34f96f1d9b111be50040a208/recovered.edits/0000000000000005182.temp region=9cef9e7d34f96f1d9b111be50040a208
2014-07-01 15:54:31,405 DEBUG [RS_LOG_REPLAY_OPS-sceplus-vm48:60020-0-Writer-2] wal.HLogSplitter: Creating writer path=hdfs://master:54310/hbase/data/default/usertable/122e92eabfab38cb15a9965b21db25c5/recovered.edits/0000000000000004536.temp region=122e92eabfab38cb15a9965b21db25c5
2014-07-01 15:54:31,432 DEBUG [RS_LOG_REPLAY_OPS-sceplus-vm48:60020-0-Writer-1] wal.HLogSplitter: Creating writer path=hdfs://master:54310/hbase/data/default/usertable/667c1f2c34fe37d48ba25a789d8aba98/recovered.edits/0000000000000004501.temp region=667c1f2c34fe37d48ba25a789d8aba98
2014-07-01 15:54:31,468 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:60020-1] wal.HLogSplitter: Finishing writing output logs and closing down.
2014-07-01 15:54:31,469 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:60020-1] wal.HLogSplitter: Waiting for split writer threads to finish
2014-07-01 15:54:31,473 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:60020-1] wal.HLogSplitter: Split writers finished
2014-07-01 15:54:31,474 DEBUG [RS_LOG_REPLAY_OPS-sceplus-vm48:60020-1] wal.HLogSplitter: Submitting close of hdfs://master:54310/hbase/data/default/usertable/122e92eabfab38cb15a9965b21db25c5/recovered.edits/0000000000000004602.temp
2014-07-01 15:54:31,474 DEBUG [RS_LOG_REPLAY_OPS-sceplus-vm48:60020-1] wal.HLogSplitter: Submitting close of hdfs://master:54310/hbase/data/default/usertable/667c1f2c34fe37d48ba25a789d8aba98/recovered.edits/0000000000000004567.temp
2014-07-01 15:54:31,474 DEBUG [split-log-closeStream-1] wal.HLogSplitter: Closing hdfs://master:54310/hbase/data/default/usertable/122e92eabfab38cb15a9965b21db25c5/recovered.edits/0000000000000004602.temp
2014-07-01 15:54:31,474 DEBUG [RS_LOG_REPLAY_OPS-sceplus-vm48:60020-1] wal.HLogSplitter: Submitting close of hdfs://master:54310/hbase/data/default/usertable/93253e37c284f3174309fede3f32339f/recovered.edits/0000000000000004609.temp
2014-07-01 15:54:31,474 DEBUG [split-log-closeStream-2] wal.HLogSplitter: Closing hdfs://master:54310/hbase/data/default/usertable/667c1f2c34fe37d48ba25a789d8aba98/recovered.edits/0000000000000004567.temp
2014-07-01 15:54:31,474 DEBUG [RS_LOG_REPLAY_OPS-sceplus-vm48:60020-1] wal.HLogSplitter: Submitting close of hdfs://master:54310/hbase/data/default/usertable/9cef9e7d34f96f1d9b111be50040a208/recovered.edits/0000000000000005392.temp
2014-07-01 15:54:31,474 DEBUG [RS_LOG_REPLAY_OPS-sceplus-vm48:60020-1] wal.HLogSplitter: Submitting close of hdfs://master:54310/hbase/data/default/usertable/bb79ae773f71289e0745068f68a0abd9/recovered.edits/0000000000000004556.temp
2014-07-01 15:54:31,474 DEBUG [split-log-closeStream-3] wal.HLogSplitter: Closing hdfs://master:54310/hbase/data/default/usertable/93253e37c284f3174309fede3f32339f/recovered.edits/0000000000000004609.temp
2014-07-01 15:54:31,478 DEBUG [RS_LOG_REPLAY_OPS-sceplus-vm48:60020-0-Writer-1] wal.HLogSplitter: Creating writer path=hdfs://master:54310/hbase/data/default/usertable/93253e37c284f3174309fede3f32339f/recovered.edits/0000000000000004544.temp region=93253e37c284f3174309fede3f32339f
2014-07-01 15:54:31,484 INFO  [split-log-closeStream-1] wal.HLogSplitter: Closed wap hdfs://master:54310/hbase/data/default/usertable/122e92eabfab38cb15a9965b21db25c5/recovered.edits/0000000000000004602.temp (wrote 15 edits in 156ms)
2014-07-01 15:54:31,486 INFO  [split-log-closeStream-2] wal.HLogSplitter: Closed wap hdfs://master:54310/hbase/data/default/usertable/667c1f2c34fe37d48ba25a789d8aba98/recovered.edits/0000000000000004567.temp (wrote 16 edits in 170ms)
2014-07-01 15:54:31,488 INFO  [split-log-closeStream-3] wal.HLogSplitter: Closed wap hdfs://master:54310/hbase/data/default/usertable/93253e37c284f3174309fede3f32339f/recovered.edits/0000000000000004609.temp (wrote 14 edits in 139ms)
2014-07-01 15:54:31,490 DEBUG [split-log-closeStream-2] wal.HLogSplitter: Rename hdfs://master:54310/hbase/data/default/usertable/667c1f2c34fe37d48ba25a789d8aba98/recovered.edits/0000000000000004567.temp to hdfs://master:54310/hbase/data/default/usertable/667c1f2c34fe37d48ba25a789d8aba98/recovered.edits/0000000000000004582
2014-07-01 15:54:31,490 DEBUG [split-log-closeStream-1] wal.HLogSplitter: Rename hdfs://master:54310/hbase/data/default/usertable/122e92eabfab38cb15a9965b21db25c5/recovered.edits/0000000000000004602.temp to hdfs://master:54310/hbase/data/default/usertable/122e92eabfab38cb15a9965b21db25c5/recovered.edits/0000000000000004616
2014-07-01 15:54:31,490 DEBUG [split-log-closeStream-2] wal.HLogSplitter: Closing hdfs://master:54310/hbase/data/default/usertable/9cef9e7d34f96f1d9b111be50040a208/recovered.edits/0000000000000005392.temp
2014-07-01 15:54:31,490 DEBUG [split-log-closeStream-1] wal.HLogSplitter: Closing hdfs://master:54310/hbase/data/default/usertable/bb79ae773f71289e0745068f68a0abd9/recovered.edits/0000000000000004556.temp
2014-07-01 15:54:31,495 INFO  [split-log-closeStream-1] wal.HLogSplitter: Closed wap hdfs://master:54310/hbase/data/default/usertable/bb79ae773f71289e0745068f68a0abd9/recovered.edits/0000000000000004556.temp (wrote 16 edits in 57ms)
2014-07-01 15:54:31,496 INFO  [split-log-closeStream-2] wal.HLogSplitter: Closed wap hdfs://master:54310/hbase/data/default/usertable/9cef9e7d34f96f1d9b111be50040a208/recovered.edits/0000000000000005392.temp (wrote 55 edits in 136ms)
2014-07-01 15:54:31,497 DEBUG [split-log-closeStream-3] wal.HLogSplitter: Rename hdfs://master:54310/hbase/data/default/usertable/93253e37c284f3174309fede3f32339f/recovered.edits/0000000000000004609.temp to hdfs://master:54310/hbase/data/default/usertable/93253e37c284f3174309fede3f32339f/recovered.edits/0000000000000004622
2014-07-01 15:54:31,498 DEBUG [split-log-closeStream-1] wal.HLogSplitter: Rename hdfs://master:54310/hbase/data/default/usertable/bb79ae773f71289e0745068f68a0abd9/recovered.edits/0000000000000004556.temp to hdfs://master:54310/hbase/data/default/usertable/bb79ae773f71289e0745068f68a0abd9/recovered.edits/0000000000000004571
2014-07-01 15:54:31,543 DEBUG [split-log-closeStream-2] wal.HLogSplitter: Rename hdfs://master:54310/hbase/data/default/usertable/9cef9e7d34f96f1d9b111be50040a208/recovered.edits/0000000000000005392.temp to hdfs://master:54310/hbase/data/default/usertable/9cef9e7d34f96f1d9b111be50040a208/recovered.edits/0000000000000005446
2014-07-01 15:54:31,544 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:60020-1] wal.HLogSplitter: Processed 116 edits across 5 regions; log file=hdfs://master:54310/hbase/WALs/slave1,60020,1404253868846-splitting/slave1%2C60020%2C1404253868846.1404254480320 is corrupted = false progress failed = false
2014-07-01 15:54:31,549 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:60020-1] handler.HLogSplitterHandler: successfully transitioned task /hbase/splitWAL/WALs%2Fslave1%2C60020%2C1404253868846-splitting%2Fslave1%252C60020%252C1404253868846.1404254480320 to final state DONE sceplus-vm48.almaden.ibm.com,60020,1404255227840
2014-07-01 15:54:31,549 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:60020-1] handler.HLogSplitterHandler: worker sceplus-vm48.almaden.ibm.com,60020,1404255227840 done with task /hbase/splitWAL/WALs%2Fslave1%2C60020%2C1404253868846-splitting%2Fslave1%252C60020%252C1404253868846.1404254480320 in 1010ms
2014-07-01 15:54:31,874 INFO  [SplitLogWorker-sceplus-vm48.almaden.ibm.com,60020,1404255227840] regionserver.SplitLogWorker: worker sceplus-vm48.almaden.ibm.com,60020,1404255227840 acquired task /hbase/splitWAL/WALs%2Fslave1%2C60020%2C1404253868846-splitting%2Fslave1%252C60020%252C1404253868846.1404254465527
2014-07-01 15:54:31,894 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:60020-1] wal.HLogSplitter: Splitting hlog: hdfs://master:54310/hbase/WALs/slave1,60020,1404253868846-splitting/slave1%2C60020%2C1404253868846.1404254465527, length=69066440
2014-07-01 15:54:31,894 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:60020-1] wal.HLogSplitter: DistributedLogReplay = false
2014-07-01 15:54:31,898 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:60020-1] util.FSHDFSUtils: Recovering lease on dfs file hdfs://master:54310/hbase/WALs/slave1,60020,1404253868846-splitting/slave1%2C60020%2C1404253868846.1404254465527
2014-07-01 15:54:31,899 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:60020-1] util.FSHDFSUtils: recoverLease=true, attempt=0 on file=hdfs://master:54310/hbase/WALs/slave1,60020,1404253868846-splitting/slave1%2C60020%2C1404253868846.1404254465527 after 1ms
2014-07-01 15:54:31,942 DEBUG [RS_LOG_REPLAY_OPS-sceplus-vm48:60020-1-Writer-0] wal.HLogSplitter: Writer thread Thread[RS_LOG_REPLAY_OPS-sceplus-vm48:60020-1-Writer-0,5,main]: starting
2014-07-01 15:54:31,942 DEBUG [RS_LOG_REPLAY_OPS-sceplus-vm48:60020-1-Writer-1] wal.HLogSplitter: Writer thread Thread[RS_LOG_REPLAY_OPS-sceplus-vm48:60020-1-Writer-1,5,main]: starting
2014-07-01 15:54:31,942 DEBUG [RS_LOG_REPLAY_OPS-sceplus-vm48:60020-1-Writer-2] wal.HLogSplitter: Writer thread Thread[RS_LOG_REPLAY_OPS-sceplus-vm48:60020-1-Writer-2,5,main]: starting
2014-07-01 15:54:31,970 DEBUG [RS_LOG_REPLAY_OPS-sceplus-vm48:60020-1-Writer-2] wal.HLogSplitter: Creating writer path=hdfs://master:54310/hbase/data/default/usertable/667c1f2c34fe37d48ba25a789d8aba98/recovered.edits/0000000000000004484.temp region=667c1f2c34fe37d48ba25a789d8aba98
2014-07-01 15:54:32,003 DEBUG [RS_LOG_REPLAY_OPS-sceplus-vm48:60020-1-Writer-0] wal.HLogSplitter: Creating writer path=hdfs://master:54310/hbase/data/default/usertable/9cef9e7d34f96f1d9b111be50040a208/recovered.edits/0000000000000005136.temp region=9cef9e7d34f96f1d9b111be50040a208
2014-07-01 15:54:32,012 DEBUG [RS_LOG_REPLAY_OPS-sceplus-vm48:60020-1-Writer-1] wal.HLogSplitter: Creating writer path=hdfs://master:54310/hbase/data/default/usertable/bb79ae773f71289e0745068f68a0abd9/recovered.edits/0000000000000004473.temp region=bb79ae773f71289e0745068f68a0abd9
2014-07-01 15:54:32,022 DEBUG [RS_LOG_REPLAY_OPS-sceplus-vm48:60020-1-Writer-2] wal.HLogSplitter: Creating writer path=hdfs://master:54310/hbase/data/default/usertable/93253e37c284f3174309fede3f32339f/recovered.edits/0000000000000004527.temp region=93253e37c284f3174309fede3f32339f
2014-07-01 15:54:32,036 DEBUG [RS_LOG_REPLAY_OPS-sceplus-vm48:60020-1-Writer-0] wal.HLogSplitter: Creating writer path=hdfs://master:54310/hbase/data/default/usertable/122e92eabfab38cb15a9965b21db25c5/recovered.edits/0000000000000004520.temp region=122e92eabfab38cb15a9965b21db25c5
2014-07-01 15:54:32,080 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:60020-0] wal.HLogSplitter: Finishing writing output logs and closing down.
2014-07-01 15:54:32,080 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:60020-0] wal.HLogSplitter: Waiting for split writer threads to finish
2014-07-01 15:54:32,090 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:60020-0] wal.HLogSplitter: Split writers finished
2014-07-01 15:54:32,090 DEBUG [RS_LOG_REPLAY_OPS-sceplus-vm48:60020-0] wal.HLogSplitter: Submitting close of hdfs://master:54310/hbase/data/default/usertable/122e92eabfab38cb15a9965b21db25c5/recovered.edits/0000000000000004536.temp
2014-07-01 15:54:32,090 DEBUG [RS_LOG_REPLAY_OPS-sceplus-vm48:60020-0] wal.HLogSplitter: Submitting close of hdfs://master:54310/hbase/data/default/usertable/667c1f2c34fe37d48ba25a789d8aba98/recovered.edits/0000000000000004501.temp
2014-07-01 15:54:32,090 DEBUG [split-log-closeStream-1] wal.HLogSplitter: Closing hdfs://master:54310/hbase/data/default/usertable/122e92eabfab38cb15a9965b21db25c5/recovered.edits/0000000000000004536.temp
2014-07-01 15:54:32,091 DEBUG [RS_LOG_REPLAY_OPS-sceplus-vm48:60020-0] wal.HLogSplitter: Submitting close of hdfs://master:54310/hbase/data/default/usertable/93253e37c284f3174309fede3f32339f/recovered.edits/0000000000000004544.temp
2014-07-01 15:54:32,091 DEBUG [split-log-closeStream-2] wal.HLogSplitter: Closing hdfs://master:54310/hbase/data/default/usertable/667c1f2c34fe37d48ba25a789d8aba98/recovered.edits/0000000000000004501.temp
2014-07-01 15:54:32,091 DEBUG [RS_LOG_REPLAY_OPS-sceplus-vm48:60020-0] wal.HLogSplitter: Submitting close of hdfs://master:54310/hbase/data/default/usertable/9cef9e7d34f96f1d9b111be50040a208/recovered.edits/0000000000000005182.temp
2014-07-01 15:54:32,091 DEBUG [RS_LOG_REPLAY_OPS-sceplus-vm48:60020-0] wal.HLogSplitter: Submitting close of hdfs://master:54310/hbase/data/default/usertable/bb79ae773f71289e0745068f68a0abd9/recovered.edits/0000000000000004488.temp
2014-07-01 15:54:32,091 DEBUG [split-log-closeStream-3] wal.HLogSplitter: Closing hdfs://master:54310/hbase/data/default/usertable/93253e37c284f3174309fede3f32339f/recovered.edits/0000000000000004544.temp
2014-07-01 15:54:32,101 INFO  [split-log-closeStream-1] wal.HLogSplitter: Closed wap hdfs://master:54310/hbase/data/default/usertable/122e92eabfab38cb15a9965b21db25c5/recovered.edits/0000000000000004536.temp (wrote 19 edits in 184ms)
2014-07-01 15:54:32,102 INFO  [split-log-closeStream-2] wal.HLogSplitter: Closed wap hdfs://master:54310/hbase/data/default/usertable/667c1f2c34fe37d48ba25a789d8aba98/recovered.edits/0000000000000004501.temp (wrote 17 edits in 145ms)
2014-07-01 15:54:32,103 INFO  [split-log-closeStream-3] wal.HLogSplitter: Closed wap hdfs://master:54310/hbase/data/default/usertable/93253e37c284f3174309fede3f32339f/recovered.edits/0000000000000004544.temp (wrote 19 edits in 180ms)
2014-07-01 15:54:32,105 DEBUG [split-log-closeStream-1] wal.HLogSplitter: Rename hdfs://master:54310/hbase/data/default/usertable/122e92eabfab38cb15a9965b21db25c5/recovered.edits/0000000000000004536.temp to hdfs://master:54310/hbase/data/default/usertable/122e92eabfab38cb15a9965b21db25c5/recovered.edits/0000000000000004554
2014-07-01 15:54:32,105 DEBUG [split-log-closeStream-1] wal.HLogSplitter: Closing hdfs://master:54310/hbase/data/default/usertable/9cef9e7d34f96f1d9b111be50040a208/recovered.edits/0000000000000005182.temp
2014-07-01 15:54:32,106 DEBUG [split-log-closeStream-2] wal.HLogSplitter: Rename hdfs://master:54310/hbase/data/default/usertable/667c1f2c34fe37d48ba25a789d8aba98/recovered.edits/0000000000000004501.temp to hdfs://master:54310/hbase/data/default/usertable/667c1f2c34fe37d48ba25a789d8aba98/recovered.edits/0000000000000004517
2014-07-01 15:54:32,106 DEBUG [split-log-closeStream-2] wal.HLogSplitter: Closing hdfs://master:54310/hbase/data/default/usertable/bb79ae773f71289e0745068f68a0abd9/recovered.edits/0000000000000004488.temp
2014-07-01 15:54:32,110 DEBUG [split-log-closeStream-3] wal.HLogSplitter: Rename hdfs://master:54310/hbase/data/default/usertable/93253e37c284f3174309fede3f32339f/recovered.edits/0000000000000004544.temp to hdfs://master:54310/hbase/data/default/usertable/93253e37c284f3174309fede3f32339f/recovered.edits/0000000000000004562
2014-07-01 15:54:32,112 INFO  [split-log-closeStream-1] wal.HLogSplitter: Closed wap hdfs://master:54310/hbase/data/default/usertable/9cef9e7d34f96f1d9b111be50040a208/recovered.edits/0000000000000005182.temp (wrote 52 edits in 152ms)
2014-07-01 15:54:32,113 INFO  [split-log-closeStream-2] wal.HLogSplitter: Closed wap hdfs://master:54310/hbase/data/default/usertable/bb79ae773f71289e0745068f68a0abd9/recovered.edits/0000000000000004488.temp (wrote 20 edits in 68ms)
2014-07-01 15:54:32,115 DEBUG [split-log-closeStream-1] wal.HLogSplitter: Rename hdfs://master:54310/hbase/data/default/usertable/9cef9e7d34f96f1d9b111be50040a208/recovered.edits/0000000000000005182.temp to hdfs://master:54310/hbase/data/default/usertable/9cef9e7d34f96f1d9b111be50040a208/recovered.edits/0000000000000005233
2014-07-01 15:54:32,162 DEBUG [split-log-closeStream-2] wal.HLogSplitter: Rename hdfs://master:54310/hbase/data/default/usertable/bb79ae773f71289e0745068f68a0abd9/recovered.edits/0000000000000004488.temp to hdfs://master:54310/hbase/data/default/usertable/bb79ae773f71289e0745068f68a0abd9/recovered.edits/0000000000000004507
2014-07-01 15:54:32,162 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:60020-0] wal.HLogSplitter: Processed 127 edits across 5 regions; log file=hdfs://master:54310/hbase/WALs/slave1,60020,1404253868846-splitting/slave1%2C60020%2C1404253868846.1404254467933 is corrupted = false progress failed = false
2014-07-01 15:54:32,168 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:60020-0] handler.HLogSplitterHandler: successfully transitioned task /hbase/splitWAL/WALs%2Fslave1%2C60020%2C1404253868846-splitting%2Fslave1%252C60020%252C1404253868846.1404254467933 to final state DONE sceplus-vm48.almaden.ibm.com,60020,1404255227840
2014-07-01 15:54:32,168 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:60020-0] handler.HLogSplitterHandler: worker sceplus-vm48.almaden.ibm.com,60020,1404255227840 done with task /hbase/splitWAL/WALs%2Fslave1%2C60020%2C1404253868846-splitting%2Fslave1%252C60020%252C1404253868846.1404254467933 in 870ms
2014-07-01 15:54:32,501 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:60020-1] wal.HLogSplitter: Finishing writing output logs and closing down.
2014-07-01 15:54:32,501 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:60020-1] wal.HLogSplitter: Waiting for split writer threads to finish
2014-07-01 15:54:32,508 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:60020-1] wal.HLogSplitter: Split writers finished
2014-07-01 15:54:32,508 DEBUG [RS_LOG_REPLAY_OPS-sceplus-vm48:60020-1] wal.HLogSplitter: Submitting close of hdfs://master:54310/hbase/data/default/usertable/122e92eabfab38cb15a9965b21db25c5/recovered.edits/0000000000000004520.temp
2014-07-01 15:54:32,508 DEBUG [RS_LOG_REPLAY_OPS-sceplus-vm48:60020-1] wal.HLogSplitter: Submitting close of hdfs://master:54310/hbase/data/default/usertable/667c1f2c34fe37d48ba25a789d8aba98/recovered.edits/0000000000000004484.temp
2014-07-01 15:54:32,508 DEBUG [split-log-closeStream-1] wal.HLogSplitter: Closing hdfs://master:54310/hbase/data/default/usertable/122e92eabfab38cb15a9965b21db25c5/recovered.edits/0000000000000004520.temp
2014-07-01 15:54:32,508 DEBUG [RS_LOG_REPLAY_OPS-sceplus-vm48:60020-1] wal.HLogSplitter: Submitting close of hdfs://master:54310/hbase/data/default/usertable/93253e37c284f3174309fede3f32339f/recovered.edits/0000000000000004527.temp
2014-07-01 15:54:32,508 DEBUG [split-log-closeStream-2] wal.HLogSplitter: Closing hdfs://master:54310/hbase/data/default/usertable/667c1f2c34fe37d48ba25a789d8aba98/recovered.edits/0000000000000004484.temp
2014-07-01 15:54:32,508 DEBUG [RS_LOG_REPLAY_OPS-sceplus-vm48:60020-1] wal.HLogSplitter: Submitting close of hdfs://master:54310/hbase/data/default/usertable/9cef9e7d34f96f1d9b111be50040a208/recovered.edits/0000000000000005136.temp
2014-07-01 15:54:32,509 DEBUG [split-log-closeStream-3] wal.HLogSplitter: Closing hdfs://master:54310/hbase/data/default/usertable/93253e37c284f3174309fede3f32339f/recovered.edits/0000000000000004527.temp
2014-07-01 15:54:32,509 DEBUG [RS_LOG_REPLAY_OPS-sceplus-vm48:60020-1] wal.HLogSplitter: Submitting close of hdfs://master:54310/hbase/data/default/usertable/bb79ae773f71289e0745068f68a0abd9/recovered.edits/0000000000000004473.temp
2014-07-01 15:54:32,515 INFO  [split-log-closeStream-1] wal.HLogSplitter: Closed wap hdfs://master:54310/hbase/data/default/usertable/122e92eabfab38cb15a9965b21db25c5/recovered.edits/0000000000000004520.temp (wrote 15 edits in 165ms)
2014-07-01 15:54:32,516 INFO  [split-log-closeStream-3] wal.HLogSplitter: Closed wap hdfs://master:54310/hbase/data/default/usertable/93253e37c284f3174309fede3f32339f/recovered.edits/0000000000000004527.temp (wrote 17 edits in 171ms)
2014-07-01 15:54:32,516 INFO  [split-log-closeStream-2] wal.HLogSplitter: Closed wap hdfs://master:54310/hbase/data/default/usertable/667c1f2c34fe37d48ba25a789d8aba98/recovered.edits/0000000000000004484.temp (wrote 17 edits in 176ms)
2014-07-01 15:54:32,519 DEBUG [split-log-closeStream-1] wal.HLogSplitter: Rename hdfs://master:54310/hbase/data/default/usertable/122e92eabfab38cb15a9965b21db25c5/recovered.edits/0000000000000004520.temp to hdfs://master:54310/hbase/data/default/usertable/122e92eabfab38cb15a9965b21db25c5/recovered.edits/0000000000000004535
2014-07-01 15:54:32,519 DEBUG [split-log-closeStream-1] wal.HLogSplitter: Closing hdfs://master:54310/hbase/data/default/usertable/9cef9e7d34f96f1d9b111be50040a208/recovered.edits/0000000000000005136.temp
2014-07-01 15:54:32,519 DEBUG [split-log-closeStream-3] wal.HLogSplitter: Rename hdfs://master:54310/hbase/data/default/usertable/93253e37c284f3174309fede3f32339f/recovered.edits/0000000000000004527.temp to hdfs://master:54310/hbase/data/default/usertable/93253e37c284f3174309fede3f32339f/recovered.edits/0000000000000004543
2014-07-01 15:54:32,519 DEBUG [split-log-closeStream-3] wal.HLogSplitter: Closing hdfs://master:54310/hbase/data/default/usertable/bb79ae773f71289e0745068f68a0abd9/recovered.edits/0000000000000004473.temp
2014-07-01 15:54:32,523 DEBUG [split-log-closeStream-2] wal.HLogSplitter: Rename hdfs://master:54310/hbase/data/default/usertable/667c1f2c34fe37d48ba25a789d8aba98/recovered.edits/0000000000000004484.temp to hdfs://master:54310/hbase/data/default/usertable/667c1f2c34fe37d48ba25a789d8aba98/recovered.edits/0000000000000004500
2014-07-01 15:54:32,524 INFO  [split-log-closeStream-1] wal.HLogSplitter: Closed wap hdfs://master:54310/hbase/data/default/usertable/9cef9e7d34f96f1d9b111be50040a208/recovered.edits/0000000000000005136.temp (wrote 46 edits in 187ms)
2014-07-01 15:54:32,525 INFO  [split-log-closeStream-3] wal.HLogSplitter: Closed wap hdfs://master:54310/hbase/data/default/usertable/bb79ae773f71289e0745068f68a0abd9/recovered.edits/0000000000000004473.temp (wrote 15 edits in 56ms)
2014-07-01 15:54:32,532 DEBUG [split-log-closeStream-1] wal.HLogSplitter: Rename hdfs://master:54310/hbase/data/default/usertable/9cef9e7d34f96f1d9b111be50040a208/recovered.edits/0000000000000005136.temp to hdfs://master:54310/hbase/data/default/usertable/9cef9e7d34f96f1d9b111be50040a208/recovered.edits/0000000000000005181
2014-07-01 15:54:32,570 DEBUG [split-log-closeStream-3] wal.HLogSplitter: Rename hdfs://master:54310/hbase/data/default/usertable/bb79ae773f71289e0745068f68a0abd9/recovered.edits/0000000000000004473.temp to hdfs://master:54310/hbase/data/default/usertable/bb79ae773f71289e0745068f68a0abd9/recovered.edits/0000000000000004487
2014-07-01 15:54:32,571 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:60020-1] wal.HLogSplitter: Processed 110 edits across 5 regions; log file=hdfs://master:54310/hbase/WALs/slave1,60020,1404253868846-splitting/slave1%2C60020%2C1404253868846.1404254465527 is corrupted = false progress failed = false
2014-07-01 15:54:32,574 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:60020-1] handler.HLogSplitterHandler: successfully transitioned task /hbase/splitWAL/WALs%2Fslave1%2C60020%2C1404253868846-splitting%2Fslave1%252C60020%252C1404253868846.1404254465527 to final state DONE sceplus-vm48.almaden.ibm.com,60020,1404255227840
2014-07-01 15:54:32,575 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:60020-1] handler.HLogSplitterHandler: worker sceplus-vm48.almaden.ibm.com,60020,1404255227840 done with task /hbase/splitWAL/WALs%2Fslave1%2C60020%2C1404253868846-splitting%2Fslave1%252C60020%252C1404253868846.1404254465527 in 700ms
2014-07-01 15:54:32,810 INFO  [SplitLogWorker-sceplus-vm48.almaden.ibm.com,60020,1404255227840] regionserver.SplitLogWorker: worker sceplus-vm48.almaden.ibm.com,60020,1404255227840 acquired task /hbase/splitWAL/WALs%2Fsceplus-vm48.almaden.ibm.com%2C60020%2C1404253870530-splitting%2Fsceplus-vm48.almaden.ibm.com%252C60020%252C1404253870530.1404254449756
2014-07-01 15:54:32,831 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:60020-0] wal.HLogSplitter: Splitting hlog: hdfs://master:54310/hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1404253870530-splitting/sceplus-vm48.almaden.ibm.com%2C60020%2C1404253870530.1404254449756, length=72565195
2014-07-01 15:54:32,831 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:60020-0] wal.HLogSplitter: DistributedLogReplay = false
2014-07-01 15:54:32,837 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:60020-0] util.FSHDFSUtils: Recovering lease on dfs file hdfs://master:54310/hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1404253870530-splitting/sceplus-vm48.almaden.ibm.com%2C60020%2C1404253870530.1404254449756
2014-07-01 15:54:32,839 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:60020-0] util.FSHDFSUtils: recoverLease=true, attempt=0 on file=hdfs://master:54310/hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1404253870530-splitting/sceplus-vm48.almaden.ibm.com%2C60020%2C1404253870530.1404254449756 after 2ms
2014-07-01 15:54:32,868 DEBUG [RS_LOG_REPLAY_OPS-sceplus-vm48:60020-0-Writer-0] wal.HLogSplitter: Writer thread Thread[RS_LOG_REPLAY_OPS-sceplus-vm48:60020-0-Writer-0,5,main]: starting
2014-07-01 15:54:32,868 DEBUG [RS_LOG_REPLAY_OPS-sceplus-vm48:60020-0-Writer-1] wal.HLogSplitter: Writer thread Thread[RS_LOG_REPLAY_OPS-sceplus-vm48:60020-0-Writer-1,5,main]: starting
2014-07-01 15:54:32,869 DEBUG [RS_LOG_REPLAY_OPS-sceplus-vm48:60020-0-Writer-2] wal.HLogSplitter: Writer thread Thread[RS_LOG_REPLAY_OPS-sceplus-vm48:60020-0-Writer-2,5,main]: starting
2014-07-01 15:54:32,903 DEBUG [RS_LOG_REPLAY_OPS-sceplus-vm48:60020-0-Writer-2] wal.HLogSplitter: Creating writer path=hdfs://master:54310/hbase/data/default/usertable/c72fc9d54f680bcd54e9fa888600346b/recovered.edits/0000000000000001697.temp region=c72fc9d54f680bcd54e9fa888600346b
2014-07-01 15:54:32,912 DEBUG [RS_LOG_REPLAY_OPS-sceplus-vm48:60020-0-Writer-0] wal.HLogSplitter: Creating writer path=hdfs://master:54310/hbase/data/default/usertable/1dc0aa61f0e3f43a53375118eb48213f/recovered.edits/0000000000000001687.temp region=1dc0aa61f0e3f43a53375118eb48213f
2014-07-01 15:54:32,922 DEBUG [RS_LOG_REPLAY_OPS-sceplus-vm48:60020-0-Writer-1] wal.HLogSplitter: Creating writer path=hdfs://master:54310/hbase/data/default/usertable/52290d336af5a689eef6040bafbe6b15/recovered.edits/0000000000000001677.temp region=52290d336af5a689eef6040bafbe6b15
2014-07-01 15:54:33,017 DEBUG [RS_LOG_REPLAY_OPS-sceplus-vm48:60020-0-Writer-2] wal.HLogSplitter: Creating writer path=hdfs://master:54310/hbase/data/default/usertable/99b73df170b514f6773e742a495216c4/recovered.edits/0000000000000001677.temp region=99b73df170b514f6773e742a495216c4
2014-07-01 15:54:33,360 DEBUG [regionserver60020-EventThread] regionserver.SplitLogWorker: tasks arrived or departed
2014-07-01 15:54:33,380 INFO  [SplitLogWorker-sceplus-vm48.almaden.ibm.com,60020,1404255227840] regionserver.SplitLogWorker: worker sceplus-vm48.almaden.ibm.com,60020,1404255227840 acquired task /hbase/splitWAL/WALs%2Fsceplus-vm48.almaden.ibm.com%2C60020%2C1404253870530-splitting%2Fsceplus-vm48.almaden.ibm.com%252C60020%252C1404253870530.1404254443569
2014-07-01 15:54:33,399 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:60020-1] wal.HLogSplitter: Splitting hlog: hdfs://master:54310/hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1404253870530-splitting/sceplus-vm48.almaden.ibm.com%2C60020%2C1404253870530.1404254443569, length=79227369
2014-07-01 15:54:33,400 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:60020-1] wal.HLogSplitter: DistributedLogReplay = false
2014-07-01 15:54:33,404 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:60020-1] util.FSHDFSUtils: Recovering lease on dfs file hdfs://master:54310/hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1404253870530-splitting/sceplus-vm48.almaden.ibm.com%2C60020%2C1404253870530.1404254443569
2014-07-01 15:54:33,405 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:60020-1] util.FSHDFSUtils: recoverLease=true, attempt=0 on file=hdfs://master:54310/hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1404253870530-splitting/sceplus-vm48.almaden.ibm.com%2C60020%2C1404253870530.1404254443569 after 1ms
2014-07-01 15:54:33,422 DEBUG [RS_LOG_REPLAY_OPS-sceplus-vm48:60020-1-Writer-0] wal.HLogSplitter: Writer thread Thread[RS_LOG_REPLAY_OPS-sceplus-vm48:60020-1-Writer-0,5,main]: starting
2014-07-01 15:54:33,422 DEBUG [RS_LOG_REPLAY_OPS-sceplus-vm48:60020-1-Writer-1] wal.HLogSplitter: Writer thread Thread[RS_LOG_REPLAY_OPS-sceplus-vm48:60020-1-Writer-1,5,main]: starting
2014-07-01 15:54:33,422 DEBUG [RS_LOG_REPLAY_OPS-sceplus-vm48:60020-1-Writer-2] wal.HLogSplitter: Writer thread Thread[RS_LOG_REPLAY_OPS-sceplus-vm48:60020-1-Writer-2,5,main]: starting
2014-07-01 15:54:33,452 DEBUG [RS_LOG_REPLAY_OPS-sceplus-vm48:60020-1-Writer-2] wal.HLogSplitter: Creating writer path=hdfs://master:54310/hbase/data/default/usertable/52290d336af5a689eef6040bafbe6b15/recovered.edits/0000000000000001639.temp region=52290d336af5a689eef6040bafbe6b15
2014-07-01 15:54:33,464 DEBUG [RS_LOG_REPLAY_OPS-sceplus-vm48:60020-1-Writer-0] wal.HLogSplitter: Creating writer path=hdfs://master:54310/hbase/data/default/usertable/99b73df170b514f6773e742a495216c4/recovered.edits/0000000000000001629.temp region=99b73df170b514f6773e742a495216c4
2014-07-01 15:54:33,496 DEBUG [RS_LOG_REPLAY_OPS-sceplus-vm48:60020-1-Writer-2] wal.HLogSplitter: Creating writer path=hdfs://master:54310/hbase/data/default/usertable/c72fc9d54f680bcd54e9fa888600346b/recovered.edits/0000000000000001654.temp region=c72fc9d54f680bcd54e9fa888600346b
2014-07-01 15:54:33,538 DEBUG [RS_LOG_REPLAY_OPS-sceplus-vm48:60020-1-Writer-1] wal.HLogSplitter: Creating writer path=hdfs://master:54310/hbase/data/default/usertable/1dc0aa61f0e3f43a53375118eb48213f/recovered.edits/0000000000000001646.temp region=1dc0aa61f0e3f43a53375118eb48213f
2014-07-01 15:54:33,542 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:60020-0] wal.HLogSplitter: Finishing writing output logs and closing down.
2014-07-01 15:54:33,542 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:60020-0] wal.HLogSplitter: Waiting for split writer threads to finish
2014-07-01 15:54:33,556 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:60020-0] wal.HLogSplitter: Split writers finished
2014-07-01 15:54:33,556 DEBUG [RS_LOG_REPLAY_OPS-sceplus-vm48:60020-0] wal.HLogSplitter: Submitting close of hdfs://master:54310/hbase/data/default/usertable/1dc0aa61f0e3f43a53375118eb48213f/recovered.edits/0000000000000001687.temp
2014-07-01 15:54:33,556 DEBUG [RS_LOG_REPLAY_OPS-sceplus-vm48:60020-0] wal.HLogSplitter: Submitting close of hdfs://master:54310/hbase/data/default/usertable/52290d336af5a689eef6040bafbe6b15/recovered.edits/0000000000000001677.temp
2014-07-01 15:54:33,557 DEBUG [split-log-closeStream-1] wal.HLogSplitter: Closing hdfs://master:54310/hbase/data/default/usertable/1dc0aa61f0e3f43a53375118eb48213f/recovered.edits/0000000000000001687.temp
2014-07-01 15:54:33,557 DEBUG [RS_LOG_REPLAY_OPS-sceplus-vm48:60020-0] wal.HLogSplitter: Submitting close of hdfs://master:54310/hbase/data/default/usertable/99b73df170b514f6773e742a495216c4/recovered.edits/0000000000000001677.temp
2014-07-01 15:54:33,557 DEBUG [split-log-closeStream-2] wal.HLogSplitter: Closing hdfs://master:54310/hbase/data/default/usertable/52290d336af5a689eef6040bafbe6b15/recovered.edits/0000000000000001677.temp
2014-07-01 15:54:33,557 DEBUG [RS_LOG_REPLAY_OPS-sceplus-vm48:60020-0] wal.HLogSplitter: Submitting close of hdfs://master:54310/hbase/data/default/usertable/c72fc9d54f680bcd54e9fa888600346b/recovered.edits/0000000000000001697.temp
2014-07-01 15:54:33,557 DEBUG [split-log-closeStream-3] wal.HLogSplitter: Closing hdfs://master:54310/hbase/data/default/usertable/99b73df170b514f6773e742a495216c4/recovered.edits/0000000000000001677.temp
2014-07-01 15:54:33,566 INFO  [split-log-closeStream-3] wal.HLogSplitter: Closed wap hdfs://master:54310/hbase/data/default/usertable/99b73df170b514f6773e742a495216c4/recovered.edits/0000000000000001677.temp (wrote 15 edits in 141ms)
2014-07-01 15:54:33,566 INFO  [split-log-closeStream-2] wal.HLogSplitter: Closed wap hdfs://master:54310/hbase/data/default/usertable/52290d336af5a689eef6040bafbe6b15/recovered.edits/0000000000000001677.temp (wrote 24 edits in 253ms)
2014-07-01 15:54:33,567 INFO  [split-log-closeStream-1] wal.HLogSplitter: Closed wap hdfs://master:54310/hbase/data/default/usertable/1dc0aa61f0e3f43a53375118eb48213f/recovered.edits/0000000000000001687.temp (wrote 19 edits in 171ms)
2014-07-01 15:54:33,570 DEBUG [split-log-closeStream-3] wal.HLogSplitter: Rename hdfs://master:54310/hbase/data/default/usertable/99b73df170b514f6773e742a495216c4/recovered.edits/0000000000000001677.temp to hdfs://master:54310/hbase/data/default/usertable/99b73df170b514f6773e742a495216c4/recovered.edits/0000000000000001691
2014-07-01 15:54:33,570 DEBUG [split-log-closeStream-3] wal.HLogSplitter: Closing hdfs://master:54310/hbase/data/default/usertable/c72fc9d54f680bcd54e9fa888600346b/recovered.edits/0000000000000001697.temp
2014-07-01 15:54:33,570 DEBUG [split-log-closeStream-2] wal.HLogSplitter: Rename hdfs://master:54310/hbase/data/default/usertable/52290d336af5a689eef6040bafbe6b15/recovered.edits/0000000000000001677.temp to hdfs://master:54310/hbase/data/default/usertable/52290d336af5a689eef6040bafbe6b15/recovered.edits/0000000000000001700
2014-07-01 15:54:33,575 DEBUG [split-log-closeStream-1] wal.HLogSplitter: Rename hdfs://master:54310/hbase/data/default/usertable/1dc0aa61f0e3f43a53375118eb48213f/recovered.edits/0000000000000001687.temp to hdfs://master:54310/hbase/data/default/usertable/1dc0aa61f0e3f43a53375118eb48213f/recovered.edits/0000000000000001705
2014-07-01 15:54:33,614 INFO  [split-log-closeStream-3] wal.HLogSplitter: Closed wap hdfs://master:54310/hbase/data/default/usertable/c72fc9d54f680bcd54e9fa888600346b/recovered.edits/0000000000000001697.temp (wrote 14 edits in 147ms)
2014-07-01 15:54:33,849 DEBUG [split-log-closeStream-3] wal.HLogSplitter: Rename hdfs://master:54310/hbase/data/default/usertable/c72fc9d54f680bcd54e9fa888600346b/recovered.edits/0000000000000001697.temp to hdfs://master:54310/hbase/data/default/usertable/c72fc9d54f680bcd54e9fa888600346b/recovered.edits/0000000000000001710
2014-07-01 15:54:33,850 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:60020-0] wal.HLogSplitter: Processed 72 edits across 4 regions; log file=hdfs://master:54310/hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1404253870530-splitting/sceplus-vm48.almaden.ibm.com%2C60020%2C1404253870530.1404254449756 is corrupted = false progress failed = false
2014-07-01 15:54:33,856 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:60020-0] handler.HLogSplitterHandler: successfully transitioned task /hbase/splitWAL/WALs%2Fsceplus-vm48.almaden.ibm.com%2C60020%2C1404253870530-splitting%2Fsceplus-vm48.almaden.ibm.com%252C60020%252C1404253870530.1404254449756 to final state DONE sceplus-vm48.almaden.ibm.com,60020,1404255227840
2014-07-01 15:54:33,857 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:60020-0] handler.HLogSplitterHandler: worker sceplus-vm48.almaden.ibm.com,60020,1404255227840 done with task /hbase/splitWAL/WALs%2Fsceplus-vm48.almaden.ibm.com%2C60020%2C1404253870530-splitting%2Fsceplus-vm48.almaden.ibm.com%252C60020%252C1404253870530.1404254449756 in 1046ms
2014-07-01 15:54:33,896 INFO  [SplitLogWorker-sceplus-vm48.almaden.ibm.com,60020,1404255227840] regionserver.SplitLogWorker: worker sceplus-vm48.almaden.ibm.com,60020,1404255227840 acquired task /hbase/splitWAL/WALs%2Fsceplus-vm48.almaden.ibm.com%2C60020%2C1404253870530-splitting%2Fsceplus-vm48.almaden.ibm.com%252C60020%252C1404253870530.1404254421797
2014-07-01 15:54:33,914 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:60020-0] wal.HLogSplitter: Splitting hlog: hdfs://master:54310/hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1404253870530-splitting/sceplus-vm48.almaden.ibm.com%2C60020%2C1404253870530.1404254421797, length=68202963
2014-07-01 15:54:33,914 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:60020-0] wal.HLogSplitter: DistributedLogReplay = false
2014-07-01 15:54:33,918 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:60020-0] util.FSHDFSUtils: Recovering lease on dfs file hdfs://master:54310/hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1404253870530-splitting/sceplus-vm48.almaden.ibm.com%2C60020%2C1404253870530.1404254421797
2014-07-01 15:54:33,919 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:60020-0] util.FSHDFSUtils: recoverLease=true, attempt=0 on file=hdfs://master:54310/hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1404253870530-splitting/sceplus-vm48.almaden.ibm.com%2C60020%2C1404253870530.1404254421797 after 1ms
2014-07-01 15:54:33,938 DEBUG [RS_LOG_REPLAY_OPS-sceplus-vm48:60020-0-Writer-0] wal.HLogSplitter: Writer thread Thread[RS_LOG_REPLAY_OPS-sceplus-vm48:60020-0-Writer-0,5,main]: starting
2014-07-01 15:54:33,938 DEBUG [RS_LOG_REPLAY_OPS-sceplus-vm48:60020-0-Writer-1] wal.HLogSplitter: Writer thread Thread[RS_LOG_REPLAY_OPS-sceplus-vm48:60020-0-Writer-1,5,main]: starting
2014-07-01 15:54:33,938 DEBUG [RS_LOG_REPLAY_OPS-sceplus-vm48:60020-0-Writer-2] wal.HLogSplitter: Writer thread Thread[RS_LOG_REPLAY_OPS-sceplus-vm48:60020-0-Writer-2,5,main]: starting
2014-07-01 15:54:33,967 DEBUG [RS_LOG_REPLAY_OPS-sceplus-vm48:60020-0-Writer-2] wal.HLogSplitter: Creating writer path=hdfs://master:54310/hbase/data/default/usertable/1dc0aa61f0e3f43a53375118eb48213f/recovered.edits/0000000000000001567.temp region=1dc0aa61f0e3f43a53375118eb48213f
2014-07-01 15:54:33,996 DEBUG [RS_LOG_REPLAY_OPS-sceplus-vm48:60020-0-Writer-0] wal.HLogSplitter: Creating writer path=hdfs://master:54310/hbase/data/default/usertable/52290d336af5a689eef6040bafbe6b15/recovered.edits/0000000000000001561.temp region=52290d336af5a689eef6040bafbe6b15
2014-07-01 15:54:34,008 DEBUG [RS_LOG_REPLAY_OPS-sceplus-vm48:60020-0-Writer-1] wal.HLogSplitter: Creating writer path=hdfs://master:54310/hbase/data/default/usertable/99b73df170b514f6773e742a495216c4/recovered.edits/0000000000000001557.temp region=99b73df170b514f6773e742a495216c4
2014-07-01 15:54:34,040 DEBUG [RS_LOG_REPLAY_OPS-sceplus-vm48:60020-0-Writer-0] wal.HLogSplitter: Creating writer path=hdfs://master:54310/hbase/data/default/usertable/c72fc9d54f680bcd54e9fa888600346b/recovered.edits/0000000000000001575.temp region=c72fc9d54f680bcd54e9fa888600346b
2014-07-01 15:54:34,281 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:60020-1] wal.HLogSplitter: Finishing writing output logs and closing down.
2014-07-01 15:54:34,281 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:60020-1] wal.HLogSplitter: Waiting for split writer threads to finish
2014-07-01 15:54:34,312 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:60020-1] wal.HLogSplitter: Split writers finished
2014-07-01 15:54:34,312 DEBUG [RS_LOG_REPLAY_OPS-sceplus-vm48:60020-1] wal.HLogSplitter: Submitting close of hdfs://master:54310/hbase/data/default/usertable/1dc0aa61f0e3f43a53375118eb48213f/recovered.edits/0000000000000001646.temp
2014-07-01 15:54:34,313 DEBUG [RS_LOG_REPLAY_OPS-sceplus-vm48:60020-1] wal.HLogSplitter: Submitting close of hdfs://master:54310/hbase/data/default/usertable/52290d336af5a689eef6040bafbe6b15/recovered.edits/0000000000000001639.temp
2014-07-01 15:54:34,313 DEBUG [split-log-closeStream-1] wal.HLogSplitter: Closing hdfs://master:54310/hbase/data/default/usertable/1dc0aa61f0e3f43a53375118eb48213f/recovered.edits/0000000000000001646.temp
2014-07-01 15:54:34,313 DEBUG [RS_LOG_REPLAY_OPS-sceplus-vm48:60020-1] wal.HLogSplitter: Submitting close of hdfs://master:54310/hbase/data/default/usertable/99b73df170b514f6773e742a495216c4/recovered.edits/0000000000000001629.temp
2014-07-01 15:54:34,313 DEBUG [split-log-closeStream-2] wal.HLogSplitter: Closing hdfs://master:54310/hbase/data/default/usertable/52290d336af5a689eef6040bafbe6b15/recovered.edits/0000000000000001639.temp
2014-07-01 15:54:34,313 DEBUG [RS_LOG_REPLAY_OPS-sceplus-vm48:60020-1] wal.HLogSplitter: Submitting close of hdfs://master:54310/hbase/data/default/usertable/c72fc9d54f680bcd54e9fa888600346b/recovered.edits/0000000000000001654.temp
2014-07-01 15:54:34,314 DEBUG [split-log-closeStream-3] wal.HLogSplitter: Closing hdfs://master:54310/hbase/data/default/usertable/99b73df170b514f6773e742a495216c4/recovered.edits/0000000000000001629.temp
2014-07-01 15:54:34,323 INFO  [split-log-closeStream-1] wal.HLogSplitter: Closed wap hdfs://master:54310/hbase/data/default/usertable/1dc0aa61f0e3f43a53375118eb48213f/recovered.edits/0000000000000001646.temp (wrote 22 edits in 257ms)
2014-07-01 15:54:34,324 INFO  [split-log-closeStream-3] wal.HLogSplitter: Closed wap hdfs://master:54310/hbase/data/default/usertable/99b73df170b514f6773e742a495216c4/recovered.edits/0000000000000001629.temp (wrote 19 edits in 260ms)
2014-07-01 15:54:34,325 INFO  [split-log-closeStream-2] wal.HLogSplitter: Closed wap hdfs://master:54310/hbase/data/default/usertable/52290d336af5a689eef6040bafbe6b15/recovered.edits/0000000000000001639.temp (wrote 20 edits in 286ms)
2014-07-01 15:54:34,328 DEBUG [split-log-closeStream-1] wal.HLogSplitter: Rename hdfs://master:54310/hbase/data/default/usertable/1dc0aa61f0e3f43a53375118eb48213f/recovered.edits/0000000000000001646.temp to hdfs://master:54310/hbase/data/default/usertable/1dc0aa61f0e3f43a53375118eb48213f/recovered.edits/0000000000000001667
2014-07-01 15:54:34,328 DEBUG [split-log-closeStream-1] wal.HLogSplitter: Closing hdfs://master:54310/hbase/data/default/usertable/c72fc9d54f680bcd54e9fa888600346b/recovered.edits/0000000000000001654.temp
2014-07-01 15:54:34,329 DEBUG [split-log-closeStream-3] wal.HLogSplitter: Rename hdfs://master:54310/hbase/data/default/usertable/99b73df170b514f6773e742a495216c4/recovered.edits/0000000000000001629.temp to hdfs://master:54310/hbase/data/default/usertable/99b73df170b514f6773e742a495216c4/recovered.edits/0000000000000001647
2014-07-01 15:54:34,333 DEBUG [split-log-closeStream-2] wal.HLogSplitter: Rename hdfs://master:54310/hbase/data/default/usertable/52290d336af5a689eef6040bafbe6b15/recovered.edits/0000000000000001639.temp to hdfs://master:54310/hbase/data/default/usertable/52290d336af5a689eef6040bafbe6b15/recovered.edits/0000000000000001658
2014-07-01 15:54:34,371 INFO  [split-log-closeStream-1] wal.HLogSplitter: Closed wap hdfs://master:54310/hbase/data/default/usertable/c72fc9d54f680bcd54e9fa888600346b/recovered.edits/0000000000000001654.temp (wrote 17 edits in 291ms)
2014-07-01 15:54:34,375 DEBUG [split-log-closeStream-1] wal.HLogSplitter: Rename hdfs://master:54310/hbase/data/default/usertable/c72fc9d54f680bcd54e9fa888600346b/recovered.edits/0000000000000001654.temp to hdfs://master:54310/hbase/data/default/usertable/c72fc9d54f680bcd54e9fa888600346b/recovered.edits/0000000000000001670
2014-07-01 15:54:34,375 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:60020-1] wal.HLogSplitter: Processed 78 edits across 4 regions; log file=hdfs://master:54310/hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1404253870530-splitting/sceplus-vm48.almaden.ibm.com%2C60020%2C1404253870530.1404254443569 is corrupted = false progress failed = false
2014-07-01 15:54:34,382 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:60020-1] handler.HLogSplitterHandler: successfully transitioned task /hbase/splitWAL/WALs%2Fsceplus-vm48.almaden.ibm.com%2C60020%2C1404253870530-splitting%2Fsceplus-vm48.almaden.ibm.com%252C60020%252C1404253870530.1404254443569 to final state DONE sceplus-vm48.almaden.ibm.com,60020,1404255227840
2014-07-01 15:54:34,382 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:60020-1] handler.HLogSplitterHandler: worker sceplus-vm48.almaden.ibm.com,60020,1404255227840 done with task /hbase/splitWAL/WALs%2Fsceplus-vm48.almaden.ibm.com%2C60020%2C1404253870530-splitting%2Fsceplus-vm48.almaden.ibm.com%252C60020%252C1404253870530.1404254443569 in 1001ms
2014-07-01 15:54:34,797 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:60020-0] wal.HLogSplitter: Finishing writing output logs and closing down.
2014-07-01 15:54:34,797 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:60020-0] wal.HLogSplitter: Waiting for split writer threads to finish
2014-07-01 15:54:34,803 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:60020-0] wal.HLogSplitter: Split writers finished
2014-07-01 15:54:34,803 DEBUG [RS_LOG_REPLAY_OPS-sceplus-vm48:60020-0] wal.HLogSplitter: Submitting close of hdfs://master:54310/hbase/data/default/usertable/1dc0aa61f0e3f43a53375118eb48213f/recovered.edits/0000000000000001567.temp
2014-07-01 15:54:34,803 DEBUG [RS_LOG_REPLAY_OPS-sceplus-vm48:60020-0] wal.HLogSplitter: Submitting close of hdfs://master:54310/hbase/data/default/usertable/52290d336af5a689eef6040bafbe6b15/recovered.edits/0000000000000001561.temp
2014-07-01 15:54:34,803 DEBUG [split-log-closeStream-1] wal.HLogSplitter: Closing hdfs://master:54310/hbase/data/default/usertable/1dc0aa61f0e3f43a53375118eb48213f/recovered.edits/0000000000000001567.temp
2014-07-01 15:54:34,803 DEBUG [RS_LOG_REPLAY_OPS-sceplus-vm48:60020-0] wal.HLogSplitter: Submitting close of hdfs://master:54310/hbase/data/default/usertable/99b73df170b514f6773e742a495216c4/recovered.edits/0000000000000001557.temp
2014-07-01 15:54:34,803 DEBUG [split-log-closeStream-2] wal.HLogSplitter: Closing hdfs://master:54310/hbase/data/default/usertable/52290d336af5a689eef6040bafbe6b15/recovered.edits/0000000000000001561.temp
2014-07-01 15:54:34,804 DEBUG [RS_LOG_REPLAY_OPS-sceplus-vm48:60020-0] wal.HLogSplitter: Submitting close of hdfs://master:54310/hbase/data/default/usertable/c72fc9d54f680bcd54e9fa888600346b/recovered.edits/0000000000000001575.temp
2014-07-01 15:54:34,804 DEBUG [split-log-closeStream-3] wal.HLogSplitter: Closing hdfs://master:54310/hbase/data/default/usertable/99b73df170b514f6773e742a495216c4/recovered.edits/0000000000000001557.temp
2014-07-01 15:54:34,810 INFO  [split-log-closeStream-1] wal.HLogSplitter: Closed wap hdfs://master:54310/hbase/data/default/usertable/1dc0aa61f0e3f43a53375118eb48213f/recovered.edits/0000000000000001567.temp (wrote 18 edits in 201ms)
2014-07-01 15:54:34,812 INFO  [split-log-closeStream-3] wal.HLogSplitter: Closed wap hdfs://master:54310/hbase/data/default/usertable/99b73df170b514f6773e742a495216c4/recovered.edits/0000000000000001557.temp (wrote 23 edits in 253ms)
2014-07-01 15:54:34,812 INFO  [split-log-closeStream-2] wal.HLogSplitter: Closed wap hdfs://master:54310/hbase/data/default/usertable/52290d336af5a689eef6040bafbe6b15/recovered.edits/0000000000000001561.temp (wrote 20 edits in 205ms)
2014-07-01 15:54:34,815 DEBUG [split-log-closeStream-1] wal.HLogSplitter: Rename hdfs://master:54310/hbase/data/default/usertable/1dc0aa61f0e3f43a53375118eb48213f/recovered.edits/0000000000000001567.temp to hdfs://master:54310/hbase/data/default/usertable/1dc0aa61f0e3f43a53375118eb48213f/recovered.edits/0000000000000001584
2014-07-01 15:54:34,815 DEBUG [split-log-closeStream-1] wal.HLogSplitter: Closing hdfs://master:54310/hbase/data/default/usertable/c72fc9d54f680bcd54e9fa888600346b/recovered.edits/0000000000000001575.temp
2014-07-01 15:54:34,815 DEBUG [split-log-closeStream-3] wal.HLogSplitter: Rename hdfs://master:54310/hbase/data/default/usertable/99b73df170b514f6773e742a495216c4/recovered.edits/0000000000000001557.temp to hdfs://master:54310/hbase/data/default/usertable/99b73df170b514f6773e742a495216c4/recovered.edits/0000000000000001579
2014-07-01 15:54:34,820 DEBUG [split-log-closeStream-2] wal.HLogSplitter: Rename hdfs://master:54310/hbase/data/default/usertable/52290d336af5a689eef6040bafbe6b15/recovered.edits/0000000000000001561.temp to hdfs://master:54310/hbase/data/default/usertable/52290d336af5a689eef6040bafbe6b15/recovered.edits/0000000000000001580
2014-07-01 15:54:34,822 INFO  [SplitLogWorker-sceplus-vm48.almaden.ibm.com,60020,1404255227840] regionserver.SplitLogWorker: worker sceplus-vm48.almaden.ibm.com,60020,1404255227840 acquired task /hbase/splitWAL/WALs%2Fslave1%2C60020%2C1404253868846-splitting%2Fslave1%252C60020%252C1404253868846.1404254541293
2014-07-01 15:54:34,824 INFO  [split-log-closeStream-1] wal.HLogSplitter: Closed wap hdfs://master:54310/hbase/data/default/usertable/c72fc9d54f680bcd54e9fa888600346b/recovered.edits/0000000000000001575.temp (wrote 13 edits in 138ms)
2014-07-01 15:54:34,840 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:60020-1] wal.HLogSplitter: Splitting hlog: hdfs://master:54310/hbase/WALs/slave1,60020,1404253868846-splitting/slave1%2C60020%2C1404253868846.1404254541293, length=68129094
2014-07-01 15:54:34,840 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:60020-1] wal.HLogSplitter: DistributedLogReplay = false
2014-07-01 15:54:34,850 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:60020-1] util.FSHDFSUtils: Recovering lease on dfs file hdfs://master:54310/hbase/WALs/slave1,60020,1404253868846-splitting/slave1%2C60020%2C1404253868846.1404254541293
2014-07-01 15:54:34,851 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:60020-1] util.FSHDFSUtils: recoverLease=true, attempt=0 on file=hdfs://master:54310/hbase/WALs/slave1,60020,1404253868846-splitting/slave1%2C60020%2C1404253868846.1404254541293 after 0ms
2014-07-01 15:54:34,858 DEBUG [split-log-closeStream-1] wal.HLogSplitter: Rename hdfs://master:54310/hbase/data/default/usertable/c72fc9d54f680bcd54e9fa888600346b/recovered.edits/0000000000000001575.temp to hdfs://master:54310/hbase/data/default/usertable/c72fc9d54f680bcd54e9fa888600346b/recovered.edits/0000000000000001587
2014-07-01 15:54:34,858 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:60020-0] wal.HLogSplitter: Processed 74 edits across 4 regions; log file=hdfs://master:54310/hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1404253870530-splitting/sceplus-vm48.almaden.ibm.com%2C60020%2C1404253870530.1404254421797 is corrupted = false progress failed = false
2014-07-01 15:54:34,863 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:60020-0] handler.HLogSplitterHandler: successfully transitioned task /hbase/splitWAL/WALs%2Fsceplus-vm48.almaden.ibm.com%2C60020%2C1404253870530-splitting%2Fsceplus-vm48.almaden.ibm.com%252C60020%252C1404253870530.1404254421797 to final state DONE sceplus-vm48.almaden.ibm.com,60020,1404255227840
2014-07-01 15:54:34,863 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:60020-0] handler.HLogSplitterHandler: worker sceplus-vm48.almaden.ibm.com,60020,1404255227840 done with task /hbase/splitWAL/WALs%2Fsceplus-vm48.almaden.ibm.com%2C60020%2C1404253870530-splitting%2Fsceplus-vm48.almaden.ibm.com%252C60020%252C1404253870530.1404254421797 in 967ms
2014-07-01 15:54:34,914 DEBUG [RS_LOG_REPLAY_OPS-sceplus-vm48:60020-1-Writer-0] wal.HLogSplitter: Writer thread Thread[RS_LOG_REPLAY_OPS-sceplus-vm48:60020-1-Writer-0,5,main]: starting
2014-07-01 15:54:34,914 DEBUG [RS_LOG_REPLAY_OPS-sceplus-vm48:60020-1-Writer-1] wal.HLogSplitter: Writer thread Thread[RS_LOG_REPLAY_OPS-sceplus-vm48:60020-1-Writer-1,5,main]: starting
2014-07-01 15:54:34,914 DEBUG [RS_LOG_REPLAY_OPS-sceplus-vm48:60020-1-Writer-2] wal.HLogSplitter: Writer thread Thread[RS_LOG_REPLAY_OPS-sceplus-vm48:60020-1-Writer-2,5,main]: starting
2014-07-01 15:54:34,957 DEBUG [RS_LOG_REPLAY_OPS-sceplus-vm48:60020-1-Writer-2] wal.HLogSplitter: Creating writer path=hdfs://master:54310/hbase/data/default/usertable/9cef9e7d34f96f1d9b111be50040a208/recovered.edits/0000000000000005642.temp region=9cef9e7d34f96f1d9b111be50040a208
2014-07-01 15:54:34,963 DEBUG [RS_LOG_REPLAY_OPS-sceplus-vm48:60020-1-Writer-0] wal.HLogSplitter: Creating writer path=hdfs://master:54310/hbase/data/default/usertable/667c1f2c34fe37d48ba25a789d8aba98/recovered.edits/0000000000000004648.temp region=667c1f2c34fe37d48ba25a789d8aba98
2014-07-01 15:54:34,964 DEBUG [RS_LOG_REPLAY_OPS-sceplus-vm48:60020-1-Writer-1] wal.HLogSplitter: Creating writer path=hdfs://master:54310/hbase/data/default/usertable/bb79ae773f71289e0745068f68a0abd9/recovered.edits/0000000000000004637.temp region=bb79ae773f71289e0745068f68a0abd9
2014-07-01 15:54:34,984 DEBUG [RS_LOG_REPLAY_OPS-sceplus-vm48:60020-1-Writer-2] wal.HLogSplitter: Creating writer path=hdfs://master:54310/hbase/data/default/usertable/93253e37c284f3174309fede3f32339f/recovered.edits/0000000000000004691.temp region=93253e37c284f3174309fede3f32339f
2014-07-01 15:54:34,985 DEBUG [RS_LOG_REPLAY_OPS-sceplus-vm48:60020-1-Writer-1] wal.HLogSplitter: Creating writer path=hdfs://master:54310/hbase/data/default/usertable/122e92eabfab38cb15a9965b21db25c5/recovered.edits/0000000000000004685.temp region=122e92eabfab38cb15a9965b21db25c5
2014-07-01 15:54:35,404 INFO  [SplitLogWorker-sceplus-vm48.almaden.ibm.com,60020,1404255227840] regionserver.SplitLogWorker: worker sceplus-vm48.almaden.ibm.com,60020,1404255227840 acquired task /hbase/splitWAL/WALs%2Fsceplus-vm48.almaden.ibm.com%2C60020%2C1404253870530-splitting%2Fsceplus-vm48.almaden.ibm.com%252C60020%252C1404253870530.1404254416705
2014-07-01 15:54:35,424 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:60020-0] wal.HLogSplitter: Splitting hlog: hdfs://master:54310/hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1404253870530-splitting/sceplus-vm48.almaden.ibm.com%2C60020%2C1404253870530.1404254416705, length=76333432
2014-07-01 15:54:35,424 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:60020-0] wal.HLogSplitter: DistributedLogReplay = false
2014-07-01 15:54:35,430 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:60020-0] util.FSHDFSUtils: Recovering lease on dfs file hdfs://master:54310/hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1404253870530-splitting/sceplus-vm48.almaden.ibm.com%2C60020%2C1404253870530.1404254416705
2014-07-01 15:54:35,431 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:60020-0] util.FSHDFSUtils: recoverLease=true, attempt=0 on file=hdfs://master:54310/hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1404253870530-splitting/sceplus-vm48.almaden.ibm.com%2C60020%2C1404253870530.1404254416705 after 1ms
2014-07-01 15:54:35,477 DEBUG [RS_LOG_REPLAY_OPS-sceplus-vm48:60020-0-Writer-0] wal.HLogSplitter: Writer thread Thread[RS_LOG_REPLAY_OPS-sceplus-vm48:60020-0-Writer-0,5,main]: starting
2014-07-01 15:54:35,477 DEBUG [RS_LOG_REPLAY_OPS-sceplus-vm48:60020-0-Writer-1] wal.HLogSplitter: Writer thread Thread[RS_LOG_REPLAY_OPS-sceplus-vm48:60020-0-Writer-1,5,main]: starting
2014-07-01 15:54:35,479 DEBUG [RS_LOG_REPLAY_OPS-sceplus-vm48:60020-0-Writer-2] wal.HLogSplitter: Writer thread Thread[RS_LOG_REPLAY_OPS-sceplus-vm48:60020-0-Writer-2,5,main]: starting
2014-07-01 15:54:35,512 DEBUG [RS_LOG_REPLAY_OPS-sceplus-vm48:60020-0-Writer-2] wal.HLogSplitter: Creating writer path=hdfs://master:54310/hbase/data/default/usertable/52290d336af5a689eef6040bafbe6b15/recovered.edits/0000000000000001526.temp region=52290d336af5a689eef6040bafbe6b15
2014-07-01 15:54:35,522 DEBUG [RS_LOG_REPLAY_OPS-sceplus-vm48:60020-0-Writer-0] wal.HLogSplitter: Creating writer path=hdfs://master:54310/hbase/data/default/usertable/99b73df170b514f6773e742a495216c4/recovered.edits/0000000000000001513.temp region=99b73df170b514f6773e742a495216c4
2014-07-01 15:54:35,537 DEBUG [RS_LOG_REPLAY_OPS-sceplus-vm48:60020-0-Writer-1] wal.HLogSplitter: Creating writer path=hdfs://master:54310/hbase/data/default/usertable/c72fc9d54f680bcd54e9fa888600346b/recovered.edits/0000000000000001537.temp region=c72fc9d54f680bcd54e9fa888600346b
2014-07-01 15:54:35,568 DEBUG [RS_LOG_REPLAY_OPS-sceplus-vm48:60020-0-Writer-0] wal.HLogSplitter: Creating writer path=hdfs://master:54310/hbase/data/default/usertable/1dc0aa61f0e3f43a53375118eb48213f/recovered.edits/0000000000000001532.temp region=1dc0aa61f0e3f43a53375118eb48213f
2014-07-01 15:54:35,738 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:60020-1] wal.HLogSplitter: Finishing writing output logs and closing down.
2014-07-01 15:54:35,738 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:60020-1] wal.HLogSplitter: Waiting for split writer threads to finish
2014-07-01 15:54:35,746 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:60020-1] wal.HLogSplitter: Split writers finished
2014-07-01 15:54:35,746 DEBUG [RS_LOG_REPLAY_OPS-sceplus-vm48:60020-1] wal.HLogSplitter: Submitting close of hdfs://master:54310/hbase/data/default/usertable/122e92eabfab38cb15a9965b21db25c5/recovered.edits/0000000000000004685.temp
2014-07-01 15:54:35,746 DEBUG [RS_LOG_REPLAY_OPS-sceplus-vm48:60020-1] wal.HLogSplitter: Submitting close of hdfs://master:54310/hbase/data/default/usertable/667c1f2c34fe37d48ba25a789d8aba98/recovered.edits/0000000000000004648.temp
2014-07-01 15:54:35,746 DEBUG [RS_LOG_REPLAY_OPS-sceplus-vm48:60020-1] wal.HLogSplitter: Submitting close of hdfs://master:54310/hbase/data/default/usertable/93253e37c284f3174309fede3f32339f/recovered.edits/0000000000000004691.temp
2014-07-01 15:54:35,746 DEBUG [split-log-closeStream-1] wal.HLogSplitter: Closing hdfs://master:54310/hbase/data/default/usertable/122e92eabfab38cb15a9965b21db25c5/recovered.edits/0000000000000004685.temp
2014-07-01 15:54:35,747 DEBUG [RS_LOG_REPLAY_OPS-sceplus-vm48:60020-1] wal.HLogSplitter: Submitting close of hdfs://master:54310/hbase/data/default/usertable/9cef9e7d34f96f1d9b111be50040a208/recovered.edits/0000000000000005642.temp
2014-07-01 15:54:35,747 DEBUG [split-log-closeStream-2] wal.HLogSplitter: Closing hdfs://master:54310/hbase/data/default/usertable/667c1f2c34fe37d48ba25a789d8aba98/recovered.edits/0000000000000004648.temp
2014-07-01 15:54:35,747 DEBUG [split-log-closeStream-3] wal.HLogSplitter: Closing hdfs://master:54310/hbase/data/default/usertable/93253e37c284f3174309fede3f32339f/recovered.edits/0000000000000004691.temp
2014-07-01 15:54:35,747 DEBUG [RS_LOG_REPLAY_OPS-sceplus-vm48:60020-1] wal.HLogSplitter: Submitting close of hdfs://master:54310/hbase/data/default/usertable/bb79ae773f71289e0745068f68a0abd9/recovered.edits/0000000000000004637.temp
2014-07-01 15:54:35,753 INFO  [split-log-closeStream-2] wal.HLogSplitter: Closed wap hdfs://master:54310/hbase/data/default/usertable/667c1f2c34fe37d48ba25a789d8aba98/recovered.edits/0000000000000004648.temp (wrote 18 edits in 179ms)
2014-07-01 15:54:35,754 INFO  [split-log-closeStream-1] wal.HLogSplitter: Closed wap hdfs://master:54310/hbase/data/default/usertable/122e92eabfab38cb15a9965b21db25c5/recovered.edits/0000000000000004685.temp (wrote 16 edits in 190ms)
2014-07-01 15:54:35,754 INFO  [split-log-closeStream-3] wal.HLogSplitter: Closed wap hdfs://master:54310/hbase/data/default/usertable/93253e37c284f3174309fede3f32339f/recovered.edits/0000000000000004691.temp (wrote 16 edits in 145ms)
2014-07-01 15:54:35,757 DEBUG [split-log-closeStream-2] wal.HLogSplitter: Rename hdfs://master:54310/hbase/data/default/usertable/667c1f2c34fe37d48ba25a789d8aba98/recovered.edits/0000000000000004648.temp to hdfs://master:54310/hbase/data/default/usertable/667c1f2c34fe37d48ba25a789d8aba98/recovered.edits/0000000000000004665
2014-07-01 15:54:35,757 DEBUG [split-log-closeStream-2] wal.HLogSplitter: Closing hdfs://master:54310/hbase/data/default/usertable/9cef9e7d34f96f1d9b111be50040a208/recovered.edits/0000000000000005642.temp
2014-07-01 15:54:35,758 DEBUG [split-log-closeStream-1] wal.HLogSplitter: Rename hdfs://master:54310/hbase/data/default/usertable/122e92eabfab38cb15a9965b21db25c5/recovered.edits/0000000000000004685.temp to hdfs://master:54310/hbase/data/default/usertable/122e92eabfab38cb15a9965b21db25c5/recovered.edits/0000000000000004700
2014-07-01 15:54:35,758 DEBUG [split-log-closeStream-1] wal.HLogSplitter: Closing hdfs://master:54310/hbase/data/default/usertable/bb79ae773f71289e0745068f68a0abd9/recovered.edits/0000000000000004637.temp
2014-07-01 15:54:35,761 DEBUG [split-log-closeStream-3] wal.HLogSplitter: Rename hdfs://master:54310/hbase/data/default/usertable/93253e37c284f3174309fede3f32339f/recovered.edits/0000000000000004691.temp to hdfs://master:54310/hbase/data/default/usertable/93253e37c284f3174309fede3f32339f/recovered.edits/0000000000000004706
2014-07-01 15:54:35,762 INFO  [split-log-closeStream-2] wal.HLogSplitter: Closed wap hdfs://master:54310/hbase/data/default/usertable/9cef9e7d34f96f1d9b111be50040a208/recovered.edits/0000000000000005642.temp (wrote 47 edits in 170ms)
2014-07-01 15:54:35,764 INFO  [split-log-closeStream-1] wal.HLogSplitter: Closed wap hdfs://master:54310/hbase/data/default/usertable/bb79ae773f71289e0745068f68a0abd9/recovered.edits/0000000000000004637.temp (wrote 16 edits in 77ms)
2014-07-01 15:54:35,766 DEBUG [split-log-closeStream-2] wal.HLogSplitter: Rename hdfs://master:54310/hbase/data/default/usertable/9cef9e7d34f96f1d9b111be50040a208/recovered.edits/0000000000000005642.temp to hdfs://master:54310/hbase/data/default/usertable/9cef9e7d34f96f1d9b111be50040a208/recovered.edits/0000000000000005688
2014-07-01 15:54:35,802 DEBUG [split-log-closeStream-1] wal.HLogSplitter: Rename hdfs://master:54310/hbase/data/default/usertable/bb79ae773f71289e0745068f68a0abd9/recovered.edits/0000000000000004637.temp to hdfs://master:54310/hbase/data/default/usertable/bb79ae773f71289e0745068f68a0abd9/recovered.edits/0000000000000004652
2014-07-01 15:54:35,803 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:60020-1] wal.HLogSplitter: Processed 113 edits across 5 regions; log file=hdfs://master:54310/hbase/WALs/slave1,60020,1404253868846-splitting/slave1%2C60020%2C1404253868846.1404254541293 is corrupted = false progress failed = false
2014-07-01 15:54:35,807 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:60020-1] handler.HLogSplitterHandler: successfully transitioned task /hbase/splitWAL/WALs%2Fslave1%2C60020%2C1404253868846-splitting%2Fslave1%252C60020%252C1404253868846.1404254541293 to final state DONE sceplus-vm48.almaden.ibm.com,60020,1404255227840
2014-07-01 15:54:35,807 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:60020-1] handler.HLogSplitterHandler: worker sceplus-vm48.almaden.ibm.com,60020,1404255227840 done with task /hbase/splitWAL/WALs%2Fslave1%2C60020%2C1404253868846-splitting%2Fslave1%252C60020%252C1404253868846.1404254541293 in 984ms
2014-07-01 15:54:36,394 INFO  [SplitLogWorker-sceplus-vm48.almaden.ibm.com,60020,1404255227840] regionserver.SplitLogWorker: worker sceplus-vm48.almaden.ibm.com,60020,1404255227840 acquired task /hbase/splitWAL/WALs%2Fsceplus-vm48.almaden.ibm.com%2C60020%2C1404253870530-splitting%2Fsceplus-vm48.almaden.ibm.com%252C60020%252C1404253870530.1404254452902
2014-07-01 15:54:36,415 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:60020-1] wal.HLogSplitter: Splitting hlog: hdfs://master:54310/hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1404253870530-splitting/sceplus-vm48.almaden.ibm.com%2C60020%2C1404253870530.1404254452902, length=74522602
2014-07-01 15:54:36,415 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:60020-1] wal.HLogSplitter: DistributedLogReplay = false
2014-07-01 15:54:36,418 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:60020-1] util.FSHDFSUtils: Recovering lease on dfs file hdfs://master:54310/hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1404253870530-splitting/sceplus-vm48.almaden.ibm.com%2C60020%2C1404253870530.1404254452902
2014-07-01 15:54:36,419 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:60020-1] util.FSHDFSUtils: recoverLease=true, attempt=0 on file=hdfs://master:54310/hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1404253870530-splitting/sceplus-vm48.almaden.ibm.com%2C60020%2C1404253870530.1404254452902 after 1ms
2014-07-01 15:54:36,419 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:60020-0] wal.HLogSplitter: Finishing writing output logs and closing down.
2014-07-01 15:54:36,419 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:60020-0] wal.HLogSplitter: Waiting for split writer threads to finish
2014-07-01 15:54:36,425 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:60020-0] wal.HLogSplitter: Split writers finished
2014-07-01 15:54:36,425 DEBUG [RS_LOG_REPLAY_OPS-sceplus-vm48:60020-0] wal.HLogSplitter: Submitting close of hdfs://master:54310/hbase/data/default/usertable/1dc0aa61f0e3f43a53375118eb48213f/recovered.edits/0000000000000001532.temp
2014-07-01 15:54:36,426 DEBUG [RS_LOG_REPLAY_OPS-sceplus-vm48:60020-0] wal.HLogSplitter: Submitting close of hdfs://master:54310/hbase/data/default/usertable/52290d336af5a689eef6040bafbe6b15/recovered.edits/0000000000000001526.temp
2014-07-01 15:54:36,426 DEBUG [split-log-closeStream-1] wal.HLogSplitter: Closing hdfs://master:54310/hbase/data/default/usertable/1dc0aa61f0e3f43a53375118eb48213f/recovered.edits/0000000000000001532.temp
2014-07-01 15:54:36,426 DEBUG [RS_LOG_REPLAY_OPS-sceplus-vm48:60020-0] wal.HLogSplitter: Submitting close of hdfs://master:54310/hbase/data/default/usertable/99b73df170b514f6773e742a495216c4/recovered.edits/0000000000000001513.temp
2014-07-01 15:54:36,426 DEBUG [split-log-closeStream-2] wal.HLogSplitter: Closing hdfs://master:54310/hbase/data/default/usertable/52290d336af5a689eef6040bafbe6b15/recovered.edits/0000000000000001526.temp
2014-07-01 15:54:36,426 DEBUG [RS_LOG_REPLAY_OPS-sceplus-vm48:60020-0] wal.HLogSplitter: Submitting close of hdfs://master:54310/hbase/data/default/usertable/c72fc9d54f680bcd54e9fa888600346b/recovered.edits/0000000000000001537.temp
2014-07-01 15:54:36,426 DEBUG [split-log-closeStream-3] wal.HLogSplitter: Closing hdfs://master:54310/hbase/data/default/usertable/99b73df170b514f6773e742a495216c4/recovered.edits/0000000000000001513.temp
2014-07-01 15:54:36,432 INFO  [split-log-closeStream-1] wal.HLogSplitter: Closed wap hdfs://master:54310/hbase/data/default/usertable/1dc0aa61f0e3f43a53375118eb48213f/recovered.edits/0000000000000001532.temp (wrote 16 edits in 158ms)
2014-07-01 15:54:36,433 INFO  [split-log-closeStream-2] wal.HLogSplitter: Closed wap hdfs://master:54310/hbase/data/default/usertable/52290d336af5a689eef6040bafbe6b15/recovered.edits/0000000000000001526.temp (wrote 11 edits in 97ms)
2014-07-01 15:54:36,434 INFO  [split-log-closeStream-3] wal.HLogSplitter: Closed wap hdfs://master:54310/hbase/data/default/usertable/99b73df170b514f6773e742a495216c4/recovered.edits/0000000000000001513.temp (wrote 24 edits in 218ms)
2014-07-01 15:54:36,437 DEBUG [split-log-closeStream-1] wal.HLogSplitter: Rename hdfs://master:54310/hbase/data/default/usertable/1dc0aa61f0e3f43a53375118eb48213f/recovered.edits/0000000000000001532.temp to hdfs://master:54310/hbase/data/default/usertable/1dc0aa61f0e3f43a53375118eb48213f/recovered.edits/0000000000000001547
2014-07-01 15:54:36,437 DEBUG [split-log-closeStream-1] wal.HLogSplitter: Closing hdfs://master:54310/hbase/data/default/usertable/c72fc9d54f680bcd54e9fa888600346b/recovered.edits/0000000000000001537.temp
2014-07-01 15:54:36,437 DEBUG [split-log-closeStream-2] wal.HLogSplitter: Rename hdfs://master:54310/hbase/data/default/usertable/52290d336af5a689eef6040bafbe6b15/recovered.edits/0000000000000001526.temp to hdfs://master:54310/hbase/data/default/usertable/52290d336af5a689eef6040bafbe6b15/recovered.edits/0000000000000001536
2014-07-01 15:54:36,439 DEBUG [split-log-closeStream-3] wal.HLogSplitter: Rename hdfs://master:54310/hbase/data/default/usertable/99b73df170b514f6773e742a495216c4/recovered.edits/0000000000000001513.temp to hdfs://master:54310/hbase/data/default/usertable/99b73df170b514f6773e742a495216c4/recovered.edits/0000000000000001536
2014-07-01 15:54:36,442 DEBUG [RS_LOG_REPLAY_OPS-sceplus-vm48:60020-1-Writer-0] wal.HLogSplitter: Writer thread Thread[RS_LOG_REPLAY_OPS-sceplus-vm48:60020-1-Writer-0,5,main]: starting
2014-07-01 15:54:36,442 DEBUG [RS_LOG_REPLAY_OPS-sceplus-vm48:60020-1-Writer-1] wal.HLogSplitter: Writer thread Thread[RS_LOG_REPLAY_OPS-sceplus-vm48:60020-1-Writer-1,5,main]: starting
2014-07-01 15:54:36,443 DEBUG [RS_LOG_REPLAY_OPS-sceplus-vm48:60020-1-Writer-2] wal.HLogSplitter: Writer thread Thread[RS_LOG_REPLAY_OPS-sceplus-vm48:60020-1-Writer-2,5,main]: starting
2014-07-01 15:54:36,459 INFO  [split-log-closeStream-1] wal.HLogSplitter: Closed wap hdfs://master:54310/hbase/data/default/usertable/c72fc9d54f680bcd54e9fa888600346b/recovered.edits/0000000000000001537.temp (wrote 24 edits in 188ms)
2014-07-01 15:54:36,463 DEBUG [split-log-closeStream-1] wal.HLogSplitter: Rename hdfs://master:54310/hbase/data/default/usertable/c72fc9d54f680bcd54e9fa888600346b/recovered.edits/0000000000000001537.temp to hdfs://master:54310/hbase/data/default/usertable/c72fc9d54f680bcd54e9fa888600346b/recovered.edits/0000000000000001560
2014-07-01 15:54:36,463 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:60020-0] wal.HLogSplitter: Processed 75 edits across 4 regions; log file=hdfs://master:54310/hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1404253870530-splitting/sceplus-vm48.almaden.ibm.com%2C60020%2C1404253870530.1404254416705 is corrupted = false progress failed = false
2014-07-01 15:54:36,469 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:60020-0] handler.HLogSplitterHandler: successfully transitioned task /hbase/splitWAL/WALs%2Fsceplus-vm48.almaden.ibm.com%2C60020%2C1404253870530-splitting%2Fsceplus-vm48.almaden.ibm.com%252C60020%252C1404253870530.1404254416705 to final state DONE sceplus-vm48.almaden.ibm.com,60020,1404255227840
2014-07-01 15:54:36,469 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:60020-0] handler.HLogSplitterHandler: worker sceplus-vm48.almaden.ibm.com,60020,1404255227840 done with task /hbase/splitWAL/WALs%2Fsceplus-vm48.almaden.ibm.com%2C60020%2C1404253870530-splitting%2Fsceplus-vm48.almaden.ibm.com%252C60020%252C1404253870530.1404254416705 in 1064ms
2014-07-01 15:54:36,495 DEBUG [RS_LOG_REPLAY_OPS-sceplus-vm48:60020-1-Writer-2] wal.HLogSplitter: Creating writer path=hdfs://master:54310/hbase/data/default/usertable/c72fc9d54f680bcd54e9fa888600346b/recovered.edits/0000000000000001711.temp region=c72fc9d54f680bcd54e9fa888600346b
2014-07-01 15:54:36,538 DEBUG [RS_LOG_REPLAY_OPS-sceplus-vm48:60020-1-Writer-1] wal.HLogSplitter: Creating writer path=hdfs://master:54310/hbase/data/default/usertable/99b73df170b514f6773e742a495216c4/recovered.edits/0000000000000001692.temp region=99b73df170b514f6773e742a495216c4
2014-07-01 15:54:36,545 DEBUG [RS_LOG_REPLAY_OPS-sceplus-vm48:60020-1-Writer-0] wal.HLogSplitter: Creating writer path=hdfs://master:54310/hbase/data/default/usertable/1dc0aa61f0e3f43a53375118eb48213f/recovered.edits/0000000000000001706.temp region=1dc0aa61f0e3f43a53375118eb48213f
2014-07-01 15:54:36,630 DEBUG [RS_LOG_REPLAY_OPS-sceplus-vm48:60020-1-Writer-2] wal.HLogSplitter: Creating writer path=hdfs://master:54310/hbase/data/default/usertable/52290d336af5a689eef6040bafbe6b15/recovered.edits/0000000000000001701.temp region=52290d336af5a689eef6040bafbe6b15
2014-07-01 15:54:37,071 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:60020-1] wal.HLogSplitter: Finishing writing output logs and closing down.
2014-07-01 15:54:37,071 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:60020-1] wal.HLogSplitter: Waiting for split writer threads to finish
2014-07-01 15:54:37,077 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:60020-1] wal.HLogSplitter: Split writers finished
2014-07-01 15:54:37,077 DEBUG [RS_LOG_REPLAY_OPS-sceplus-vm48:60020-1] wal.HLogSplitter: Submitting close of hdfs://master:54310/hbase/data/default/usertable/1dc0aa61f0e3f43a53375118eb48213f/recovered.edits/0000000000000001706.temp
2014-07-01 15:54:37,078 DEBUG [RS_LOG_REPLAY_OPS-sceplus-vm48:60020-1] wal.HLogSplitter: Submitting close of hdfs://master:54310/hbase/data/default/usertable/52290d336af5a689eef6040bafbe6b15/recovered.edits/0000000000000001701.temp
2014-07-01 15:54:37,078 DEBUG [split-log-closeStream-1] wal.HLogSplitter: Closing hdfs://master:54310/hbase/data/default/usertable/1dc0aa61f0e3f43a53375118eb48213f/recovered.edits/0000000000000001706.temp
2014-07-01 15:54:37,078 DEBUG [RS_LOG_REPLAY_OPS-sceplus-vm48:60020-1] wal.HLogSplitter: Submitting close of hdfs://master:54310/hbase/data/default/usertable/99b73df170b514f6773e742a495216c4/recovered.edits/0000000000000001692.temp
2014-07-01 15:54:37,078 DEBUG [split-log-closeStream-2] wal.HLogSplitter: Closing hdfs://master:54310/hbase/data/default/usertable/52290d336af5a689eef6040bafbe6b15/recovered.edits/0000000000000001701.temp
2014-07-01 15:54:37,078 DEBUG [RS_LOG_REPLAY_OPS-sceplus-vm48:60020-1] wal.HLogSplitter: Submitting close of hdfs://master:54310/hbase/data/default/usertable/c72fc9d54f680bcd54e9fa888600346b/recovered.edits/0000000000000001711.temp
2014-07-01 15:54:37,078 DEBUG [split-log-closeStream-3] wal.HLogSplitter: Closing hdfs://master:54310/hbase/data/default/usertable/99b73df170b514f6773e742a495216c4/recovered.edits/0000000000000001692.temp
2014-07-01 15:54:37,241 INFO  [split-log-closeStream-1] wal.HLogSplitter: Closed wap hdfs://master:54310/hbase/data/default/usertable/1dc0aa61f0e3f43a53375118eb48213f/recovered.edits/0000000000000001706.temp (wrote 15 edits in 123ms)
2014-07-01 15:54:37,243 INFO  [SplitLogWorker-sceplus-vm48.almaden.ibm.com,60020,1404255227840] regionserver.SplitLogWorker: worker sceplus-vm48.almaden.ibm.com,60020,1404255227840 acquired task /hbase/splitWAL/WALs%2Fsceplus-vm48.almaden.ibm.com%2C60020%2C1404253870530-splitting%2Fsceplus-vm48.almaden.ibm.com%252C60020%252C1404253870530.1404254411653
2014-07-01 15:54:37,243 INFO  [split-log-closeStream-2] wal.HLogSplitter: Closed wap hdfs://master:54310/hbase/data/default/usertable/52290d336af5a689eef6040bafbe6b15/recovered.edits/0000000000000001701.temp (wrote 14 edits in 115ms)
2014-07-01 15:54:37,243 INFO  [split-log-closeStream-3] wal.HLogSplitter: Closed wap hdfs://master:54310/hbase/data/default/usertable/99b73df170b514f6773e742a495216c4/recovered.edits/0000000000000001692.temp (wrote 22 edits in 240ms)
2014-07-01 15:54:37,247 DEBUG [split-log-closeStream-1] wal.HLogSplitter: Rename hdfs://master:54310/hbase/data/default/usertable/1dc0aa61f0e3f43a53375118eb48213f/recovered.edits/0000000000000001706.temp to hdfs://master:54310/hbase/data/default/usertable/1dc0aa61f0e3f43a53375118eb48213f/recovered.edits/0000000000000001720
2014-07-01 15:54:37,248 DEBUG [split-log-closeStream-1] wal.HLogSplitter: Closing hdfs://master:54310/hbase/data/default/usertable/c72fc9d54f680bcd54e9fa888600346b/recovered.edits/0000000000000001711.temp
2014-07-01 15:54:37,251 DEBUG [split-log-closeStream-3] wal.HLogSplitter: Rename hdfs://master:54310/hbase/data/default/usertable/99b73df170b514f6773e742a495216c4/recovered.edits/0000000000000001692.temp to hdfs://master:54310/hbase/data/default/usertable/99b73df170b514f6773e742a495216c4/recovered.edits/0000000000000001713
2014-07-01 15:54:37,251 DEBUG [split-log-closeStream-2] wal.HLogSplitter: Rename hdfs://master:54310/hbase/data/default/usertable/52290d336af5a689eef6040bafbe6b15/recovered.edits/0000000000000001701.temp to hdfs://master:54310/hbase/data/default/usertable/52290d336af5a689eef6040bafbe6b15/recovered.edits/0000000000000001714
2014-07-01 15:54:37,263 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:60020-0] wal.HLogSplitter: Splitting hlog: hdfs://master:54310/hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1404253870530-splitting/sceplus-vm48.almaden.ibm.com%2C60020%2C1404253870530.1404254411653, length=72721694
2014-07-01 15:54:37,263 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:60020-0] wal.HLogSplitter: DistributedLogReplay = false
2014-07-01 15:54:37,268 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:60020-0] util.FSHDFSUtils: Recovering lease on dfs file hdfs://master:54310/hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1404253870530-splitting/sceplus-vm48.almaden.ibm.com%2C60020%2C1404253870530.1404254411653
2014-07-01 15:54:37,268 INFO  [split-log-closeStream-1] wal.HLogSplitter: Closed wap hdfs://master:54310/hbase/data/default/usertable/c72fc9d54f680bcd54e9fa888600346b/recovered.edits/0000000000000001711.temp (wrote 24 edits in 207ms)
2014-07-01 15:54:37,269 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:60020-0] util.FSHDFSUtils: recoverLease=true, attempt=0 on file=hdfs://master:54310/hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1404253870530-splitting/sceplus-vm48.almaden.ibm.com%2C60020%2C1404253870530.1404254411653 after 1ms
2014-07-01 15:54:37,318 DEBUG [split-log-closeStream-1] wal.HLogSplitter: Rename hdfs://master:54310/hbase/data/default/usertable/c72fc9d54f680bcd54e9fa888600346b/recovered.edits/0000000000000001711.temp to hdfs://master:54310/hbase/data/default/usertable/c72fc9d54f680bcd54e9fa888600346b/recovered.edits/0000000000000001734
2014-07-01 15:54:37,319 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:60020-1] wal.HLogSplitter: Processed 75 edits across 4 regions; log file=hdfs://master:54310/hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1404253870530-splitting/sceplus-vm48.almaden.ibm.com%2C60020%2C1404253870530.1404254452902 is corrupted = false progress failed = false
2014-07-01 15:54:37,324 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:60020-1] handler.HLogSplitterHandler: successfully transitioned task /hbase/splitWAL/WALs%2Fsceplus-vm48.almaden.ibm.com%2C60020%2C1404253870530-splitting%2Fsceplus-vm48.almaden.ibm.com%252C60020%252C1404253870530.1404254452902 to final state DONE sceplus-vm48.almaden.ibm.com,60020,1404255227840
2014-07-01 15:54:37,324 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:60020-1] handler.HLogSplitterHandler: worker sceplus-vm48.almaden.ibm.com,60020,1404255227840 done with task /hbase/splitWAL/WALs%2Fsceplus-vm48.almaden.ibm.com%2C60020%2C1404253870530-splitting%2Fsceplus-vm48.almaden.ibm.com%252C60020%252C1404253870530.1404254452902 in 929ms
2014-07-01 15:54:37,330 DEBUG [RS_LOG_REPLAY_OPS-sceplus-vm48:60020-0-Writer-0] wal.HLogSplitter: Writer thread Thread[RS_LOG_REPLAY_OPS-sceplus-vm48:60020-0-Writer-0,5,main]: starting
2014-07-01 15:54:37,331 DEBUG [RS_LOG_REPLAY_OPS-sceplus-vm48:60020-0-Writer-1] wal.HLogSplitter: Writer thread Thread[RS_LOG_REPLAY_OPS-sceplus-vm48:60020-0-Writer-1,5,main]: starting
2014-07-01 15:54:37,331 DEBUG [RS_LOG_REPLAY_OPS-sceplus-vm48:60020-0-Writer-2] wal.HLogSplitter: Writer thread Thread[RS_LOG_REPLAY_OPS-sceplus-vm48:60020-0-Writer-2,5,main]: starting
2014-07-01 15:54:37,370 DEBUG [RS_LOG_REPLAY_OPS-sceplus-vm48:60020-0-Writer-2] wal.HLogSplitter: Creating writer path=hdfs://master:54310/hbase/data/default/usertable/52290d336af5a689eef6040bafbe6b15/recovered.edits/0000000000000001480.temp region=52290d336af5a689eef6040bafbe6b15
2014-07-01 15:54:37,388 DEBUG [RS_LOG_REPLAY_OPS-sceplus-vm48:60020-0-Writer-0] wal.HLogSplitter: Creating writer path=hdfs://master:54310/hbase/data/default/usertable/1dc0aa61f0e3f43a53375118eb48213f/recovered.edits/0000000000000001491.temp region=1dc0aa61f0e3f43a53375118eb48213f
2014-07-01 15:54:37,441 DEBUG [RS_LOG_REPLAY_OPS-sceplus-vm48:60020-0-Writer-2] wal.HLogSplitter: Creating writer path=hdfs://master:54310/hbase/data/default/usertable/99b73df170b514f6773e742a495216c4/recovered.edits/0000000000000001479.temp region=99b73df170b514f6773e742a495216c4
2014-07-01 15:54:37,687 DEBUG [RS_LOG_REPLAY_OPS-sceplus-vm48:60020-0-Writer-2] wal.HLogSplitter: Creating writer path=hdfs://master:54310/hbase/data/default/usertable/c72fc9d54f680bcd54e9fa888600346b/recovered.edits/0000000000000001501.temp region=c72fc9d54f680bcd54e9fa888600346b
2014-07-01 15:54:38,207 INFO  [SplitLogWorker-sceplus-vm48.almaden.ibm.com,60020,1404255227840] regionserver.SplitLogWorker: worker sceplus-vm48.almaden.ibm.com,60020,1404255227840 acquired task /hbase/splitWAL/WALs%2Fsceplus-vm48.almaden.ibm.com%2C60020%2C1404253870530-splitting%2Fsceplus-vm48.almaden.ibm.com%252C60020%252C1404253870530.1404254481109
2014-07-01 15:54:38,224 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:60020-1] wal.HLogSplitter: Splitting hlog: hdfs://master:54310/hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1404253870530-splitting/sceplus-vm48.almaden.ibm.com%2C60020%2C1404253870530.1404254481109, length=74634914
2014-07-01 15:54:38,224 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:60020-1] wal.HLogSplitter: DistributedLogReplay = false
2014-07-01 15:54:38,256 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:60020-1] util.FSHDFSUtils: Recovering lease on dfs file hdfs://master:54310/hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1404253870530-splitting/sceplus-vm48.almaden.ibm.com%2C60020%2C1404253870530.1404254481109
2014-07-01 15:54:38,257 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:60020-1] util.FSHDFSUtils: recoverLease=true, attempt=0 on file=hdfs://master:54310/hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1404253870530-splitting/sceplus-vm48.almaden.ibm.com%2C60020%2C1404253870530.1404254481109 after 0ms
2014-07-01 15:54:38,274 DEBUG [RS_LOG_REPLAY_OPS-sceplus-vm48:60020-1-Writer-0] wal.HLogSplitter: Writer thread Thread[RS_LOG_REPLAY_OPS-sceplus-vm48:60020-1-Writer-0,5,main]: starting
2014-07-01 15:54:38,274 DEBUG [RS_LOG_REPLAY_OPS-sceplus-vm48:60020-1-Writer-1] wal.HLogSplitter: Writer thread Thread[RS_LOG_REPLAY_OPS-sceplus-vm48:60020-1-Writer-1,5,main]: starting
2014-07-01 15:54:38,275 DEBUG [RS_LOG_REPLAY_OPS-sceplus-vm48:60020-1-Writer-2] wal.HLogSplitter: Writer thread Thread[RS_LOG_REPLAY_OPS-sceplus-vm48:60020-1-Writer-2,5,main]: starting
2014-07-01 15:54:38,313 DEBUG [RS_LOG_REPLAY_OPS-sceplus-vm48:60020-1-Writer-2] wal.HLogSplitter: Creating writer path=hdfs://master:54310/hbase/data/default/usertable/52290d336af5a689eef6040bafbe6b15/recovered.edits/0000000000000001870.temp region=52290d336af5a689eef6040bafbe6b15
2014-07-01 15:54:38,323 DEBUG [RS_LOG_REPLAY_OPS-sceplus-vm48:60020-1-Writer-0] wal.HLogSplitter: Creating writer path=hdfs://master:54310/hbase/data/default/usertable/1dc0aa61f0e3f43a53375118eb48213f/recovered.edits/0000000000000001877.temp region=1dc0aa61f0e3f43a53375118eb48213f
2014-07-01 15:54:38,361 DEBUG [RS_LOG_REPLAY_OPS-sceplus-vm48:60020-1-Writer-0] wal.HLogSplitter: Creating writer path=hdfs://master:54310/hbase/data/default/usertable/c72fc9d54f680bcd54e9fa888600346b/recovered.edits/0000000000000001883.temp region=c72fc9d54f680bcd54e9fa888600346b
2014-07-01 15:54:38,814 DEBUG [RS_LOG_REPLAY_OPS-sceplus-vm48:60020-1-Writer-0] wal.HLogSplitter: Creating writer path=hdfs://master:54310/hbase/data/default/usertable/99b73df170b514f6773e742a495216c4/recovered.edits/0000000000000001870.temp region=99b73df170b514f6773e742a495216c4
2014-07-01 15:54:38,982 DEBUG [SplitLogWorker-sceplus-vm48.almaden.ibm.com,60020,1404255227840] regionserver.SplitLogWorker: Current region server sceplus-vm48.almaden.ibm.com,60020,1404255227840 has 2 tasks in progress and can't take more.
2014-07-01 15:54:38,984 DEBUG [SplitLogWorker-sceplus-vm48.almaden.ibm.com,60020,1404255227840] regionserver.SplitLogWorker: Current region server sceplus-vm48.almaden.ibm.com,60020,1404255227840 has 2 tasks in progress and can't take more.
2014-07-01 15:54:39,586 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:60020-0] wal.HLogSplitter: Finishing writing output logs and closing down.
2014-07-01 15:54:39,586 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:60020-0] wal.HLogSplitter: Waiting for split writer threads to finish
2014-07-01 15:54:39,594 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:60020-0] wal.HLogSplitter: Split writers finished
2014-07-01 15:54:39,594 DEBUG [RS_LOG_REPLAY_OPS-sceplus-vm48:60020-0] wal.HLogSplitter: Submitting close of hdfs://master:54310/hbase/data/default/usertable/1dc0aa61f0e3f43a53375118eb48213f/recovered.edits/0000000000000001491.temp
2014-07-01 15:54:39,595 DEBUG [RS_LOG_REPLAY_OPS-sceplus-vm48:60020-0] wal.HLogSplitter: Submitting close of hdfs://master:54310/hbase/data/default/usertable/52290d336af5a689eef6040bafbe6b15/recovered.edits/0000000000000001480.temp
2014-07-01 15:54:39,595 DEBUG [split-log-closeStream-1] wal.HLogSplitter: Closing hdfs://master:54310/hbase/data/default/usertable/1dc0aa61f0e3f43a53375118eb48213f/recovered.edits/0000000000000001491.temp
2014-07-01 15:54:39,595 DEBUG [RS_LOG_REPLAY_OPS-sceplus-vm48:60020-0] wal.HLogSplitter: Submitting close of hdfs://master:54310/hbase/data/default/usertable/99b73df170b514f6773e742a495216c4/recovered.edits/0000000000000001479.temp
2014-07-01 15:54:39,595 DEBUG [split-log-closeStream-2] wal.HLogSplitter: Closing hdfs://master:54310/hbase/data/default/usertable/52290d336af5a689eef6040bafbe6b15/recovered.edits/0000000000000001480.temp
2014-07-01 15:54:39,595 DEBUG [RS_LOG_REPLAY_OPS-sceplus-vm48:60020-0] wal.HLogSplitter: Submitting close of hdfs://master:54310/hbase/data/default/usertable/c72fc9d54f680bcd54e9fa888600346b/recovered.edits/0000000000000001501.temp
2014-07-01 15:54:39,595 DEBUG [split-log-closeStream-3] wal.HLogSplitter: Closing hdfs://master:54310/hbase/data/default/usertable/99b73df170b514f6773e742a495216c4/recovered.edits/0000000000000001479.temp
2014-07-01 15:54:39,796 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:60020-1] wal.HLogSplitter: Finishing writing output logs and closing down.
2014-07-01 15:54:39,797 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:60020-1] wal.HLogSplitter: Waiting for split writer threads to finish
2014-07-01 15:54:39,803 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:60020-1] wal.HLogSplitter: Split writers finished
2014-07-01 15:54:39,804 DEBUG [RS_LOG_REPLAY_OPS-sceplus-vm48:60020-1] wal.HLogSplitter: Submitting close of hdfs://master:54310/hbase/data/default/usertable/1dc0aa61f0e3f43a53375118eb48213f/recovered.edits/0000000000000001877.temp
2014-07-01 15:54:39,804 DEBUG [RS_LOG_REPLAY_OPS-sceplus-vm48:60020-1] wal.HLogSplitter: Submitting close of hdfs://master:54310/hbase/data/default/usertable/52290d336af5a689eef6040bafbe6b15/recovered.edits/0000000000000001870.temp
2014-07-01 15:54:39,804 DEBUG [split-log-closeStream-1] wal.HLogSplitter: Closing hdfs://master:54310/hbase/data/default/usertable/1dc0aa61f0e3f43a53375118eb48213f/recovered.edits/0000000000000001877.temp
2014-07-01 15:54:39,804 DEBUG [RS_LOG_REPLAY_OPS-sceplus-vm48:60020-1] wal.HLogSplitter: Submitting close of hdfs://master:54310/hbase/data/default/usertable/99b73df170b514f6773e742a495216c4/recovered.edits/0000000000000001870.temp
2014-07-01 15:54:39,804 DEBUG [split-log-closeStream-2] wal.HLogSplitter: Closing hdfs://master:54310/hbase/data/default/usertable/52290d336af5a689eef6040bafbe6b15/recovered.edits/0000000000000001870.temp
2014-07-01 15:54:39,804 DEBUG [RS_LOG_REPLAY_OPS-sceplus-vm48:60020-1] wal.HLogSplitter: Submitting close of hdfs://master:54310/hbase/data/default/usertable/c72fc9d54f680bcd54e9fa888600346b/recovered.edits/0000000000000001883.temp
2014-07-01 15:54:39,805 DEBUG [split-log-closeStream-3] wal.HLogSplitter: Closing hdfs://master:54310/hbase/data/default/usertable/99b73df170b514f6773e742a495216c4/recovered.edits/0000000000000001870.temp
2014-07-01 15:54:39,837 INFO  [split-log-closeStream-2] wal.HLogSplitter: Closed wap hdfs://master:54310/hbase/data/default/usertable/52290d336af5a689eef6040bafbe6b15/recovered.edits/0000000000000001480.temp (wrote 21 edits in 392ms)
2014-07-01 15:54:39,837 INFO  [split-log-closeStream-1] wal.HLogSplitter: Closed wap hdfs://master:54310/hbase/data/default/usertable/1dc0aa61f0e3f43a53375118eb48213f/recovered.edits/0000000000000001491.temp (wrote 20 edits in 325ms)
2014-07-01 15:54:39,837 INFO  [split-log-closeStream-3] wal.HLogSplitter: Closed wap hdfs://master:54310/hbase/data/default/usertable/99b73df170b514f6773e742a495216c4/recovered.edits/0000000000000001479.temp (wrote 15 edits in 215ms)
2014-07-01 15:54:39,839 DEBUG [regionserver60020-EventThread] regionserver.SplitLogWorker: tasks arrived or departed
2014-07-01 15:54:39,842 DEBUG [SplitLogWorker-sceplus-vm48.almaden.ibm.com,60020,1404255227840] regionserver.SplitLogWorker: Current region server sceplus-vm48.almaden.ibm.com,60020,1404255227840 has 2 tasks in progress and can't take more.
2014-07-01 15:54:39,853 INFO  [split-log-closeStream-3] wal.HLogSplitter: Closed wap hdfs://master:54310/hbase/data/default/usertable/99b73df170b514f6773e742a495216c4/recovered.edits/0000000000000001870.temp (wrote 15 edits in 513ms)
2014-07-01 15:54:39,853 INFO  [split-log-closeStream-2] wal.HLogSplitter: Closed wap hdfs://master:54310/hbase/data/default/usertable/52290d336af5a689eef6040bafbe6b15/recovered.edits/0000000000000001870.temp (wrote 20 edits in 167ms)
2014-07-01 15:54:39,854 INFO  [split-log-closeStream-1] wal.HLogSplitter: Closed wap hdfs://master:54310/hbase/data/default/usertable/1dc0aa61f0e3f43a53375118eb48213f/recovered.edits/0000000000000001877.temp (wrote 19 edits in 174ms)
2014-07-01 15:54:40,622 DEBUG [split-log-closeStream-1] wal.HLogSplitter: Rename hdfs://master:54310/hbase/data/default/usertable/1dc0aa61f0e3f43a53375118eb48213f/recovered.edits/0000000000000001491.temp to hdfs://master:54310/hbase/data/default/usertable/1dc0aa61f0e3f43a53375118eb48213f/recovered.edits/0000000000000001510
2014-07-01 15:54:40,622 DEBUG [split-log-closeStream-1] wal.HLogSplitter: Closing hdfs://master:54310/hbase/data/default/usertable/c72fc9d54f680bcd54e9fa888600346b/recovered.edits/0000000000000001501.temp
2014-07-01 15:54:40,627 DEBUG [split-log-closeStream-2] wal.HLogSplitter: Rename hdfs://master:54310/hbase/data/default/usertable/52290d336af5a689eef6040bafbe6b15/recovered.edits/0000000000000001480.temp to hdfs://master:54310/hbase/data/default/usertable/52290d336af5a689eef6040bafbe6b15/recovered.edits/0000000000000001500
2014-07-01 15:54:40,780 DEBUG [split-log-closeStream-1] wal.HLogSplitter: Rename hdfs://master:54310/hbase/data/default/usertable/1dc0aa61f0e3f43a53375118eb48213f/recovered.edits/0000000000000001877.temp to hdfs://master:54310/hbase/data/default/usertable/1dc0aa61f0e3f43a53375118eb48213f/recovered.edits/0000000000000001895
2014-07-01 15:54:40,780 DEBUG [split-log-closeStream-2] wal.HLogSplitter: Rename hdfs://master:54310/hbase/data/default/usertable/52290d336af5a689eef6040bafbe6b15/recovered.edits/0000000000000001870.temp to hdfs://master:54310/hbase/data/default/usertable/52290d336af5a689eef6040bafbe6b15/recovered.edits/0000000000000001889
2014-07-01 15:54:40,780 DEBUG [split-log-closeStream-1] wal.HLogSplitter: Closing hdfs://master:54310/hbase/data/default/usertable/c72fc9d54f680bcd54e9fa888600346b/recovered.edits/0000000000000001883.temp
2014-07-01 15:54:40,780 DEBUG [split-log-closeStream-3] wal.HLogSplitter: Rename hdfs://master:54310/hbase/data/default/usertable/99b73df170b514f6773e742a495216c4/recovered.edits/0000000000000001479.temp to hdfs://master:54310/hbase/data/default/usertable/99b73df170b514f6773e742a495216c4/recovered.edits/0000000000000001493
2014-07-01 15:54:40,780 DEBUG [split-log-closeStream-3] wal.HLogSplitter: Rename hdfs://master:54310/hbase/data/default/usertable/99b73df170b514f6773e742a495216c4/recovered.edits/0000000000000001870.temp to hdfs://master:54310/hbase/data/default/usertable/99b73df170b514f6773e742a495216c4/recovered.edits/0000000000000001884
2014-07-01 15:54:40,781 INFO  [split-log-closeStream-1] wal.HLogSplitter: Closed wap hdfs://master:54310/hbase/data/default/usertable/c72fc9d54f680bcd54e9fa888600346b/recovered.edits/0000000000000001501.temp (wrote 17 edits in 204ms)
2014-07-01 15:54:40,786 DEBUG [split-log-closeStream-1] wal.HLogSplitter: Rename hdfs://master:54310/hbase/data/default/usertable/c72fc9d54f680bcd54e9fa888600346b/recovered.edits/0000000000000001501.temp to hdfs://master:54310/hbase/data/default/usertable/c72fc9d54f680bcd54e9fa888600346b/recovered.edits/0000000000000001517
2014-07-01 15:54:40,786 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:60020-0] wal.HLogSplitter: Processed 73 edits across 4 regions; log file=hdfs://master:54310/hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1404253870530-splitting/sceplus-vm48.almaden.ibm.com%2C60020%2C1404253870530.1404254411653 is corrupted = false progress failed = false
2014-07-01 15:54:40,788 INFO  [split-log-closeStream-1] wal.HLogSplitter: Closed wap hdfs://master:54310/hbase/data/default/usertable/c72fc9d54f680bcd54e9fa888600346b/recovered.edits/0000000000000001883.temp (wrote 19 edits in 261ms)
2014-07-01 15:54:40,792 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:60020-0] handler.HLogSplitterHandler: successfully transitioned task /hbase/splitWAL/WALs%2Fsceplus-vm48.almaden.ibm.com%2C60020%2C1404253870530-splitting%2Fsceplus-vm48.almaden.ibm.com%252C60020%252C1404253870530.1404254411653 to final state DONE sceplus-vm48.almaden.ibm.com,60020,1404255227840
2014-07-01 15:54:40,792 DEBUG [split-log-closeStream-1] wal.HLogSplitter: Rename hdfs://master:54310/hbase/data/default/usertable/c72fc9d54f680bcd54e9fa888600346b/recovered.edits/0000000000000001883.temp to hdfs://master:54310/hbase/data/default/usertable/c72fc9d54f680bcd54e9fa888600346b/recovered.edits/0000000000000001901
2014-07-01 15:54:40,792 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:60020-0] handler.HLogSplitterHandler: worker sceplus-vm48.almaden.ibm.com,60020,1404255227840 done with task /hbase/splitWAL/WALs%2Fsceplus-vm48.almaden.ibm.com%2C60020%2C1404253870530-splitting%2Fsceplus-vm48.almaden.ibm.com%252C60020%252C1404253870530.1404254411653 in 3547ms
2014-07-01 15:54:40,792 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:60020-1] wal.HLogSplitter: Processed 73 edits across 4 regions; log file=hdfs://master:54310/hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1404253870530-splitting/sceplus-vm48.almaden.ibm.com%2C60020%2C1404253870530.1404254481109 is corrupted = false progress failed = false
2014-07-01 15:54:40,796 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:60020-1] handler.HLogSplitterHandler: successfully transitioned task /hbase/splitWAL/WALs%2Fsceplus-vm48.almaden.ibm.com%2C60020%2C1404253870530-splitting%2Fsceplus-vm48.almaden.ibm.com%252C60020%252C1404253870530.1404254481109 to final state DONE sceplus-vm48.almaden.ibm.com,60020,1404255227840
2014-07-01 15:54:40,797 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:60020-1] handler.HLogSplitterHandler: worker sceplus-vm48.almaden.ibm.com,60020,1404255227840 done with task /hbase/splitWAL/WALs%2Fsceplus-vm48.almaden.ibm.com%2C60020%2C1404253870530-splitting%2Fsceplus-vm48.almaden.ibm.com%252C60020%252C1404253870530.1404254481109 in 2590ms
2014-07-01 15:54:40,809 DEBUG [regionserver60020-EventThread] regionserver.SplitLogWorker: tasks arrived or departed
2014-07-01 15:54:40,814 INFO  [SplitLogWorker-sceplus-vm48.almaden.ibm.com,60020,1404255227840] regionserver.SplitLogWorker: worker sceplus-vm48.almaden.ibm.com,60020,1404255227840 acquired task /hbase/splitWAL/WALs%2Fsceplus-vm48.almaden.ibm.com%2C60020%2C1404253870530-splitting%2Fsceplus-vm48.almaden.ibm.com%252C60020%252C1404253870530.1404254437507
2014-07-01 15:54:40,826 DEBUG [regionserver60020-EventThread] regionserver.SplitLogWorker: tasks arrived or departed
2014-07-01 15:54:40,831 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:60020-0] wal.HLogSplitter: Splitting hlog: hdfs://master:54310/hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1404253870530-splitting/sceplus-vm48.almaden.ibm.com%2C60020%2C1404253870530.1404254437507, length=82475965
2014-07-01 15:54:40,831 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:60020-0] wal.HLogSplitter: DistributedLogReplay = false
2014-07-01 15:54:40,834 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:60020-0] util.FSHDFSUtils: Recovering lease on dfs file hdfs://master:54310/hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1404253870530-splitting/sceplus-vm48.almaden.ibm.com%2C60020%2C1404253870530.1404254437507
2014-07-01 15:54:40,835 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:60020-0] util.FSHDFSUtils: recoverLease=true, attempt=0 on file=hdfs://master:54310/hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1404253870530-splitting/sceplus-vm48.almaden.ibm.com%2C60020%2C1404253870530.1404254437507 after 1ms
2014-07-01 15:54:40,873 DEBUG [RS_LOG_REPLAY_OPS-sceplus-vm48:60020-0-Writer-0] wal.HLogSplitter: Writer thread Thread[RS_LOG_REPLAY_OPS-sceplus-vm48:60020-0-Writer-0,5,main]: starting
2014-07-01 15:54:40,873 DEBUG [RS_LOG_REPLAY_OPS-sceplus-vm48:60020-0-Writer-1] wal.HLogSplitter: Writer thread Thread[RS_LOG_REPLAY_OPS-sceplus-vm48:60020-0-Writer-1,5,main]: starting
2014-07-01 15:54:40,874 DEBUG [RS_LOG_REPLAY_OPS-sceplus-vm48:60020-0-Writer-2] wal.HLogSplitter: Writer thread Thread[RS_LOG_REPLAY_OPS-sceplus-vm48:60020-0-Writer-2,5,main]: starting
2014-07-01 15:54:40,902 DEBUG [RS_LOG_REPLAY_OPS-sceplus-vm48:60020-0-Writer-2] wal.HLogSplitter: Creating writer path=hdfs://master:54310/hbase/data/default/usertable/52290d336af5a689eef6040bafbe6b15/recovered.edits/0000000000000001596.temp region=52290d336af5a689eef6040bafbe6b15
2014-07-01 15:54:40,909 DEBUG [RS_LOG_REPLAY_OPS-sceplus-vm48:60020-0-Writer-0] wal.HLogSplitter: Creating writer path=hdfs://master:54310/hbase/data/default/usertable/99b73df170b514f6773e742a495216c4/recovered.edits/0000000000000001595.temp region=99b73df170b514f6773e742a495216c4
2014-07-01 15:54:40,939 DEBUG [RS_LOG_REPLAY_OPS-sceplus-vm48:60020-0-Writer-1] wal.HLogSplitter: Creating writer path=hdfs://master:54310/hbase/data/default/usertable/c72fc9d54f680bcd54e9fa888600346b/recovered.edits/0000000000000001611.temp region=c72fc9d54f680bcd54e9fa888600346b
2014-07-01 15:54:40,973 DEBUG [RS_LOG_REPLAY_OPS-sceplus-vm48:60020-0-Writer-0] wal.HLogSplitter: Creating writer path=hdfs://master:54310/hbase/data/default/usertable/1dc0aa61f0e3f43a53375118eb48213f/recovered.edits/0000000000000001606.temp region=1dc0aa61f0e3f43a53375118eb48213f
2014-07-01 15:54:41,680 INFO  [SplitLogWorker-sceplus-vm48.almaden.ibm.com,60020,1404255227840] regionserver.SplitLogWorker: worker sceplus-vm48.almaden.ibm.com,60020,1404255227840 acquired task /hbase/splitWAL/WALs%2Fsceplus-vm48.almaden.ibm.com%2C60020%2C1404253870530-splitting%2Fsceplus-vm48.almaden.ibm.com%252C60020%252C1404253870530.1404254549308
2014-07-01 15:54:41,699 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:60020-1] wal.HLogSplitter: Splitting hlog: hdfs://master:54310/hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1404253870530-splitting/sceplus-vm48.almaden.ibm.com%2C60020%2C1404253870530.1404254549308, length=0
2014-07-01 15:54:41,699 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:60020-1] wal.HLogSplitter: DistributedLogReplay = false
2014-07-01 15:54:41,854 WARN  [RS_LOG_REPLAY_OPS-sceplus-vm48:60020-1] wal.HLogSplitter: File hdfs://master:54310/hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1404253870530-splitting/sceplus-vm48.almaden.ibm.com%2C60020%2C1404253870530.1404254549308 might be still open, length is 0
2014-07-01 15:54:41,854 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:60020-1] util.FSHDFSUtils: Recovering lease on dfs file hdfs://master:54310/hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1404253870530-splitting/sceplus-vm48.almaden.ibm.com%2C60020%2C1404253870530.1404254549308
2014-07-01 15:54:41,856 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:60020-1] util.FSHDFSUtils: recoverLease=false, attempt=0 on file=hdfs://master:54310/hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1404253870530-splitting/sceplus-vm48.almaden.ibm.com%2C60020%2C1404253870530.1404254549308 after 2ms
2014-07-01 15:54:42,607 DEBUG [SplitLogWorker-sceplus-vm48.almaden.ibm.com,60020,1404255227840] regionserver.SplitLogWorker: Current region server sceplus-vm48.almaden.ibm.com,60020,1404255227840 has 2 tasks in progress and can't take more.
2014-07-01 15:54:42,609 DEBUG [SplitLogWorker-sceplus-vm48.almaden.ibm.com,60020,1404255227840] regionserver.SplitLogWorker: Current region server sceplus-vm48.almaden.ibm.com,60020,1404255227840 has 2 tasks in progress and can't take more.
2014-07-01 15:54:43,407 DEBUG [regionserver60020-EventThread] regionserver.SplitLogWorker: tasks arrived or departed
2014-07-01 15:54:43,410 DEBUG [SplitLogWorker-sceplus-vm48.almaden.ibm.com,60020,1404255227840] regionserver.SplitLogWorker: Current region server sceplus-vm48.almaden.ibm.com,60020,1404255227840 has 2 tasks in progress and can't take more.
2014-07-01 15:54:44,643 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:60020-0] wal.HLogSplitter: Finishing writing output logs and closing down.
2014-07-01 15:54:44,643 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:60020-0] wal.HLogSplitter: Waiting for split writer threads to finish
2014-07-01 15:54:44,663 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:60020-0] wal.HLogSplitter: Split writers finished
2014-07-01 15:54:44,663 DEBUG [RS_LOG_REPLAY_OPS-sceplus-vm48:60020-0] wal.HLogSplitter: Submitting close of hdfs://master:54310/hbase/data/default/usertable/1dc0aa61f0e3f43a53375118eb48213f/recovered.edits/0000000000000001606.temp
2014-07-01 15:54:44,664 DEBUG [RS_LOG_REPLAY_OPS-sceplus-vm48:60020-0] wal.HLogSplitter: Submitting close of hdfs://master:54310/hbase/data/default/usertable/52290d336af5a689eef6040bafbe6b15/recovered.edits/0000000000000001596.temp
2014-07-01 15:54:44,664 DEBUG [split-log-closeStream-1] wal.HLogSplitter: Closing hdfs://master:54310/hbase/data/default/usertable/1dc0aa61f0e3f43a53375118eb48213f/recovered.edits/0000000000000001606.temp
2014-07-01 15:54:44,664 DEBUG [RS_LOG_REPLAY_OPS-sceplus-vm48:60020-0] wal.HLogSplitter: Submitting close of hdfs://master:54310/hbase/data/default/usertable/99b73df170b514f6773e742a495216c4/recovered.edits/0000000000000001595.temp
2014-07-01 15:54:44,664 DEBUG [split-log-closeStream-2] wal.HLogSplitter: Closing hdfs://master:54310/hbase/data/default/usertable/52290d336af5a689eef6040bafbe6b15/recovered.edits/0000000000000001596.temp
2014-07-01 15:54:44,664 DEBUG [RS_LOG_REPLAY_OPS-sceplus-vm48:60020-0] wal.HLogSplitter: Submitting close of hdfs://master:54310/hbase/data/default/usertable/c72fc9d54f680bcd54e9fa888600346b/recovered.edits/0000000000000001611.temp
2014-07-01 15:54:44,665 DEBUG [split-log-closeStream-3] wal.HLogSplitter: Closing hdfs://master:54310/hbase/data/default/usertable/99b73df170b514f6773e742a495216c4/recovered.edits/0000000000000001595.temp
2014-07-01 15:54:44,671 INFO  [split-log-closeStream-1] wal.HLogSplitter: Closed wap hdfs://master:54310/hbase/data/default/usertable/1dc0aa61f0e3f43a53375118eb48213f/recovered.edits/0000000000000001606.temp (wrote 20 edits in 235ms)
2014-07-01 15:54:44,672 INFO  [split-log-closeStream-3] wal.HLogSplitter: Closed wap hdfs://master:54310/hbase/data/default/usertable/99b73df170b514f6773e742a495216c4/recovered.edits/0000000000000001595.temp (wrote 18 edits in 205ms)
2014-07-01 15:54:44,675 DEBUG [split-log-closeStream-1] wal.HLogSplitter: Rename hdfs://master:54310/hbase/data/default/usertable/1dc0aa61f0e3f43a53375118eb48213f/recovered.edits/0000000000000001606.temp to hdfs://master:54310/hbase/data/default/usertable/1dc0aa61f0e3f43a53375118eb48213f/recovered.edits/0000000000000001625
2014-07-01 15:54:44,676 DEBUG [split-log-closeStream-1] wal.HLogSplitter: Closing hdfs://master:54310/hbase/data/default/usertable/c72fc9d54f680bcd54e9fa888600346b/recovered.edits/0000000000000001611.temp
2014-07-01 15:54:44,680 DEBUG [split-log-closeStream-3] wal.HLogSplitter: Rename hdfs://master:54310/hbase/data/default/usertable/99b73df170b514f6773e742a495216c4/recovered.edits/0000000000000001595.temp to hdfs://master:54310/hbase/data/default/usertable/99b73df170b514f6773e742a495216c4/recovered.edits/0000000000000001612
2014-07-01 15:54:44,718 INFO  [split-log-closeStream-1] wal.HLogSplitter: Closed wap hdfs://master:54310/hbase/data/default/usertable/c72fc9d54f680bcd54e9fa888600346b/recovered.edits/0000000000000001611.temp (wrote 25 edits in 236ms)
2014-07-01 15:54:44,722 DEBUG [split-log-closeStream-1] wal.HLogSplitter: Rename hdfs://master:54310/hbase/data/default/usertable/c72fc9d54f680bcd54e9fa888600346b/recovered.edits/0000000000000001611.temp to hdfs://master:54310/hbase/data/default/usertable/c72fc9d54f680bcd54e9fa888600346b/recovered.edits/0000000000000001635
2014-07-01 15:54:44,865 DEBUG [regionserver60020-EventThread] regionserver.SplitLogWorker: tasks arrived or departed
2014-07-01 15:54:44,868 DEBUG [SplitLogWorker-sceplus-vm48.almaden.ibm.com,60020,1404255227840] regionserver.SplitLogWorker: Current region server sceplus-vm48.almaden.ibm.com,60020,1404255227840 has 2 tasks in progress and can't take more.
2014-07-01 15:54:45,074 INFO  [split-log-closeStream-2] wal.HLogSplitter: Closed wap hdfs://master:54310/hbase/data/default/usertable/52290d336af5a689eef6040bafbe6b15/recovered.edits/0000000000000001596.temp (wrote 19 edits in 216ms)
2014-07-01 15:54:45,079 DEBUG [split-log-closeStream-2] wal.HLogSplitter: Rename hdfs://master:54310/hbase/data/default/usertable/52290d336af5a689eef6040bafbe6b15/recovered.edits/0000000000000001596.temp to hdfs://master:54310/hbase/data/default/usertable/52290d336af5a689eef6040bafbe6b15/recovered.edits/0000000000000001614
2014-07-01 15:54:45,079 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:60020-0] wal.HLogSplitter: Processed 82 edits across 4 regions; log file=hdfs://master:54310/hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1404253870530-splitting/sceplus-vm48.almaden.ibm.com%2C60020%2C1404253870530.1404254437507 is corrupted = false progress failed = false
2014-07-01 15:54:45,087 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:60020-0] handler.HLogSplitterHandler: successfully transitioned task /hbase/splitWAL/WALs%2Fsceplus-vm48.almaden.ibm.com%2C60020%2C1404253870530-splitting%2Fsceplus-vm48.almaden.ibm.com%252C60020%252C1404253870530.1404254437507 to final state DONE sceplus-vm48.almaden.ibm.com,60020,1404255227840
2014-07-01 15:54:45,087 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:60020-0] handler.HLogSplitterHandler: worker sceplus-vm48.almaden.ibm.com,60020,1404255227840 done with task /hbase/splitWAL/WALs%2Fsceplus-vm48.almaden.ibm.com%2C60020%2C1404253870530-splitting%2Fsceplus-vm48.almaden.ibm.com%252C60020%252C1404253870530.1404254437507 in 4273ms
2014-07-01 15:54:45,103 DEBUG [regionserver60020-EventThread] regionserver.SplitLogWorker: tasks arrived or departed
2014-07-01 15:54:45,110 INFO  [SplitLogWorker-sceplus-vm48.almaden.ibm.com,60020,1404255227840] regionserver.SplitLogWorker: worker sceplus-vm48.almaden.ibm.com,60020,1404255227840 acquired task /hbase/splitWAL/WALs%2Fslave1%2C60020%2C1404253868846-splitting%2Fslave1%252C60020%252C1404253868846.1404254543413
2014-07-01 15:54:45,127 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:60020-0] wal.HLogSplitter: Splitting hlog: hdfs://master:54310/hbase/WALs/slave1,60020,1404253868846-splitting/slave1%2C60020%2C1404253868846.1404254543413, length=65805357
2014-07-01 15:54:45,127 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:60020-0] wal.HLogSplitter: DistributedLogReplay = false
2014-07-01 15:54:45,132 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:60020-0] util.FSHDFSUtils: Recovering lease on dfs file hdfs://master:54310/hbase/WALs/slave1,60020,1404253868846-splitting/slave1%2C60020%2C1404253868846.1404254543413
2014-07-01 15:54:45,133 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:60020-0] util.FSHDFSUtils: recoverLease=true, attempt=0 on file=hdfs://master:54310/hbase/WALs/slave1,60020,1404253868846-splitting/slave1%2C60020%2C1404253868846.1404254543413 after 1ms
2014-07-01 15:54:45,157 DEBUG [RS_LOG_REPLAY_OPS-sceplus-vm48:60020-0-Writer-0] wal.HLogSplitter: Writer thread Thread[RS_LOG_REPLAY_OPS-sceplus-vm48:60020-0-Writer-0,5,main]: starting
2014-07-01 15:54:45,157 DEBUG [RS_LOG_REPLAY_OPS-sceplus-vm48:60020-0-Writer-1] wal.HLogSplitter: Writer thread Thread[RS_LOG_REPLAY_OPS-sceplus-vm48:60020-0-Writer-1,5,main]: starting
2014-07-01 15:54:45,158 DEBUG [RS_LOG_REPLAY_OPS-sceplus-vm48:60020-0-Writer-2] wal.HLogSplitter: Writer thread Thread[RS_LOG_REPLAY_OPS-sceplus-vm48:60020-0-Writer-2,5,main]: starting
2014-07-01 15:54:45,172 DEBUG [RS_LOG_REPLAY_OPS-sceplus-vm48:60020-0-Writer-2] wal.HLogSplitter: Creating writer path=hdfs://master:54310/hbase/data/default/usertable/bb79ae773f71289e0745068f68a0abd9/recovered.edits/0000000000000004653.temp region=bb79ae773f71289e0745068f68a0abd9
2014-07-01 15:54:45,181 DEBUG [RS_LOG_REPLAY_OPS-sceplus-vm48:60020-0-Writer-1] wal.HLogSplitter: Creating writer path=hdfs://master:54310/hbase/data/default/usertable/93253e37c284f3174309fede3f32339f/recovered.edits/0000000000000004707.temp region=93253e37c284f3174309fede3f32339f
2014-07-01 15:54:45,186 DEBUG [RS_LOG_REPLAY_OPS-sceplus-vm48:60020-0-Writer-0] wal.HLogSplitter: Creating writer path=hdfs://master:54310/hbase/data/default/usertable/9cef9e7d34f96f1d9b111be50040a208/recovered.edits/0000000000000005689.temp region=9cef9e7d34f96f1d9b111be50040a208
2014-07-01 15:54:45,191 DEBUG [RS_LOG_REPLAY_OPS-sceplus-vm48:60020-0-Writer-2] wal.HLogSplitter: Creating writer path=hdfs://master:54310/hbase/data/default/usertable/122e92eabfab38cb15a9965b21db25c5/recovered.edits/0000000000000004701.temp region=122e92eabfab38cb15a9965b21db25c5
2014-07-01 15:54:45,220 DEBUG [RS_LOG_REPLAY_OPS-sceplus-vm48:60020-0-Writer-0] wal.HLogSplitter: Creating writer path=hdfs://master:54310/hbase/data/default/usertable/667c1f2c34fe37d48ba25a789d8aba98/recovered.edits/0000000000000004666.temp region=667c1f2c34fe37d48ba25a789d8aba98
2014-07-01 15:54:45,400 DEBUG [regionserver60020-EventThread] regionserver.SplitLogWorker: tasks arrived or departed
2014-07-01 15:54:45,815 DEBUG [SplitLogWorker-sceplus-vm48.almaden.ibm.com,60020,1404255227840] regionserver.SplitLogWorker: Current region server sceplus-vm48.almaden.ibm.com,60020,1404255227840 has 2 tasks in progress and can't take more.
2014-07-01 15:54:45,818 DEBUG [SplitLogWorker-sceplus-vm48.almaden.ibm.com,60020,1404255227840] regionserver.SplitLogWorker: Current region server sceplus-vm48.almaden.ibm.com,60020,1404255227840 has 2 tasks in progress and can't take more.
2014-07-01 15:54:45,858 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:60020-1] util.FSHDFSUtils: recoverLease=true, attempt=1 on file=hdfs://master:54310/hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1404253870530-splitting/sceplus-vm48.almaden.ibm.com%2C60020%2C1404253870530.1404254549308 after 4004ms
2014-07-01 15:54:45,879 DEBUG [RS_LOG_REPLAY_OPS-sceplus-vm48:60020-1-Writer-0] wal.HLogSplitter: Writer thread Thread[RS_LOG_REPLAY_OPS-sceplus-vm48:60020-1-Writer-0,5,main]: starting
2014-07-01 15:54:45,880 DEBUG [RS_LOG_REPLAY_OPS-sceplus-vm48:60020-1-Writer-1] wal.HLogSplitter: Writer thread Thread[RS_LOG_REPLAY_OPS-sceplus-vm48:60020-1-Writer-1,5,main]: starting
2014-07-01 15:54:45,880 DEBUG [RS_LOG_REPLAY_OPS-sceplus-vm48:60020-1-Writer-2] wal.HLogSplitter: Writer thread Thread[RS_LOG_REPLAY_OPS-sceplus-vm48:60020-1-Writer-2,5,main]: starting
2014-07-01 15:54:45,913 DEBUG [RS_LOG_REPLAY_OPS-sceplus-vm48:60020-1-Writer-2] wal.HLogSplitter: Creating writer path=hdfs://master:54310/hbase/data/default/usertable/c72fc9d54f680bcd54e9fa888600346b/recovered.edits/0000000000000002002.temp region=c72fc9d54f680bcd54e9fa888600346b
2014-07-01 15:54:45,935 DEBUG [RS_LOG_REPLAY_OPS-sceplus-vm48:60020-1-Writer-1] wal.HLogSplitter: Creating writer path=hdfs://master:54310/hbase/data/default/usertable/52290d336af5a689eef6040bafbe6b15/recovered.edits/0000000000000001993.temp region=52290d336af5a689eef6040bafbe6b15
2014-07-01 15:54:45,941 DEBUG [RS_LOG_REPLAY_OPS-sceplus-vm48:60020-1-Writer-0] wal.HLogSplitter: Creating writer path=hdfs://master:54310/hbase/data/default/usertable/99b73df170b514f6773e742a495216c4/recovered.edits/0000000000000001993.temp region=99b73df170b514f6773e742a495216c4
2014-07-01 15:54:45,961 DEBUG [RS_LOG_REPLAY_OPS-sceplus-vm48:60020-1-Writer-1] wal.HLogSplitter: Creating writer path=hdfs://master:54310/hbase/data/default/usertable/1dc0aa61f0e3f43a53375118eb48213f/recovered.edits/0000000000000001998.temp region=1dc0aa61f0e3f43a53375118eb48213f
2014-07-01 15:54:46,188 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:60020-0] wal.HLogSplitter: Finishing writing output logs and closing down.
2014-07-01 15:54:46,188 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:60020-0] wal.HLogSplitter: Waiting for split writer threads to finish
2014-07-01 15:54:46,195 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:60020-0] wal.HLogSplitter: Split writers finished
2014-07-01 15:54:46,196 DEBUG [RS_LOG_REPLAY_OPS-sceplus-vm48:60020-0] wal.HLogSplitter: Submitting close of hdfs://master:54310/hbase/data/default/usertable/122e92eabfab38cb15a9965b21db25c5/recovered.edits/0000000000000004701.temp
2014-07-01 15:54:46,196 DEBUG [RS_LOG_REPLAY_OPS-sceplus-vm48:60020-0] wal.HLogSplitter: Submitting close of hdfs://master:54310/hbase/data/default/usertable/667c1f2c34fe37d48ba25a789d8aba98/recovered.edits/0000000000000004666.temp
2014-07-01 15:54:46,196 DEBUG [split-log-closeStream-1] wal.HLogSplitter: Closing hdfs://master:54310/hbase/data/default/usertable/122e92eabfab38cb15a9965b21db25c5/recovered.edits/0000000000000004701.temp
2014-07-01 15:54:46,196 DEBUG [RS_LOG_REPLAY_OPS-sceplus-vm48:60020-0] wal.HLogSplitter: Submitting close of hdfs://master:54310/hbase/data/default/usertable/93253e37c284f3174309fede3f32339f/recovered.edits/0000000000000004707.temp
2014-07-01 15:54:46,196 DEBUG [split-log-closeStream-2] wal.HLogSplitter: Closing hdfs://master:54310/hbase/data/default/usertable/667c1f2c34fe37d48ba25a789d8aba98/recovered.edits/0000000000000004666.temp
2014-07-01 15:54:46,196 DEBUG [RS_LOG_REPLAY_OPS-sceplus-vm48:60020-0] wal.HLogSplitter: Submitting close of hdfs://master:54310/hbase/data/default/usertable/9cef9e7d34f96f1d9b111be50040a208/recovered.edits/0000000000000005689.temp
2014-07-01 15:54:46,197 DEBUG [RS_LOG_REPLAY_OPS-sceplus-vm48:60020-0] wal.HLogSplitter: Submitting close of hdfs://master:54310/hbase/data/default/usertable/bb79ae773f71289e0745068f68a0abd9/recovered.edits/0000000000000004653.temp
2014-07-01 15:54:46,197 DEBUG [split-log-closeStream-3] wal.HLogSplitter: Closing hdfs://master:54310/hbase/data/default/usertable/93253e37c284f3174309fede3f32339f/recovered.edits/0000000000000004707.temp
2014-07-01 15:54:46,400 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:60020-1] wal.HLogSplitter: Finishing writing output logs and closing down.
2014-07-01 15:54:46,400 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:60020-1] wal.HLogSplitter: Waiting for split writer threads to finish
2014-07-01 15:54:46,407 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:60020-1] wal.HLogSplitter: Split writers finished
2014-07-01 15:54:46,407 DEBUG [RS_LOG_REPLAY_OPS-sceplus-vm48:60020-1] wal.HLogSplitter: Submitting close of hdfs://master:54310/hbase/data/default/usertable/1dc0aa61f0e3f43a53375118eb48213f/recovered.edits/0000000000000001998.temp
2014-07-01 15:54:46,408 DEBUG [RS_LOG_REPLAY_OPS-sceplus-vm48:60020-1] wal.HLogSplitter: Submitting close of hdfs://master:54310/hbase/data/default/usertable/52290d336af5a689eef6040bafbe6b15/recovered.edits/0000000000000001993.temp
2014-07-01 15:54:46,408 DEBUG [split-log-closeStream-1] wal.HLogSplitter: Closing hdfs://master:54310/hbase/data/default/usertable/1dc0aa61f0e3f43a53375118eb48213f/recovered.edits/0000000000000001998.temp
2014-07-01 15:54:46,408 DEBUG [RS_LOG_REPLAY_OPS-sceplus-vm48:60020-1] wal.HLogSplitter: Submitting close of hdfs://master:54310/hbase/data/default/usertable/99b73df170b514f6773e742a495216c4/recovered.edits/0000000000000001993.temp
2014-07-01 15:54:46,408 DEBUG [split-log-closeStream-2] wal.HLogSplitter: Closing hdfs://master:54310/hbase/data/default/usertable/52290d336af5a689eef6040bafbe6b15/recovered.edits/0000000000000001993.temp
2014-07-01 15:54:46,408 DEBUG [RS_LOG_REPLAY_OPS-sceplus-vm48:60020-1] wal.HLogSplitter: Submitting close of hdfs://master:54310/hbase/data/default/usertable/c72fc9d54f680bcd54e9fa888600346b/recovered.edits/0000000000000002002.temp
2014-07-01 15:54:46,408 DEBUG [split-log-closeStream-3] wal.HLogSplitter: Closing hdfs://master:54310/hbase/data/default/usertable/99b73df170b514f6773e742a495216c4/recovered.edits/0000000000000001993.temp
2014-07-01 15:54:46,440 INFO  [split-log-closeStream-1] wal.HLogSplitter: Closed wap hdfs://master:54310/hbase/data/default/usertable/122e92eabfab38cb15a9965b21db25c5/recovered.edits/0000000000000004701.temp (wrote 15 edits in 138ms)
2014-07-01 15:54:46,458 DEBUG [regionserver60020-EventThread] regionserver.SplitLogWorker: tasks arrived or departed
2014-07-01 15:54:46,458 INFO  [split-log-closeStream-1] wal.HLogSplitter: Closed wap hdfs://master:54310/hbase/data/default/usertable/1dc0aa61f0e3f43a53375118eb48213f/recovered.edits/0000000000000001998.temp (wrote 9 edits in 90ms)
2014-07-01 15:54:46,459 INFO  [split-log-closeStream-3] wal.HLogSplitter: Closed wap hdfs://master:54310/hbase/data/default/usertable/99b73df170b514f6773e742a495216c4/recovered.edits/0000000000000001993.temp (wrote 10 edits in 98ms)
2014-07-01 15:54:46,459 INFO  [split-log-closeStream-3] wal.HLogSplitter: Closed wap hdfs://master:54310/hbase/data/default/usertable/93253e37c284f3174309fede3f32339f/recovered.edits/0000000000000004707.temp (wrote 17 edits in 129ms)
2014-07-01 15:54:46,459 INFO  [split-log-closeStream-2] wal.HLogSplitter: Closed wap hdfs://master:54310/hbase/data/default/usertable/52290d336af5a689eef6040bafbe6b15/recovered.edits/0000000000000001993.temp (wrote 10 edits in 94ms)
2014-07-01 15:54:46,459 INFO  [split-log-closeStream-2] wal.HLogSplitter: Closed wap hdfs://master:54310/hbase/data/default/usertable/667c1f2c34fe37d48ba25a789d8aba98/recovered.edits/0000000000000004666.temp (wrote 15 edits in 139ms)
2014-07-01 15:54:46,459 DEBUG [SplitLogWorker-sceplus-vm48.almaden.ibm.com,60020,1404255227840] regionserver.SplitLogWorker: Current region server sceplus-vm48.almaden.ibm.com,60020,1404255227840 has 2 tasks in progress and can't take more.
2014-07-01 15:54:46,459 DEBUG [split-log-closeStream-1] wal.HLogSplitter: Rename hdfs://master:54310/hbase/data/default/usertable/122e92eabfab38cb15a9965b21db25c5/recovered.edits/0000000000000004701.temp to hdfs://master:54310/hbase/data/default/usertable/122e92eabfab38cb15a9965b21db25c5/recovered.edits/0000000000000004715
2014-07-01 15:54:46,460 DEBUG [split-log-closeStream-1] wal.HLogSplitter: Closing hdfs://master:54310/hbase/data/default/usertable/9cef9e7d34f96f1d9b111be50040a208/recovered.edits/0000000000000005689.temp
2014-07-01 15:54:46,463 DEBUG [split-log-closeStream-1] wal.HLogSplitter: Rename hdfs://master:54310/hbase/data/default/usertable/1dc0aa61f0e3f43a53375118eb48213f/recovered.edits/0000000000000001998.temp to hdfs://master:54310/hbase/data/default/usertable/1dc0aa61f0e3f43a53375118eb48213f/recovered.edits/0000000000000002007
2014-07-01 15:54:46,463 DEBUG [split-log-closeStream-1] wal.HLogSplitter: Closing hdfs://master:54310/hbase/data/default/usertable/c72fc9d54f680bcd54e9fa888600346b/recovered.edits/0000000000000002002.temp
2014-07-01 15:54:46,464 DEBUG [split-log-closeStream-3] wal.HLogSplitter: Rename hdfs://master:54310/hbase/data/default/usertable/99b73df170b514f6773e742a495216c4/recovered.edits/0000000000000001993.temp to hdfs://master:54310/hbase/data/default/usertable/99b73df170b514f6773e742a495216c4/recovered.edits/0000000000000002002
2014-07-01 15:54:46,464 DEBUG [split-log-closeStream-3] wal.HLogSplitter: Rename hdfs://master:54310/hbase/data/default/usertable/93253e37c284f3174309fede3f32339f/recovered.edits/0000000000000004707.temp to hdfs://master:54310/hbase/data/default/usertable/93253e37c284f3174309fede3f32339f/recovered.edits/0000000000000004723
2014-07-01 15:54:46,464 DEBUG [split-log-closeStream-3] wal.HLogSplitter: Closing hdfs://master:54310/hbase/data/default/usertable/bb79ae773f71289e0745068f68a0abd9/recovered.edits/0000000000000004653.temp
2014-07-01 15:54:46,467 DEBUG [split-log-closeStream-2] wal.HLogSplitter: Rename hdfs://master:54310/hbase/data/default/usertable/667c1f2c34fe37d48ba25a789d8aba98/recovered.edits/0000000000000004666.temp to hdfs://master:54310/hbase/data/default/usertable/667c1f2c34fe37d48ba25a789d8aba98/recovered.edits/0000000000000004681
2014-07-01 15:54:46,467 DEBUG [split-log-closeStream-2] wal.HLogSplitter: Rename hdfs://master:54310/hbase/data/default/usertable/52290d336af5a689eef6040bafbe6b15/recovered.edits/0000000000000001993.temp to hdfs://master:54310/hbase/data/default/usertable/52290d336af5a689eef6040bafbe6b15/recovered.edits/0000000000000002002
2014-07-01 15:54:46,467 INFO  [split-log-closeStream-1] wal.HLogSplitter: Closed wap hdfs://master:54310/hbase/data/default/usertable/9cef9e7d34f96f1d9b111be50040a208/recovered.edits/0000000000000005689.temp (wrote 47 edits in 127ms)
2014-07-01 15:54:46,469 INFO  [split-log-closeStream-1] wal.HLogSplitter: Closed wap hdfs://master:54310/hbase/data/default/usertable/c72fc9d54f680bcd54e9fa888600346b/recovered.edits/0000000000000002002.temp (wrote 8 edits in 121ms)
2014-07-01 15:54:46,470 INFO  [split-log-closeStream-3] wal.HLogSplitter: Closed wap hdfs://master:54310/hbase/data/default/usertable/bb79ae773f71289e0745068f68a0abd9/recovered.edits/0000000000000004653.temp (wrote 17 edits in 60ms)
2014-07-01 15:54:46,471 DEBUG [split-log-closeStream-1] wal.HLogSplitter: Rename hdfs://master:54310/hbase/data/default/usertable/9cef9e7d34f96f1d9b111be50040a208/recovered.edits/0000000000000005689.temp to hdfs://master:54310/hbase/data/default/usertable/9cef9e7d34f96f1d9b111be50040a208/recovered.edits/0000000000000005735
2014-07-01 15:54:46,472 DEBUG [split-log-closeStream-1] wal.HLogSplitter: Rename hdfs://master:54310/hbase/data/default/usertable/c72fc9d54f680bcd54e9fa888600346b/recovered.edits/0000000000000002002.temp to hdfs://master:54310/hbase/data/default/usertable/c72fc9d54f680bcd54e9fa888600346b/recovered.edits/0000000000000002009
2014-07-01 15:54:46,472 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:60020-1] wal.HLogSplitter: Processed 37 edits across 4 regions; log file=hdfs://master:54310/hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1404253870530-splitting/sceplus-vm48.almaden.ibm.com%2C60020%2C1404253870530.1404254549308 is corrupted = false progress failed = false
2014-07-01 15:54:46,475 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:60020-1] handler.HLogSplitterHandler: successfully transitioned task /hbase/splitWAL/WALs%2Fsceplus-vm48.almaden.ibm.com%2C60020%2C1404253870530-splitting%2Fsceplus-vm48.almaden.ibm.com%252C60020%252C1404253870530.1404254549308 to final state DONE sceplus-vm48.almaden.ibm.com,60020,1404255227840
2014-07-01 15:54:46,476 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:60020-1] handler.HLogSplitterHandler: worker sceplus-vm48.almaden.ibm.com,60020,1404255227840 done with task /hbase/splitWAL/WALs%2Fsceplus-vm48.almaden.ibm.com%2C60020%2C1404253870530-splitting%2Fsceplus-vm48.almaden.ibm.com%252C60020%252C1404253870530.1404254549308 in 4795ms
2014-07-01 15:54:46,488 DEBUG [regionserver60020-EventThread] regionserver.SplitLogWorker: tasks arrived or departed
2014-07-01 15:54:46,500 INFO  [SplitLogWorker-sceplus-vm48.almaden.ibm.com,60020,1404255227840] regionserver.SplitLogWorker: worker sceplus-vm48.almaden.ibm.com,60020,1404255227840 acquired task /hbase/splitWAL/WALs%2Fsceplus-vm48.almaden.ibm.com%2C60020%2C1404253870530-splitting%2Fsceplus-vm48.almaden.ibm.com%252C60020%252C1404253870530.1404254471168
2014-07-01 15:54:46,501 DEBUG [split-log-closeStream-3] wal.HLogSplitter: Rename hdfs://master:54310/hbase/data/default/usertable/bb79ae773f71289e0745068f68a0abd9/recovered.edits/0000000000000004653.temp to hdfs://master:54310/hbase/data/default/usertable/bb79ae773f71289e0745068f68a0abd9/recovered.edits/0000000000000004669
2014-07-01 15:54:46,501 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:60020-0] wal.HLogSplitter: Processed 111 edits across 5 regions; log file=hdfs://master:54310/hbase/WALs/slave1,60020,1404253868846-splitting/slave1%2C60020%2C1404253868846.1404254543413 is corrupted = false progress failed = false
2014-07-01 15:54:46,507 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:60020-0] handler.HLogSplitterHandler: successfully transitioned task /hbase/splitWAL/WALs%2Fslave1%2C60020%2C1404253868846-splitting%2Fslave1%252C60020%252C1404253868846.1404254543413 to final state DONE sceplus-vm48.almaden.ibm.com,60020,1404255227840
2014-07-01 15:54:46,507 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:60020-0] handler.HLogSplitterHandler: worker sceplus-vm48.almaden.ibm.com,60020,1404255227840 done with task /hbase/splitWAL/WALs%2Fslave1%2C60020%2C1404253868846-splitting%2Fslave1%252C60020%252C1404253868846.1404254543413 in 1397ms
2014-07-01 15:54:46,520 DEBUG [regionserver60020-EventThread] regionserver.SplitLogWorker: tasks arrived or departed
2014-07-01 15:54:46,555 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:60020-1] wal.HLogSplitter: Splitting hlog: hdfs://master:54310/hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1404253870530-splitting/sceplus-vm48.almaden.ibm.com%2C60020%2C1404253870530.1404254471168, length=81024835
2014-07-01 15:54:46,555 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:60020-1] wal.HLogSplitter: DistributedLogReplay = false
2014-07-01 15:54:46,558 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:60020-1] util.FSHDFSUtils: Recovering lease on dfs file hdfs://master:54310/hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1404253870530-splitting/sceplus-vm48.almaden.ibm.com%2C60020%2C1404253870530.1404254471168
2014-07-01 15:54:46,560 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:60020-1] util.FSHDFSUtils: recoverLease=true, attempt=0 on file=hdfs://master:54310/hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1404253870530-splitting/sceplus-vm48.almaden.ibm.com%2C60020%2C1404253870530.1404254471168 after 2ms
2014-07-01 15:54:46,584 DEBUG [RS_LOG_REPLAY_OPS-sceplus-vm48:60020-1-Writer-0] wal.HLogSplitter: Writer thread Thread[RS_LOG_REPLAY_OPS-sceplus-vm48:60020-1-Writer-0,5,main]: starting
2014-07-01 15:54:46,584 DEBUG [RS_LOG_REPLAY_OPS-sceplus-vm48:60020-1-Writer-1] wal.HLogSplitter: Writer thread Thread[RS_LOG_REPLAY_OPS-sceplus-vm48:60020-1-Writer-1,5,main]: starting
2014-07-01 15:54:46,585 DEBUG [RS_LOG_REPLAY_OPS-sceplus-vm48:60020-1-Writer-2] wal.HLogSplitter: Writer thread Thread[RS_LOG_REPLAY_OPS-sceplus-vm48:60020-1-Writer-2,5,main]: starting
2014-07-01 15:54:46,634 DEBUG [RS_LOG_REPLAY_OPS-sceplus-vm48:60020-1-Writer-2] wal.HLogSplitter: Creating writer path=hdfs://master:54310/hbase/data/default/usertable/1dc0aa61f0e3f43a53375118eb48213f/recovered.edits/0000000000000001821.temp region=1dc0aa61f0e3f43a53375118eb48213f
2014-07-01 15:54:46,655 DEBUG [RS_LOG_REPLAY_OPS-sceplus-vm48:60020-1-Writer-1] wal.HLogSplitter: Creating writer path=hdfs://master:54310/hbase/data/default/usertable/52290d336af5a689eef6040bafbe6b15/recovered.edits/0000000000000001811.temp region=52290d336af5a689eef6040bafbe6b15
2014-07-01 15:54:46,767 DEBUG [RS_LOG_REPLAY_OPS-sceplus-vm48:60020-1-Writer-2] wal.HLogSplitter: Creating writer path=hdfs://master:54310/hbase/data/default/usertable/99b73df170b514f6773e742a495216c4/recovered.edits/0000000000000001804.temp region=99b73df170b514f6773e742a495216c4
2014-07-01 15:54:47,072 DEBUG [RS_LOG_REPLAY_OPS-sceplus-vm48:60020-1-Writer-1] wal.HLogSplitter: Creating writer path=hdfs://master:54310/hbase/data/default/usertable/c72fc9d54f680bcd54e9fa888600346b/recovered.edits/0000000000000001832.temp region=c72fc9d54f680bcd54e9fa888600346b
2014-07-01 15:54:47,671 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:60020-1] wal.HLogSplitter: Finishing writing output logs and closing down.
2014-07-01 15:54:47,671 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:60020-1] wal.HLogSplitter: Waiting for split writer threads to finish
2014-07-01 15:54:47,677 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:60020-1] wal.HLogSplitter: Split writers finished
2014-07-01 15:54:47,678 DEBUG [RS_LOG_REPLAY_OPS-sceplus-vm48:60020-1] wal.HLogSplitter: Submitting close of hdfs://master:54310/hbase/data/default/usertable/1dc0aa61f0e3f43a53375118eb48213f/recovered.edits/0000000000000001821.temp
2014-07-01 15:54:47,678 DEBUG [RS_LOG_REPLAY_OPS-sceplus-vm48:60020-1] wal.HLogSplitter: Submitting close of hdfs://master:54310/hbase/data/default/usertable/52290d336af5a689eef6040bafbe6b15/recovered.edits/0000000000000001811.temp
2014-07-01 15:54:47,678 DEBUG [split-log-closeStream-1] wal.HLogSplitter: Closing hdfs://master:54310/hbase/data/default/usertable/1dc0aa61f0e3f43a53375118eb48213f/recovered.edits/0000000000000001821.temp
2014-07-01 15:54:47,678 DEBUG [RS_LOG_REPLAY_OPS-sceplus-vm48:60020-1] wal.HLogSplitter: Submitting close of hdfs://master:54310/hbase/data/default/usertable/99b73df170b514f6773e742a495216c4/recovered.edits/0000000000000001804.temp
2014-07-01 15:54:47,678 DEBUG [split-log-closeStream-2] wal.HLogSplitter: Closing hdfs://master:54310/hbase/data/default/usertable/52290d336af5a689eef6040bafbe6b15/recovered.edits/0000000000000001811.temp
2014-07-01 15:54:47,679 DEBUG [RS_LOG_REPLAY_OPS-sceplus-vm48:60020-1] wal.HLogSplitter: Submitting close of hdfs://master:54310/hbase/data/default/usertable/c72fc9d54f680bcd54e9fa888600346b/recovered.edits/0000000000000001832.temp
2014-07-01 15:54:47,679 DEBUG [split-log-closeStream-3] wal.HLogSplitter: Closing hdfs://master:54310/hbase/data/default/usertable/99b73df170b514f6773e742a495216c4/recovered.edits/0000000000000001804.temp
2014-07-01 15:54:47,751 INFO  [split-log-closeStream-3] wal.HLogSplitter: Closed wap hdfs://master:54310/hbase/data/default/usertable/99b73df170b514f6773e742a495216c4/recovered.edits/0000000000000001804.temp (wrote 23 edits in 299ms)
2014-07-01 15:54:47,751 INFO  [split-log-closeStream-1] wal.HLogSplitter: Closed wap hdfs://master:54310/hbase/data/default/usertable/1dc0aa61f0e3f43a53375118eb48213f/recovered.edits/0000000000000001821.temp (wrote 23 edits in 236ms)
2014-07-01 15:54:47,751 INFO  [split-log-closeStream-2] wal.HLogSplitter: Closed wap hdfs://master:54310/hbase/data/default/usertable/52290d336af5a689eef6040bafbe6b15/recovered.edits/0000000000000001811.temp (wrote 21 edits in 327ms)
2014-07-01 15:54:47,753 INFO  [SplitLogWorker-sceplus-vm48.almaden.ibm.com,60020,1404255227840] regionserver.SplitLogWorker: worker sceplus-vm48.almaden.ibm.com,60020,1404255227840 acquired task /hbase/splitWAL/WALs%2Fsceplus-vm48.almaden.ibm.com%2C60020%2C1404253870530-splitting%2Fsceplus-vm48.almaden.ibm.com%252C60020%252C1404253870530.1404254540321
2014-07-01 15:54:47,756 DEBUG [split-log-closeStream-3] wal.HLogSplitter: Rename hdfs://master:54310/hbase/data/default/usertable/99b73df170b514f6773e742a495216c4/recovered.edits/0000000000000001804.temp to hdfs://master:54310/hbase/data/default/usertable/99b73df170b514f6773e742a495216c4/recovered.edits/0000000000000001826
2014-07-01 15:54:47,757 DEBUG [split-log-closeStream-3] wal.HLogSplitter: Closing hdfs://master:54310/hbase/data/default/usertable/c72fc9d54f680bcd54e9fa888600346b/recovered.edits/0000000000000001832.temp
2014-07-01 15:54:47,760 DEBUG [split-log-closeStream-1] wal.HLogSplitter: Rename hdfs://master:54310/hbase/data/default/usertable/1dc0aa61f0e3f43a53375118eb48213f/recovered.edits/0000000000000001821.temp to hdfs://master:54310/hbase/data/default/usertable/1dc0aa61f0e3f43a53375118eb48213f/recovered.edits/0000000000000001843
2014-07-01 15:54:47,760 DEBUG [split-log-closeStream-2] wal.HLogSplitter: Rename hdfs://master:54310/hbase/data/default/usertable/52290d336af5a689eef6040bafbe6b15/recovered.edits/0000000000000001811.temp to hdfs://master:54310/hbase/data/default/usertable/52290d336af5a689eef6040bafbe6b15/recovered.edits/0000000000000001831
2014-07-01 15:54:47,770 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:60020-0] wal.HLogSplitter: Splitting hlog: hdfs://master:54310/hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1404253870530-splitting/sceplus-vm48.almaden.ibm.com%2C60020%2C1404253870530.1404254540321, length=69481408
2014-07-01 15:54:47,770 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:60020-0] wal.HLogSplitter: DistributedLogReplay = false
2014-07-01 15:54:47,774 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:60020-0] util.FSHDFSUtils: Recovering lease on dfs file hdfs://master:54310/hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1404253870530-splitting/sceplus-vm48.almaden.ibm.com%2C60020%2C1404253870530.1404254540321
2014-07-01 15:54:47,774 INFO  [split-log-closeStream-3] wal.HLogSplitter: Closed wap hdfs://master:54310/hbase/data/default/usertable/c72fc9d54f680bcd54e9fa888600346b/recovered.edits/0000000000000001832.temp (wrote 15 edits in 201ms)
2014-07-01 15:54:47,775 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:60020-0] util.FSHDFSUtils: recoverLease=true, attempt=0 on file=hdfs://master:54310/hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1404253870530-splitting/sceplus-vm48.almaden.ibm.com%2C60020%2C1404253870530.1404254540321 after 1ms
2014-07-01 15:54:47,827 DEBUG [RS_LOG_REPLAY_OPS-sceplus-vm48:60020-0-Writer-0] wal.HLogSplitter: Writer thread Thread[RS_LOG_REPLAY_OPS-sceplus-vm48:60020-0-Writer-0,5,main]: starting
2014-07-01 15:54:47,827 DEBUG [RS_LOG_REPLAY_OPS-sceplus-vm48:60020-0-Writer-1] wal.HLogSplitter: Writer thread Thread[RS_LOG_REPLAY_OPS-sceplus-vm48:60020-0-Writer-1,5,main]: starting
2014-07-01 15:54:47,828 DEBUG [RS_LOG_REPLAY_OPS-sceplus-vm48:60020-0-Writer-2] wal.HLogSplitter: Writer thread Thread[RS_LOG_REPLAY_OPS-sceplus-vm48:60020-0-Writer-2,5,main]: starting
2014-07-01 15:54:47,834 DEBUG [split-log-closeStream-3] wal.HLogSplitter: Rename hdfs://master:54310/hbase/data/default/usertable/c72fc9d54f680bcd54e9fa888600346b/recovered.edits/0000000000000001832.temp to hdfs://master:54310/hbase/data/default/usertable/c72fc9d54f680bcd54e9fa888600346b/recovered.edits/0000000000000001846
2014-07-01 15:54:47,835 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:60020-1] wal.HLogSplitter: Processed 82 edits across 4 regions; log file=hdfs://master:54310/hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1404253870530-splitting/sceplus-vm48.almaden.ibm.com%2C60020%2C1404253870530.1404254471168 is corrupted = false progress failed = false
2014-07-01 15:54:47,840 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:60020-1] handler.HLogSplitterHandler: successfully transitioned task /hbase/splitWAL/WALs%2Fsceplus-vm48.almaden.ibm.com%2C60020%2C1404253870530-splitting%2Fsceplus-vm48.almaden.ibm.com%252C60020%252C1404253870530.1404254471168 to final state DONE sceplus-vm48.almaden.ibm.com,60020,1404255227840
2014-07-01 15:54:47,840 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:60020-1] handler.HLogSplitterHandler: worker sceplus-vm48.almaden.ibm.com,60020,1404255227840 done with task /hbase/splitWAL/WALs%2Fsceplus-vm48.almaden.ibm.com%2C60020%2C1404253870530-splitting%2Fsceplus-vm48.almaden.ibm.com%252C60020%252C1404253870530.1404254471168 in 1340ms
2014-07-01 15:54:47,884 DEBUG [RS_LOG_REPLAY_OPS-sceplus-vm48:60020-0-Writer-2] wal.HLogSplitter: Creating writer path=hdfs://master:54310/hbase/data/default/usertable/99b73df170b514f6773e742a495216c4/recovered.edits/0000000000000001942.temp region=99b73df170b514f6773e742a495216c4
2014-07-01 15:54:47,912 DEBUG [RS_LOG_REPLAY_OPS-sceplus-vm48:60020-0-Writer-1] wal.HLogSplitter: Creating writer path=hdfs://master:54310/hbase/data/default/usertable/52290d336af5a689eef6040bafbe6b15/recovered.edits/0000000000000001943.temp region=52290d336af5a689eef6040bafbe6b15
2014-07-01 15:54:47,931 DEBUG [RS_LOG_REPLAY_OPS-sceplus-vm48:60020-0-Writer-0] wal.HLogSplitter: Creating writer path=hdfs://master:54310/hbase/data/default/usertable/c72fc9d54f680bcd54e9fa888600346b/recovered.edits/0000000000000001952.temp region=c72fc9d54f680bcd54e9fa888600346b
2014-07-01 15:54:48,049 DEBUG [RS_LOG_REPLAY_OPS-sceplus-vm48:60020-0-Writer-2] wal.HLogSplitter: Creating writer path=hdfs://master:54310/hbase/data/default/usertable/1dc0aa61f0e3f43a53375118eb48213f/recovered.edits/0000000000000001948.temp region=1dc0aa61f0e3f43a53375118eb48213f
2014-07-01 15:54:48,641 INFO  [SplitLogWorker-sceplus-vm48.almaden.ibm.com,60020,1404255227840] regionserver.SplitLogWorker: worker sceplus-vm48.almaden.ibm.com,60020,1404255227840 acquired task /hbase/splitWAL/WALs%2Fslave1%2C60020%2C1404253868846-splitting%2Fslave1%252C60020%252C1404253868846.1404254533427
2014-07-01 15:54:48,661 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:60020-1] wal.HLogSplitter: Splitting hlog: hdfs://master:54310/hbase/WALs/slave1,60020,1404253868846-splitting/slave1%2C60020%2C1404253868846.1404254533427, length=67322834
2014-07-01 15:54:48,661 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:60020-1] wal.HLogSplitter: DistributedLogReplay = false
2014-07-01 15:54:48,667 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:60020-1] util.FSHDFSUtils: Recovering lease on dfs file hdfs://master:54310/hbase/WALs/slave1,60020,1404253868846-splitting/slave1%2C60020%2C1404253868846.1404254533427
2014-07-01 15:54:48,669 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:60020-1] util.FSHDFSUtils: recoverLease=true, attempt=0 on file=hdfs://master:54310/hbase/WALs/slave1,60020,1404253868846-splitting/slave1%2C60020%2C1404253868846.1404254533427 after 2ms
2014-07-01 15:54:48,716 DEBUG [RS_LOG_REPLAY_OPS-sceplus-vm48:60020-1-Writer-0] wal.HLogSplitter: Writer thread Thread[RS_LOG_REPLAY_OPS-sceplus-vm48:60020-1-Writer-0,5,main]: starting
2014-07-01 15:54:48,716 DEBUG [RS_LOG_REPLAY_OPS-sceplus-vm48:60020-1-Writer-1] wal.HLogSplitter: Writer thread Thread[RS_LOG_REPLAY_OPS-sceplus-vm48:60020-1-Writer-1,5,main]: starting
2014-07-01 15:54:48,716 DEBUG [RS_LOG_REPLAY_OPS-sceplus-vm48:60020-1-Writer-2] wal.HLogSplitter: Writer thread Thread[RS_LOG_REPLAY_OPS-sceplus-vm48:60020-1-Writer-2,5,main]: starting
2014-07-01 15:54:48,739 DEBUG [RS_LOG_REPLAY_OPS-sceplus-vm48:60020-1-Writer-2] wal.HLogSplitter: Creating writer path=hdfs://master:54310/hbase/data/default/usertable/9cef9e7d34f96f1d9b111be50040a208/recovered.edits/0000000000000005539.temp region=9cef9e7d34f96f1d9b111be50040a208
2014-07-01 15:54:48,761 DEBUG [RS_LOG_REPLAY_OPS-sceplus-vm48:60020-1-Writer-0] wal.HLogSplitter: Creating writer path=hdfs://master:54310/hbase/data/default/usertable/122e92eabfab38cb15a9965b21db25c5/recovered.edits/0000000000000004652.temp region=122e92eabfab38cb15a9965b21db25c5
2014-07-01 15:54:48,771 DEBUG [RS_LOG_REPLAY_OPS-sceplus-vm48:60020-1-Writer-1] wal.HLogSplitter: Creating writer path=hdfs://master:54310/hbase/data/default/usertable/667c1f2c34fe37d48ba25a789d8aba98/recovered.edits/0000000000000004617.temp region=667c1f2c34fe37d48ba25a789d8aba98
2014-07-01 15:54:48,797 DEBUG [RS_LOG_REPLAY_OPS-sceplus-vm48:60020-1-Writer-2] wal.HLogSplitter: Creating writer path=hdfs://master:54310/hbase/data/default/usertable/bb79ae773f71289e0745068f68a0abd9/recovered.edits/0000000000000004606.temp region=bb79ae773f71289e0745068f68a0abd9
2014-07-01 15:54:48,816 DEBUG [RS_LOG_REPLAY_OPS-sceplus-vm48:60020-1-Writer-0] wal.HLogSplitter: Creating writer path=hdfs://master:54310/hbase/data/default/usertable/93253e37c284f3174309fede3f32339f/recovered.edits/0000000000000004660.temp region=93253e37c284f3174309fede3f32339f
2014-07-01 15:54:48,967 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:60020-0] wal.HLogSplitter: Finishing writing output logs and closing down.
2014-07-01 15:54:48,968 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:60020-0] wal.HLogSplitter: Waiting for split writer threads to finish
2014-07-01 15:54:48,973 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:60020-0] wal.HLogSplitter: Split writers finished
2014-07-01 15:54:48,973 DEBUG [RS_LOG_REPLAY_OPS-sceplus-vm48:60020-0] wal.HLogSplitter: Submitting close of hdfs://master:54310/hbase/data/default/usertable/1dc0aa61f0e3f43a53375118eb48213f/recovered.edits/0000000000000001948.temp
2014-07-01 15:54:48,974 DEBUG [RS_LOG_REPLAY_OPS-sceplus-vm48:60020-0] wal.HLogSplitter: Submitting close of hdfs://master:54310/hbase/data/default/usertable/52290d336af5a689eef6040bafbe6b15/recovered.edits/0000000000000001943.temp
2014-07-01 15:54:48,974 DEBUG [split-log-closeStream-1] wal.HLogSplitter: Closing hdfs://master:54310/hbase/data/default/usertable/1dc0aa61f0e3f43a53375118eb48213f/recovered.edits/0000000000000001948.temp
2014-07-01 15:54:48,975 DEBUG [RS_LOG_REPLAY_OPS-sceplus-vm48:60020-0] wal.HLogSplitter: Submitting close of hdfs://master:54310/hbase/data/default/usertable/99b73df170b514f6773e742a495216c4/recovered.edits/0000000000000001942.temp
2014-07-01 15:54:48,975 DEBUG [split-log-closeStream-2] wal.HLogSplitter: Closing hdfs://master:54310/hbase/data/default/usertable/52290d336af5a689eef6040bafbe6b15/recovered.edits/0000000000000001943.temp
2014-07-01 15:54:48,975 DEBUG [RS_LOG_REPLAY_OPS-sceplus-vm48:60020-0] wal.HLogSplitter: Submitting close of hdfs://master:54310/hbase/data/default/usertable/c72fc9d54f680bcd54e9fa888600346b/recovered.edits/0000000000000001952.temp
2014-07-01 15:54:48,975 DEBUG [split-log-closeStream-3] wal.HLogSplitter: Closing hdfs://master:54310/hbase/data/default/usertable/99b73df170b514f6773e742a495216c4/recovered.edits/0000000000000001942.temp
2014-07-01 15:54:48,984 INFO  [split-log-closeStream-1] wal.HLogSplitter: Closed wap hdfs://master:54310/hbase/data/default/usertable/1dc0aa61f0e3f43a53375118eb48213f/recovered.edits/0000000000000001948.temp (wrote 16 edits in 310ms)
2014-07-01 15:54:48,985 INFO  [split-log-closeStream-3] wal.HLogSplitter: Closed wap hdfs://master:54310/hbase/data/default/usertable/99b73df170b514f6773e742a495216c4/recovered.edits/0000000000000001942.temp (wrote 17 edits in 287ms)
2014-07-01 15:54:48,986 INFO  [split-log-closeStream-2] wal.HLogSplitter: Closed wap hdfs://master:54310/hbase/data/default/usertable/52290d336af5a689eef6040bafbe6b15/recovered.edits/0000000000000001943.temp (wrote 17 edits in 288ms)
2014-07-01 15:54:48,989 DEBUG [split-log-closeStream-1] wal.HLogSplitter: Rename hdfs://master:54310/hbase/data/default/usertable/1dc0aa61f0e3f43a53375118eb48213f/recovered.edits/0000000000000001948.temp to hdfs://master:54310/hbase/data/default/usertable/1dc0aa61f0e3f43a53375118eb48213f/recovered.edits/0000000000000001963
2014-07-01 15:54:48,989 DEBUG [split-log-closeStream-1] wal.HLogSplitter: Closing hdfs://master:54310/hbase/data/default/usertable/c72fc9d54f680bcd54e9fa888600346b/recovered.edits/0000000000000001952.temp
2014-07-01 15:54:48,990 DEBUG [split-log-closeStream-3] wal.HLogSplitter: Rename hdfs://master:54310/hbase/data/default/usertable/99b73df170b514f6773e742a495216c4/recovered.edits/0000000000000001942.temp to hdfs://master:54310/hbase/data/default/usertable/99b73df170b514f6773e742a495216c4/recovered.edits/0000000000000001958
2014-07-01 15:54:49,004 DEBUG [split-log-closeStream-2] wal.HLogSplitter: Rename hdfs://master:54310/hbase/data/default/usertable/52290d336af5a689eef6040bafbe6b15/recovered.edits/0000000000000001943.temp to hdfs://master:54310/hbase/data/default/usertable/52290d336af5a689eef6040bafbe6b15/recovered.edits/0000000000000001959
2014-07-01 15:54:49,041 INFO  [split-log-closeStream-1] wal.HLogSplitter: Closed wap hdfs://master:54310/hbase/data/default/usertable/c72fc9d54f680bcd54e9fa888600346b/recovered.edits/0000000000000001952.temp (wrote 19 edits in 294ms)
2014-07-01 15:54:49,081 DEBUG [split-log-closeStream-1] wal.HLogSplitter: Rename hdfs://master:54310/hbase/data/default/usertable/c72fc9d54f680bcd54e9fa888600346b/recovered.edits/0000000000000001952.temp to hdfs://master:54310/hbase/data/default/usertable/c72fc9d54f680bcd54e9fa888600346b/recovered.edits/0000000000000001970
2014-07-01 15:54:49,082 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:60020-0] wal.HLogSplitter: Processed 69 edits across 4 regions; log file=hdfs://master:54310/hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1404253870530-splitting/sceplus-vm48.almaden.ibm.com%2C60020%2C1404253870530.1404254540321 is corrupted = false progress failed = false
2014-07-01 15:54:49,086 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:60020-0] handler.HLogSplitterHandler: successfully transitioned task /hbase/splitWAL/WALs%2Fsceplus-vm48.almaden.ibm.com%2C60020%2C1404253870530-splitting%2Fsceplus-vm48.almaden.ibm.com%252C60020%252C1404253870530.1404254540321 to final state DONE sceplus-vm48.almaden.ibm.com,60020,1404255227840
2014-07-01 15:54:49,086 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:60020-0] handler.HLogSplitterHandler: worker sceplus-vm48.almaden.ibm.com,60020,1404255227840 done with task /hbase/splitWAL/WALs%2Fsceplus-vm48.almaden.ibm.com%2C60020%2C1404253870530-splitting%2Fsceplus-vm48.almaden.ibm.com%252C60020%252C1404253870530.1404254540321 in 1333ms
2014-07-01 15:54:49,603 INFO  [SplitLogWorker-sceplus-vm48.almaden.ibm.com,60020,1404255227840] regionserver.SplitLogWorker: worker sceplus-vm48.almaden.ibm.com,60020,1404255227840 acquired task /hbase/splitWAL/WALs%2Fslave1%2C60020%2C1404253868846-splitting%2Fslave1%252C60020%252C1404253868846.1404254547570
2014-07-01 15:54:49,630 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:60020-0] wal.HLogSplitter: Splitting hlog: hdfs://master:54310/hbase/WALs/slave1,60020,1404253868846-splitting/slave1%2C60020%2C1404253868846.1404254547570, length=0
2014-07-01 15:54:49,630 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:60020-0] wal.HLogSplitter: DistributedLogReplay = false
2014-07-01 15:54:49,644 WARN  [RS_LOG_REPLAY_OPS-sceplus-vm48:60020-0] wal.HLogSplitter: File hdfs://master:54310/hbase/WALs/slave1,60020,1404253868846-splitting/slave1%2C60020%2C1404253868846.1404254547570 might be still open, length is 0
2014-07-01 15:54:49,644 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:60020-0] util.FSHDFSUtils: Recovering lease on dfs file hdfs://master:54310/hbase/WALs/slave1,60020,1404253868846-splitting/slave1%2C60020%2C1404253868846.1404254547570
2014-07-01 15:54:49,647 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:60020-0] util.FSHDFSUtils: recoverLease=false, attempt=0 on file=hdfs://master:54310/hbase/WALs/slave1,60020,1404253868846-splitting/slave1%2C60020%2C1404253868846.1404254547570 after 3ms
2014-07-01 15:54:50,286 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:60020-1] wal.HLogSplitter: Finishing writing output logs and closing down.
2014-07-01 15:54:50,286 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:60020-1] wal.HLogSplitter: Waiting for split writer threads to finish
2014-07-01 15:54:50,287 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:60020-1] wal.HLogSplitter: Split writers finished
2014-07-01 15:54:50,288 DEBUG [RS_LOG_REPLAY_OPS-sceplus-vm48:60020-1] wal.HLogSplitter: Submitting close of hdfs://master:54310/hbase/data/default/usertable/122e92eabfab38cb15a9965b21db25c5/recovered.edits/0000000000000004652.temp
2014-07-01 15:54:50,288 DEBUG [RS_LOG_REPLAY_OPS-sceplus-vm48:60020-1] wal.HLogSplitter: Submitting close of hdfs://master:54310/hbase/data/default/usertable/667c1f2c34fe37d48ba25a789d8aba98/recovered.edits/0000000000000004617.temp
2014-07-01 15:54:50,288 DEBUG [split-log-closeStream-1] wal.HLogSplitter: Closing hdfs://master:54310/hbase/data/default/usertable/122e92eabfab38cb15a9965b21db25c5/recovered.edits/0000000000000004652.temp
2014-07-01 15:54:50,288 DEBUG [RS_LOG_REPLAY_OPS-sceplus-vm48:60020-1] wal.HLogSplitter: Submitting close of hdfs://master:54310/hbase/data/default/usertable/93253e37c284f3174309fede3f32339f/recovered.edits/0000000000000004660.temp
2014-07-01 15:54:50,288 DEBUG [split-log-closeStream-2] wal.HLogSplitter: Closing hdfs://master:54310/hbase/data/default/usertable/667c1f2c34fe37d48ba25a789d8aba98/recovered.edits/0000000000000004617.temp
2014-07-01 15:54:50,288 DEBUG [RS_LOG_REPLAY_OPS-sceplus-vm48:60020-1] wal.HLogSplitter: Submitting close of hdfs://master:54310/hbase/data/default/usertable/9cef9e7d34f96f1d9b111be50040a208/recovered.edits/0000000000000005539.temp
2014-07-01 15:54:50,288 DEBUG [RS_LOG_REPLAY_OPS-sceplus-vm48:60020-1] wal.HLogSplitter: Submitting close of hdfs://master:54310/hbase/data/default/usertable/bb79ae773f71289e0745068f68a0abd9/recovered.edits/0000000000000004606.temp
2014-07-01 15:54:50,288 DEBUG [split-log-closeStream-3] wal.HLogSplitter: Closing hdfs://master:54310/hbase/data/default/usertable/93253e37c284f3174309fede3f32339f/recovered.edits/0000000000000004660.temp
2014-07-01 15:54:50,293 INFO  [split-log-closeStream-1] wal.HLogSplitter: Closed wap hdfs://master:54310/hbase/data/default/usertable/122e92eabfab38cb15a9965b21db25c5/recovered.edits/0000000000000004652.temp (wrote 16 edits in 224ms)
2014-07-01 15:54:50,299 INFO  [split-log-closeStream-3] wal.HLogSplitter: Closed wap hdfs://master:54310/hbase/data/default/usertable/93253e37c284f3174309fede3f32339f/recovered.edits/0000000000000004660.temp (wrote 15 edits in 420ms)
2014-07-01 15:54:50,300 INFO  [split-log-closeStream-2] wal.HLogSplitter: Closed wap hdfs://master:54310/hbase/data/default/usertable/667c1f2c34fe37d48ba25a789d8aba98/recovered.edits/0000000000000004617.temp (wrote 16 edits in 317ms)
2014-07-01 15:54:50,304 DEBUG [split-log-closeStream-1] wal.HLogSplitter: Rename hdfs://master:54310/hbase/data/default/usertable/122e92eabfab38cb15a9965b21db25c5/recovered.edits/0000000000000004652.temp to hdfs://master:54310/hbase/data/default/usertable/122e92eabfab38cb15a9965b21db25c5/recovered.edits/0000000000000004667
2014-07-01 15:54:50,304 DEBUG [split-log-closeStream-1] wal.HLogSplitter: Closing hdfs://master:54310/hbase/data/default/usertable/9cef9e7d34f96f1d9b111be50040a208/recovered.edits/0000000000000005539.temp
2014-07-01 15:54:50,310 DEBUG [split-log-closeStream-3] wal.HLogSplitter: Rename hdfs://master:54310/hbase/data/default/usertable/93253e37c284f3174309fede3f32339f/recovered.edits/0000000000000004660.temp to hdfs://master:54310/hbase/data/default/usertable/93253e37c284f3174309fede3f32339f/recovered.edits/0000000000000004674
2014-07-01 15:54:50,311 DEBUG [split-log-closeStream-3] wal.HLogSplitter: Closing hdfs://master:54310/hbase/data/default/usertable/bb79ae773f71289e0745068f68a0abd9/recovered.edits/0000000000000004606.temp
2014-07-01 15:54:50,311 DEBUG [split-log-closeStream-2] wal.HLogSplitter: Rename hdfs://master:54310/hbase/data/default/usertable/667c1f2c34fe37d48ba25a789d8aba98/recovered.edits/0000000000000004617.temp to hdfs://master:54310/hbase/data/default/usertable/667c1f2c34fe37d48ba25a789d8aba98/recovered.edits/0000000000000004632
2014-07-01 15:54:50,313 INFO  [split-log-closeStream-1] wal.HLogSplitter: Closed wap hdfs://master:54310/hbase/data/default/usertable/9cef9e7d34f96f1d9b111be50040a208/recovered.edits/0000000000000005539.temp (wrote 51 edits in 552ms)
2014-07-01 15:54:50,315 INFO  [split-log-closeStream-3] wal.HLogSplitter: Closed wap hdfs://master:54310/hbase/data/default/usertable/bb79ae773f71289e0745068f68a0abd9/recovered.edits/0000000000000004606.temp (wrote 16 edits in 92ms)
2014-07-01 15:54:50,318 DEBUG [split-log-closeStream-1] wal.HLogSplitter: Rename hdfs://master:54310/hbase/data/default/usertable/9cef9e7d34f96f1d9b111be50040a208/recovered.edits/0000000000000005539.temp to hdfs://master:54310/hbase/data/default/usertable/9cef9e7d34f96f1d9b111be50040a208/recovered.edits/0000000000000005589
2014-07-01 15:54:50,355 DEBUG [split-log-closeStream-3] wal.HLogSplitter: Rename hdfs://master:54310/hbase/data/default/usertable/bb79ae773f71289e0745068f68a0abd9/recovered.edits/0000000000000004606.temp to hdfs://master:54310/hbase/data/default/usertable/bb79ae773f71289e0745068f68a0abd9/recovered.edits/0000000000000004621
2014-07-01 15:54:50,355 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:60020-1] wal.HLogSplitter: Processed 114 edits across 5 regions; log file=hdfs://master:54310/hbase/WALs/slave1,60020,1404253868846-splitting/slave1%2C60020%2C1404253868846.1404254533427 is corrupted = false progress failed = false
2014-07-01 15:54:50,361 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:60020-1] handler.HLogSplitterHandler: successfully transitioned task /hbase/splitWAL/WALs%2Fslave1%2C60020%2C1404253868846-splitting%2Fslave1%252C60020%252C1404253868846.1404254533427 to final state DONE sceplus-vm48.almaden.ibm.com,60020,1404255227840
2014-07-01 15:54:50,361 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:60020-1] handler.HLogSplitterHandler: worker sceplus-vm48.almaden.ibm.com,60020,1404255227840 done with task /hbase/splitWAL/WALs%2Fslave1%2C60020%2C1404253868846-splitting%2Fslave1%252C60020%252C1404253868846.1404254533427 in 1719ms
2014-07-01 15:54:50,453 INFO  [SplitLogWorker-sceplus-vm48.almaden.ibm.com,60020,1404255227840] regionserver.SplitLogWorker: worker sceplus-vm48.almaden.ibm.com,60020,1404255227840 acquired task /hbase/splitWAL/WALs%2Fsceplus-vm48.almaden.ibm.com%2C60020%2C1404253870530-splitting%2Fsceplus-vm48.almaden.ibm.com%252C60020%252C1404253870530.1404254405087
2014-07-01 15:54:50,475 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:60020-1] wal.HLogSplitter: Splitting hlog: hdfs://master:54310/hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1404253870530-splitting/sceplus-vm48.almaden.ibm.com%2C60020%2C1404253870530.1404254405087, length=84122033
2014-07-01 15:54:50,475 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:60020-1] wal.HLogSplitter: DistributedLogReplay = false
2014-07-01 15:54:50,478 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:60020-1] util.FSHDFSUtils: Recovering lease on dfs file hdfs://master:54310/hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1404253870530-splitting/sceplus-vm48.almaden.ibm.com%2C60020%2C1404253870530.1404254405087
2014-07-01 15:54:50,479 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:60020-1] util.FSHDFSUtils: recoverLease=true, attempt=0 on file=hdfs://master:54310/hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1404253870530-splitting/sceplus-vm48.almaden.ibm.com%2C60020%2C1404253870530.1404254405087 after 1ms
2014-07-01 15:54:50,516 DEBUG [RS_LOG_REPLAY_OPS-sceplus-vm48:60020-1-Writer-0] wal.HLogSplitter: Writer thread Thread[RS_LOG_REPLAY_OPS-sceplus-vm48:60020-1-Writer-0,5,main]: starting
2014-07-01 15:54:50,516 DEBUG [RS_LOG_REPLAY_OPS-sceplus-vm48:60020-1-Writer-1] wal.HLogSplitter: Writer thread Thread[RS_LOG_REPLAY_OPS-sceplus-vm48:60020-1-Writer-1,5,main]: starting
2014-07-01 15:54:50,517 DEBUG [RS_LOG_REPLAY_OPS-sceplus-vm48:60020-1-Writer-2] wal.HLogSplitter: Writer thread Thread[RS_LOG_REPLAY_OPS-sceplus-vm48:60020-1-Writer-2,5,main]: starting
2014-07-01 15:54:50,552 DEBUG [RS_LOG_REPLAY_OPS-sceplus-vm48:60020-1-Writer-2] wal.HLogSplitter: Creating writer path=hdfs://master:54310/hbase/data/default/usertable/99b73df170b514f6773e742a495216c4/recovered.edits/0000000000000001437.temp region=99b73df170b514f6773e742a495216c4
2014-07-01 15:54:50,556 DEBUG [RS_LOG_REPLAY_OPS-sceplus-vm48:60020-1-Writer-0] wal.HLogSplitter: Creating writer path=hdfs://master:54310/hbase/data/default/usertable/1dc0aa61f0e3f43a53375118eb48213f/recovered.edits/0000000000000001443.temp region=1dc0aa61f0e3f43a53375118eb48213f
2014-07-01 15:54:50,583 DEBUG [RS_LOG_REPLAY_OPS-sceplus-vm48:60020-1-Writer-1] wal.HLogSplitter: Creating writer path=hdfs://master:54310/hbase/data/default/usertable/c72fc9d54f680bcd54e9fa888600346b/recovered.edits/0000000000000001452.temp region=c72fc9d54f680bcd54e9fa888600346b
2014-07-01 15:54:51,099 DEBUG [SplitLogWorker-sceplus-vm48.almaden.ibm.com,60020,1404255227840] regionserver.SplitLogWorker: Current region server sceplus-vm48.almaden.ibm.com,60020,1404255227840 has 2 tasks in progress and can't take more.
2014-07-01 15:54:51,101 DEBUG [SplitLogWorker-sceplus-vm48.almaden.ibm.com,60020,1404255227840] regionserver.SplitLogWorker: Current region server sceplus-vm48.almaden.ibm.com,60020,1404255227840 has 2 tasks in progress and can't take more.
2014-07-01 15:54:52,027 DEBUG [RS_LOG_REPLAY_OPS-sceplus-vm48:60020-1-Writer-2] wal.HLogSplitter: Creating writer path=hdfs://master:54310/hbase/data/default/usertable/52290d336af5a689eef6040bafbe6b15/recovered.edits/0000000000000001438.temp region=52290d336af5a689eef6040bafbe6b15
2014-07-01 15:54:52,037 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:60020-1] wal.HLogSplitter: Finishing writing output logs and closing down.
2014-07-01 15:54:52,037 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:60020-1] wal.HLogSplitter: Waiting for split writer threads to finish
2014-07-01 15:54:52,202 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:60020-1] wal.HLogSplitter: Split writers finished
2014-07-01 15:54:52,202 DEBUG [RS_LOG_REPLAY_OPS-sceplus-vm48:60020-1] wal.HLogSplitter: Submitting close of hdfs://master:54310/hbase/data/default/usertable/1dc0aa61f0e3f43a53375118eb48213f/recovered.edits/0000000000000001443.temp
2014-07-01 15:54:52,203 DEBUG [RS_LOG_REPLAY_OPS-sceplus-vm48:60020-1] wal.HLogSplitter: Submitting close of hdfs://master:54310/hbase/data/default/usertable/52290d336af5a689eef6040bafbe6b15/recovered.edits/0000000000000001438.temp
2014-07-01 15:54:52,203 DEBUG [split-log-closeStream-1] wal.HLogSplitter: Closing hdfs://master:54310/hbase/data/default/usertable/1dc0aa61f0e3f43a53375118eb48213f/recovered.edits/0000000000000001443.temp
2014-07-01 15:54:52,203 DEBUG [RS_LOG_REPLAY_OPS-sceplus-vm48:60020-1] wal.HLogSplitter: Submitting close of hdfs://master:54310/hbase/data/default/usertable/99b73df170b514f6773e742a495216c4/recovered.edits/0000000000000001437.temp
2014-07-01 15:54:52,203 DEBUG [split-log-closeStream-2] wal.HLogSplitter: Closing hdfs://master:54310/hbase/data/default/usertable/52290d336af5a689eef6040bafbe6b15/recovered.edits/0000000000000001438.temp
2014-07-01 15:54:52,203 DEBUG [RS_LOG_REPLAY_OPS-sceplus-vm48:60020-1] wal.HLogSplitter: Submitting close of hdfs://master:54310/hbase/data/default/usertable/c72fc9d54f680bcd54e9fa888600346b/recovered.edits/0000000000000001452.temp
2014-07-01 15:54:52,203 DEBUG [split-log-closeStream-3] wal.HLogSplitter: Closing hdfs://master:54310/hbase/data/default/usertable/99b73df170b514f6773e742a495216c4/recovered.edits/0000000000000001437.temp
2014-07-01 15:54:52,726 INFO  [split-log-closeStream-2] wal.HLogSplitter: Closed wap hdfs://master:54310/hbase/data/default/usertable/52290d336af5a689eef6040bafbe6b15/recovered.edits/0000000000000001438.temp (wrote 19 edits in 1568ms)
2014-07-01 15:54:52,726 INFO  [split-log-closeStream-1] wal.HLogSplitter: Closed wap hdfs://master:54310/hbase/data/default/usertable/1dc0aa61f0e3f43a53375118eb48213f/recovered.edits/0000000000000001443.temp (wrote 23 edits in 287ms)
2014-07-01 15:54:52,726 INFO  [split-log-closeStream-3] wal.HLogSplitter: Closed wap hdfs://master:54310/hbase/data/default/usertable/99b73df170b514f6773e742a495216c4/recovered.edits/0000000000000001437.temp (wrote 17 edits in 333ms)
2014-07-01 15:54:52,731 DEBUG [split-log-closeStream-3] wal.HLogSplitter: Rename hdfs://master:54310/hbase/data/default/usertable/99b73df170b514f6773e742a495216c4/recovered.edits/0000000000000001437.temp to hdfs://master:54310/hbase/data/default/usertable/99b73df170b514f6773e742a495216c4/recovered.edits/0000000000000001453
2014-07-01 15:54:52,732 DEBUG [split-log-closeStream-3] wal.HLogSplitter: Closing hdfs://master:54310/hbase/data/default/usertable/c72fc9d54f680bcd54e9fa888600346b/recovered.edits/0000000000000001452.temp
2014-07-01 15:54:52,733 DEBUG [split-log-closeStream-1] wal.HLogSplitter: Rename hdfs://master:54310/hbase/data/default/usertable/1dc0aa61f0e3f43a53375118eb48213f/recovered.edits/0000000000000001443.temp to hdfs://master:54310/hbase/data/default/usertable/1dc0aa61f0e3f43a53375118eb48213f/recovered.edits/0000000000000001465
2014-07-01 15:54:52,735 DEBUG [split-log-closeStream-2] wal.HLogSplitter: Rename hdfs://master:54310/hbase/data/default/usertable/52290d336af5a689eef6040bafbe6b15/recovered.edits/0000000000000001438.temp to hdfs://master:54310/hbase/data/default/usertable/52290d336af5a689eef6040bafbe6b15/recovered.edits/0000000000000001456
2014-07-01 15:54:52,774 INFO  [split-log-closeStream-3] wal.HLogSplitter: Closed wap hdfs://master:54310/hbase/data/default/usertable/c72fc9d54f680bcd54e9fa888600346b/recovered.edits/0000000000000001452.temp (wrote 25 edits in 277ms)
2014-07-01 15:54:52,778 DEBUG [split-log-closeStream-3] wal.HLogSplitter: Rename hdfs://master:54310/hbase/data/default/usertable/c72fc9d54f680bcd54e9fa888600346b/recovered.edits/0000000000000001452.temp to hdfs://master:54310/hbase/data/default/usertable/c72fc9d54f680bcd54e9fa888600346b/recovered.edits/0000000000000001476
2014-07-01 15:54:52,778 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:60020-1] wal.HLogSplitter: Processed 84 edits across 4 regions; log file=hdfs://master:54310/hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1404253870530-splitting/sceplus-vm48.almaden.ibm.com%2C60020%2C1404253870530.1404254405087 is corrupted = false progress failed = false
2014-07-01 15:54:52,789 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:60020-1] handler.HLogSplitterHandler: successfully transitioned task /hbase/splitWAL/WALs%2Fsceplus-vm48.almaden.ibm.com%2C60020%2C1404253870530-splitting%2Fsceplus-vm48.almaden.ibm.com%252C60020%252C1404253870530.1404254405087 to final state DONE sceplus-vm48.almaden.ibm.com,60020,1404255227840
2014-07-01 15:54:52,789 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:60020-1] handler.HLogSplitterHandler: worker sceplus-vm48.almaden.ibm.com,60020,1404255227840 done with task /hbase/splitWAL/WALs%2Fsceplus-vm48.almaden.ibm.com%2C60020%2C1404253870530-splitting%2Fsceplus-vm48.almaden.ibm.com%252C60020%252C1404253870530.1404254405087 in 2336ms
2014-07-01 15:54:52,803 DEBUG [regionserver60020-EventThread] regionserver.SplitLogWorker: tasks arrived or departed
2014-07-01 15:54:52,811 DEBUG [regionserver60020-EventThread] regionserver.SplitLogWorker: tasks arrived or departed
2014-07-01 15:54:52,815 INFO  [SplitLogWorker-sceplus-vm48.almaden.ibm.com,60020,1404255227840] regionserver.SplitLogWorker: worker sceplus-vm48.almaden.ibm.com,60020,1404255227840 acquired task /hbase/splitWAL/WALs%2Fsceplus-vm48.almaden.ibm.com%2C60020%2C1404253870530-splitting%2Fsceplus-vm48.almaden.ibm.com%252C60020%252C1404253870530.1404254456728
2014-07-01 15:54:52,831 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:60020-1] wal.HLogSplitter: Splitting hlog: hdfs://master:54310/hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1404253870530-splitting/sceplus-vm48.almaden.ibm.com%2C60020%2C1404253870530.1404254456728, length=74276803
2014-07-01 15:54:52,831 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:60020-1] wal.HLogSplitter: DistributedLogReplay = false
2014-07-01 15:54:52,835 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:60020-1] util.FSHDFSUtils: Recovering lease on dfs file hdfs://master:54310/hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1404253870530-splitting/sceplus-vm48.almaden.ibm.com%2C60020%2C1404253870530.1404254456728
2014-07-01 15:54:52,836 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:60020-1] util.FSHDFSUtils: recoverLease=true, attempt=0 on file=hdfs://master:54310/hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1404253870530-splitting/sceplus-vm48.almaden.ibm.com%2C60020%2C1404253870530.1404254456728 after 1ms
2014-07-01 15:54:52,894 DEBUG [RS_LOG_REPLAY_OPS-sceplus-vm48:60020-1-Writer-0] wal.HLogSplitter: Writer thread Thread[RS_LOG_REPLAY_OPS-sceplus-vm48:60020-1-Writer-0,5,main]: starting
2014-07-01 15:54:52,894 DEBUG [RS_LOG_REPLAY_OPS-sceplus-vm48:60020-1-Writer-1] wal.HLogSplitter: Writer thread Thread[RS_LOG_REPLAY_OPS-sceplus-vm48:60020-1-Writer-1,5,main]: starting
2014-07-01 15:54:52,895 DEBUG [RS_LOG_REPLAY_OPS-sceplus-vm48:60020-1-Writer-2] wal.HLogSplitter: Writer thread Thread[RS_LOG_REPLAY_OPS-sceplus-vm48:60020-1-Writer-2,5,main]: starting
2014-07-01 15:54:52,935 DEBUG [RS_LOG_REPLAY_OPS-sceplus-vm48:60020-1-Writer-2] wal.HLogSplitter: Creating writer path=hdfs://master:54310/hbase/data/default/usertable/1dc0aa61f0e3f43a53375118eb48213f/recovered.edits/0000000000000001721.temp region=1dc0aa61f0e3f43a53375118eb48213f
2014-07-01 15:54:53,264 DEBUG [RS_LOG_REPLAY_OPS-sceplus-vm48:60020-1-Writer-0] wal.HLogSplitter: Creating writer path=hdfs://master:54310/hbase/data/default/usertable/c72fc9d54f680bcd54e9fa888600346b/recovered.edits/0000000000000001735.temp region=c72fc9d54f680bcd54e9fa888600346b
2014-07-01 15:54:53,302 DEBUG [RS_LOG_REPLAY_OPS-sceplus-vm48:60020-1-Writer-0] wal.HLogSplitter: Creating writer path=hdfs://master:54310/hbase/data/default/usertable/99b73df170b514f6773e742a495216c4/recovered.edits/0000000000000001714.temp region=99b73df170b514f6773e742a495216c4
2014-07-01 15:54:53,313 DEBUG [RS_LOG_REPLAY_OPS-sceplus-vm48:60020-1-Writer-1] wal.HLogSplitter: Creating writer path=hdfs://master:54310/hbase/data/default/usertable/52290d336af5a689eef6040bafbe6b15/recovered.edits/0000000000000001715.temp region=52290d336af5a689eef6040bafbe6b15
2014-07-01 15:54:53,327 DEBUG [SplitLogWorker-sceplus-vm48.almaden.ibm.com,60020,1404255227840] regionserver.SplitLogWorker: Current region server sceplus-vm48.almaden.ibm.com,60020,1404255227840 has 2 tasks in progress and can't take more.
2014-07-01 15:54:53,330 DEBUG [SplitLogWorker-sceplus-vm48.almaden.ibm.com,60020,1404255227840] regionserver.SplitLogWorker: Current region server sceplus-vm48.almaden.ibm.com,60020,1404255227840 has 2 tasks in progress and can't take more.
2014-07-01 15:54:53,649 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:60020-0] util.FSHDFSUtils: recoverLease=true, attempt=1 on file=hdfs://master:54310/hbase/WALs/slave1,60020,1404253868846-splitting/slave1%2C60020%2C1404253868846.1404254547570 after 4005ms
2014-07-01 15:54:53,684 DEBUG [RS_LOG_REPLAY_OPS-sceplus-vm48:60020-0-Writer-0] wal.HLogSplitter: Writer thread Thread[RS_LOG_REPLAY_OPS-sceplus-vm48:60020-0-Writer-0,5,main]: starting
2014-07-01 15:54:53,686 DEBUG [RS_LOG_REPLAY_OPS-sceplus-vm48:60020-0-Writer-1] wal.HLogSplitter: Writer thread Thread[RS_LOG_REPLAY_OPS-sceplus-vm48:60020-0-Writer-1,5,main]: starting
2014-07-01 15:54:53,686 DEBUG [RS_LOG_REPLAY_OPS-sceplus-vm48:60020-0-Writer-2] wal.HLogSplitter: Writer thread Thread[RS_LOG_REPLAY_OPS-sceplus-vm48:60020-0-Writer-2,5,main]: starting
2014-07-01 15:54:53,706 DEBUG [RS_LOG_REPLAY_OPS-sceplus-vm48:60020-0-Writer-2] wal.HLogSplitter: Creating writer path=hdfs://master:54310/hbase/data/default/usertable/122e92eabfab38cb15a9965b21db25c5/recovered.edits/0000000000000004716.temp region=122e92eabfab38cb15a9965b21db25c5
2014-07-01 15:54:53,713 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:60020-1] wal.HLogSplitter: Finishing writing output logs and closing down.
2014-07-01 15:54:53,713 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:60020-1] wal.HLogSplitter: Waiting for split writer threads to finish
2014-07-01 15:54:53,734 DEBUG [RS_LOG_REPLAY_OPS-sceplus-vm48:60020-0-Writer-0] wal.HLogSplitter: Creating writer path=hdfs://master:54310/hbase/data/default/usertable/667c1f2c34fe37d48ba25a789d8aba98/recovered.edits/0000000000000004682.temp region=667c1f2c34fe37d48ba25a789d8aba98
2014-07-01 15:54:53,738 DEBUG [RS_LOG_REPLAY_OPS-sceplus-vm48:60020-0-Writer-1] wal.HLogSplitter: Creating writer path=hdfs://master:54310/hbase/data/default/usertable/bb79ae773f71289e0745068f68a0abd9/recovered.edits/0000000000000004670.temp region=bb79ae773f71289e0745068f68a0abd9
2014-07-01 15:54:53,751 DEBUG [RS_LOG_REPLAY_OPS-sceplus-vm48:60020-0-Writer-2] wal.HLogSplitter: Creating writer path=hdfs://master:54310/hbase/data/default/usertable/9cef9e7d34f96f1d9b111be50040a208/recovered.edits/0000000000000005736.temp region=9cef9e7d34f96f1d9b111be50040a208
2014-07-01 15:54:53,793 DEBUG [RS_LOG_REPLAY_OPS-sceplus-vm48:60020-0-Writer-1] wal.HLogSplitter: Creating writer path=hdfs://master:54310/hbase/data/default/usertable/93253e37c284f3174309fede3f32339f/recovered.edits/0000000000000004724.temp region=93253e37c284f3174309fede3f32339f
2014-07-01 15:54:53,802 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:60020-1] wal.HLogSplitter: Split writers finished
2014-07-01 15:54:53,803 DEBUG [RS_LOG_REPLAY_OPS-sceplus-vm48:60020-1] wal.HLogSplitter: Submitting close of hdfs://master:54310/hbase/data/default/usertable/1dc0aa61f0e3f43a53375118eb48213f/recovered.edits/0000000000000001721.temp
2014-07-01 15:54:53,803 DEBUG [RS_LOG_REPLAY_OPS-sceplus-vm48:60020-1] wal.HLogSplitter: Submitting close of hdfs://master:54310/hbase/data/default/usertable/52290d336af5a689eef6040bafbe6b15/recovered.edits/0000000000000001715.temp
2014-07-01 15:54:53,803 DEBUG [split-log-closeStream-1] wal.HLogSplitter: Closing hdfs://master:54310/hbase/data/default/usertable/1dc0aa61f0e3f43a53375118eb48213f/recovered.edits/0000000000000001721.temp
2014-07-01 15:54:53,803 DEBUG [RS_LOG_REPLAY_OPS-sceplus-vm48:60020-1] wal.HLogSplitter: Submitting close of hdfs://master:54310/hbase/data/default/usertable/99b73df170b514f6773e742a495216c4/recovered.edits/0000000000000001714.temp
2014-07-01 15:54:53,803 DEBUG [split-log-closeStream-2] wal.HLogSplitter: Closing hdfs://master:54310/hbase/data/default/usertable/52290d336af5a689eef6040bafbe6b15/recovered.edits/0000000000000001715.temp
2014-07-01 15:54:53,803 DEBUG [RS_LOG_REPLAY_OPS-sceplus-vm48:60020-1] wal.HLogSplitter: Submitting close of hdfs://master:54310/hbase/data/default/usertable/c72fc9d54f680bcd54e9fa888600346b/recovered.edits/0000000000000001735.temp
2014-07-01 15:54:53,803 DEBUG [split-log-closeStream-3] wal.HLogSplitter: Closing hdfs://master:54310/hbase/data/default/usertable/99b73df170b514f6773e742a495216c4/recovered.edits/0000000000000001714.temp
2014-07-01 15:54:53,808 INFO  [split-log-closeStream-1] wal.HLogSplitter: Closed wap hdfs://master:54310/hbase/data/default/usertable/1dc0aa61f0e3f43a53375118eb48213f/recovered.edits/0000000000000001721.temp (wrote 24 edits in 193ms)
2014-07-01 15:54:53,815 INFO  [split-log-closeStream-2] wal.HLogSplitter: Closed wap hdfs://master:54310/hbase/data/default/usertable/52290d336af5a689eef6040bafbe6b15/recovered.edits/0000000000000001715.temp (wrote 21 edits in 193ms)
2014-07-01 15:54:53,817 INFO  [split-log-closeStream-3] wal.HLogSplitter: Closed wap hdfs://master:54310/hbase/data/default/usertable/99b73df170b514f6773e742a495216c4/recovered.edits/0000000000000001714.temp (wrote 14 edits in 342ms)
2014-07-01 15:54:53,819 DEBUG [split-log-closeStream-1] wal.HLogSplitter: Rename hdfs://master:54310/hbase/data/default/usertable/1dc0aa61f0e3f43a53375118eb48213f/recovered.edits/0000000000000001721.temp to hdfs://master:54310/hbase/data/default/usertable/1dc0aa61f0e3f43a53375118eb48213f/recovered.edits/0000000000000001744
2014-07-01 15:54:53,819 DEBUG [split-log-closeStream-1] wal.HLogSplitter: Closing hdfs://master:54310/hbase/data/default/usertable/c72fc9d54f680bcd54e9fa888600346b/recovered.edits/0000000000000001735.temp
2014-07-01 15:54:53,819 DEBUG [split-log-closeStream-2] wal.HLogSplitter: Rename hdfs://master:54310/hbase/data/default/usertable/52290d336af5a689eef6040bafbe6b15/recovered.edits/0000000000000001715.temp to hdfs://master:54310/hbase/data/default/usertable/52290d336af5a689eef6040bafbe6b15/recovered.edits/0000000000000001735
2014-07-01 15:54:53,823 INFO  [split-log-closeStream-1] wal.HLogSplitter: Closed wap hdfs://master:54310/hbase/data/default/usertable/c72fc9d54f680bcd54e9fa888600346b/recovered.edits/0000000000000001735.temp (wrote 15 edits in 146ms)
2014-07-01 15:54:53,823 DEBUG [split-log-closeStream-3] wal.HLogSplitter: Rename hdfs://master:54310/hbase/data/default/usertable/99b73df170b514f6773e742a495216c4/recovered.edits/0000000000000001714.temp to hdfs://master:54310/hbase/data/default/usertable/99b73df170b514f6773e742a495216c4/recovered.edits/0000000000000001727
2014-07-01 15:54:53,865 DEBUG [split-log-closeStream-1] wal.HLogSplitter: Rename hdfs://master:54310/hbase/data/default/usertable/c72fc9d54f680bcd54e9fa888600346b/recovered.edits/0000000000000001735.temp to hdfs://master:54310/hbase/data/default/usertable/c72fc9d54f680bcd54e9fa888600346b/recovered.edits/0000000000000001749
2014-07-01 15:54:53,865 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:60020-1] wal.HLogSplitter: Processed 74 edits across 4 regions; log file=hdfs://master:54310/hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1404253870530-splitting/sceplus-vm48.almaden.ibm.com%2C60020%2C1404253870530.1404254456728 is corrupted = false progress failed = false
2014-07-01 15:54:53,869 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:60020-1] handler.HLogSplitterHandler: successfully transitioned task /hbase/splitWAL/WALs%2Fsceplus-vm48.almaden.ibm.com%2C60020%2C1404253870530-splitting%2Fsceplus-vm48.almaden.ibm.com%252C60020%252C1404253870530.1404254456728 to final state DONE sceplus-vm48.almaden.ibm.com,60020,1404255227840
2014-07-01 15:54:53,869 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:60020-1] handler.HLogSplitterHandler: worker sceplus-vm48.almaden.ibm.com,60020,1404255227840 done with task /hbase/splitWAL/WALs%2Fsceplus-vm48.almaden.ibm.com%2C60020%2C1404253870530-splitting%2Fsceplus-vm48.almaden.ibm.com%252C60020%252C1404253870530.1404254456728 in 1054ms
2014-07-01 15:54:53,883 DEBUG [regionserver60020-EventThread] regionserver.SplitLogWorker: tasks arrived or departed
2014-07-01 15:54:53,888 DEBUG [SplitLogWorker-sceplus-vm48.almaden.ibm.com,60020,1404255227840] regionserver.SplitLogWorker: Current region server sceplus-vm48.almaden.ibm.com,60020,1404255227840 has 1 tasks in progress and can't take more.
2014-07-01 15:54:53,937 INFO  [Priority.RpcServer.handler=1,port=60020] regionserver.HRegionServer: Open usertable,user4,1404228265526.1dc0aa61f0e3f43a53375118eb48213f.
2014-07-01 15:54:54,079 INFO  [Priority.RpcServer.handler=1,port=60020] regionserver.HRegionServer: Open usertable,user8,1404228265526.99b73df170b514f6773e742a495216c4.
2014-07-01 15:54:54,080 DEBUG [RS_OPEN_REGION-sceplus-vm48:60020-0] zookeeper.ZKAssign: regionserver:60020-0x46f4224a300001, quorum=sceplus-vm49.almaden.ibm.com:2181,sceplus-vm48.almaden.ibm.com:2181, baseZNode=/hbase Transitioning 1dc0aa61f0e3f43a53375118eb48213f from M_ZK_REGION_OFFLINE to RS_ZK_REGION_OPENING
2014-07-01 15:54:54,080 DEBUG [RS_OPEN_REGION-sceplus-vm48:60020-1] zookeeper.ZKAssign: regionserver:60020-0x46f4224a300001, quorum=sceplus-vm49.almaden.ibm.com:2181,sceplus-vm48.almaden.ibm.com:2181, baseZNode=/hbase Transitioning 99b73df170b514f6773e742a495216c4 from M_ZK_REGION_OFFLINE to RS_ZK_REGION_OPENING
2014-07-01 15:54:54,084 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:60020-0] wal.HLogSplitter: Finishing writing output logs and closing down.
2014-07-01 15:54:54,084 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:60020-0] wal.HLogSplitter: Waiting for split writer threads to finish
2014-07-01 15:54:54,087 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:60020-0] wal.HLogSplitter: Split writers finished
2014-07-01 15:54:54,087 DEBUG [RS_LOG_REPLAY_OPS-sceplus-vm48:60020-0] wal.HLogSplitter: Submitting close of hdfs://master:54310/hbase/data/default/usertable/122e92eabfab38cb15a9965b21db25c5/recovered.edits/0000000000000004716.temp
2014-07-01 15:54:54,088 DEBUG [RS_LOG_REPLAY_OPS-sceplus-vm48:60020-0] wal.HLogSplitter: Submitting close of hdfs://master:54310/hbase/data/default/usertable/667c1f2c34fe37d48ba25a789d8aba98/recovered.edits/0000000000000004682.temp
2014-07-01 15:54:54,088 DEBUG [split-log-closeStream-1] wal.HLogSplitter: Closing hdfs://master:54310/hbase/data/default/usertable/122e92eabfab38cb15a9965b21db25c5/recovered.edits/0000000000000004716.temp
2014-07-01 15:54:54,088 DEBUG [RS_LOG_REPLAY_OPS-sceplus-vm48:60020-0] wal.HLogSplitter: Submitting close of hdfs://master:54310/hbase/data/default/usertable/93253e37c284f3174309fede3f32339f/recovered.edits/0000000000000004724.temp
2014-07-01 15:54:54,088 DEBUG [split-log-closeStream-2] wal.HLogSplitter: Closing hdfs://master:54310/hbase/data/default/usertable/667c1f2c34fe37d48ba25a789d8aba98/recovered.edits/0000000000000004682.temp
2014-07-01 15:54:54,088 DEBUG [split-log-closeStream-3] wal.HLogSplitter: Closing hdfs://master:54310/hbase/data/default/usertable/93253e37c284f3174309fede3f32339f/recovered.edits/0000000000000004724.temp
2014-07-01 15:54:54,088 DEBUG [RS_LOG_REPLAY_OPS-sceplus-vm48:60020-0] wal.HLogSplitter: Submitting close of hdfs://master:54310/hbase/data/default/usertable/9cef9e7d34f96f1d9b111be50040a208/recovered.edits/0000000000000005736.temp
2014-07-01 15:54:54,090 DEBUG [RS_LOG_REPLAY_OPS-sceplus-vm48:60020-0] wal.HLogSplitter: Submitting close of hdfs://master:54310/hbase/data/default/usertable/bb79ae773f71289e0745068f68a0abd9/recovered.edits/0000000000000004670.temp
2014-07-01 15:54:54,095 INFO  [split-log-closeStream-2] wal.HLogSplitter: Closed wap hdfs://master:54310/hbase/data/default/usertable/667c1f2c34fe37d48ba25a789d8aba98/recovered.edits/0000000000000004682.temp (wrote 8 edits in 104ms)
2014-07-01 15:54:54,096 INFO  [split-log-closeStream-1] wal.HLogSplitter: Closed wap hdfs://master:54310/hbase/data/default/usertable/122e92eabfab38cb15a9965b21db25c5/recovered.edits/0000000000000004716.temp (wrote 9 edits in 75ms)
2014-07-01 15:54:54,098 DEBUG [RS_OPEN_REGION-sceplus-vm48:60020-1] zookeeper.ZKAssign: regionserver:60020-0x46f4224a300001, quorum=sceplus-vm49.almaden.ibm.com:2181,sceplus-vm48.almaden.ibm.com:2181, baseZNode=/hbase Transitioned node 99b73df170b514f6773e742a495216c4 from M_ZK_REGION_OFFLINE to RS_ZK_REGION_OPENING
2014-07-01 15:54:54,098 DEBUG [RS_OPEN_REGION-sceplus-vm48:60020-0] zookeeper.ZKAssign: regionserver:60020-0x46f4224a300001, quorum=sceplus-vm49.almaden.ibm.com:2181,sceplus-vm48.almaden.ibm.com:2181, baseZNode=/hbase Transitioned node 1dc0aa61f0e3f43a53375118eb48213f from M_ZK_REGION_OFFLINE to RS_ZK_REGION_OPENING
2014-07-01 15:54:54,099 DEBUG [split-log-closeStream-2] wal.HLogSplitter: Rename hdfs://master:54310/hbase/data/default/usertable/667c1f2c34fe37d48ba25a789d8aba98/recovered.edits/0000000000000004682.temp to hdfs://master:54310/hbase/data/default/usertable/667c1f2c34fe37d48ba25a789d8aba98/recovered.edits/0000000000000004689
2014-07-01 15:54:54,099 DEBUG [split-log-closeStream-2] wal.HLogSplitter: Closing hdfs://master:54310/hbase/data/default/usertable/9cef9e7d34f96f1d9b111be50040a208/recovered.edits/0000000000000005736.temp
2014-07-01 15:54:54,099 DEBUG [RS_OPEN_REGION-sceplus-vm48:60020-1] regionserver.HRegion: Opening region: {ENCODED => 99b73df170b514f6773e742a495216c4, NAME => 'usertable,user8,1404228265526.99b73df170b514f6773e742a495216c4.', STARTKEY => 'user8', ENDKEY => 'user9'}
2014-07-01 15:54:54,099 DEBUG [RS_OPEN_REGION-sceplus-vm48:60020-0] regionserver.HRegion: Opening region: {ENCODED => 1dc0aa61f0e3f43a53375118eb48213f, NAME => 'usertable,user4,1404228265526.1dc0aa61f0e3f43a53375118eb48213f.', STARTKEY => 'user4', ENDKEY => 'user5'}
2014-07-01 15:54:54,100 DEBUG [split-log-closeStream-1] wal.HLogSplitter: Rename hdfs://master:54310/hbase/data/default/usertable/122e92eabfab38cb15a9965b21db25c5/recovered.edits/0000000000000004716.temp to hdfs://master:54310/hbase/data/default/usertable/122e92eabfab38cb15a9965b21db25c5/recovered.edits/0000000000000004724
2014-07-01 15:54:54,100 DEBUG [split-log-closeStream-1] wal.HLogSplitter: Closing hdfs://master:54310/hbase/data/default/usertable/bb79ae773f71289e0745068f68a0abd9/recovered.edits/0000000000000004670.temp
2014-07-01 15:54:54,105 INFO  [split-log-closeStream-1] wal.HLogSplitter: Closed wap hdfs://master:54310/hbase/data/default/usertable/bb79ae773f71289e0745068f68a0abd9/recovered.edits/0000000000000004670.temp (wrote 9 edits in 49ms)
2014-07-01 15:54:54,105 INFO  [split-log-closeStream-2] wal.HLogSplitter: Closed wap hdfs://master:54310/hbase/data/default/usertable/9cef9e7d34f96f1d9b111be50040a208/recovered.edits/0000000000000005736.temp (wrote 22 edits in 69ms)
2014-07-01 15:54:54,108 DEBUG [split-log-closeStream-1] wal.HLogSplitter: Rename hdfs://master:54310/hbase/data/default/usertable/bb79ae773f71289e0745068f68a0abd9/recovered.edits/0000000000000004670.temp to hdfs://master:54310/hbase/data/default/usertable/bb79ae773f71289e0745068f68a0abd9/recovered.edits/0000000000000004678
2014-07-01 15:54:54,109 DEBUG [split-log-closeStream-2] wal.HLogSplitter: Rename hdfs://master:54310/hbase/data/default/usertable/9cef9e7d34f96f1d9b111be50040a208/recovered.edits/0000000000000005736.temp to hdfs://master:54310/hbase/data/default/usertable/9cef9e7d34f96f1d9b111be50040a208/recovered.edits/0000000000000005757
2014-07-01 15:54:54,131 INFO  [RS_OPEN_REGION-sceplus-vm48:60020-0] impl.MetricsSourceAdapter: MBean for source RegionServer,sub=Regions registered.
2014-07-01 15:54:54,131 DEBUG [RS_OPEN_REGION-sceplus-vm48:60020-0] regionserver.MetricsRegionSourceImpl: Creating new MetricsRegionSourceImpl for table usertable 1dc0aa61f0e3f43a53375118eb48213f
2014-07-01 15:54:54,131 DEBUG [RS_OPEN_REGION-sceplus-vm48:60020-1] regionserver.MetricsRegionSourceImpl: Creating new MetricsRegionSourceImpl for table usertable 99b73df170b514f6773e742a495216c4
2014-07-01 15:54:54,132 DEBUG [RS_OPEN_REGION-sceplus-vm48:60020-0] regionserver.HRegion: Instantiated usertable,user4,1404228265526.1dc0aa61f0e3f43a53375118eb48213f.
2014-07-01 15:54:54,132 DEBUG [RS_OPEN_REGION-sceplus-vm48:60020-1] regionserver.HRegion: Instantiated usertable,user8,1404228265526.99b73df170b514f6773e742a495216c4.
2014-07-01 15:54:54,141 INFO  [RS_OPEN_REGION-sceplus-vm48:60020-1] util.NativeCodeLoader: Loaded the native-hadoop library
2014-07-01 15:54:54,142 INFO  [RS_OPEN_REGION-sceplus-vm48:60020-1] zlib.ZlibFactory: Successfully loaded & initialized native-zlib library
2014-07-01 15:54:54,149 INFO  [RS_OPEN_REGION-sceplus-vm48:60020-1] compress.CodecPool: Got brand-new compressor
2014-07-01 15:54:54,149 INFO  [RS_OPEN_REGION-sceplus-vm48:60020-0] compress.CodecPool: Got brand-new compressor
2014-07-01 15:54:54,242 INFO  [StoreOpener-1dc0aa61f0e3f43a53375118eb48213f-1] compactions.CompactionConfiguration: size [268435456, 9223372036854775807); files [3, 10); ratio 1.200000; off-peak ratio 5.000000; throttle point 5368709120; delete expired; major period 0, major jitter 0.500000
2014-07-01 15:54:54,242 INFO  [StoreOpener-99b73df170b514f6773e742a495216c4-1] compactions.CompactionConfiguration: size [268435456, 9223372036854775807); files [3, 10); ratio 1.200000; off-peak ratio 5.000000; throttle point 5368709120; delete expired; major period 0, major jitter 0.500000
2014-07-01 15:54:54,276 INFO  [StoreFileOpenerThread-family-1] util.ChecksumType: Checksum using org.apache.hadoop.util.PureJavaCrc32
2014-07-01 15:54:54,311 INFO  [StoreFileOpenerThread-family-1] compress.CodecPool: Got brand-new decompressor
2014-07-01 15:54:54,311 INFO  [StoreFileOpenerThread-family-1] compress.CodecPool: Got brand-new decompressor
2014-07-01 15:54:54,322 DEBUG [StoreOpener-1dc0aa61f0e3f43a53375118eb48213f-1] regionserver.HStore: loaded hdfs://master:54310/hbase/data/default/usertable/1dc0aa61f0e3f43a53375118eb48213f/family/74395d51dde5439eaae7432124c91d62, isReference=false, isBulkLoadResult=false, seqid=2006, majorCompaction=true
2014-07-01 15:54:54,322 DEBUG [StoreOpener-99b73df170b514f6773e742a495216c4-1] regionserver.HStore: loaded hdfs://master:54310/hbase/data/default/usertable/99b73df170b514f6773e742a495216c4/family/8d6bc579814b4b25b9449173a9101b1b, isReference=false, isBulkLoadResult=false, seqid=1930, majorCompaction=true
2014-07-01 15:54:54,409 DEBUG [RS_OPEN_REGION-sceplus-vm48:60020-1] regionserver.HRegion: Found 31 recovered edits file(s) under hdfs://master:54310/hbase/data/default/usertable/99b73df170b514f6773e742a495216c4
2014-07-01 15:54:54,409 DEBUG [RS_OPEN_REGION-sceplus-vm48:60020-0] regionserver.HRegion: Found 31 recovered edits file(s) under hdfs://master:54310/hbase/data/default/usertable/1dc0aa61f0e3f43a53375118eb48213f
2014-07-01 15:54:54,411 DEBUG [RS_OPEN_REGION-sceplus-vm48:60020-1] regionserver.HRegion: Maximum sequenceid for this log is 1436 and minimum sequenceid for the region is 1930, skipped the whole file, path=hdfs://master:54310/hbase/data/default/usertable/99b73df170b514f6773e742a495216c4/recovered.edits/0000000000000001436
2014-07-01 15:54:54,411 DEBUG [RS_OPEN_REGION-sceplus-vm48:60020-0] regionserver.HRegion: Maximum sequenceid for this log is 1442 and minimum sequenceid for the region is 2006, skipped the whole file, path=hdfs://master:54310/hbase/data/default/usertable/1dc0aa61f0e3f43a53375118eb48213f/recovered.edits/0000000000000001442
2014-07-01 15:54:54,413 DEBUG [RS_OPEN_REGION-sceplus-vm48:60020-1] regionserver.HRegion: Maximum sequenceid for this log is 1453 and minimum sequenceid for the region is 1930, skipped the whole file, path=hdfs://master:54310/hbase/data/default/usertable/99b73df170b514f6773e742a495216c4/recovered.edits/0000000000000001453
2014-07-01 15:54:54,413 DEBUG [RS_OPEN_REGION-sceplus-vm48:60020-0] regionserver.HRegion: Maximum sequenceid for this log is 1465 and minimum sequenceid for the region is 2006, skipped the whole file, path=hdfs://master:54310/hbase/data/default/usertable/1dc0aa61f0e3f43a53375118eb48213f/recovered.edits/0000000000000001465
2014-07-01 15:54:54,414 DEBUG [RS_OPEN_REGION-sceplus-vm48:60020-1] regionserver.HRegion: Maximum sequenceid for this log is 1478 and minimum sequenceid for the region is 1930, skipped the whole file, path=hdfs://master:54310/hbase/data/default/usertable/99b73df170b514f6773e742a495216c4/recovered.edits/0000000000000001478
2014-07-01 15:54:54,415 DEBUG [RS_OPEN_REGION-sceplus-vm48:60020-0] regionserver.HRegion: Maximum sequenceid for this log is 1490 and minimum sequenceid for the region is 2006, skipped the whole file, path=hdfs://master:54310/hbase/data/default/usertable/1dc0aa61f0e3f43a53375118eb48213f/recovered.edits/0000000000000001490
2014-07-01 15:54:54,416 DEBUG [RS_OPEN_REGION-sceplus-vm48:60020-1] regionserver.HRegion: Maximum sequenceid for this log is 1493 and minimum sequenceid for the region is 1930, skipped the whole file, path=hdfs://master:54310/hbase/data/default/usertable/99b73df170b514f6773e742a495216c4/recovered.edits/0000000000000001493
2014-07-01 15:54:54,416 DEBUG [RS_OPEN_REGION-sceplus-vm48:60020-0] regionserver.HRegion: Maximum sequenceid for this log is 1510 and minimum sequenceid for the region is 2006, skipped the whole file, path=hdfs://master:54310/hbase/data/default/usertable/1dc0aa61f0e3f43a53375118eb48213f/recovered.edits/0000000000000001510
2014-07-01 15:54:54,418 DEBUG [RS_OPEN_REGION-sceplus-vm48:60020-1] regionserver.HRegion: Maximum sequenceid for this log is 1512 and minimum sequenceid for the region is 1930, skipped the whole file, path=hdfs://master:54310/hbase/data/default/usertable/99b73df170b514f6773e742a495216c4/recovered.edits/0000000000000001512
2014-07-01 15:54:54,418 DEBUG [RS_OPEN_REGION-sceplus-vm48:60020-0] regionserver.HRegion: Maximum sequenceid for this log is 1531 and minimum sequenceid for the region is 2006, skipped the whole file, path=hdfs://master:54310/hbase/data/default/usertable/1dc0aa61f0e3f43a53375118eb48213f/recovered.edits/0000000000000001531
2014-07-01 15:54:54,419 DEBUG [RS_OPEN_REGION-sceplus-vm48:60020-1] regionserver.HRegion: Maximum sequenceid for this log is 1536 and minimum sequenceid for the region is 1930, skipped the whole file, path=hdfs://master:54310/hbase/data/default/usertable/99b73df170b514f6773e742a495216c4/recovered.edits/0000000000000001536
2014-07-01 15:54:54,420 DEBUG [RS_OPEN_REGION-sceplus-vm48:60020-0] regionserver.HRegion: Maximum sequenceid for this log is 1547 and minimum sequenceid for the region is 2006, skipped the whole file, path=hdfs://master:54310/hbase/data/default/usertable/1dc0aa61f0e3f43a53375118eb48213f/recovered.edits/0000000000000001547
2014-07-01 15:54:54,421 DEBUG [RS_OPEN_REGION-sceplus-vm48:60020-1] regionserver.HRegion: Maximum sequenceid for this log is 1556 and minimum sequenceid for the region is 1930, skipped the whole file, path=hdfs://master:54310/hbase/data/default/usertable/99b73df170b514f6773e742a495216c4/recovered.edits/0000000000000001556
2014-07-01 15:54:54,422 DEBUG [RS_OPEN_REGION-sceplus-vm48:60020-0] regionserver.HRegion: Maximum sequenceid for this log is 1566 and minimum sequenceid for the region is 2006, skipped the whole file, path=hdfs://master:54310/hbase/data/default/usertable/1dc0aa61f0e3f43a53375118eb48213f/recovered.edits/0000000000000001566
2014-07-01 15:54:54,423 DEBUG [RS_OPEN_REGION-sceplus-vm48:60020-1] regionserver.HRegion: Maximum sequenceid for this log is 1579 and minimum sequenceid for the region is 1930, skipped the whole file, path=hdfs://master:54310/hbase/data/default/usertable/99b73df170b514f6773e742a495216c4/recovered.edits/0000000000000001579
2014-07-01 15:54:54,423 DEBUG [RS_OPEN_REGION-sceplus-vm48:60020-0] regionserver.HRegion: Maximum sequenceid for this log is 1584 and minimum sequenceid for the region is 2006, skipped the whole file, path=hdfs://master:54310/hbase/data/default/usertable/1dc0aa61f0e3f43a53375118eb48213f/recovered.edits/0000000000000001584
2014-07-01 15:54:54,425 DEBUG [RS_OPEN_REGION-sceplus-vm48:60020-1] regionserver.HRegion: Maximum sequenceid for this log is 1594 and minimum sequenceid for the region is 1930, skipped the whole file, path=hdfs://master:54310/hbase/data/default/usertable/99b73df170b514f6773e742a495216c4/recovered.edits/0000000000000001594
2014-07-01 15:54:54,425 DEBUG [RS_OPEN_REGION-sceplus-vm48:60020-0] regionserver.HRegion: Maximum sequenceid for this log is 1605 and minimum sequenceid for the region is 2006, skipped the whole file, path=hdfs://master:54310/hbase/data/default/usertable/1dc0aa61f0e3f43a53375118eb48213f/recovered.edits/0000000000000001605
2014-07-01 15:54:54,426 DEBUG [RS_OPEN_REGION-sceplus-vm48:60020-1] regionserver.HRegion: Maximum sequenceid for this log is 1612 and minimum sequenceid for the region is 1930, skipped the whole file, path=hdfs://master:54310/hbase/data/default/usertable/99b73df170b514f6773e742a495216c4/recovered.edits/0000000000000001612
2014-07-01 15:54:54,427 DEBUG [RS_OPEN_REGION-sceplus-vm48:60020-0] regionserver.HRegion: Maximum sequenceid for this log is 1625 and minimum sequenceid for the region is 2006, skipped the whole file, path=hdfs://master:54310/hbase/data/default/usertable/1dc0aa61f0e3f43a53375118eb48213f/recovered.edits/0000000000000001625
2014-07-01 15:54:54,428 DEBUG [RS_OPEN_REGION-sceplus-vm48:60020-1] regionserver.HRegion: Maximum sequenceid for this log is 1628 and minimum sequenceid for the region is 1930, skipped the whole file, path=hdfs://master:54310/hbase/data/default/usertable/99b73df170b514f6773e742a495216c4/recovered.edits/0000000000000001628
2014-07-01 15:54:54,428 DEBUG [RS_OPEN_REGION-sceplus-vm48:60020-0] regionserver.HRegion: Maximum sequenceid for this log is 1645 and minimum sequenceid for the region is 2006, skipped the whole file, path=hdfs://master:54310/hbase/data/default/usertable/1dc0aa61f0e3f43a53375118eb48213f/recovered.edits/0000000000000001645
2014-07-01 15:54:54,430 DEBUG [RS_OPEN_REGION-sceplus-vm48:60020-1] regionserver.HRegion: Maximum sequenceid for this log is 1647 and minimum sequenceid for the region is 1930, skipped the whole file, path=hdfs://master:54310/hbase/data/default/usertable/99b73df170b514f6773e742a495216c4/recovered.edits/0000000000000001647
2014-07-01 15:54:54,430 DEBUG [RS_OPEN_REGION-sceplus-vm48:60020-0] regionserver.HRegion: Maximum sequenceid for this log is 1667 and minimum sequenceid for the region is 2006, skipped the whole file, path=hdfs://master:54310/hbase/data/default/usertable/1dc0aa61f0e3f43a53375118eb48213f/recovered.edits/0000000000000001667
2014-07-01 15:54:54,431 DEBUG [RS_OPEN_REGION-sceplus-vm48:60020-1] regionserver.HRegion: Maximum sequenceid for this log is 1676 and minimum sequenceid for the region is 1930, skipped the whole file, path=hdfs://master:54310/hbase/data/default/usertable/99b73df170b514f6773e742a495216c4/recovered.edits/0000000000000001676
2014-07-01 15:54:54,432 DEBUG [RS_OPEN_REGION-sceplus-vm48:60020-0] regionserver.HRegion: Maximum sequenceid for this log is 1686 and minimum sequenceid for the region is 2006, skipped the whole file, path=hdfs://master:54310/hbase/data/default/usertable/1dc0aa61f0e3f43a53375118eb48213f/recovered.edits/0000000000000001686
2014-07-01 15:54:54,433 DEBUG [RS_OPEN_REGION-sceplus-vm48:60020-1] regionserver.HRegion: Maximum sequenceid for this log is 1691 and minimum sequenceid for the region is 1930, skipped the whole file, path=hdfs://master:54310/hbase/data/default/usertable/99b73df170b514f6773e742a495216c4/recovered.edits/0000000000000001691
2014-07-01 15:54:54,433 DEBUG [RS_OPEN_REGION-sceplus-vm48:60020-0] regionserver.HRegion: Maximum sequenceid for this log is 1705 and minimum sequenceid for the region is 2006, skipped the whole file, path=hdfs://master:54310/hbase/data/default/usertable/1dc0aa61f0e3f43a53375118eb48213f/recovered.edits/0000000000000001705
2014-07-01 15:54:54,435 DEBUG [RS_OPEN_REGION-sceplus-vm48:60020-1] regionserver.HRegion: Maximum sequenceid for this log is 1713 and minimum sequenceid for the region is 1930, skipped the whole file, path=hdfs://master:54310/hbase/data/default/usertable/99b73df170b514f6773e742a495216c4/recovered.edits/0000000000000001713
2014-07-01 15:54:54,435 DEBUG [RS_OPEN_REGION-sceplus-vm48:60020-0] regionserver.HRegion: Maximum sequenceid for this log is 1720 and minimum sequenceid for the region is 2006, skipped the whole file, path=hdfs://master:54310/hbase/data/default/usertable/1dc0aa61f0e3f43a53375118eb48213f/recovered.edits/0000000000000001720
2014-07-01 15:54:54,437 DEBUG [RS_OPEN_REGION-sceplus-vm48:60020-1] regionserver.HRegion: Maximum sequenceid for this log is 1727 and minimum sequenceid for the region is 1930, skipped the whole file, path=hdfs://master:54310/hbase/data/default/usertable/99b73df170b514f6773e742a495216c4/recovered.edits/0000000000000001727
2014-07-01 15:54:54,437 DEBUG [RS_OPEN_REGION-sceplus-vm48:60020-0] regionserver.HRegion: Maximum sequenceid for this log is 1744 and minimum sequenceid for the region is 2006, skipped the whole file, path=hdfs://master:54310/hbase/data/default/usertable/1dc0aa61f0e3f43a53375118eb48213f/recovered.edits/0000000000000001744
2014-07-01 15:54:54,438 DEBUG [RS_OPEN_REGION-sceplus-vm48:60020-1] regionserver.HRegion: Maximum sequenceid for this log is 1748 and minimum sequenceid for the region is 1930, skipped the whole file, path=hdfs://master:54310/hbase/data/default/usertable/99b73df170b514f6773e742a495216c4/recovered.edits/0000000000000001748
2014-07-01 15:54:54,439 DEBUG [RS_OPEN_REGION-sceplus-vm48:60020-0] regionserver.HRegion: Maximum sequenceid for this log is 1762 and minimum sequenceid for the region is 2006, skipped the whole file, path=hdfs://master:54310/hbase/data/default/usertable/1dc0aa61f0e3f43a53375118eb48213f/recovered.edits/0000000000000001762
2014-07-01 15:54:54,440 DEBUG [RS_OPEN_REGION-sceplus-vm48:60020-1] regionserver.HRegion: Maximum sequenceid for this log is 1767 and minimum sequenceid for the region is 1930, skipped the whole file, path=hdfs://master:54310/hbase/data/default/usertable/99b73df170b514f6773e742a495216c4/recovered.edits/0000000000000001767
2014-07-01 15:54:54,440 DEBUG [RS_OPEN_REGION-sceplus-vm48:60020-0] regionserver.HRegion: Maximum sequenceid for this log is 1784 and minimum sequenceid for the region is 2006, skipped the whole file, path=hdfs://master:54310/hbase/data/default/usertable/1dc0aa61f0e3f43a53375118eb48213f/recovered.edits/0000000000000001784
2014-07-01 15:54:54,442 DEBUG [RS_OPEN_REGION-sceplus-vm48:60020-1] regionserver.HRegion: Maximum sequenceid for this log is 1784 and minimum sequenceid for the region is 1930, skipped the whole file, path=hdfs://master:54310/hbase/data/default/usertable/99b73df170b514f6773e742a495216c4/recovered.edits/0000000000000001784
2014-07-01 15:54:54,442 DEBUG [RS_OPEN_REGION-sceplus-vm48:60020-0] regionserver.HRegion: Maximum sequenceid for this log is 1802 and minimum sequenceid for the region is 2006, skipped the whole file, path=hdfs://master:54310/hbase/data/default/usertable/1dc0aa61f0e3f43a53375118eb48213f/recovered.edits/0000000000000001802
2014-07-01 15:54:54,443 DEBUG [RS_OPEN_REGION-sceplus-vm48:60020-1] regionserver.HRegion: Maximum sequenceid for this log is 1803 and minimum sequenceid for the region is 1930, skipped the whole file, path=hdfs://master:54310/hbase/data/default/usertable/99b73df170b514f6773e742a495216c4/recovered.edits/0000000000000001803
2014-07-01 15:54:54,444 DEBUG [RS_OPEN_REGION-sceplus-vm48:60020-0] regionserver.HRegion: Maximum sequenceid for this log is 1820 and minimum sequenceid for the region is 2006, skipped the whole file, path=hdfs://master:54310/hbase/data/default/usertable/1dc0aa61f0e3f43a53375118eb48213f/recovered.edits/0000000000000001820
2014-07-01 15:54:54,445 DEBUG [RS_OPEN_REGION-sceplus-vm48:60020-1] regionserver.HRegion: Maximum sequenceid for this log is 1826 and minimum sequenceid for the region is 1930, skipped the whole file, path=hdfs://master:54310/hbase/data/default/usertable/99b73df170b514f6773e742a495216c4/recovered.edits/0000000000000001826
2014-07-01 15:54:54,445 DEBUG [RS_OPEN_REGION-sceplus-vm48:60020-0] regionserver.HRegion: Maximum sequenceid for this log is 1843 and minimum sequenceid for the region is 2006, skipped the whole file, path=hdfs://master:54310/hbase/data/default/usertable/1dc0aa61f0e3f43a53375118eb48213f/recovered.edits/0000000000000001843
2014-07-01 15:54:54,446 DEBUG [RS_OPEN_REGION-sceplus-vm48:60020-1] regionserver.HRegion: Maximum sequenceid for this log is 1844 and minimum sequenceid for the region is 1930, skipped the whole file, path=hdfs://master:54310/hbase/data/default/usertable/99b73df170b514f6773e742a495216c4/recovered.edits/0000000000000001844
2014-07-01 15:54:54,447 DEBUG [RS_OPEN_REGION-sceplus-vm48:60020-0] regionserver.HRegion: Maximum sequenceid for this log is 1861 and minimum sequenceid for the region is 2006, skipped the whole file, path=hdfs://master:54310/hbase/data/default/usertable/1dc0aa61f0e3f43a53375118eb48213f/recovered.edits/0000000000000001861
2014-07-01 15:54:54,448 DEBUG [RS_OPEN_REGION-sceplus-vm48:60020-1] regionserver.HRegion: Maximum sequenceid for this log is 1869 and minimum sequenceid for the region is 1930, skipped the whole file, path=hdfs://master:54310/hbase/data/default/usertable/99b73df170b514f6773e742a495216c4/recovered.edits/0000000000000001869
2014-07-01 15:54:54,495 DEBUG [RS_OPEN_REGION-sceplus-vm48:60020-0] regionserver.HRegion: Maximum sequenceid for this log is 1876 and minimum sequenceid for the region is 2006, skipped the whole file, path=hdfs://master:54310/hbase/data/default/usertable/1dc0aa61f0e3f43a53375118eb48213f/recovered.edits/0000000000000001876
2014-07-01 15:54:54,499 INFO  [split-log-closeStream-3] wal.HLogSplitter: Closed wap hdfs://master:54310/hbase/data/default/usertable/93253e37c284f3174309fede3f32339f/recovered.edits/0000000000000004724.temp (wrote 7 edits in 180ms)
2014-07-01 15:54:54,499 DEBUG [RS_OPEN_REGION-sceplus-vm48:60020-1] regionserver.HRegion: Maximum sequenceid for this log is 1884 and minimum sequenceid for the region is 1930, skipped the whole file, path=hdfs://master:54310/hbase/data/default/usertable/99b73df170b514f6773e742a495216c4/recovered.edits/0000000000000001884
2014-07-01 15:54:54,499 DEBUG [RS_OPEN_REGION-sceplus-vm48:60020-0] regionserver.HRegion: Maximum sequenceid for this log is 1895 and minimum sequenceid for the region is 2006, skipped the whole file, path=hdfs://master:54310/hbase/data/default/usertable/1dc0aa61f0e3f43a53375118eb48213f/recovered.edits/0000000000000001895
2014-07-01 15:54:54,500 DEBUG [RS_OPEN_REGION-sceplus-vm48:60020-1] regionserver.HRegion: Maximum sequenceid for this log is 1904 and minimum sequenceid for the region is 1930, skipped the whole file, path=hdfs://master:54310/hbase/data/default/usertable/99b73df170b514f6773e742a495216c4/recovered.edits/0000000000000001904
2014-07-01 15:54:54,501 DEBUG [RS_OPEN_REGION-sceplus-vm48:60020-0] regionserver.HRegion: Maximum sequenceid for this log is 1912 and minimum sequenceid for the region is 2006, skipped the whole file, path=hdfs://master:54310/hbase/data/default/usertable/1dc0aa61f0e3f43a53375118eb48213f/recovered.edits/0000000000000001912
2014-07-01 15:54:54,502 DEBUG [split-log-closeStream-3] wal.HLogSplitter: Rename hdfs://master:54310/hbase/data/default/usertable/93253e37c284f3174309fede3f32339f/recovered.edits/0000000000000004724.temp to hdfs://master:54310/hbase/data/default/usertable/93253e37c284f3174309fede3f32339f/recovered.edits/0000000000000004730
2014-07-01 15:54:54,502 DEBUG [RS_OPEN_REGION-sceplus-vm48:60020-1] regionserver.HRegion: Maximum sequenceid for this log is 1921 and minimum sequenceid for the region is 1930, skipped the whole file, path=hdfs://master:54310/hbase/data/default/usertable/99b73df170b514f6773e742a495216c4/recovered.edits/0000000000000001921
2014-07-01 15:54:54,502 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:60020-0] wal.HLogSplitter: Processed 55 edits across 5 regions; log file=hdfs://master:54310/hbase/WALs/slave1,60020,1404253868846-splitting/slave1%2C60020%2C1404253868846.1404254547570 is corrupted = false progress failed = false
2014-07-01 15:54:54,502 DEBUG [RS_OPEN_REGION-sceplus-vm48:60020-0] regionserver.HRegion: Maximum sequenceid for this log is 1930 and minimum sequenceid for the region is 2006, skipped the whole file, path=hdfs://master:54310/hbase/data/default/usertable/1dc0aa61f0e3f43a53375118eb48213f/recovered.edits/0000000000000001930
2014-07-01 15:54:54,504 INFO  [RS_OPEN_REGION-sceplus-vm48:60020-1] regionserver.HRegion: Replaying edits from hdfs://master:54310/hbase/data/default/usertable/99b73df170b514f6773e742a495216c4/recovered.edits/0000000000000001941
2014-07-01 15:54:54,504 DEBUG [RS_OPEN_REGION-sceplus-vm48:60020-0] regionserver.HRegion: Maximum sequenceid for this log is 1947 and minimum sequenceid for the region is 2006, skipped the whole file, path=hdfs://master:54310/hbase/data/default/usertable/1dc0aa61f0e3f43a53375118eb48213f/recovered.edits/0000000000000001947
2014-07-01 15:54:54,514 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:60020-0] handler.HLogSplitterHandler: successfully transitioned task /hbase/splitWAL/WALs%2Fslave1%2C60020%2C1404253868846-splitting%2Fslave1%252C60020%252C1404253868846.1404254547570 to final state DONE sceplus-vm48.almaden.ibm.com,60020,1404255227840
2014-07-01 15:54:54,514 INFO  [RS_LOG_REPLAY_OPS-sceplus-vm48:60020-0] handler.HLogSplitterHandler: worker sceplus-vm48.almaden.ibm.com,60020,1404255227840 done with task /hbase/splitWAL/WALs%2Fslave1%2C60020%2C1404253868846-splitting%2Fslave1%252C60020%252C1404253868846.1404254547570 in 4911ms
2014-07-01 15:54:54,534 DEBUG [regionserver60020-EventThread] regionserver.SplitLogWorker: tasks arrived or departed
2014-07-01 15:54:54,547 DEBUG [RS_OPEN_REGION-sceplus-vm48:60020-0] regionserver.HRegion: Maximum sequenceid for this log is 1963 and minimum sequenceid for the region is 2006, skipped the whole file, path=hdfs://master:54310/hbase/data/default/usertable/1dc0aa61f0e3f43a53375118eb48213f/recovered.edits/0000000000000001963
2014-07-01 15:54:54,547 INFO  [Priority.RpcServer.handler=2,port=60020] regionserver.HRegionServer: Open usertable,,1404228265525.9cef9e7d34f96f1d9b111be50040a208.
2014-07-01 15:54:54,549 DEBUG [RS_OPEN_REGION-sceplus-vm48:60020-0] regionserver.HRegion: Maximum sequenceid for this log is 1980 and minimum sequenceid for the region is 2006, skipped the whole file, path=hdfs://master:54310/hbase/data/default/usertable/1dc0aa61f0e3f43a53375118eb48213f/recovered.edits/0000000000000001980
2014-07-01 15:54:54,549 INFO  [Priority.RpcServer.handler=2,port=60020] regionserver.HRegionServer: Open usertable,user5,1404228265526.122e92eabfab38cb15a9965b21db25c5.
2014-07-01 15:54:54,550 DEBUG [RS_OPEN_REGION-sceplus-vm48:60020-2] zookeeper.ZKAssign: regionserver:60020-0x46f4224a300001, quorum=sceplus-vm49.almaden.ibm.com:2181,sceplus-vm48.almaden.ibm.com:2181, baseZNode=/hbase Transitioning 9cef9e7d34f96f1d9b111be50040a208 from M_ZK_REGION_OFFLINE to RS_ZK_REGION_OPENING
2014-07-01 15:54:54,550 INFO  [Priority.RpcServer.handler=2,port=60020] regionserver.HRegionServer: Open usertable,user9,1404228265526.bb79ae773f71289e0745068f68a0abd9.
2014-07-01 15:54:54,551 DEBUG [RS_OPEN_REGION-sceplus-vm48:60020-0] regionserver.HRegion: Maximum sequenceid for this log is 1997 and minimum sequenceid for the region is 2006, skipped the whole file, path=hdfs://master:54310/hbase/data/default/usertable/1dc0aa61f0e3f43a53375118eb48213f/recovered.edits/0000000000000001997
2014-07-01 15:54:54,552 INFO  [RS_OPEN_REGION-sceplus-vm48:60020-0] regionserver.HRegion: Replaying edits from hdfs://master:54310/hbase/data/default/usertable/1dc0aa61f0e3f43a53375118eb48213f/recovered.edits/0000000000000002007
2014-07-01 15:54:54,555 DEBUG [RS_OPEN_REGION-sceplus-vm48:60020-2] zookeeper.ZKAssign: regionserver:60020-0x46f4224a300001, quorum=sceplus-vm49.almaden.ibm.com:2181,sceplus-vm48.almaden.ibm.com:2181, baseZNode=/hbase Transitioned node 9cef9e7d34f96f1d9b111be50040a208 from M_ZK_REGION_OFFLINE to RS_ZK_REGION_OPENING
2014-07-01 15:54:54,555 DEBUG [RS_OPEN_REGION-sceplus-vm48:60020-2] regionserver.HRegion: Opening region: {ENCODED => 9cef9e7d34f96f1d9b111be50040a208, NAME => 'usertable,,1404228265525.9cef9e7d34f96f1d9b111be50040a208.', STARTKEY => '', ENDKEY => 'user2'}
2014-07-01 15:54:54,555 DEBUG [RS_OPEN_REGION-sceplus-vm48:60020-2] regionserver.MetricsRegionSourceImpl: Creating new MetricsRegionSourceImpl for table usertable 9cef9e7d34f96f1d9b111be50040a208
2014-07-01 15:54:54,556 DEBUG [RS_OPEN_REGION-sceplus-vm48:60020-2] regionserver.HRegion: Instantiated usertable,,1404228265525.9cef9e7d34f96f1d9b111be50040a208.
2014-07-01 15:54:54,561 INFO  [StoreOpener-9cef9e7d34f96f1d9b111be50040a208-1] compactions.CompactionConfiguration: size [268435456, 9223372036854775807); files [3, 10); ratio 1.200000; off-peak ratio 5.000000; throttle point 5368709120; delete expired; major period 0, major jitter 0.500000
2014-07-01 15:54:54,600 DEBUG [StoreOpener-9cef9e7d34f96f1d9b111be50040a208-1] regionserver.HStore: loaded hdfs://master:54310/hbase/data/default/usertable/9cef9e7d34f96f1d9b111be50040a208/family/fd1299745de743a1aedf8d741c8f5e6a, isReference=false, isBulkLoadResult=false, seqid=5476, majorCompaction=true
2014-07-01 15:54:54,621 DEBUG [RS_OPEN_REGION-sceplus-vm48:60020-2] regionserver.HRegion: Found 17 recovered edits file(s) under hdfs://master:54310/hbase/data/default/usertable/9cef9e7d34f96f1d9b111be50040a208
2014-07-01 15:54:54,623 DEBUG [RS_OPEN_REGION-sceplus-vm48:60020-2] regionserver.HRegion: Maximum sequenceid for this log is 4965 and minimum sequenceid for the region is 5476, skipped the whole file, path=hdfs://master:54310/hbase/data/default/usertable/9cef9e7d34f96f1d9b111be50040a208/recovered.edits/0000000000000004965
2014-07-01 15:54:54,625 DEBUG [RS_OPEN_REGION-sceplus-vm48:60020-2] regionserver.HRegion: Maximum sequenceid for this log is 5024 and minimum sequenceid for the region is 5476, skipped the whole file, path=hdfs://master:54310/hbase/data/default/usertable/9cef9e7d34f96f1d9b111be50040a208/recovered.edits/0000000000000005024
2014-07-01 15:54:54,627 DEBUG [RS_OPEN_REGION-sceplus-vm48:60020-2] regionserver.HRegion: Maximum sequenceid for this log is 5083 and minimum sequenceid for the region is 5476, skipped the whole file, path=hdfs://master:54310/hbase/data/default/usertable/9cef9e7d34f96f1d9b111be50040a208/recovered.edits/0000000000000005083
2014-07-01 15:54:54,628 DEBUG [RS_OPEN_REGION-sceplus-vm48:60020-2] regionserver.HRegion: Maximum sequenceid for this log is 5135 and minimum sequenceid for the region is 5476, skipped the whole file, path=hdfs://master:54310/hbase/data/default/usertable/9cef9e7d34f96f1d9b111be50040a208/recovered.edits/0000000000000005135
2014-07-01 15:54:54,630 DEBUG [RS_OPEN_REGION-sceplus-vm48:60020-2] regionserver.HRegion: Maximum sequenceid for this log is 5181 and minimum sequenceid for the region is 5476, skipped the whole file, path=hdfs://master:54310/hbase/data/default/usertable/9cef9e7d34f96f1d9b111be50040a208/recovered.edits/0000000000000005181
2014-07-01 15:54:54,631 DEBUG [RS_OPEN_REGION-sceplus-vm48:60020-2] regionserver.HRegion: Maximum sequenceid for this log is 5233 and minimum sequenceid for the region is 5476, skipped the whole file, path=hdfs://master:54310/hbase/data/default/usertable/9cef9e7d34f96f1d9b111be50040a208/recovered.edits/0000000000000005233
2014-07-01 15:54:54,633 DEBUG [RS_OPEN_REGION-sceplus-vm48:60020-2] regionserver.HRegion: Maximum sequenceid for this log is 5285 and minimum sequenceid for the region is 5476, skipped the whole file, path=hdfs://master:54310/hbase/data/default/usertable/9cef9e7d34f96f1d9b111be50040a208/recovered.edits/0000000000000005285
2014-07-01 15:54:54,634 DEBUG [RS_OPEN_REGION-sceplus-vm48:60020-2] regionserver.HRegion: Maximum sequenceid for this log is 5336 and minimum sequenceid for the region is 5476, skipped the whole file, path=hdfs://master:54310/hbase/data/default/usertable/9cef9e7d34f96f1d9b111be50040a208/recovered.edits/0000000000000005336
2014-07-01 15:54:54,636 DEBUG [RS_OPEN_REGION-sceplus-vm48:60020-2] regionserver.HRegion: Maximum sequenceid for this log is 5391 and minimum sequenceid for the region is 5476, skipped the whole file, path=hdfs://master:54310/hbase/data/default/usertable/9cef9e7d34f96f1d9b111be50040a208/recovered.edits/0000000000000005391
2014-07-01 15:54:54,638 DEBUG [RS_OPEN_REGION-sceplus-vm48:60020-2] regionserver.HRegion: Maximum sequenceid for this log is 5446 and minimum sequenceid for the region is 5476, skipped the whole file, path=hdfs://master:54310/hbase/data/default/usertable/9cef9e7d34f96f1d9b111be50040a208/recovered.edits/0000000000000005446
2014-07-01 15:54:54,639 INFO  [RS_OPEN_REGION-sceplus-vm48:60020-2] regionserver.HRegion: Replaying edits from hdfs://master:54310/hbase/data/default/usertable/9cef9e7d34f96f1d9b111be50040a208/recovered.edits/0000000000000005496
2014-07-01 15:54:54,669 DEBUG [RS_OPEN_REGION-sceplus-vm48:60020-0] regionserver.HStore: Completing compaction from the WAL marker
2014-07-01 15:54:54,671 DEBUG [RS_OPEN_REGION-sceplus-vm48:60020-0] regionserver.HStore: Removing store files after compaction...
2014-07-01 15:54:54,674 DEBUG [RS_OPEN_REGION-sceplus-vm48:60020-0] backup.HFileArchiver: No store files to dispose, done!
2014-07-01 15:54:54,674 DEBUG [RS_OPEN_REGION-sceplus-vm48:60020-0] zookeeper.ZKAssign: regionserver:60020-0x46f4224a300001, quorum=sceplus-vm49.almaden.ibm.com:2181,sceplus-vm48.almaden.ibm.com:2181, baseZNode=/hbase Attempting to retransition opening state of node 1dc0aa61f0e3f43a53375118eb48213f
2014-07-01 15:54:54,678 DEBUG [RS_OPEN_REGION-sceplus-vm48:60020-0] regionserver.HRegion: Applied 0, skipped 50061, firstSequenceidInLog=1998, maxSequenceidInLog=2007, path=hdfs://master:54310/hbase/data/default/usertable/1dc0aa61f0e3f43a53375118eb48213f/recovered.edits/0000000000000002007
2014-07-01 15:54:54,678 DEBUG [RS_OPEN_REGION-sceplus-vm48:60020-0] regionserver.HRegion: Empty memstore size for the current region usertable,user4,1404228265526.1dc0aa61f0e3f43a53375118eb48213f.
2014-07-01 15:54:54,681 DEBUG [RS_OPEN_REGION-sceplus-vm48:60020-0] regionserver.HRegion: Deleted recovered.edits file=hdfs://master:54310/hbase/data/default/usertable/1dc0aa61f0e3f43a53375118eb48213f/recovered.edits/0000000000000001442
2014-07-01 15:54:54,682 DEBUG [RS_OPEN_REGION-sceplus-vm48:60020-0] regionserver.HRegion: Deleted recovered.edits file=hdfs://master:54310/hbase/data/default/usertable/1dc0aa61f0e3f43a53375118eb48213f/recovered.edits/0000000000000001465
2014-07-01 15:54:54,683 DEBUG [RS_OPEN_REGION-sceplus-vm48:60020-0] regionserver.HRegion: Deleted recovered.edits file=hdfs://master:54310/hbase/data/default/usertable/1dc0aa61f0e3f43a53375118eb48213f/recovered.edits/0000000000000001490
2014-07-01 15:54:54,685 DEBUG [RS_OPEN_REGION-sceplus-vm48:60020-0] regionserver.HRegion: Deleted recovered.edits file=hdfs://master:54310/hbase/data/default/usertable/1dc0aa61f0e3f43a53375118eb48213f/recovered.edits/0000000000000001510
2014-07-01 15:54:54,686 DEBUG [RS_OPEN_REGION-sceplus-vm48:60020-0] regionserver.HRegion: Deleted recovered.edits file=hdfs://master:54310/hbase/data/default/usertable/1dc0aa61f0e3f43a53375118eb48213f/recovered.edits/0000000000000001531
2014-07-01 15:54:54,687 DEBUG [RS_OPEN_REGION-sceplus-vm48:60020-0] regionserver.HRegion: Deleted recovered.edits file=hdfs://master:54310/hbase/data/default/usertable/1dc0aa61f0e3f43a53375118eb48213f/recovered.edits/0000000000000001547
2014-07-01 15:54:54,689 DEBUG [RS_OPEN_REGION-sceplus-vm48:60020-0] regionserver.HRegion: Deleted recovered.edits file=hdfs://master:54310/hbase/data/default/usertable/1dc0aa61f0e3f43a53375118eb48213f/recovered.edits/0000000000000001566
2014-07-01 15:54:54,690 DEBUG [RS_OPEN_REGION-sceplus-vm48:60020-0] regionserver.HRegion: Deleted recovered.edits file=hdfs://master:54310/hbase/data/default/usertable/1dc0aa61f0e3f43a53375118eb48213f/recovered.edits/0000000000000001584
2014-07-01 15:54:54,691 DEBUG [RS_OPEN_REGION-sceplus-vm48:60020-0] regionserver.HRegion: Deleted recovered.edits file=hdfs://master:54310/hbase/data/default/usertable/1dc0aa61f0e3f43a53375118eb48213f/recovered.edits/0000000000000001605
2014-07-01 15:54:54,692 DEBUG [RS_OPEN_REGION-sceplus-vm48:60020-0] regionserver.HRegion: Deleted recovered.edits file=hdfs://master:54310/hbase/data/default/usertable/1dc0aa61f0e3f43a53375118eb48213f/recovered.edits/0000000000000001625
2014-07-01 15:54:54,693 DEBUG [RS_OPEN_REGION-sceplus-vm48:60020-0] regionserver.HRegion: Deleted recovered.edits file=hdfs://master:54310/hbase/data/default/usertable/1dc0aa61f0e3f43a53375118eb48213f/recovered.edits/0000000000000001645
2014-07-01 15:54:54,695 DEBUG [RS_OPEN_REGION-sceplus-vm48:60020-0] regionserver.HRegion: Deleted recovered.edits file=hdfs://master:54310/hbase/data/default/usertable/1dc0aa61f0e3f43a53375118eb48213f/recovered.edits/0000000000000001667
2014-07-01 15:54:54,696 DEBUG [RS_OPEN_REGION-sceplus-vm48:60020-0] regionserver.HRegion: Deleted recovered.edits file=hdfs://master:54310/hbase/data/default/usertable/1dc0aa61f0e3f43a53375118eb48213f/recovered.edits/0000000000000001686
2014-07-01 15:54:54,697 DEBUG [RS_OPEN_REGION-sceplus-vm48:60020-0] regionserver.HRegion: Deleted recovered.edits file=hdfs://master:54310/hbase/data/default/usertable/1dc0aa61f0e3f43a53375118eb48213f/recovered.edits/0000000000000001705
2014-07-01 15:54:54,698 DEBUG [RS_OPEN_REGION-sceplus-vm48:60020-0] regionserver.HRegion: Deleted recovered.edits file=hdfs://master:54310/hbase/data/default/usertable/1dc0aa61f0e3f43a53375118eb48213f/recovered.edits/0000000000000001720
2014-07-01 15:54:54,699 DEBUG [RS_OPEN_REGION-sceplus-vm48:60020-0] regionserver.HRegion: Deleted recovered.edits file=hdfs://master:54310/hbase/data/default/usertable/1dc0aa61f0e3f43a53375118eb48213f/recovered.edits/0000000000000001744
2014-07-01 15:54:54,700 DEBUG [RS_OPEN_REGION-sceplus-vm48:60020-0] regionserver.HRegion: Deleted recovered.edits file=hdfs://master:54310/hbase/data/default/usertable/1dc0aa61f0e3f43a53375118eb48213f/recovered.edits/0000000000000001762
2014-07-01 15:54:54,701 DEBUG [RS_OPEN_REGION-sceplus-vm48:60020-0] regionserver.HRegion: Deleted recovered.edits file=hdfs://master:54310/hbase/data/default/usertable/1dc0aa61f0e3f43a53375118eb48213f/recovered.edits/0000000000000001784
2014-07-01 15:54:54,703 DEBUG [RS_OPEN_REGION-sceplus-vm48:60020-0] regionserver.HRegion: Deleted recovered.edits file=hdfs://master:54310/hbase/data/default/usertable/1dc0aa61f0e3f43a53375118eb48213f/recovered.edits/0000000000000001802
2014-07-01 15:54:54,704 DEBUG [RS_OPEN_REGION-sceplus-vm48:60020-0] regionserver.HRegion: Deleted recovered.edits file=hdfs://master:54310/hbase/data/default/usertable/1dc0aa61f0e3f43a53375118eb48213f/recovered.edits/0000000000000001820
2014-07-01 15:54:54,705 DEBUG [RS_OPEN_REGION-sceplus-vm48:60020-0] regionserver.HRegion: Deleted recovered.edits file=hdfs://master:54310/hbase/data/default/usertable/1dc0aa61f0e3f43a53375118eb48213f/recovered.edits/0000000000000001843
2014-07-01 15:54:54,706 DEBUG [RS_OPEN_REGION-sceplus-vm48:60020-0] regionserver.HRegion: Deleted recovered.edits file=hdfs://master:54310/hbase/data/default/usertable/1dc0aa61f0e3f43a53375118eb48213f/recovered.edits/0000000000000001861
2014-07-01 15:54:54,707 DEBUG [RS_OPEN_REGION-sceplus-vm48:60020-0] regionserver.HRegion: Deleted recovered.edits file=hdfs://master:54310/hbase/data/default/usertable/1dc0aa61f0e3f43a53375118eb48213f/recovered.edits/0000000000000001876
2014-07-01 15:54:54,708 DEBUG [RS_OPEN_REGION-sceplus-vm48:60020-0] regionserver.HRegion: Deleted recovered.edits file=hdfs://master:54310/hbase/data/default/usertable/1dc0aa61f0e3f43a53375118eb48213f/recovered.edits/0000000000000001895
2014-07-01 15:54:54,709 DEBUG [RS_OPEN_REGION-sceplus-vm48:60020-0] regionserver.HRegion: Deleted recovered.edits file=hdfs://master:54310/hbase/data/default/usertable/1dc0aa61f0e3f43a53375118eb48213f/recovered.edits/0000000000000001912
2014-07-01 15:54:54,711 DEBUG [RS_OPEN_REGION-sceplus-vm48:60020-0] regionserver.HRegion: Deleted recovered.edits file=hdfs://master:54310/hbase/data/default/usertable/1dc0aa61f0e3f43a53375118eb48213f/recovered.edits/0000000000000001930
2014-07-01 15:54:54,712 DEBUG [RS_OPEN_REGION-sceplus-vm48:60020-0] regionserver.HRegion: Deleted recovered.edits file=hdfs://master:54310/hbase/data/default/usertable/1dc0aa61f0e3f43a53375118eb48213f/recovered.edits/0000000000000001947
2014-07-01 15:54:54,713 DEBUG [RS_OPEN_REGION-sceplus-vm48:60020-0] regionserver.HRegion: Deleted recovered.edits file=hdfs://master:54310/hbase/data/default/usertable/1dc0aa61f0e3f43a53375118eb48213f/recovered.edits/0000000000000001963
2014-07-01 15:54:54,714 DEBUG [RS_OPEN_REGION-sceplus-vm48:60020-0] regionserver.HRegion: Deleted recovered.edits file=hdfs://master:54310/hbase/data/default/usertable/1dc0aa61f0e3f43a53375118eb48213f/recovered.edits/0000000000000001980
2014-07-01 15:54:54,715 DEBUG [RS_OPEN_REGION-sceplus-vm48:60020-0] regionserver.HRegion: Deleted recovered.edits file=hdfs://master:54310/hbase/data/default/usertable/1dc0aa61f0e3f43a53375118eb48213f/recovered.edits/0000000000000001997
2014-07-01 15:54:54,716 DEBUG [RS_OPEN_REGION-sceplus-vm48:60020-0] regionserver.HRegion: Deleted recovered.edits file=hdfs://master:54310/hbase/data/default/usertable/1dc0aa61f0e3f43a53375118eb48213f/recovered.edits/0000000000000002007
2014-07-01 15:54:54,721 INFO  [RS_OPEN_REGION-sceplus-vm48:60020-0] regionserver.HRegion: Onlined 1dc0aa61f0e3f43a53375118eb48213f; next sequenceid=2008
2014-07-01 15:54:54,721 DEBUG [RS_OPEN_REGION-sceplus-vm48:60020-0] zookeeper.ZKAssign: regionserver:60020-0x46f4224a300001, quorum=sceplus-vm49.almaden.ibm.com:2181,sceplus-vm48.almaden.ibm.com:2181, baseZNode=/hbase Attempting to retransition opening state of node 1dc0aa61f0e3f43a53375118eb48213f
2014-07-01 15:54:54,725 INFO  [PostOpenDeployTasks:1dc0aa61f0e3f43a53375118eb48213f] regionserver.HRegionServer: Post open deploy tasks for region=usertable,user4,1404228265526.1dc0aa61f0e3f43a53375118eb48213f.
2014-07-01 15:54:54,818 INFO  [PostOpenDeployTasks:1dc0aa61f0e3f43a53375118eb48213f] catalog.MetaEditor: Updated row usertable,user4,1404228265526.1dc0aa61f0e3f43a53375118eb48213f. with server=sceplus-vm48.almaden.ibm.com,60020,1404255227840
2014-07-01 15:54:54,819 INFO  [PostOpenDeployTasks:1dc0aa61f0e3f43a53375118eb48213f] regionserver.HRegionServer: Finished post open deploy task for usertable,user4,1404228265526.1dc0aa61f0e3f43a53375118eb48213f.
2014-07-01 15:54:54,819 DEBUG [RS_OPEN_REGION-sceplus-vm48:60020-0] zookeeper.ZKAssign: regionserver:60020-0x46f4224a300001, quorum=sceplus-vm49.almaden.ibm.com:2181,sceplus-vm48.almaden.ibm.com:2181, baseZNode=/hbase Transitioning 1dc0aa61f0e3f43a53375118eb48213f from RS_ZK_REGION_OPENING to RS_ZK_REGION_OPENED
2014-07-01 15:54:54,841 DEBUG [RS_OPEN_REGION-sceplus-vm48:60020-0] zookeeper.ZKAssign: regionserver:60020-0x46f4224a300001, quorum=sceplus-vm49.almaden.ibm.com:2181,sceplus-vm48.almaden.ibm.com:2181, baseZNode=/hbase Transitioned node 1dc0aa61f0e3f43a53375118eb48213f from RS_ZK_REGION_OPENING to RS_ZK_REGION_OPENED
2014-07-01 15:54:54,841 DEBUG [RS_OPEN_REGION-sceplus-vm48:60020-0] handler.OpenRegionHandler: Transitioned 1dc0aa61f0e3f43a53375118eb48213f to OPENED in zk on sceplus-vm48.almaden.ibm.com,60020,1404255227840
2014-07-01 15:54:54,841 DEBUG [RS_OPEN_REGION-sceplus-vm48:60020-0] handler.OpenRegionHandler: Opened usertable,user4,1404228265526.1dc0aa61f0e3f43a53375118eb48213f. on sceplus-vm48.almaden.ibm.com,60020,1404255227840
2014-07-01 15:54:54,842 DEBUG [RS_OPEN_REGION-sceplus-vm48:60020-0] zookeeper.ZKAssign: regionserver:60020-0x46f4224a300001, quorum=sceplus-vm49.almaden.ibm.com:2181,sceplus-vm48.almaden.ibm.com:2181, baseZNode=/hbase Transitioning 122e92eabfab38cb15a9965b21db25c5 from M_ZK_REGION_OFFLINE to RS_ZK_REGION_OPENING
2014-07-01 15:54:54,847 DEBUG [RS_OPEN_REGION-sceplus-vm48:60020-0] zookeeper.ZKAssign: regionserver:60020-0x46f4224a300001, quorum=sceplus-vm49.almaden.ibm.com:2181,sceplus-vm48.almaden.ibm.com:2181, baseZNode=/hbase Transitioned node 122e92eabfab38cb15a9965b21db25c5 from M_ZK_REGION_OFFLINE to RS_ZK_REGION_OPENING
2014-07-01 15:54:54,847 DEBUG [RS_OPEN_REGION-sceplus-vm48:60020-0] regionserver.HRegion: Opening region: {ENCODED => 122e92eabfab38cb15a9965b21db25c5, NAME => 'usertable,user5,1404228265526.122e92eabfab38cb15a9965b21db25c5.', STARTKEY => 'user5', ENDKEY => 'user6'}
2014-07-01 15:54:54,848 DEBUG [RS_OPEN_REGION-sceplus-vm48:60020-0] regionserver.MetricsRegionSourceImpl: Creating new MetricsRegionSourceImpl for table usertable 122e92eabfab38cb15a9965b21db25c5
2014-07-01 15:54:54,848 DEBUG [RS_OPEN_REGION-sceplus-vm48:60020-0] regionserver.HRegion: Instantiated usertable,user5,1404228265526.122e92eabfab38cb15a9965b21db25c5.
2014-07-01 15:54:54,853 INFO  [StoreOpener-122e92eabfab38cb15a9965b21db25c5-1] compactions.CompactionConfiguration: size [268435456, 9223372036854775807); files [3, 10); ratio 1.200000; off-peak ratio 5.000000; throttle point 5368709120; delete expired; major period 0, major jitter 0.500000
2014-07-01 15:54:54,880 DEBUG [StoreOpener-122e92eabfab38cb15a9965b21db25c5-1] regionserver.HStore: loaded hdfs://master:54310/hbase/data/default/usertable/122e92eabfab38cb15a9965b21db25c5/family/f4a8142044e841dcb47eae4bbbacffe5, isReference=false, isBulkLoadResult=false, seqid=4674, majorCompaction=true
2014-07-01 15:54:54,897 DEBUG [RS_OPEN_REGION-sceplus-vm48:60020-0] regionserver.HRegion: Found 17 recovered edits file(s) under hdfs://master:54310/hbase/data/default/usertable/122e92eabfab38cb15a9965b21db25c5
2014-07-01 15:54:54,898 DEBUG [RS_OPEN_REGION-sceplus-vm48:60020-0] regionserver.HRegion: Maximum sequenceid for this log is 4474 and minimum sequenceid for the region is 4674, skipped the whole file, path=hdfs://master:54310/hbase/data/default/usertable/122e92eabfab38cb15a9965b21db25c5/recovered.edits/0000000000000004474
2014-07-01 15:54:54,900 DEBUG [RS_OPEN_REGION-sceplus-vm48:60020-0] regionserver.HRegion: Maximum sequenceid for this log is 4488 and minimum sequenceid for the region is 4674, skipped the whole file, path=hdfs://master:54310/hbase/data/default/usertable/122e92eabfab38cb15a9965b21db25c5/recovered.edits/0000000000000004488
2014-07-01 15:54:54,901 DEBUG [RS_OPEN_REGION-sceplus-vm48:60020-0] regionserver.HRegion: Maximum sequenceid for this log is 4504 and minimum sequenceid for the region is 4674, skipped the whole file, path=hdfs://master:54310/hbase/data/default/usertable/122e92eabfab38cb15a9965b21db25c5/recovered.edits/0000000000000004504
2014-07-01 15:54:54,903 DEBUG [RS_OPEN_REGION-sceplus-vm48:60020-0] regionserver.HRegion: Maximum sequenceid for this log is 4519 and minimum sequenceid for the region is 4674, skipped the whole file, path=hdfs://master:54310/hbase/data/default/usertable/122e92eabfab38cb15a9965b21db25c5/recovered.edits/0000000000000004519
2014-07-01 15:54:54,904 DEBUG [RS_OPEN_REGION-sceplus-vm48:60020-0] regionserver.HRegion: Maximum sequenceid for this log is 4535 and minimum sequenceid for the region is 4674, skipped the whole file, path=hdfs://master:54310/hbase/data/default/usertable/122e92eabfab38cb15a9965b21db25c5/recovered.edits/0000000000000004535
2014-07-01 15:54:54,906 DEBUG [RS_OPEN_REGION-sceplus-vm48:60020-0] regionserver.HRegion: Maximum sequenceid for this log is 4554 and minimum sequenceid for the region is 4674, skipped the whole file, path=hdfs://master:54310/hbase/data/default/usertable/122e92eabfab38cb15a9965b21db25c5/recovered.edits/0000000000000004554
2014-07-01 15:54:54,907 DEBUG [RS_OPEN_REGION-sceplus-vm48:60020-0] regionserver.HRegion: Maximum sequenceid for this log is 4571 and minimum sequenceid for the region is 4674, skipped the whole file, path=hdfs://master:54310/hbase/data/default/usertable/122e92eabfab38cb15a9965b21db25c5/recovered.edits/0000000000000004571
2014-07-01 15:54:54,909 DEBUG [RS_OPEN_REGION-sceplus-vm48:60020-0] regionserver.HRegion: Maximum sequenceid for this log is 4586 and minimum sequenceid for the region is 4674, skipped the whole file, path=hdfs://master:54310/hbase/data/default/usertable/122e92eabfab38cb15a9965b21db25c5/recovered.edits/0000000000000004586
2014-07-01 15:54:54,910 DEBUG [RS_OPEN_REGION-sceplus-vm48:60020-0] regionserver.HRegion: Maximum sequenceid for this log is 4601 and minimum sequenceid for the region is 4674, skipped the whole file, path=hdfs://master:54310/hbase/data/default/usertable/122e92eabfab38cb15a9965b21db25c5/recovered.edits/0000000000000004601
2014-07-01 15:54:54,911 DEBUG [RS_OPEN_REGION-sceplus-vm48:60020-0] regionserver.HRegion: Maximum sequenceid for this log is 4616 and minimum sequenceid for the region is 4674, skipped the whole file, path=hdfs://master:54310/hbase/data/default/usertable/122e92eabfab38cb15a9965b21db25c5/recovered.edits/0000000000000004616
2014-07-01 15:54:54,913 DEBUG [RS_OPEN_REGION-sceplus-vm48:60020-0] regionserver.HRegion: Maximum sequenceid for this log is 4634 and minimum sequenceid for the region is 4674, skipped the whole file, path=hdfs://master:54310/hbase/data/default/usertable/122e92eabfab38cb15a9965b21db25c5/recovered.edits/0000000000000004634
2014-07-01 15:54:54,914 DEBUG [RS_OPEN_REGION-sceplus-vm48:60020-0] regionserver.HRegion: Maximum sequenceid for this log is 4651 and minimum sequenceid for the region is 4674, skipped the whole file, path=hdfs://master:54310/hbase/data/default/usertable/122e92eabfab38cb15a9965b21db25c5/recovered.edits/0000000000000004651
2014-07-01 15:54:54,916 DEBUG [RS_OPEN_REGION-sceplus-vm48:60020-0] regionserver.HRegion: Maximum sequenceid for this log is 4667 and minimum sequenceid for the region is 4674, skipped the whole file, path=hdfs://master:54310/hbase/data/default/usertable/122e92eabfab38cb15a9965b21db25c5/recovered.edits/0000000000000004667
2014-07-01 15:54:54,917 INFO  [RS_OPEN_REGION-sceplus-vm48:60020-0] regionserver.HRegion: Replaying edits from hdfs://master:54310/hbase/data/default/usertable/122e92eabfab38cb15a9965b21db25c5/recovered.edits/0000000000000004684
2014-07-01 15:54:55,130 DEBUG [RS_OPEN_REGION-sceplus-vm48:60020-2] zookeeper.ZKAssign: regionserver:60020-0x46f4224a300001, quorum=sceplus-vm49.almaden.ibm.com:2181,sceplus-vm48.almaden.ibm.com:2181, baseZNode=/hbase Attempting to retransition opening state of node 9cef9e7d34f96f1d9b111be50040a208
2014-07-01 15:54:55,135 DEBUG [RS_OPEN_REGION-sceplus-vm48:60020-2] regionserver.HRegion: Applied 51050, skipped 51710, firstSequenceidInLog=5447, maxSequenceidInLog=5496, path=hdfs://master:54310/hbase/data/default/usertable/9cef9e7d34f96f1d9b111be50040a208/recovered.edits/0000000000000005496
2014-07-01 15:54:55,138 INFO  [RS_OPEN_REGION-sceplus-vm48:60020-2] regionserver.HRegion: Replaying edits from hdfs://master:54310/hbase/data/default/usertable/9cef9e7d34f96f1d9b111be50040a208/recovered.edits/0000000000000005538
2014-07-01 15:54:55,298 DEBUG [RS_OPEN_REGION-sceplus-vm48:60020-1] zookeeper.ZKAssign: regionserver:60020-0x46f4224a300001, quorum=sceplus-vm49.almaden.ibm.com:2181,sceplus-vm48.almaden.ibm.com:2181, baseZNode=/hbase Attempting to retransition opening state of node 99b73df170b514f6773e742a495216c4
2014-07-01 15:54:55,303 DEBUG [RS_OPEN_REGION-sceplus-vm48:60020-1] regionserver.HRegion: Applied 70320, skipped 45870, firstSequenceidInLog=1922, maxSequenceidInLog=1941, path=hdfs://master:54310/hbase/data/default/usertable/99b73df170b514f6773e742a495216c4/recovered.edits/0000000000000001941
2014-07-01 15:54:55,305 INFO  [RS_OPEN_REGION-sceplus-vm48:60020-1] regionserver.HRegion: Replaying edits from hdfs://master:54310/hbase/data/default/usertable/99b73df170b514f6773e742a495216c4/recovered.edits/0000000000000001958
2014-07-01 15:54:55,381 DEBUG [RS_OPEN_REGION-sceplus-vm48:60020-0] zookeeper.ZKAssign: regionserver:60020-0x46f4224a300001, quorum=sceplus-vm49.almaden.ibm.com:2181,sceplus-vm48.almaden.ibm.com:2181, baseZNode=/hbase Attempting to retransition opening state of node 122e92eabfab38cb15a9965b21db25c5
2014-07-01 15:54:55,384 DEBUG [RS_OPEN_REGION-sceplus-vm48:60020-0] regionserver.HRegion: Applied 63210, skipped 38420, firstSequenceidInLog=4668, maxSequenceidInLog=4684, path=hdfs://master:54310/hbase/data/default/usertable/122e92eabfab38cb15a9965b21db25c5/recovered.edits/0000000000000004684
2014-07-01 15:54:55,386 INFO  [RS_OPEN_REGION-sceplus-vm48:60020-0] regionserver.HRegion: Replaying edits from hdfs://master:54310/hbase/data/default/usertable/122e92eabfab38cb15a9965b21db25c5/recovered.edits/0000000000000004700
2014-07-01 15:54:55,737 DEBUG [RS_OPEN_REGION-sceplus-vm48:60020-2] zookeeper.ZKAssign: regionserver:60020-0x46f4224a300001, quorum=sceplus-vm49.almaden.ibm.com:2181,sceplus-vm48.almaden.ibm.com:2181, baseZNode=/hbase Attempting to retransition opening state of node 9cef9e7d34f96f1d9b111be50040a208
2014-07-01 15:54:55,741 DEBUG [RS_OPEN_REGION-sceplus-vm48:60020-2] regionserver.HRegion: Applied 82750, skipped 0, firstSequenceidInLog=5497, maxSequenceidInLog=5538, path=hdfs://master:54310/hbase/data/default/usertable/9cef9e7d34f96f1d9b111be50040a208/recovered.edits/0000000000000005538
2014-07-01 15:54:55,766 INFO  [RS_OPEN_REGION-sceplus-vm48:60020-2] regionserver.HRegion: Replaying edits from hdfs://master:54310/hbase/data/default/usertable/9cef9e7d34f96f1d9b111be50040a208/recovered.edits/0000000000000005589
2014-07-01 15:54:56,039 DEBUG [RS_OPEN_REGION-sceplus-vm48:60020-0] zookeeper.ZKAssign: regionserver:60020-0x46f4224a300001, quorum=sceplus-vm49.almaden.ibm.com:2181,sceplus-vm48.almaden.ibm.com:2181, baseZNode=/hbase Attempting to retransition opening state of node 122e92eabfab38cb15a9965b21db25c5
2014-07-01 15:54:56,041 DEBUG [RS_OPEN_REGION-sceplus-vm48:60020-0] regionserver.HRegion: Applied 98770, skipped 0, firstSequenceidInLog=4685, maxSequenceidInLog=4700, path=hdfs://master:54310/hbase/data/default/usertable/122e92eabfab38cb15a9965b21db25c5/recovered.edits/0000000000000004700
2014-07-01 15:54:56,043 INFO  [RS_OPEN_REGION-sceplus-vm48:60020-0] regionserver.HRegion: Replaying edits from hdfs://master:54310/hbase/data/default/usertable/122e92eabfab38cb15a9965b21db25c5/recovered.edits/0000000000000004715
2014-07-01 15:54:56,074 DEBUG [RS_OPEN_REGION-sceplus-vm48:60020-1] zookeeper.ZKAssign: regionserver:60020-0x46f4224a300001, quorum=sceplus-vm49.almaden.ibm.com:2181,sceplus-vm48.almaden.ibm.com:2181, baseZNode=/hbase Attempting to retransition opening state of node 99b73df170b514f6773e742a495216c4
2014-07-01 15:54:56,076 DEBUG [RS_OPEN_REGION-sceplus-vm48:60020-1] regionserver.HRegion: Applied 108520, skipped 0, firstSequenceidInLog=1942, maxSequenceidInLog=1958, path=hdfs://master:54310/hbase/data/default/usertable/99b73df170b514f6773e742a495216c4/recovered.edits/0000000000000001958
2014-07-01 15:54:56,078 INFO  [RS_OPEN_REGION-sceplus-vm48:60020-1] regionserver.HRegion: Replaying edits from hdfs://master:54310/hbase/data/default/usertable/99b73df170b514f6773e742a495216c4/recovered.edits/0000000000000001976
2014-07-01 15:54:56,421 DEBUG [RS_OPEN_REGION-sceplus-vm48:60020-2] zookeeper.ZKAssign: regionserver:60020-0x46f4224a300001, quorum=sceplus-vm49.almaden.ibm.com:2181,sceplus-vm48.almaden.ibm.com:2181, baseZNode=/hbase Attempting to retransition opening state of node 9cef9e7d34f96f1d9b111be50040a208
2014-07-01 15:54:56,424 DEBUG [RS_OPEN_REGION-sceplus-vm48:60020-2] regionserver.HRegion: Applied 94540, skipped 0, firstSequenceidInLog=5539, maxSequenceidInLog=5589, path=hdfs://master:54310/hbase/data/default/usertable/9cef9e7d34f96f1d9b111be50040a208/recovered.edits/0000000000000005589
2014-07-01 15:54:56,427 INFO  [RS_OPEN_REGION-sceplus-vm48:60020-2] regionserver.HRegion: Replaying edits from hdfs://master:54310/hbase/data/default/usertable/9cef9e7d34f96f1d9b111be50040a208/recovered.edits/0000000000000005641
2014-07-01 15:54:56,849 DEBUG [RS_OPEN_REGION-sceplus-vm48:60020-0] zookeeper.ZKAssign: regionserver:60020-0x46f4224a300001, quorum=sceplus-vm49.almaden.ibm.com:2181,sceplus-vm48.almaden.ibm.com:2181, baseZNode=/hbase Attempting to retransition opening state of node 122e92eabfab38cb15a9965b21db25c5
2014-07-01 15:54:56,851 DEBUG [RS_OPEN_REGION-sceplus-vm48:60020-0] regionserver.HRegion: Applied 93070, skipped 0, firstSequenceidInLog=4701, maxSequenceidInLog=4715, path=hdfs://master:54310/hbase/data/default/usertable/122e92eabfab38cb15a9965b21db25c5/recovered.edits/0000000000000004715
2014-07-01 15:54:56,854 INFO  [RS_OPEN_REGION-sceplus-vm48:60020-0] regionserver.HRegion: Replaying edits from hdfs://master:54310/hbase/data/default/usertable/122e92eabfab38cb15a9965b21db25c5/recovered.edits/0000000000000004724
2014-07-01 15:54:56,876 INFO  [Priority.RpcServer.handler=3,port=60020] regionserver.HRegionServer: Close 1dc0aa61f0e3f43a53375118eb48213f, via zk=yes, znode version=0, on null
2014-07-01 15:54:56,877 DEBUG [RS_CLOSE_REGION-sceplus-vm48:60020-0] handler.CloseRegionHandler: Processing close of usertable,user4,1404228265526.1dc0aa61f0e3f43a53375118eb48213f.
2014-07-01 15:54:56,881 DEBUG [RS_CLOSE_REGION-sceplus-vm48:60020-0] regionserver.HRegion: Closing usertable,user4,1404228265526.1dc0aa61f0e3f43a53375118eb48213f.: disabling compactions & flushes
2014-07-01 15:54:56,881 DEBUG [RS_CLOSE_REGION-sceplus-vm48:60020-0] regionserver.HRegion: Updates disabled for region usertable,user4,1404228265526.1dc0aa61f0e3f43a53375118eb48213f.
2014-07-01 15:54:56,967 INFO  [StoreCloserThread-usertable,user4,1404228265526.1dc0aa61f0e3f43a53375118eb48213f.-1] regionserver.HStore: Closed family
2014-07-01 15:54:56,971 INFO  [RS_CLOSE_REGION-sceplus-vm48:60020-0] regionserver.HRegion: Closed usertable,user4,1404228265526.1dc0aa61f0e3f43a53375118eb48213f.
2014-07-01 15:54:56,971 DEBUG [RS_CLOSE_REGION-sceplus-vm48:60020-0] zookeeper.ZKAssign: regionserver:60020-0x46f4224a300001, quorum=sceplus-vm49.almaden.ibm.com:2181,sceplus-vm48.almaden.ibm.com:2181, baseZNode=/hbase Transitioning 1dc0aa61f0e3f43a53375118eb48213f from M_ZK_REGION_CLOSING to RS_ZK_REGION_CLOSED
2014-07-01 15:54:56,976 DEBUG [RS_CLOSE_REGION-sceplus-vm48:60020-0] zookeeper.ZKAssign: regionserver:60020-0x46f4224a300001, quorum=sceplus-vm49.almaden.ibm.com:2181,sceplus-vm48.almaden.ibm.com:2181, baseZNode=/hbase Transitioned node 1dc0aa61f0e3f43a53375118eb48213f from M_ZK_REGION_CLOSING to RS_ZK_REGION_CLOSED
2014-07-01 15:54:56,976 DEBUG [RS_CLOSE_REGION-sceplus-vm48:60020-0] handler.CloseRegionHandler: Set closed state in zk for usertable,user4,1404228265526.1dc0aa61f0e3f43a53375118eb48213f. on sceplus-vm48.almaden.ibm.com,60020,1404255227840
2014-07-01 15:54:56,976 DEBUG [RS_CLOSE_REGION-sceplus-vm48:60020-0] handler.CloseRegionHandler: Closed usertable,user4,1404228265526.1dc0aa61f0e3f43a53375118eb48213f.
2014-07-01 15:54:57,057 DEBUG [RS_OPEN_REGION-sceplus-vm48:60020-1] zookeeper.ZKAssign: regionserver:60020-0x46f4224a300001, quorum=sceplus-vm49.almaden.ibm.com:2181,sceplus-vm48.almaden.ibm.com:2181, baseZNode=/hbase Attempting to retransition opening state of node 99b73df170b514f6773e742a495216c4
2014-07-01 15:54:57,060 DEBUG [RS_OPEN_REGION-sceplus-vm48:60020-1] regionserver.HRegion: Applied 109600, skipped 0, firstSequenceidInLog=1959, maxSequenceidInLog=1976, path=hdfs://master:54310/hbase/data/default/usertable/99b73df170b514f6773e742a495216c4/recovered.edits/0000000000000001976
2014-07-01 15:54:57,063 INFO  [RS_OPEN_REGION-sceplus-vm48:60020-1] regionserver.HRegion: Replaying edits from hdfs://master:54310/hbase/data/default/usertable/99b73df170b514f6773e742a495216c4/recovered.edits/0000000000000001992
2014-07-01 15:54:57,311 DEBUG [RS_OPEN_REGION-sceplus-vm48:60020-2] zookeeper.ZKAssign: regionserver:60020-0x46f4224a300001, quorum=sceplus-vm49.almaden.ibm.com:2181,sceplus-vm48.almaden.ibm.com:2181, baseZNode=/hbase Attempting to retransition opening state of node 9cef9e7d34f96f1d9b111be50040a208
2014-07-01 15:54:57,313 DEBUG [RS_OPEN_REGION-sceplus-vm48:60020-2] regionserver.HRegion: Applied 94870, skipped 0, firstSequenceidInLog=5590, maxSequenceidInLog=5641, path=hdfs://master:54310/hbase/data/default/usertable/9cef9e7d34f96f1d9b111be50040a208/recovered.edits/0000000000000005641
2014-07-01 15:54:57,315 INFO  [RS_OPEN_REGION-sceplus-vm48:60020-2] regionserver.HRegion: Replaying edits from hdfs://master:54310/hbase/data/default/usertable/9cef9e7d34f96f1d9b111be50040a208/recovered.edits/0000000000000005688
2014-07-01 15:54:57,320 DEBUG [RS_OPEN_REGION-sceplus-vm48:60020-0] regionserver.HStore: Completing compaction from the WAL marker
2014-07-01 15:54:57,322 DEBUG [RS_OPEN_REGION-sceplus-vm48:60020-0] regionserver.HStore: Removing store files after compaction...
2014-07-01 15:54:57,322 DEBUG [RS_OPEN_REGION-sceplus-vm48:60020-0] backup.HFileArchiver: No store files to dispose, done!
2014-07-01 15:54:57,322 DEBUG [RS_OPEN_REGION-sceplus-vm48:60020-0] zookeeper.ZKAssign: regionserver:60020-0x46f4224a300001, quorum=sceplus-vm49.almaden.ibm.com:2181,sceplus-vm48.almaden.ibm.com:2181, baseZNode=/hbase Attempting to retransition opening state of node 122e92eabfab38cb15a9965b21db25c5
2014-07-01 15:54:57,324 DEBUG [RS_OPEN_REGION-sceplus-vm48:60020-0] regionserver.HRegion: Applied 50930, skipped 1, firstSequenceidInLog=4716, maxSequenceidInLog=4724, path=hdfs://master:54310/hbase/data/default/usertable/122e92eabfab38cb15a9965b21db25c5/recovered.edits/0000000000000004724
2014-07-01 15:54:57,324 DEBUG [RS_OPEN_REGION-sceplus-vm48:60020-0] regionserver.HRegion: Started memstore flush for usertable,user5,1404228265526.122e92eabfab38cb15a9965b21db25c5., current region memstore size 84.0m; wal is null, using passed sequenceid=4724
2014-07-01 15:54:57,381 DEBUG [RS_OPEN_REGION-sceplus-vm48:60020-0] util.FSUtils: DFS Client does not support most favored nodes create; using default create
2014-07-01 15:54:57,869 DEBUG [RS_OPEN_REGION-sceplus-vm48:60020-1] zookeeper.ZKAssign: regionserver:60020-0x46f4224a300001, quorum=sceplus-vm49.almaden.ibm.com:2181,sceplus-vm48.almaden.ibm.com:2181, baseZNode=/hbase Attempting to retransition opening state of node 99b73df170b514f6773e742a495216c4
2014-07-01 15:54:57,949 DEBUG [RS_OPEN_REGION-sceplus-vm48:60020-1] regionserver.HRegion: Applied 101970, skipped 0, firstSequenceidInLog=1977, maxSequenceidInLog=1992, path=hdfs://master:54310/hbase/data/default/usertable/99b73df170b514f6773e742a495216c4/recovered.edits/0000000000000001992
2014-07-01 15:54:57,952 INFO  [RS_OPEN_REGION-sceplus-vm48:60020-1] regionserver.HRegion: Replaying edits from hdfs://master:54310/hbase/data/default/usertable/99b73df170b514f6773e742a495216c4/recovered.edits/0000000000000002002
2014-07-01 15:54:57,990 DEBUG [RS_OPEN_REGION-sceplus-vm48:60020-2] zookeeper.ZKAssign: regionserver:60020-0x46f4224a300001, quorum=sceplus-vm49.almaden.ibm.com:2181,sceplus-vm48.almaden.ibm.com:2181, baseZNode=/hbase Attempting to retransition opening state of node 9cef9e7d34f96f1d9b111be50040a208
2014-07-01 15:54:58,156 DEBUG [RS_OPEN_REGION-sceplus-vm48:60020-2] regionserver.HRegion: Applied 91190, skipped 0, firstSequenceidInLog=5642, maxSequenceidInLog=5688, path=hdfs://master:54310/hbase/data/default/usertable/9cef9e7d34f96f1d9b111be50040a208/recovered.edits/0000000000000005688
2014-07-01 15:54:58,158 INFO  [RS_OPEN_REGION-sceplus-vm48:60020-2] regionserver.HRegion: Replaying edits from hdfs://master:54310/hbase/data/default/usertable/9cef9e7d34f96f1d9b111be50040a208/recovered.edits/0000000000000005735
2014-07-01 15:54:58,384 DEBUG [RS_OPEN_REGION-sceplus-vm48:60020-1] regionserver.HStore: Completing compaction from the WAL marker
2014-07-01 15:54:58,385 DEBUG [RS_OPEN_REGION-sceplus-vm48:60020-1] regionserver.HStore: Removing store files after compaction...
2014-07-01 15:54:58,386 DEBUG [RS_OPEN_REGION-sceplus-vm48:60020-1] backup.HFileArchiver: No store files to dispose, done!
2014-07-01 15:54:58,387 DEBUG [RS_OPEN_REGION-sceplus-vm48:60020-1] zookeeper.ZKAssign: regionserver:60020-0x46f4224a300001, quorum=sceplus-vm49.almaden.ibm.com:2181,sceplus-vm48.almaden.ibm.com:2181, baseZNode=/hbase Attempting to retransition opening state of node 99b73df170b514f6773e742a495216c4
2014-07-01 15:54:58,390 DEBUG [RS_OPEN_REGION-sceplus-vm48:60020-1] regionserver.HRegion: Applied 57610, skipped 1, firstSequenceidInLog=1993, maxSequenceidInLog=2002, path=hdfs://master:54310/hbase/data/default/usertable/99b73df170b514f6773e742a495216c4/recovered.edits/0000000000000002002
2014-07-01 15:54:58,391 DEBUG [RS_OPEN_REGION-sceplus-vm48:60020-1] regionserver.HRegion: Started memstore flush for usertable,user8,1404228265526.99b73df170b514f6773e742a495216c4., current region memstore size 123.0m; wal is null, using passed sequenceid=2002
2014-07-01 15:54:58,434 DEBUG [RS_OPEN_REGION-sceplus-vm48:60020-1] util.FSUtils: DFS Client does not support most favored nodes create; using default create
2014-07-01 15:54:58,438 INFO  [RS_OPEN_REGION-sceplus-vm48:60020-1] compress.CodecPool: Got brand-new compressor
2014-07-01 15:54:58,439 INFO  [RS_OPEN_REGION-sceplus-vm48:60020-1] compress.CodecPool: Got brand-new compressor
2014-07-01 15:54:58,962 DEBUG [RS_OPEN_REGION-sceplus-vm48:60020-2] zookeeper.ZKAssign: regionserver:60020-0x46f4224a300001, quorum=sceplus-vm49.almaden.ibm.com:2181,sceplus-vm48.almaden.ibm.com:2181, baseZNode=/hbase Attempting to retransition opening state of node 9cef9e7d34f96f1d9b111be50040a208
2014-07-01 15:55:00,231 INFO  [RS_OPEN_REGION-sceplus-vm48:60020-0] regionserver.DefaultStoreFlusher: Flushed, sequenceid=4724, memsize=84.0m, hasBloomFilter=true, into tmp file hdfs://master:54310/hbase/data/default/usertable/122e92eabfab38cb15a9965b21db25c5/.tmp/138d86aaa4c24d66910e32e3e3232832
2014-07-01 15:55:00,245 DEBUG [RS_OPEN_REGION-sceplus-vm48:60020-0] regionserver.HRegionFileSystem: Committing store file hdfs://master:54310/hbase/data/default/usertable/122e92eabfab38cb15a9965b21db25c5/.tmp/138d86aaa4c24d66910e32e3e3232832 as hdfs://master:54310/hbase/data/default/usertable/122e92eabfab38cb15a9965b21db25c5/family/138d86aaa4c24d66910e32e3e3232832
2014-07-01 15:55:00,257 INFO  [RS_OPEN_REGION-sceplus-vm48:60020-0] regionserver.HStore: Added hdfs://master:54310/hbase/data/default/usertable/122e92eabfab38cb15a9965b21db25c5/family/138d86aaa4c24d66910e32e3e3232832, entries=305980, sequenceid=4724, filesize=21.8m
2014-07-01 15:55:00,258 INFO  [RS_OPEN_REGION-sceplus-vm48:60020-0] regionserver.HRegion: Finished memstore flush of ~84.0m/88120160, currentsize=0.0/0 for region usertable,user5,1404228265526.122e92eabfab38cb15a9965b21db25c5. in 2934ms, sequenceid=4724, compaction requested=false; wal=null
2014-07-01 15:55:00,260 DEBUG [RS_OPEN_REGION-sceplus-vm48:60020-0] regionserver.HRegion: Deleted recovered.edits file=hdfs://master:54310/hbase/data/default/usertable/122e92eabfab38cb15a9965b21db25c5/recovered.edits/0000000000000004474
2014-07-01 15:55:00,267 DEBUG [RS_OPEN_REGION-sceplus-vm48:60020-0] regionserver.HRegion: Deleted recovered.edits file=hdfs://master:54310/hbase/data/default/usertable/122e92eabfab38cb15a9965b21db25c5/recovered.edits/0000000000000004488
2014-07-01 15:55:00,269 DEBUG [RS_OPEN_REGION-sceplus-vm48:60020-0] regionserver.HRegion: Deleted recovered.edits file=hdfs://master:54310/hbase/data/default/usertable/122e92eabfab38cb15a9965b21db25c5/recovered.edits/0000000000000004504
2014-07-01 15:55:00,272 DEBUG [RS_OPEN_REGION-sceplus-vm48:60020-0] regionserver.HRegion: Deleted recovered.edits file=hdfs://master:54310/hbase/data/default/usertable/122e92eabfab38cb15a9965b21db25c5/recovered.edits/0000000000000004519
2014-07-01 15:55:00,276 DEBUG [RS_OPEN_REGION-sceplus-vm48:60020-0] regionserver.HRegion: Deleted recovered.edits file=hdfs://master:54310/hbase/data/default/usertable/122e92eabfab38cb15a9965b21db25c5/recovered.edits/0000000000000004535
2014-07-01 15:55:00,278 DEBUG [RS_OPEN_REGION-sceplus-vm48:60020-0] regionserver.HRegion: Deleted recovered.edits file=hdfs://master:54310/hbase/data/default/usertable/122e92eabfab38cb15a9965b21db25c5/recovered.edits/0000000000000004554
2014-07-01 15:55:00,284 DEBUG [RS_OPEN_REGION-sceplus-vm48:60020-0] regionserver.HRegion: Deleted recovered.edits file=hdfs://master:54310/hbase/data/default/usertable/122e92eabfab38cb15a9965b21db25c5/recovered.edits/0000000000000004571
2014-07-01 15:55:00,287 DEBUG [RS_OPEN_REGION-sceplus-vm48:60020-0] regionserver.HRegion: Deleted recovered.edits file=hdfs://master:54310/hbase/data/default/usertable/122e92eabfab38cb15a9965b21db25c5/recovered.edits/0000000000000004586
2014-07-01 15:55:00,289 DEBUG [RS_OPEN_REGION-sceplus-vm48:60020-0] regionserver.HRegion: Deleted recovered.edits file=hdfs://master:54310/hbase/data/default/usertable/122e92eabfab38cb15a9965b21db25c5/recovered.edits/0000000000000004601
2014-07-01 15:55:00,292 DEBUG [RS_OPEN_REGION-sceplus-vm48:60020-0] regionserver.HRegion: Deleted recovered.edits file=hdfs://master:54310/hbase/data/default/usertable/122e92eabfab38cb15a9965b21db25c5/recovered.edits/0000000000000004616
2014-07-01 15:55:00,294 DEBUG [RS_OPEN_REGION-sceplus-vm48:60020-0] regionserver.HRegion: Deleted recovered.edits file=hdfs://master:54310/hbase/data/default/usertable/122e92eabfab38cb15a9965b21db25c5/recovered.edits/0000000000000004634
2014-07-01 15:55:00,296 DEBUG [RS_OPEN_REGION-sceplus-vm48:60020-0] regionserver.HRegion: Deleted recovered.edits file=hdfs://master:54310/hbase/data/default/usertable/122e92eabfab38cb15a9965b21db25c5/recovered.edits/0000000000000004651
2014-07-01 15:55:00,297 DEBUG [RS_OPEN_REGION-sceplus-vm48:60020-0] regionserver.HRegion: Deleted recovered.edits file=hdfs://master:54310/hbase/data/default/usertable/122e92eabfab38cb15a9965b21db25c5/recovered.edits/0000000000000004667
2014-07-01 15:55:00,299 DEBUG [RS_OPEN_REGION-sceplus-vm48:60020-0] regionserver.HRegion: Deleted recovered.edits file=hdfs://master:54310/hbase/data/default/usertable/122e92eabfab38cb15a9965b21db25c5/recovered.edits/0000000000000004684
2014-07-01 15:55:00,301 DEBUG [RS_OPEN_REGION-sceplus-vm48:60020-0] regionserver.HRegion: Deleted recovered.edits file=hdfs://master:54310/hbase/data/default/usertable/122e92eabfab38cb15a9965b21db25c5/recovered.edits/0000000000000004700
2014-07-01 15:55:00,310 DEBUG [RS_OPEN_REGION-sceplus-vm48:60020-0] regionserver.HRegion: Deleted recovered.edits file=hdfs://master:54310/hbase/data/default/usertable/122e92eabfab38cb15a9965b21db25c5/recovered.edits/0000000000000004715
2014-07-01 15:55:00,314 DEBUG [RS_OPEN_REGION-sceplus-vm48:60020-0] regionserver.HRegion: Deleted recovered.edits file=hdfs://master:54310/hbase/data/default/usertable/122e92eabfab38cb15a9965b21db25c5/recovered.edits/0000000000000004724
2014-07-01 15:55:00,315 INFO  [RS_OPEN_REGION-sceplus-vm48:60020-0] regionserver.HRegion: Onlined 122e92eabfab38cb15a9965b21db25c5; next sequenceid=4725
2014-07-01 15:55:00,316 DEBUG [RS_OPEN_REGION-sceplus-vm48:60020-0] zookeeper.ZKAssign: regionserver:60020-0x46f4224a300001, quorum=sceplus-vm49.almaden.ibm.com:2181,sceplus-vm48.almaden.ibm.com:2181, baseZNode=/hbase Attempting to retransition opening state of node 122e92eabfab38cb15a9965b21db25c5
2014-07-01 15:55:00,540 DEBUG [RS_OPEN_REGION-sceplus-vm48:60020-2] regionserver.HRegion: Applied 94990, skipped 0, firstSequenceidInLog=5689, maxSequenceidInLog=5735, path=hdfs://master:54310/hbase/data/default/usertable/9cef9e7d34f96f1d9b111be50040a208/recovered.edits/0000000000000005735
2014-07-01 15:55:00,541 INFO  [PostOpenDeployTasks:122e92eabfab38cb15a9965b21db25c5] regionserver.HRegionServer: Post open deploy tasks for region=usertable,user5,1404228265526.122e92eabfab38cb15a9965b21db25c5.
2014-07-01 15:55:00,543 INFO  [RS_OPEN_REGION-sceplus-vm48:60020-2] regionserver.HRegion: Replaying edits from hdfs://master:54310/hbase/data/default/usertable/9cef9e7d34f96f1d9b111be50040a208/recovered.edits/0000000000000005757
2014-07-01 15:55:00,557 INFO  [PostOpenDeployTasks:122e92eabfab38cb15a9965b21db25c5] catalog.MetaEditor: Updated row usertable,user5,1404228265526.122e92eabfab38cb15a9965b21db25c5. with server=sceplus-vm48.almaden.ibm.com,60020,1404255227840
2014-07-01 15:55:00,557 INFO  [PostOpenDeployTasks:122e92eabfab38cb15a9965b21db25c5] regionserver.HRegionServer: Finished post open deploy task for usertable,user5,1404228265526.122e92eabfab38cb15a9965b21db25c5.
2014-07-01 15:55:00,558 DEBUG [RS_OPEN_REGION-sceplus-vm48:60020-0] zookeeper.ZKAssign: regionserver:60020-0x46f4224a300001, quorum=sceplus-vm49.almaden.ibm.com:2181,sceplus-vm48.almaden.ibm.com:2181, baseZNode=/hbase Transitioning 122e92eabfab38cb15a9965b21db25c5 from RS_ZK_REGION_OPENING to RS_ZK_REGION_OPENED
2014-07-01 15:55:00,565 DEBUG [RS_OPEN_REGION-sceplus-vm48:60020-0] zookeeper.ZKAssign: regionserver:60020-0x46f4224a300001, quorum=sceplus-vm49.almaden.ibm.com:2181,sceplus-vm48.almaden.ibm.com:2181, baseZNode=/hbase Transitioned node 122e92eabfab38cb15a9965b21db25c5 from RS_ZK_REGION_OPENING to RS_ZK_REGION_OPENED
2014-07-01 15:55:00,565 DEBUG [RS_OPEN_REGION-sceplus-vm48:60020-0] handler.OpenRegionHandler: Transitioned 122e92eabfab38cb15a9965b21db25c5 to OPENED in zk on sceplus-vm48.almaden.ibm.com,60020,1404255227840
2014-07-01 15:55:00,565 DEBUG [RS_OPEN_REGION-sceplus-vm48:60020-0] handler.OpenRegionHandler: Opened usertable,user5,1404228265526.122e92eabfab38cb15a9965b21db25c5. on sceplus-vm48.almaden.ibm.com,60020,1404255227840
2014-07-01 15:55:00,566 DEBUG [RS_OPEN_REGION-sceplus-vm48:60020-0] zookeeper.ZKAssign: regionserver:60020-0x46f4224a300001, quorum=sceplus-vm49.almaden.ibm.com:2181,sceplus-vm48.almaden.ibm.com:2181, baseZNode=/hbase Transitioning bb79ae773f71289e0745068f68a0abd9 from M_ZK_REGION_OFFLINE to RS_ZK_REGION_OPENING
2014-07-01 15:55:00,571 DEBUG [RS_OPEN_REGION-sceplus-vm48:60020-0] zookeeper.ZKAssign: regionserver:60020-0x46f4224a300001, quorum=sceplus-vm49.almaden.ibm.com:2181,sceplus-vm48.almaden.ibm.com:2181, baseZNode=/hbase Transitioned node bb79ae773f71289e0745068f68a0abd9 from M_ZK_REGION_OFFLINE to RS_ZK_REGION_OPENING
2014-07-01 15:55:00,571 DEBUG [RS_OPEN_REGION-sceplus-vm48:60020-0] regionserver.HRegion: Opening region: {ENCODED => bb79ae773f71289e0745068f68a0abd9, NAME => 'usertable,user9,1404228265526.bb79ae773f71289e0745068f68a0abd9.', STARTKEY => 'user9', ENDKEY => ''}
2014-07-01 15:55:00,572 DEBUG [RS_OPEN_REGION-sceplus-vm48:60020-0] regionserver.MetricsRegionSourceImpl: Creating new MetricsRegionSourceImpl for table usertable bb79ae773f71289e0745068f68a0abd9
2014-07-01 15:55:00,572 DEBUG [RS_OPEN_REGION-sceplus-vm48:60020-0] regionserver.HRegion: Instantiated usertable,user9,1404228265526.bb79ae773f71289e0745068f68a0abd9.
2014-07-01 15:55:00,578 INFO  [StoreOpener-bb79ae773f71289e0745068f68a0abd9-1] compactions.CompactionConfiguration: size [268435456, 9223372036854775807); files [3, 10); ratio 1.200000; off-peak ratio 5.000000; throttle point 5368709120; delete expired; major period 0, major jitter 0.500000
2014-07-01 15:55:00,580 INFO  [Priority.RpcServer.handler=4,port=60020] regionserver.HRegionServer: Close 122e92eabfab38cb15a9965b21db25c5, via zk=yes, znode version=0, on null
2014-07-01 15:55:00,581 DEBUG [RS_CLOSE_REGION-sceplus-vm48:60020-1] handler.CloseRegionHandler: Processing close of usertable,user5,1404228265526.122e92eabfab38cb15a9965b21db25c5.
2014-07-01 15:55:00,585 DEBUG [RS_CLOSE_REGION-sceplus-vm48:60020-1] regionserver.HRegion: Closing usertable,user5,1404228265526.122e92eabfab38cb15a9965b21db25c5.: disabling compactions & flushes
2014-07-01 15:55:00,585 DEBUG [RS_CLOSE_REGION-sceplus-vm48:60020-1] regionserver.HRegion: Updates disabled for region usertable,user5,1404228265526.122e92eabfab38cb15a9965b21db25c5.
2014-07-01 15:55:00,590 INFO  [StoreCloserThread-usertable,user5,1404228265526.122e92eabfab38cb15a9965b21db25c5.-1] regionserver.HStore: Closed family
2014-07-01 15:55:00,591 INFO  [RS_CLOSE_REGION-sceplus-vm48:60020-1] regionserver.HRegion: Closed usertable,user5,1404228265526.122e92eabfab38cb15a9965b21db25c5.
2014-07-01 15:55:00,591 DEBUG [RS_CLOSE_REGION-sceplus-vm48:60020-1] zookeeper.ZKAssign: regionserver:60020-0x46f4224a300001, quorum=sceplus-vm49.almaden.ibm.com:2181,sceplus-vm48.almaden.ibm.com:2181, baseZNode=/hbase Transitioning 122e92eabfab38cb15a9965b21db25c5 from M_ZK_REGION_CLOSING to RS_ZK_REGION_CLOSED
2014-07-01 15:55:00,598 DEBUG [StoreOpener-bb79ae773f71289e0745068f68a0abd9-1] regionserver.HStore: loaded hdfs://master:54310/hbase/data/default/usertable/bb79ae773f71289e0745068f68a0abd9/family/461d41f2b1274df0a6ca2ed0e39229fd, isReference=false, isBulkLoadResult=false, seqid=4417, majorCompaction=true
2014-07-01 15:55:00,600 DEBUG [RS_CLOSE_REGION-sceplus-vm48:60020-1] zookeeper.ZKAssign: regionserver:60020-0x46f4224a300001, quorum=sceplus-vm49.almaden.ibm.com:2181,sceplus-vm48.almaden.ibm.com:2181, baseZNode=/hbase Transitioned node 122e92eabfab38cb15a9965b21db25c5 from M_ZK_REGION_CLOSING to RS_ZK_REGION_CLOSED
2014-07-01 15:55:00,600 DEBUG [RS_CLOSE_REGION-sceplus-vm48:60020-1] handler.CloseRegionHandler: Set closed state in zk for usertable,user5,1404228265526.122e92eabfab38cb15a9965b21db25c5. on sceplus-vm48.almaden.ibm.com,60020,1404255227840
2014-07-01 15:55:00,600 DEBUG [RS_CLOSE_REGION-sceplus-vm48:60020-1] handler.CloseRegionHandler: Closed usertable,user5,1404228265526.122e92eabfab38cb15a9965b21db25c5.
2014-07-01 15:55:00,616 DEBUG [RS_OPEN_REGION-sceplus-vm48:60020-0] regionserver.HRegion: Found 17 recovered edits file(s) under hdfs://master:54310/hbase/data/default/usertable/bb79ae773f71289e0745068f68a0abd9
2014-07-01 15:55:00,617 INFO  [RS_OPEN_REGION-sceplus-vm48:60020-0] regionserver.HRegion: Replaying edits from hdfs://master:54310/hbase/data/default/usertable/bb79ae773f71289e0745068f68a0abd9/recovered.edits/0000000000000004426
2014-07-01 15:55:00,707 DEBUG [RS_OPEN_REGION-sceplus-vm48:60020-0] zookeeper.ZKAssign: regionserver:60020-0x46f4224a300001, quorum=sceplus-vm49.almaden.ibm.com:2181,sceplus-vm48.almaden.ibm.com:2181, baseZNode=/hbase Attempting to retransition opening state of node bb79ae773f71289e0745068f68a0abd9
2014-07-01 15:55:00,711 DEBUG [RS_OPEN_REGION-sceplus-vm48:60020-0] regionserver.HRegion: Applied 17770, skipped 10010, firstSequenceidInLog=4412, maxSequenceidInLog=4426, path=hdfs://master:54310/hbase/data/default/usertable/bb79ae773f71289e0745068f68a0abd9/recovered.edits/0000000000000004426
2014-07-01 15:55:00,713 INFO  [RS_OPEN_REGION-sceplus-vm48:60020-0] regionserver.HRegion: Replaying edits from hdfs://master:54310/hbase/data/default/usertable/bb79ae773f71289e0745068f68a0abd9/recovered.edits/0000000000000004444
2014-07-01 15:55:00,886 DEBUG [RS_OPEN_REGION-sceplus-vm48:60020-0] zookeeper.ZKAssign: regionserver:60020-0x46f4224a300001, quorum=sceplus-vm49.almaden.ibm.com:2181,sceplus-vm48.almaden.ibm.com:2181, baseZNode=/hbase Attempting to retransition opening state of node bb79ae773f71289e0745068f68a0abd9
2014-07-01 15:55:00,890 DEBUG [RS_OPEN_REGION-sceplus-vm48:60020-0] regionserver.HRegion: Applied 34800, skipped 0, firstSequenceidInLog=4427, maxSequenceidInLog=4444, path=hdfs://master:54310/hbase/data/default/usertable/bb79ae773f71289e0745068f68a0abd9/recovered.edits/0000000000000004444
2014-07-01 15:55:00,893 INFO  [RS_OPEN_REGION-sceplus-vm48:60020-0] regionserver.HRegion: Replaying edits from hdfs://master:54310/hbase/data/default/usertable/bb79ae773f71289e0745068f68a0abd9/recovered.edits/0000000000000004457
2014-07-01 15:55:00,900 DEBUG [RS_OPEN_REGION-sceplus-vm48:60020-2] regionserver.HStore: Completing compaction from the WAL marker
2014-07-01 15:55:00,902 DEBUG [RS_OPEN_REGION-sceplus-vm48:60020-2] regionserver.HStore: Removing store files after compaction...
2014-07-01 15:55:00,902 DEBUG [RS_OPEN_REGION-sceplus-vm48:60020-2] backup.HFileArchiver: No store files to dispose, done!
2014-07-01 15:55:00,902 DEBUG [RS_OPEN_REGION-sceplus-vm48:60020-2] zookeeper.ZKAssign: regionserver:60020-0x46f4224a300001, quorum=sceplus-vm49.almaden.ibm.com:2181,sceplus-vm48.almaden.ibm.com:2181, baseZNode=/hbase Attempting to retransition opening state of node 9cef9e7d34f96f1d9b111be50040a208
2014-07-01 15:55:00,904 DEBUG [RS_OPEN_REGION-sceplus-vm48:60020-2] regionserver.HRegion: Applied 38390, skipped 1, firstSequenceidInLog=5736, maxSequenceidInLog=5757, path=hdfs://master:54310/hbase/data/default/usertable/9cef9e7d34f96f1d9b111be50040a208/recovered.edits/0000000000000005757
2014-07-01 15:55:00,904 DEBUG [RS_OPEN_REGION-sceplus-vm48:60020-2] regionserver.HRegion: Started memstore flush for usertable,,1404228265525.9cef9e7d34f96f1d9b111be50040a208., current region memstore size 150.4m; wal is null, using passed sequenceid=5757
2014-07-01 15:55:00,992 DEBUG [RS_OPEN_REGION-sceplus-vm48:60020-2] util.FSUtils: DFS Client does not support most favored nodes create; using default create
2014-07-01 15:55:00,998 DEBUG [RS_OPEN_REGION-sceplus-vm48:60020-0] zookeeper.ZKAssign: regionserver:60020-0x46f4224a300001, quorum=sceplus-vm49.almaden.ibm.com:2181,sceplus-vm48.almaden.ibm.com:2181, baseZNode=/hbase Attempting to retransition opening state of node bb79ae773f71289e0745068f68a0abd9
2014-07-01 15:55:01,003 DEBUG [RS_OPEN_REGION-sceplus-vm48:60020-0] regionserver.HRegion: Applied 23650, skipped 0, firstSequenceidInLog=4445, maxSequenceidInLog=4457, path=hdfs://master:54310/hbase/data/default/usertable/bb79ae773f71289e0745068f68a0abd9/recovered.edits/0000000000000004457
2014-07-01 15:55:01,004 INFO  [RS_OPEN_REGION-sceplus-vm48:60020-0] regionserver.HRegion: Replaying edits from hdfs://master:54310/hbase/data/default/usertable/bb79ae773f71289e0745068f68a0abd9/recovered.edits/0000000000000004472
2014-07-01 15:55:01,188 DEBUG [RS_OPEN_REGION-sceplus-vm48:60020-0] zookeeper.ZKAssign: regionserver:60020-0x46f4224a300001, quorum=sceplus-vm49.almaden.ibm.com:2181,sceplus-vm48.almaden.ibm.com:2181, baseZNode=/hbase Attempting to retransition opening state of node bb79ae773f71289e0745068f68a0abd9
2014-07-01 15:55:01,192 DEBUG [RS_OPEN_REGION-sceplus-vm48:60020-0] regionserver.HRegion: Applied 29590, skipped 0, firstSequenceidInLog=4458, maxSequenceidInLog=4472, path=hdfs://master:54310/hbase/data/default/usertable/bb79ae773f71289e0745068f68a0abd9/recovered.edits/0000000000000004472
2014-07-01 15:55:01,194 INFO  [RS_OPEN_REGION-sceplus-vm48:60020-0] regionserver.HRegion: Replaying edits from hdfs://master:54310/hbase/data/default/usertable/bb79ae773f71289e0745068f68a0abd9/recovered.edits/0000000000000004487
2014-07-01 15:55:01,344 DEBUG [RS_OPEN_REGION-sceplus-vm48:60020-0] zookeeper.ZKAssign: regionserver:60020-0x46f4224a300001, quorum=sceplus-vm49.almaden.ibm.com:2181,sceplus-vm48.almaden.ibm.com:2181, baseZNode=/hbase Attempting to retransition opening state of node bb79ae773f71289e0745068f68a0abd9
2014-07-01 15:55:01,347 DEBUG [RS_OPEN_REGION-sceplus-vm48:60020-0] regionserver.HRegion: Applied 28780, skipped 0, firstSequenceidInLog=4473, maxSequenceidInLog=4487, path=hdfs://master:54310/hbase/data/default/usertable/bb79ae773f71289e0745068f68a0abd9/recovered.edits/0000000000000004487
2014-07-01 15:55:01,349 INFO  [RS_OPEN_REGION-sceplus-vm48:60020-0] regionserver.HRegion: Replaying edits from hdfs://master:54310/hbase/data/default/usertable/bb79ae773f71289e0745068f68a0abd9/recovered.edits/0000000000000004507
2014-07-01 15:55:01,565 DEBUG [RS_OPEN_REGION-sceplus-vm48:60020-0] zookeeper.ZKAssign: regionserver:60020-0x46f4224a300001, quorum=sceplus-vm49.almaden.ibm.com:2181,sceplus-vm48.almaden.ibm.com:2181, baseZNode=/hbase Attempting to retransition opening state of node bb79ae773f71289e0745068f68a0abd9
2014-07-01 15:55:01,568 DEBUG [RS_OPEN_REGION-sceplus-vm48:60020-0] regionserver.HRegion: Applied 37240, skipped 0, firstSequenceidInLog=4488, maxSequenceidInLog=4507, path=hdfs://master:54310/hbase/data/default/usertable/bb79ae773f71289e0745068f68a0abd9/recovered.edits/0000000000000004507
2014-07-01 15:55:01,570 INFO  [RS_OPEN_REGION-sceplus-vm48:60020-0] regionserver.HRegion: Replaying edits from hdfs://master:54310/hbase/data/default/usertable/bb79ae773f71289e0745068f68a0abd9/recovered.edits/0000000000000004521
2014-07-01 15:55:01,707 DEBUG [RS_OPEN_REGION-sceplus-vm48:60020-0] zookeeper.ZKAssign: regionserver:60020-0x46f4224a300001, quorum=sceplus-vm49.almaden.ibm.com:2181,sceplus-vm48.almaden.ibm.com:2181, baseZNode=/hbase Attempting to retransition opening state of node bb79ae773f71289e0745068f68a0abd9
2014-07-01 15:55:01,712 DEBUG [RS_OPEN_REGION-sceplus-vm48:60020-0] regionserver.HRegion: Applied 25340, skipped 0, firstSequenceidInLog=4508, maxSequenceidInLog=4521, path=hdfs://master:54310/hbase/data/default/usertable/bb79ae773f71289e0745068f68a0abd9/recovered.edits/0000000000000004521
2014-07-01 15:55:01,714 INFO  [RS_OPEN_REGION-sceplus-vm48:60020-0] regionserver.HRegion: Replaying edits from hdfs://master:54310/hbase/data/default/usertable/bb79ae773f71289e0745068f68a0abd9/recovered.edits/0000000000000004539
2014-07-01 15:55:01,919 DEBUG [RS_OPEN_REGION-sceplus-vm48:60020-0] zookeeper.ZKAssign: regionserver:60020-0x46f4224a300001, quorum=sceplus-vm49.almaden.ibm.com:2181,sceplus-vm48.almaden.ibm.com:2181, baseZNode=/hbase Attempting to retransition opening state of node bb79ae773f71289e0745068f68a0abd9
2014-07-01 15:55:01,922 DEBUG [RS_OPEN_REGION-sceplus-vm48:60020-0] regionserver.HRegion: Applied 34370, skipped 0, firstSequenceidInLog=4522, maxSequenceidInLog=4539, path=hdfs://master:54310/hbase/data/default/usertable/bb79ae773f71289e0745068f68a0abd9/recovered.edits/0000000000000004539
2014-07-01 15:55:01,925 INFO  [RS_OPEN_REGION-sceplus-vm48:60020-0] regionserver.HRegion: Replaying edits from hdfs://master:54310/hbase/data/default/usertable/bb79ae773f71289e0745068f68a0abd9/recovered.edits/0000000000000004555
2014-07-01 15:55:02,125 DEBUG [RS_OPEN_REGION-sceplus-vm48:60020-0] zookeeper.ZKAssign: regionserver:60020-0x46f4224a300001, quorum=sceplus-vm49.almaden.ibm.com:2181,sceplus-vm48.almaden.ibm.com:2181, baseZNode=/hbase Attempting to retransition opening state of node bb79ae773f71289e0745068f68a0abd9
2014-07-01 15:55:02,127 DEBUG [RS_OPEN_REGION-sceplus-vm48:60020-0] regionserver.HRegion: Applied 30200, skipped 0, firstSequenceidInLog=4540, maxSequenceidInLog=4555, path=hdfs://master:54310/hbase/data/default/usertable/bb79ae773f71289e0745068f68a0abd9/recovered.edits/0000000000000004555
2014-07-01 15:55:02,130 INFO  [RS_OPEN_REGION-sceplus-vm48:60020-0] regionserver.HRegion: Replaying edits from hdfs://master:54310/hbase/data/default/usertable/bb79ae773f71289e0745068f68a0abd9/recovered.edits/0000000000000004571
2014-07-01 15:55:02,199 INFO  [RS_OPEN_REGION-sceplus-vm48:60020-1] regionserver.DefaultStoreFlusher: Flushed, sequenceid=2002, memsize=123.0m, hasBloomFilter=true, into tmp file hdfs://master:54310/hbase/data/default/usertable/99b73df170b514f6773e742a495216c4/.tmp/4a2dc29027814a048bb2ce88ec2df5aa
2014-07-01 15:55:02,213 DEBUG [RS_OPEN_REGION-sceplus-vm48:60020-1] regionserver.HRegionFileSystem: Committing store file hdfs://master:54310/hbase/data/default/usertable/99b73df170b514f6773e742a495216c4/.tmp/4a2dc29027814a048bb2ce88ec2df5aa as hdfs://master:54310/hbase/data/default/usertable/99b73df170b514f6773e742a495216c4/family/4a2dc29027814a048bb2ce88ec2df5aa
2014-07-01 15:55:02,226 INFO  [RS_OPEN_REGION-sceplus-vm48:60020-1] regionserver.HStore: Added hdfs://master:54310/hbase/data/default/usertable/99b73df170b514f6773e742a495216c4/family/4a2dc29027814a048bb2ce88ec2df5aa, entries=448020, sequenceid=2002, filesize=31.9m
2014-07-01 15:55:02,226 INFO  [RS_OPEN_REGION-sceplus-vm48:60020-1] regionserver.HRegion: Finished memstore flush of ~123.0m/129025520, currentsize=0.0/0 for region usertable,user8,1404228265526.99b73df170b514f6773e742a495216c4. in 3835ms, sequenceid=2002, compaction requested=false; wal=null
2014-07-01 15:55:02,227 DEBUG [RS_OPEN_REGION-sceplus-vm48:60020-1] regionserver.HRegion: Deleted recovered.edits file=hdfs://master:54310/hbase/data/default/usertable/99b73df170b514f6773e742a495216c4/recovered.edits/0000000000000001436
2014-07-01 15:55:02,229 DEBUG [RS_OPEN_REGION-sceplus-vm48:60020-1] regionserver.HRegion: Deleted recovered.edits file=hdfs://master:54310/hbase/data/default/usertable/99b73df170b514f6773e742a495216c4/recovered.edits/0000000000000001453
2014-07-01 15:55:02,230 DEBUG [RS_OPEN_REGION-sceplus-vm48:60020-1] regionserver.HRegion: Deleted recovered.edits file=hdfs://master:54310/hbase/data/default/usertable/99b73df170b514f6773e742a495216c4/recovered.edits/0000000000000001478
2014-07-01 15:55:02,231 DEBUG [RS_OPEN_REGION-sceplus-vm48:60020-1] regionserver.HRegion: Deleted recovered.edits file=hdfs://master:54310/hbase/data/default/usertable/99b73df170b514f6773e742a495216c4/recovered.edits/0000000000000001493
2014-07-01 15:55:02,232 DEBUG [RS_OPEN_REGION-sceplus-vm48:60020-1] regionserver.HRegion: Deleted recovered.edits file=hdfs://master:54310/hbase/data/default/usertable/99b73df170b514f6773e742a495216c4/recovered.edits/0000000000000001512
2014-07-01 15:55:02,234 DEBUG [RS_OPEN_REGION-sceplus-vm48:60020-1] regionserver.HRegion: Deleted recovered.edits file=hdfs://master:54310/hbase/data/default/usertable/99b73df170b514f6773e742a495216c4/recovered.edits/0000000000000001536
2014-07-01 15:55:02,235 DEBUG [RS_OPEN_REGION-sceplus-vm48:60020-1] regionserver.HRegion: Deleted recovered.edits file=hdfs://master:54310/hbase/data/default/usertable/99b73df170b514f6773e742a495216c4/recovered.edits/0000000000000001556
2014-07-01 15:55:02,236 DEBUG [RS_OPEN_REGION-sceplus-vm48:60020-1] regionserver.HRegion: Deleted recovered.edits file=hdfs://master:54310/hbase/data/default/usertable/99b73df170b514f6773e742a495216c4/recovered.edits/0000000000000001579
2014-07-01 15:55:02,237 DEBUG [RS_OPEN_REGION-sceplus-vm48:60020-1] regionserver.HRegion: Deleted recovered.edits file=hdfs://master:54310/hbase/data/default/usertable/99b73df170b514f6773e742a495216c4/recovered.edits/0000000000000001594
2014-07-01 15:55:02,238 DEBUG [RS_OPEN_REGION-sceplus-vm48:60020-1] regionserver.HRegion: Deleted recovered.edits file=hdfs://master:54310/hbase/data/default/usertable/99b73df170b514f6773e742a495216c4/recovered.edits/0000000000000001612
2014-07-01 15:55:02,240 DEBUG [RS_OPEN_REGION-sceplus-vm48:60020-1] regionserver.HRegion: Deleted recovered.edits file=hdfs://master:54310/hbase/data/default/usertable/99b73df170b514f6773e742a495216c4/recovered.edits/0000000000000001628
2014-07-01 15:55:02,241 DEBUG [RS_OPEN_REGION-sceplus-vm48:60020-1] regionserver.HRegion: Deleted recovered.edits file=hdfs://master:54310/hbase/data/default/usertable/99b73df170b514f6773e742a495216c4/recovered.edits/0000000000000001647
2014-07-01 15:55:02,242 DEBUG [RS_OPEN_REGION-sceplus-vm48:60020-1] regionserver.HRegion: Deleted recovered.edits file=hdfs://master:54310/hbase/data/default/usertable/99b73df170b514f6773e742a495216c4/recovered.edits/0000000000000001676
2014-07-01 15:55:02,243 DEBUG [RS_OPEN_REGION-sceplus-vm48:60020-1] regionserver.HRegion: Deleted recovered.edits file=hdfs://master:54310/hbase/data/default/usertable/99b73df170b514f6773e742a495216c4/recovered.edits/0000000000000001691
2014-07-01 15:55:02,244 DEBUG [RS_OPEN_REGION-sceplus-vm48:60020-1] regionserver.HRegion: Deleted recovered.edits file=hdfs://master:54310/hbase/data/default/usertable/99b73df170b514f6773e742a495216c4/recovered.edits/0000000000000001713
2014-07-01 15:55:02,245 DEBUG [RS_OPEN_REGION-sceplus-vm48:60020-1] regionserver.HRegion: Deleted recovered.edits file=hdfs://master:54310/hbase/data/default/usertable/99b73df170b514f6773e742a495216c4/recovered.edits/0000000000000001727
2014-07-01 15:55:02,247 DEBUG [RS_OPEN_REGION-sceplus-vm48:60020-1] regionserver.HRegion: Deleted recovered.edits file=hdfs://master:54310/hbase/data/default/usertable/99b73df170b514f6773e742a495216c4/recovered.edits/0000000000000001748
2014-07-01 15:55:02,248 DEBUG [RS_OPEN_REGION-sceplus-vm48:60020-1] regionserver.HRegion: Deleted recovered.edits file=hdfs://master:54310/hbase/data/default/usertable/99b73df170b514f6773e742a495216c4/recovered.edits/0000000000000001767
2014-07-01 15:55:02,249 DEBUG [RS_OPEN_REGION-sceplus-vm48:60020-1] regionserver.HRegion: Deleted recovered.edits file=hdfs://master:54310/hbase/data/default/usertable/99b73df170b514f6773e742a495216c4/recovered.edits/0000000000000001784
2014-07-01 15:55:02,250 DEBUG [RS_OPEN_REGION-sceplus-vm48:60020-1] regionserver.HRegion: Deleted recovered.edits file=hdfs://master:54310/hbase/data/default/usertable/99b73df170b514f6773e742a495216c4/recovered.edits/0000000000000001803
2014-07-01 15:55:02,251 DEBUG [RS_OPEN_REGION-sceplus-vm48:60020-1] regionserver.HRegion: Deleted recovered.edits file=hdfs://master:54310/hbase/data/default/usertable/99b73df170b514f6773e742a495216c4/recovered.edits/0000000000000001826
2014-07-01 15:55:02,252 DEBUG [RS_OPEN_REGION-sceplus-vm48:60020-1] regionserver.HRegion: Deleted recovered.edits file=hdfs://master:54310/hbase/data/default/usertable/99b73df170b514f6773e742a495216c4/recovered.edits/0000000000000001844
2014-07-01 15:55:02,254 DEBUG [RS_OPEN_REGION-sceplus-vm48:60020-1] regionserver.HRegion: Deleted recovered.edits file=hdfs://master:54310/hbase/data/default/usertable/99b73df170b514f6773e742a495216c4/recovered.edits/0000000000000001869
2014-07-01 15:55:02,255 DEBUG [RS_OPEN_REGION-sceplus-vm48:60020-1] regionserver.HRegion: Deleted recovered.edits file=hdfs://master:54310/hbase/data/default/usertable/99b73df170b514f6773e742a495216c4/recovered.edits/0000000000000001884
2014-07-01 15:55:02,256 DEBUG [RS_OPEN_REGION-sceplus-vm48:60020-1] regionserver.HRegion: Deleted recovered.edits file=hdfs://master:54310/hbase/data/default/usertable/99b73df170b514f6773e742a495216c4/recovered.edits/0000000000000001904
2014-07-01 15:55:02,257 DEBUG [RS_OPEN_REGION-sceplus-vm48:60020-1] regionserver.HRegion: Deleted recovered.edits file=hdfs://master:54310/hbase/data/default/usertable/99b73df170b514f6773e742a495216c4/recovered.edits/0000000000000001921
2014-07-01 15:55:02,258 DEBUG [RS_OPEN_REGION-sceplus-vm48:60020-1] regionserver.HRegion: Deleted recovered.edits file=hdfs://master:54310/hbase/data/default/usertable/99b73df170b514f6773e742a495216c4/recovered.edits/0000000000000001941
2014-07-01 15:55:02,260 DEBUG [RS_OPEN_REGION-sceplus-vm48:60020-1] regionserver.HRegion: Deleted recovered.edits file=hdfs://master:54310/hbase/data/default/usertable/99b73df170b514f6773e742a495216c4/recovered.edits/0000000000000001958
2014-07-01 15:55:02,261 DEBUG [RS_OPEN_REGION-sceplus-vm48:60020-1] regionserver.HRegion: Deleted recovered.edits file=hdfs://master:54310/hbase/data/default/usertable/99b73df170b514f6773e742a495216c4/recovered.edits/0000000000000001976
2014-07-01 15:55:02,262 DEBUG [RS_OPEN_REGION-sceplus-vm48:60020-1] regionserver.HRegion: Deleted recovered.edits file=hdfs://master:54310/hbase/data/default/usertable/99b73df170b514f6773e742a495216c4/recovered.edits/0000000000000001992
2014-07-01 15:55:02,263 DEBUG [RS_OPEN_REGION-sceplus-vm48:60020-1] regionserver.HRegion: Deleted recovered.edits file=hdfs://master:54310/hbase/data/default/usertable/99b73df170b514f6773e742a495216c4/recovered.edits/0000000000000002002
2014-07-01 15:55:02,265 INFO  [RS_OPEN_REGION-sceplus-vm48:60020-1] regionserver.HRegion: Onlined 99b73df170b514f6773e742a495216c4; next sequenceid=2003
2014-07-01 15:55:02,265 DEBUG [RS_OPEN_REGION-sceplus-vm48:60020-1] zookeeper.ZKAssign: regionserver:60020-0x46f4224a300001, quorum=sceplus-vm49.almaden.ibm.com:2181,sceplus-vm48.almaden.ibm.com:2181, baseZNode=/hbase Attempting to retransition opening state of node 99b73df170b514f6773e742a495216c4
2014-07-01 15:55:02,268 INFO  [PostOpenDeployTasks:99b73df170b514f6773e742a495216c4] regionserver.HRegionServer: Post open deploy tasks for region=usertable,user8,1404228265526.99b73df170b514f6773e742a495216c4.
2014-07-01 15:55:02,276 INFO  [PostOpenDeployTasks:99b73df170b514f6773e742a495216c4] catalog.MetaEditor: Updated row usertable,user8,1404228265526.99b73df170b514f6773e742a495216c4. with server=sceplus-vm48.almaden.ibm.com,60020,1404255227840
2014-07-01 15:55:02,276 INFO  [PostOpenDeployTasks:99b73df170b514f6773e742a495216c4] regionserver.HRegionServer: Finished post open deploy task for usertable,user8,1404228265526.99b73df170b514f6773e742a495216c4.
2014-07-01 15:55:02,277 DEBUG [RS_OPEN_REGION-sceplus-vm48:60020-1] zookeeper.ZKAssign: regionserver:60020-0x46f4224a300001, quorum=sceplus-vm49.almaden.ibm.com:2181,sceplus-vm48.almaden.ibm.com:2181, baseZNode=/hbase Transitioning 99b73df170b514f6773e742a495216c4 from RS_ZK_REGION_OPENING to RS_ZK_REGION_OPENED
2014-07-01 15:55:02,284 DEBUG [RS_OPEN_REGION-sceplus-vm48:60020-1] zookeeper.ZKAssign: regionserver:60020-0x46f4224a300001, quorum=sceplus-vm49.almaden.ibm.com:2181,sceplus-vm48.almaden.ibm.com:2181, baseZNode=/hbase Transitioned node 99b73df170b514f6773e742a495216c4 from RS_ZK_REGION_OPENING to RS_ZK_REGION_OPENED
2014-07-01 15:55:02,284 DEBUG [RS_OPEN_REGION-sceplus-vm48:60020-1] handler.OpenRegionHandler: Transitioned 99b73df170b514f6773e742a495216c4 to OPENED in zk on sceplus-vm48.almaden.ibm.com,60020,1404255227840
2014-07-01 15:55:02,284 DEBUG [RS_OPEN_REGION-sceplus-vm48:60020-1] handler.OpenRegionHandler: Opened usertable,user8,1404228265526.99b73df170b514f6773e742a495216c4. on sceplus-vm48.almaden.ibm.com,60020,1404255227840
2014-07-01 15:55:02,300 INFO  [Priority.RpcServer.handler=5,port=60020] regionserver.HRegionServer: Close 99b73df170b514f6773e742a495216c4, via zk=yes, znode version=0, on null
2014-07-01 15:55:02,301 DEBUG [RS_CLOSE_REGION-sceplus-vm48:60020-2] handler.CloseRegionHandler: Processing close of usertable,user8,1404228265526.99b73df170b514f6773e742a495216c4.
2014-07-01 15:55:02,305 DEBUG [RS_CLOSE_REGION-sceplus-vm48:60020-2] regionserver.HRegion: Closing usertable,user8,1404228265526.99b73df170b514f6773e742a495216c4.: disabling compactions & flushes
2014-07-01 15:55:02,305 DEBUG [RS_CLOSE_REGION-sceplus-vm48:60020-2] regionserver.HRegion: Updates disabled for region usertable,user8,1404228265526.99b73df170b514f6773e742a495216c4.
2014-07-01 15:55:02,307 INFO  [StoreCloserThread-usertable,user8,1404228265526.99b73df170b514f6773e742a495216c4.-1] regionserver.HStore: Closed family
2014-07-01 15:55:02,308 INFO  [RS_CLOSE_REGION-sceplus-vm48:60020-2] regionserver.HRegion: Closed usertable,user8,1404228265526.99b73df170b514f6773e742a495216c4.
2014-07-01 15:55:02,308 DEBUG [RS_CLOSE_REGION-sceplus-vm48:60020-2] zookeeper.ZKAssign: regionserver:60020-0x46f4224a300001, quorum=sceplus-vm49.almaden.ibm.com:2181,sceplus-vm48.almaden.ibm.com:2181, baseZNode=/hbase Transitioning 99b73df170b514f6773e742a495216c4 from M_ZK_REGION_CLOSING to RS_ZK_REGION_CLOSED
2014-07-01 15:55:02,317 DEBUG [RS_CLOSE_REGION-sceplus-vm48:60020-2] zookeeper.ZKAssign: regionserver:60020-0x46f4224a300001, quorum=sceplus-vm49.almaden.ibm.com:2181,sceplus-vm48.almaden.ibm.com:2181, baseZNode=/hbase Transitioned node 99b73df170b514f6773e742a495216c4 from M_ZK_REGION_CLOSING to RS_ZK_REGION_CLOSED
2014-07-01 15:55:02,317 DEBUG [RS_CLOSE_REGION-sceplus-vm48:60020-2] handler.CloseRegionHandler: Set closed state in zk for usertable,user8,1404228265526.99b73df170b514f6773e742a495216c4. on sceplus-vm48.almaden.ibm.com,60020,1404255227840
2014-07-01 15:55:02,317 DEBUG [RS_CLOSE_REGION-sceplus-vm48:60020-2] handler.CloseRegionHandler: Closed usertable,user8,1404228265526.99b73df170b514f6773e742a495216c4.
2014-07-01 15:55:02,321 DEBUG [RS_OPEN_REGION-sceplus-vm48:60020-0] zookeeper.ZKAssign: regionserver:60020-0x46f4224a300001, quorum=sceplus-vm49.almaden.ibm.com:2181,sceplus-vm48.almaden.ibm.com:2181, baseZNode=/hbase Attempting to retransition opening state of node bb79ae773f71289e0745068f68a0abd9
2014-07-01 15:55:02,326 DEBUG [RS_OPEN_REGION-sceplus-vm48:60020-0] regionserver.HRegion: Applied 31340, skipped 0, firstSequenceidInLog=4556, maxSequenceidInLog=4571, path=hdfs://master:54310/hbase/data/default/usertable/bb79ae773f71289e0745068f68a0abd9/recovered.edits/0000000000000004571
2014-07-01 15:55:02,328 INFO  [RS_OPEN_REGION-sceplus-vm48:60020-0] regionserver.HRegion: Replaying edits from hdfs://master:54310/hbase/data/default/usertable/bb79ae773f71289e0745068f68a0abd9/recovered.edits/0000000000000004585
2014-07-01 15:55:02,526 DEBUG [RS_OPEN_REGION-sceplus-vm48:60020-0] zookeeper.ZKAssign: regionserver:60020-0x46f4224a300001, quorum=sceplus-vm49.almaden.ibm.com:2181,sceplus-vm48.almaden.ibm.com:2181, baseZNode=/hbase Attempting to retransition opening state of node bb79ae773f71289e0745068f68a0abd9
2014-07-01 15:55:02,529 DEBUG [RS_OPEN_REGION-sceplus-vm48:60020-0] regionserver.HRegion: Applied 26130, skipped 0, firstSequenceidInLog=4572, maxSequenceidInLog=4585, path=hdfs://master:54310/hbase/data/default/usertable/bb79ae773f71289e0745068f68a0abd9/recovered.edits/0000000000000004585
2014-07-01 15:55:02,531 INFO  [RS_OPEN_REGION-sceplus-vm48:60020-0] regionserver.HRegion: Replaying edits from hdfs://master:54310/hbase/data/default/usertable/bb79ae773f71289e0745068f68a0abd9/recovered.edits/0000000000000004605
2014-07-01 15:55:02,757 DEBUG [RS_OPEN_REGION-sceplus-vm48:60020-0] zookeeper.ZKAssign: regionserver:60020-0x46f4224a300001, quorum=sceplus-vm49.almaden.ibm.com:2181,sceplus-vm48.almaden.ibm.com:2181, baseZNode=/hbase Attempting to retransition opening state of node bb79ae773f71289e0745068f68a0abd9
2014-07-01 15:55:02,760 DEBUG [RS_OPEN_REGION-sceplus-vm48:60020-0] regionserver.HRegion: Applied 34010, skipped 0, firstSequenceidInLog=4586, maxSequenceidInLog=4605, path=hdfs://master:54310/hbase/data/default/usertable/bb79ae773f71289e0745068f68a0abd9/recovered.edits/0000000000000004605
2014-07-01 15:55:02,762 INFO  [RS_OPEN_REGION-sceplus-vm48:60020-0] regionserver.HRegion: Replaying edits from hdfs://master:54310/hbase/data/default/usertable/bb79ae773f71289e0745068f68a0abd9/recovered.edits/0000000000000004621
2014-07-01 15:55:02,985 DEBUG [RS_OPEN_REGION-sceplus-vm48:60020-0] zookeeper.ZKAssign: regionserver:60020-0x46f4224a300001, quorum=sceplus-vm49.almaden.ibm.com:2181,sceplus-vm48.almaden.ibm.com:2181, baseZNode=/hbase Attempting to retransition opening state of node bb79ae773f71289e0745068f68a0abd9
2014-07-01 15:55:02,989 DEBUG [RS_OPEN_REGION-sceplus-vm48:60020-0] regionserver.HRegion: Applied 31360, skipped 0, firstSequenceidInLog=4606, maxSequenceidInLog=4621, path=hdfs://master:54310/hbase/data/default/usertable/bb79ae773f71289e0745068f68a0abd9/recovered.edits/0000000000000004621
2014-07-01 15:55:02,991 INFO  [RS_OPEN_REGION-sceplus-vm48:60020-0] regionserver.HRegion: Replaying edits from hdfs://master:54310/hbase/data/default/usertable/bb79ae773f71289e0745068f68a0abd9/recovered.edits/0000000000000004636
2014-07-01 15:55:03,173 DEBUG [RS_OPEN_REGION-sceplus-vm48:60020-0] zookeeper.ZKAssign: regionserver:60020-0x46f4224a300001, quorum=sceplus-vm49.almaden.ibm.com:2181,sceplus-vm48.almaden.ibm.com:2181, baseZNode=/hbase Attempting to retransition opening state of node bb79ae773f71289e0745068f68a0abd9
2014-07-01 15:55:03,177 DEBUG [RS_OPEN_REGION-sceplus-vm48:60020-0] regionserver.HRegion: Applied 28520, skipped 0, firstSequenceidInLog=4622, maxSequenceidInLog=4636, path=hdfs://master:54310/hbase/data/default/usertable/bb79ae773f71289e0745068f68a0abd9/recovered.edits/0000000000000004636
2014-07-01 15:55:03,179 INFO  [RS_OPEN_REGION-sceplus-vm48:60020-0] regionserver.HRegion: Replaying edits from hdfs://master:54310/hbase/data/default/usertable/bb79ae773f71289e0745068f68a0abd9/recovered.edits/0000000000000004652
2014-07-01 15:55:03,364 DEBUG [RS_OPEN_REGION-sceplus-vm48:60020-0] zookeeper.ZKAssign: regionserver:60020-0x46f4224a300001, quorum=sceplus-vm49.almaden.ibm.com:2181,sceplus-vm48.almaden.ibm.com:2181, baseZNode=/hbase Attempting to retransition opening state of node bb79ae773f71289e0745068f68a0abd9
2014-07-01 15:55:03,369 DEBUG [RS_OPEN_REGION-sceplus-vm48:60020-0] regionserver.HRegion: Applied 29340, skipped 0, firstSequenceidInLog=4637, maxSequenceidInLog=4652, path=hdfs://master:54310/hbase/data/default/usertable/bb79ae773f71289e0745068f68a0abd9/recovered.edits/0000000000000004652
2014-07-01 15:55:03,371 INFO  [RS_OPEN_REGION-sceplus-vm48:60020-0] regionserver.HRegion: Replaying edits from hdfs://master:54310/hbase/data/default/usertable/bb79ae773f71289e0745068f68a0abd9/recovered.edits/0000000000000004669
2014-07-01 15:55:03,669 DEBUG [RS_OPEN_REGION-sceplus-vm48:60020-0] zookeeper.ZKAssign: regionserver:60020-0x46f4224a300001, quorum=sceplus-vm49.almaden.ibm.com:2181,sceplus-vm48.almaden.ibm.com:2181, baseZNode=/hbase Attempting to retransition opening state of node bb79ae773f71289e0745068f68a0abd9
2014-07-01 15:55:03,672 DEBUG [RS_OPEN_REGION-sceplus-vm48:60020-0] regionserver.HRegion: Applied 32470, skipped 0, firstSequenceidInLog=4653, maxSequenceidInLog=4669, path=hdfs://master:54310/hbase/data/default/usertable/bb79ae773f71289e0745068f68a0abd9/recovered.edits/0000000000000004669
2014-07-01 15:55:03,675 INFO  [RS_OPEN_REGION-sceplus-vm48:60020-0] regionserver.HRegion: Replaying edits from hdfs://master:54310/hbase/data/default/usertable/bb79ae773f71289e0745068f68a0abd9/recovered.edits/0000000000000004678
2014-07-01 15:55:03,784 DEBUG [RS_OPEN_REGION-sceplus-vm48:60020-0] regionserver.HStore: Completing compaction from the WAL marker
2014-07-01 15:55:03,786 DEBUG [RS_OPEN_REGION-sceplus-vm48:60020-0] regionserver.HStore: Removing store files after compaction...
2014-07-01 15:55:03,786 DEBUG [RS_OPEN_REGION-sceplus-vm48:60020-0] backup.HFileArchiver: No store files to dispose, done!
2014-07-01 15:55:03,786 DEBUG [RS_OPEN_REGION-sceplus-vm48:60020-0] zookeeper.ZKAssign: regionserver:60020-0x46f4224a300001, quorum=sceplus-vm49.almaden.ibm.com:2181,sceplus-vm48.almaden.ibm.com:2181, baseZNode=/hbase Attempting to retransition opening state of node bb79ae773f71289e0745068f68a0abd9
2014-07-01 15:55:03,790 DEBUG [RS_OPEN_REGION-sceplus-vm48:60020-0] regionserver.HRegion: Applied 15830, skipped 1, firstSequenceidInLog=4670, maxSequenceidInLog=4678, path=hdfs://master:54310/hbase/data/default/usertable/bb79ae773f71289e0745068f68a0abd9/recovered.edits/0000000000000004678
2014-07-01 15:55:03,790 DEBUG [RS_OPEN_REGION-sceplus-vm48:60020-0] regionserver.HRegion: Started memstore flush for usertable,user9,1404228265526.bb79ae773f71289e0745068f68a0abd9., current region memstore size 134.8m; wal is null, using passed sequenceid=4678
2014-07-01 15:55:03,849 DEBUG [RS_OPEN_REGION-sceplus-vm48:60020-0] util.FSUtils: DFS Client does not support most favored nodes create; using default create
2014-07-01 15:55:05,355 INFO  [RS_OPEN_REGION-sceplus-vm48:60020-2] regionserver.DefaultStoreFlusher: Flushed, sequenceid=5757, memsize=150.4m, hasBloomFilter=true, into tmp file hdfs://master:54310/hbase/data/default/usertable/9cef9e7d34f96f1d9b111be50040a208/.tmp/a2e0aec9db954541b8816bb246175550
2014-07-01 15:55:05,370 DEBUG [RS_OPEN_REGION-sceplus-vm48:60020-2] regionserver.HRegionFileSystem: Committing store file hdfs://master:54310/hbase/data/default/usertable/9cef9e7d34f96f1d9b111be50040a208/.tmp/a2e0aec9db954541b8816bb246175550 as hdfs://master:54310/hbase/data/default/usertable/9cef9e7d34f96f1d9b111be50040a208/family/a2e0aec9db954541b8816bb246175550
2014-07-01 15:55:05,382 INFO  [RS_OPEN_REGION-sceplus-vm48:60020-2] regionserver.HStore: Added hdfs://master:54310/hbase/data/default/usertable/9cef9e7d34f96f1d9b111be50040a208/family/a2e0aec9db954541b8816bb246175550, entries=547780, sequenceid=5757, filesize=39.1m
2014-07-01 15:55:05,383 INFO  [RS_OPEN_REGION-sceplus-vm48:60020-2] regionserver.HRegion: Finished memstore flush of ~150.4m/157756880, currentsize=0.0/0 for region usertable,,1404228265525.9cef9e7d34f96f1d9b111be50040a208. in 4479ms, sequenceid=5757, compaction requested=false; wal=null
2014-07-01 15:55:05,384 DEBUG [RS_OPEN_REGION-sceplus-vm48:60020-2] regionserver.HRegion: Deleted recovered.edits file=hdfs://master:54310/hbase/data/default/usertable/9cef9e7d34f96f1d9b111be50040a208/recovered.edits/0000000000000004965
2014-07-01 15:55:05,386 DEBUG [RS_OPEN_REGION-sceplus-vm48:60020-2] regionserver.HRegion: Deleted recovered.edits file=hdfs://master:54310/hbase/data/default/usertable/9cef9e7d34f96f1d9b111be50040a208/recovered.edits/0000000000000005024
2014-07-01 15:55:05,392 DEBUG [RS_OPEN_REGION-sceplus-vm48:60020-2] regionserver.HRegion: Deleted recovered.edits file=hdfs://master:54310/hbase/data/default/usertable/9cef9e7d34f96f1d9b111be50040a208/recovered.edits/0000000000000005083
2014-07-01 15:55:05,393 DEBUG [RS_OPEN_REGION-sceplus-vm48:60020-2] regionserver.HRegion: Deleted recovered.edits file=hdfs://master:54310/hbase/data/default/usertable/9cef9e7d34f96f1d9b111be50040a208/recovered.edits/0000000000000005135
2014-07-01 15:55:05,395 DEBUG [RS_OPEN_REGION-sceplus-vm48:60020-2] regionserver.HRegion: Deleted recovered.edits file=hdfs://master:54310/hbase/data/default/usertable/9cef9e7d34f96f1d9b111be50040a208/recovered.edits/0000000000000005181
2014-07-01 15:55:05,396 DEBUG [RS_OPEN_REGION-sceplus-vm48:60020-2] regionserver.HRegion: Deleted recovered.edits file=hdfs://master:54310/hbase/data/default/usertable/9cef9e7d34f96f1d9b111be50040a208/recovered.edits/0000000000000005233
2014-07-01 15:55:05,397 DEBUG [RS_OPEN_REGION-sceplus-vm48:60020-2] regionserver.HRegion: Deleted recovered.edits file=hdfs://master:54310/hbase/data/default/usertable/9cef9e7d34f96f1d9b111be50040a208/recovered.edits/0000000000000005285
2014-07-01 15:55:05,398 DEBUG [RS_OPEN_REGION-sceplus-vm48:60020-2] regionserver.HRegion: Deleted recovered.edits file=hdfs://master:54310/hbase/data/default/usertable/9cef9e7d34f96f1d9b111be50040a208/recovered.edits/0000000000000005336
2014-07-01 15:55:05,399 DEBUG [RS_OPEN_REGION-sceplus-vm48:60020-2] regionserver.HRegion: Deleted recovered.edits file=hdfs://master:54310/hbase/data/default/usertable/9cef9e7d34f96f1d9b111be50040a208/recovered.edits/0000000000000005391
2014-07-01 15:55:05,400 DEBUG [RS_OPEN_REGION-sceplus-vm48:60020-2] regionserver.HRegion: Deleted recovered.edits file=hdfs://master:54310/hbase/data/default/usertable/9cef9e7d34f96f1d9b111be50040a208/recovered.edits/0000000000000005446
2014-07-01 15:55:05,401 DEBUG [RS_OPEN_REGION-sceplus-vm48:60020-2] regionserver.HRegion: Deleted recovered.edits file=hdfs://master:54310/hbase/data/default/usertable/9cef9e7d34f96f1d9b111be50040a208/recovered.edits/0000000000000005496
2014-07-01 15:55:05,403 DEBUG [RS_OPEN_REGION-sceplus-vm48:60020-2] regionserver.HRegion: Deleted recovered.edits file=hdfs://master:54310/hbase/data/default/usertable/9cef9e7d34f96f1d9b111be50040a208/recovered.edits/0000000000000005538
2014-07-01 15:55:05,404 DEBUG [RS_OPEN_REGION-sceplus-vm48:60020-2] regionserver.HRegion: Deleted recovered.edits file=hdfs://master:54310/hbase/data/default/usertable/9cef9e7d34f96f1d9b111be50040a208/recovered.edits/0000000000000005589
2014-07-01 15:55:05,405 DEBUG [RS_OPEN_REGION-sceplus-vm48:60020-2] regionserver.HRegion: Deleted recovered.edits file=hdfs://master:54310/hbase/data/default/usertable/9cef9e7d34f96f1d9b111be50040a208/recovered.edits/0000000000000005641
2014-07-01 15:55:05,406 DEBUG [RS_OPEN_REGION-sceplus-vm48:60020-2] regionserver.HRegion: Deleted recovered.edits file=hdfs://master:54310/hbase/data/default/usertable/9cef9e7d34f96f1d9b111be50040a208/recovered.edits/0000000000000005688
2014-07-01 15:55:05,407 DEBUG [RS_OPEN_REGION-sceplus-vm48:60020-2] regionserver.HRegion: Deleted recovered.edits file=hdfs://master:54310/hbase/data/default/usertable/9cef9e7d34f96f1d9b111be50040a208/recovered.edits/0000000000000005735
2014-07-01 15:55:05,408 DEBUG [RS_OPEN_REGION-sceplus-vm48:60020-2] regionserver.HRegion: Deleted recovered.edits file=hdfs://master:54310/hbase/data/default/usertable/9cef9e7d34f96f1d9b111be50040a208/recovered.edits/0000000000000005757
2014-07-01 15:55:05,410 INFO  [RS_OPEN_REGION-sceplus-vm48:60020-2] regionserver.HRegion: Onlined 9cef9e7d34f96f1d9b111be50040a208; next sequenceid=5758
2014-07-01 15:55:05,410 DEBUG [RS_OPEN_REGION-sceplus-vm48:60020-2] zookeeper.ZKAssign: regionserver:60020-0x46f4224a300001, quorum=sceplus-vm49.almaden.ibm.com:2181,sceplus-vm48.almaden.ibm.com:2181, baseZNode=/hbase Attempting to retransition opening state of node 9cef9e7d34f96f1d9b111be50040a208
2014-07-01 15:55:05,414 INFO  [PostOpenDeployTasks:9cef9e7d34f96f1d9b111be50040a208] regionserver.HRegionServer: Post open deploy tasks for region=usertable,,1404228265525.9cef9e7d34f96f1d9b111be50040a208.
2014-07-01 15:55:05,423 INFO  [PostOpenDeployTasks:9cef9e7d34f96f1d9b111be50040a208] catalog.MetaEditor: Updated row usertable,,1404228265525.9cef9e7d34f96f1d9b111be50040a208. with server=sceplus-vm48.almaden.ibm.com,60020,1404255227840
2014-07-01 15:55:05,423 INFO  [PostOpenDeployTasks:9cef9e7d34f96f1d9b111be50040a208] regionserver.HRegionServer: Finished post open deploy task for usertable,,1404228265525.9cef9e7d34f96f1d9b111be50040a208.
2014-07-01 15:55:05,423 DEBUG [RS_OPEN_REGION-sceplus-vm48:60020-2] zookeeper.ZKAssign: regionserver:60020-0x46f4224a300001, quorum=sceplus-vm49.almaden.ibm.com:2181,sceplus-vm48.almaden.ibm.com:2181, baseZNode=/hbase Transitioning 9cef9e7d34f96f1d9b111be50040a208 from RS_ZK_REGION_OPENING to RS_ZK_REGION_OPENED
2014-07-01 15:55:05,437 DEBUG [RS_OPEN_REGION-sceplus-vm48:60020-2] zookeeper.ZKAssign: regionserver:60020-0x46f4224a300001, quorum=sceplus-vm49.almaden.ibm.com:2181,sceplus-vm48.almaden.ibm.com:2181, baseZNode=/hbase Transitioned node 9cef9e7d34f96f1d9b111be50040a208 from RS_ZK_REGION_OPENING to RS_ZK_REGION_OPENED
2014-07-01 15:55:05,437 DEBUG [RS_OPEN_REGION-sceplus-vm48:60020-2] handler.OpenRegionHandler: Transitioned 9cef9e7d34f96f1d9b111be50040a208 to OPENED in zk on sceplus-vm48.almaden.ibm.com,60020,1404255227840
2014-07-01 15:55:05,437 DEBUG [RS_OPEN_REGION-sceplus-vm48:60020-2] handler.OpenRegionHandler: Opened usertable,,1404228265525.9cef9e7d34f96f1d9b111be50040a208. on sceplus-vm48.almaden.ibm.com,60020,1404255227840
2014-07-01 15:55:05,464 INFO  [Priority.RpcServer.handler=6,port=60020] regionserver.HRegionServer: Close 9cef9e7d34f96f1d9b111be50040a208, via zk=yes, znode version=0, on null
2014-07-01 15:55:05,464 DEBUG [RS_CLOSE_REGION-sceplus-vm48:60020-0] handler.CloseRegionHandler: Processing close of usertable,,1404228265525.9cef9e7d34f96f1d9b111be50040a208.
2014-07-01 15:55:05,470 DEBUG [RS_CLOSE_REGION-sceplus-vm48:60020-0] regionserver.HRegion: Closing usertable,,1404228265525.9cef9e7d34f96f1d9b111be50040a208.: disabling compactions & flushes
2014-07-01 15:55:05,470 DEBUG [RS_CLOSE_REGION-sceplus-vm48:60020-0] regionserver.HRegion: Updates disabled for region usertable,,1404228265525.9cef9e7d34f96f1d9b111be50040a208.
2014-07-01 15:55:05,473 INFO  [StoreCloserThread-usertable,,1404228265525.9cef9e7d34f96f1d9b111be50040a208.-1] regionserver.HStore: Closed family
2014-07-01 15:55:05,473 INFO  [RS_CLOSE_REGION-sceplus-vm48:60020-0] regionserver.HRegion: Closed usertable,,1404228265525.9cef9e7d34f96f1d9b111be50040a208.
2014-07-01 15:55:05,473 DEBUG [RS_CLOSE_REGION-sceplus-vm48:60020-0] zookeeper.ZKAssign: regionserver:60020-0x46f4224a300001, quorum=sceplus-vm49.almaden.ibm.com:2181,sceplus-vm48.almaden.ibm.com:2181, baseZNode=/hbase Transitioning 9cef9e7d34f96f1d9b111be50040a208 from M_ZK_REGION_CLOSING to RS_ZK_REGION_CLOSED
2014-07-01 15:55:05,484 DEBUG [RS_CLOSE_REGION-sceplus-vm48:60020-0] zookeeper.ZKAssign: regionserver:60020-0x46f4224a300001, quorum=sceplus-vm49.almaden.ibm.com:2181,sceplus-vm48.almaden.ibm.com:2181, baseZNode=/hbase Transitioned node 9cef9e7d34f96f1d9b111be50040a208 from M_ZK_REGION_CLOSING to RS_ZK_REGION_CLOSED
2014-07-01 15:55:05,484 DEBUG [RS_CLOSE_REGION-sceplus-vm48:60020-0] handler.CloseRegionHandler: Set closed state in zk for usertable,,1404228265525.9cef9e7d34f96f1d9b111be50040a208. on sceplus-vm48.almaden.ibm.com,60020,1404255227840
2014-07-01 15:55:05,484 DEBUG [RS_CLOSE_REGION-sceplus-vm48:60020-0] handler.CloseRegionHandler: Closed usertable,,1404228265525.9cef9e7d34f96f1d9b111be50040a208.
2014-07-01 15:55:07,641 INFO  [RS_OPEN_REGION-sceplus-vm48:60020-0] regionserver.DefaultStoreFlusher: Flushed, sequenceid=4678, memsize=134.3m, hasBloomFilter=true, into tmp file hdfs://master:54310/hbase/data/default/usertable/bb79ae773f71289e0745068f68a0abd9/.tmp/8e22631d0883445dabe398fd3a95d1bd
2014-07-01 15:55:07,653 DEBUG [RS_OPEN_REGION-sceplus-vm48:60020-0] regionserver.HRegionFileSystem: Committing store file hdfs://master:54310/hbase/data/default/usertable/bb79ae773f71289e0745068f68a0abd9/.tmp/8e22631d0883445dabe398fd3a95d1bd as hdfs://master:54310/hbase/data/default/usertable/bb79ae773f71289e0745068f68a0abd9/family/8e22631d0883445dabe398fd3a95d1bd
2014-07-01 15:55:07,665 INFO  [RS_OPEN_REGION-sceplus-vm48:60020-0] regionserver.HStore: Added hdfs://master:54310/hbase/data/default/usertable/bb79ae773f71289e0745068f68a0abd9/family/8e22631d0883445dabe398fd3a95d1bd, entries=489020, sequenceid=4678, filesize=34.8m
2014-07-01 15:55:07,665 INFO  [RS_OPEN_REGION-sceplus-vm48:60020-0] regionserver.HRegion: Finished memstore flush of ~134.8m/141321360, currentsize=0.0/0 for region usertable,user9,1404228265526.bb79ae773f71289e0745068f68a0abd9. in 3875ms, sequenceid=4678, compaction requested=false; wal=null
2014-07-01 15:55:07,667 DEBUG [RS_OPEN_REGION-sceplus-vm48:60020-0] regionserver.HRegion: Deleted recovered.edits file=hdfs://master:54310/hbase/data/default/usertable/bb79ae773f71289e0745068f68a0abd9/recovered.edits/0000000000000004426
2014-07-01 15:55:07,668 DEBUG [RS_OPEN_REGION-sceplus-vm48:60020-0] regionserver.HRegion: Deleted recovered.edits file=hdfs://master:54310/hbase/data/default/usertable/bb79ae773f71289e0745068f68a0abd9/recovered.edits/0000000000000004444
2014-07-01 15:55:07,669 DEBUG [RS_OPEN_REGION-sceplus-vm48:60020-0] regionserver.HRegion: Deleted recovered.edits file=hdfs://master:54310/hbase/data/default/usertable/bb79ae773f71289e0745068f68a0abd9/recovered.edits/0000000000000004457
2014-07-01 15:55:07,671 DEBUG [RS_OPEN_REGION-sceplus-vm48:60020-0] regionserver.HRegion: Deleted recovered.edits file=hdfs://master:54310/hbase/data/default/usertable/bb79ae773f71289e0745068f68a0abd9/recovered.edits/0000000000000004472
2014-07-01 15:55:07,672 DEBUG [RS_OPEN_REGION-sceplus-vm48:60020-0] regionserver.HRegion: Deleted recovered.edits file=hdfs://master:54310/hbase/data/default/usertable/bb79ae773f71289e0745068f68a0abd9/recovered.edits/0000000000000004487
2014-07-01 15:55:07,673 DEBUG [RS_OPEN_REGION-sceplus-vm48:60020-0] regionserver.HRegion: Deleted recovered.edits file=hdfs://master:54310/hbase/data/default/usertable/bb79ae773f71289e0745068f68a0abd9/recovered.edits/0000000000000004507
2014-07-01 15:55:07,675 DEBUG [RS_OPEN_REGION-sceplus-vm48:60020-0] regionserver.HRegion: Deleted recovered.edits file=hdfs://master:54310/hbase/data/default/usertable/bb79ae773f71289e0745068f68a0abd9/recovered.edits/0000000000000004521
2014-07-01 15:55:07,676 DEBUG [RS_OPEN_REGION-sceplus-vm48:60020-0] regionserver.HRegion: Deleted recovered.edits file=hdfs://master:54310/hbase/data/default/usertable/bb79ae773f71289e0745068f68a0abd9/recovered.edits/0000000000000004539
2014-07-01 15:55:07,677 DEBUG [RS_OPEN_REGION-sceplus-vm48:60020-0] regionserver.HRegion: Deleted recovered.edits file=hdfs://master:54310/hbase/data/default/usertable/bb79ae773f71289e0745068f68a0abd9/recovered.edits/0000000000000004555
2014-07-01 15:55:07,679 DEBUG [RS_OPEN_REGION-sceplus-vm48:60020-0] regionserver.HRegion: Deleted recovered.edits file=hdfs://master:54310/hbase/data/default/usertable/bb79ae773f71289e0745068f68a0abd9/recovered.edits/0000000000000004571
2014-07-01 15:55:07,680 DEBUG [RS_OPEN_REGION-sceplus-vm48:60020-0] regionserver.HRegion: Deleted recovered.edits file=hdfs://master:54310/hbase/data/default/usertable/bb79ae773f71289e0745068f68a0abd9/recovered.edits/0000000000000004585
2014-07-01 15:55:07,681 DEBUG [RS_OPEN_REGION-sceplus-vm48:60020-0] regionserver.HRegion: Deleted recovered.edits file=hdfs://master:54310/hbase/data/default/usertable/bb79ae773f71289e0745068f68a0abd9/recovered.edits/0000000000000004605
2014-07-01 15:55:07,683 DEBUG [RS_OPEN_REGION-sceplus-vm48:60020-0] regionserver.HRegion: Deleted recovered.edits file=hdfs://master:54310/hbase/data/default/usertable/bb79ae773f71289e0745068f68a0abd9/recovered.edits/0000000000000004621
2014-07-01 15:55:07,684 DEBUG [RS_OPEN_REGION-sceplus-vm48:60020-0] regionserver.HRegion: Deleted recovered.edits file=hdfs://master:54310/hbase/data/default/usertable/bb79ae773f71289e0745068f68a0abd9/recovered.edits/0000000000000004636
2014-07-01 15:55:07,686 DEBUG [RS_OPEN_REGION-sceplus-vm48:60020-0] regionserver.HRegion: Deleted recovered.edits file=hdfs://master:54310/hbase/data/default/usertable/bb79ae773f71289e0745068f68a0abd9/recovered.edits/0000000000000004652
2014-07-01 15:55:07,687 DEBUG [RS_OPEN_REGION-sceplus-vm48:60020-0] regionserver.HRegion: Deleted recovered.edits file=hdfs://master:54310/hbase/data/default/usertable/bb79ae773f71289e0745068f68a0abd9/recovered.edits/0000000000000004669
2014-07-01 15:55:07,688 DEBUG [RS_OPEN_REGION-sceplus-vm48:60020-0] regionserver.HRegion: Deleted recovered.edits file=hdfs://master:54310/hbase/data/default/usertable/bb79ae773f71289e0745068f68a0abd9/recovered.edits/0000000000000004678
2014-07-01 15:55:07,690 INFO  [RS_OPEN_REGION-sceplus-vm48:60020-0] regionserver.HRegion: Onlined bb79ae773f71289e0745068f68a0abd9; next sequenceid=4679
2014-07-01 15:55:07,690 DEBUG [RS_OPEN_REGION-sceplus-vm48:60020-0] zookeeper.ZKAssign: regionserver:60020-0x46f4224a300001, quorum=sceplus-vm49.almaden.ibm.com:2181,sceplus-vm48.almaden.ibm.com:2181, baseZNode=/hbase Attempting to retransition opening state of node bb79ae773f71289e0745068f68a0abd9
2014-07-01 15:55:07,693 INFO  [PostOpenDeployTasks:bb79ae773f71289e0745068f68a0abd9] regionserver.HRegionServer: Post open deploy tasks for region=usertable,user9,1404228265526.bb79ae773f71289e0745068f68a0abd9.
2014-07-01 15:55:07,700 INFO  [PostOpenDeployTasks:bb79ae773f71289e0745068f68a0abd9] catalog.MetaEditor: Updated row usertable,user9,1404228265526.bb79ae773f71289e0745068f68a0abd9. with server=sceplus-vm48.almaden.ibm.com,60020,1404255227840
2014-07-01 15:55:07,700 INFO  [PostOpenDeployTasks:bb79ae773f71289e0745068f68a0abd9] regionserver.HRegionServer: Finished post open deploy task for usertable,user9,1404228265526.bb79ae773f71289e0745068f68a0abd9.
2014-07-01 15:55:07,700 DEBUG [RS_OPEN_REGION-sceplus-vm48:60020-0] zookeeper.ZKAssign: regionserver:60020-0x46f4224a300001, quorum=sceplus-vm49.almaden.ibm.com:2181,sceplus-vm48.almaden.ibm.com:2181, baseZNode=/hbase Transitioning bb79ae773f71289e0745068f68a0abd9 from RS_ZK_REGION_OPENING to RS_ZK_REGION_OPENED
2014-07-01 15:55:07,705 DEBUG [RS_OPEN_REGION-sceplus-vm48:60020-0] zookeeper.ZKAssign: regionserver:60020-0x46f4224a300001, quorum=sceplus-vm49.almaden.ibm.com:2181,sceplus-vm48.almaden.ibm.com:2181, baseZNode=/hbase Transitioned node bb79ae773f71289e0745068f68a0abd9 from RS_ZK_REGION_OPENING to RS_ZK_REGION_OPENED
2014-07-01 15:55:07,705 DEBUG [RS_OPEN_REGION-sceplus-vm48:60020-0] handler.OpenRegionHandler: Transitioned bb79ae773f71289e0745068f68a0abd9 to OPENED in zk on sceplus-vm48.almaden.ibm.com,60020,1404255227840
2014-07-01 15:55:07,705 DEBUG [RS_OPEN_REGION-sceplus-vm48:60020-0] handler.OpenRegionHandler: Opened usertable,user9,1404228265526.bb79ae773f71289e0745068f68a0abd9. on sceplus-vm48.almaden.ibm.com,60020,1404255227840
2014-07-01 15:55:07,716 INFO  [Priority.RpcServer.handler=7,port=60020] regionserver.HRegionServer: Close bb79ae773f71289e0745068f68a0abd9, via zk=yes, znode version=0, on null
2014-07-01 15:55:07,716 DEBUG [RS_CLOSE_REGION-sceplus-vm48:60020-1] handler.CloseRegionHandler: Processing close of usertable,user9,1404228265526.bb79ae773f71289e0745068f68a0abd9.
2014-07-01 15:55:07,718 DEBUG [RS_CLOSE_REGION-sceplus-vm48:60020-1] regionserver.HRegion: Closing usertable,user9,1404228265526.bb79ae773f71289e0745068f68a0abd9.: disabling compactions & flushes
2014-07-01 15:55:07,718 DEBUG [RS_CLOSE_REGION-sceplus-vm48:60020-1] regionserver.HRegion: Updates disabled for region usertable,user9,1404228265526.bb79ae773f71289e0745068f68a0abd9.
2014-07-01 15:55:07,719 INFO  [StoreCloserThread-usertable,user9,1404228265526.bb79ae773f71289e0745068f68a0abd9.-1] regionserver.HStore: Closed family
2014-07-01 15:55:07,720 INFO  [RS_CLOSE_REGION-sceplus-vm48:60020-1] regionserver.HRegion: Closed usertable,user9,1404228265526.bb79ae773f71289e0745068f68a0abd9.
2014-07-01 15:55:07,720 DEBUG [RS_CLOSE_REGION-sceplus-vm48:60020-1] zookeeper.ZKAssign: regionserver:60020-0x46f4224a300001, quorum=sceplus-vm49.almaden.ibm.com:2181,sceplus-vm48.almaden.ibm.com:2181, baseZNode=/hbase Transitioning bb79ae773f71289e0745068f68a0abd9 from M_ZK_REGION_CLOSING to RS_ZK_REGION_CLOSED
2014-07-01 15:55:07,726 DEBUG [RS_CLOSE_REGION-sceplus-vm48:60020-1] zookeeper.ZKAssign: regionserver:60020-0x46f4224a300001, quorum=sceplus-vm49.almaden.ibm.com:2181,sceplus-vm48.almaden.ibm.com:2181, baseZNode=/hbase Transitioned node bb79ae773f71289e0745068f68a0abd9 from M_ZK_REGION_CLOSING to RS_ZK_REGION_CLOSED
2014-07-01 15:55:07,726 DEBUG [RS_CLOSE_REGION-sceplus-vm48:60020-1] handler.CloseRegionHandler: Set closed state in zk for usertable,user9,1404228265526.bb79ae773f71289e0745068f68a0abd9. on sceplus-vm48.almaden.ibm.com,60020,1404255227840
2014-07-01 15:55:07,726 DEBUG [RS_CLOSE_REGION-sceplus-vm48:60020-1] handler.CloseRegionHandler: Closed usertable,user9,1404228265526.bb79ae773f71289e0745068f68a0abd9.
2014-07-01 15:58:47,929 DEBUG [LruStats #0] hfile.LruBlockCache: Total=4.16 MB, free=3.95 GB, max=3.96 GB, blocks=0, accesses=0, hits=0, hitRatio=0, cachingAccesses=0, cachingHits=0, cachingHitsRatio=0,evictions=0, evicted=0, evictedPerRun=NaN
2014-07-01 16:00:16,494 INFO  [Priority.RpcServer.handler=8,port=60020] regionserver.HRegionServer: Open usertable,user8,1404255616195.9e9a6a0fd8aef92a09081d29900a50f3.
2014-07-01 16:00:16,507 INFO  [Priority.RpcServer.handler=8,port=60020] regionserver.HRegionServer: Open usertable,,1404255616194.510634b895ee706ff306728824babf96.
2014-07-01 16:00:16,507 DEBUG [RS_OPEN_REGION-sceplus-vm48:60020-1] zookeeper.ZKAssign: regionserver:60020-0x46f4224a300001, quorum=sceplus-vm49.almaden.ibm.com:2181,sceplus-vm48.almaden.ibm.com:2181, baseZNode=/hbase Transitioning 9e9a6a0fd8aef92a09081d29900a50f3 from M_ZK_REGION_OFFLINE to RS_ZK_REGION_OPENING
2014-07-01 16:00:16,508 INFO  [Priority.RpcServer.handler=8,port=60020] regionserver.HRegionServer: Open usertable,user7,1404255616194.431aad56a67499b48936eca121e3178e.
2014-07-01 16:00:16,508 DEBUG [RS_OPEN_REGION-sceplus-vm48:60020-2] zookeeper.ZKAssign: regionserver:60020-0x46f4224a300001, quorum=sceplus-vm49.almaden.ibm.com:2181,sceplus-vm48.almaden.ibm.com:2181, baseZNode=/hbase Transitioning 510634b895ee706ff306728824babf96 from M_ZK_REGION_OFFLINE to RS_ZK_REGION_OPENING
2014-07-01 16:00:16,509 INFO  [Priority.RpcServer.handler=8,port=60020] regionserver.HRegionServer: Open usertable,user5,1404255616194.3e76136f0a8be94ccf3637676c064402.
2014-07-01 16:00:16,509 INFO  [Priority.RpcServer.handler=8,port=60020] regionserver.HRegionServer: Open usertable,user3,1404255616194.09108ac86cf3c374e79d1bb78f787b49.
2014-07-01 16:00:16,510 DEBUG [RS_OPEN_REGION-sceplus-vm48:60020-0] zookeeper.ZKAssign: regionserver:60020-0x46f4224a300001, quorum=sceplus-vm49.almaden.ibm.com:2181,sceplus-vm48.almaden.ibm.com:2181, baseZNode=/hbase Transitioning 431aad56a67499b48936eca121e3178e from M_ZK_REGION_OFFLINE to RS_ZK_REGION_OPENING
2014-07-01 16:00:16,513 DEBUG [RS_OPEN_REGION-sceplus-vm48:60020-1] zookeeper.ZKAssign: regionserver:60020-0x46f4224a300001, quorum=sceplus-vm49.almaden.ibm.com:2181,sceplus-vm48.almaden.ibm.com:2181, baseZNode=/hbase Transitioned node 9e9a6a0fd8aef92a09081d29900a50f3 from M_ZK_REGION_OFFLINE to RS_ZK_REGION_OPENING
2014-07-01 16:00:16,514 DEBUG [RS_OPEN_REGION-sceplus-vm48:60020-1] regionserver.HRegion: Opening region: {ENCODED => 9e9a6a0fd8aef92a09081d29900a50f3, NAME => 'usertable,user8,1404255616195.9e9a6a0fd8aef92a09081d29900a50f3.', STARTKEY => 'user8', ENDKEY => 'user9'}
2014-07-01 16:00:16,515 DEBUG [RS_OPEN_REGION-sceplus-vm48:60020-2] zookeeper.ZKAssign: regionserver:60020-0x46f4224a300001, quorum=sceplus-vm49.almaden.ibm.com:2181,sceplus-vm48.almaden.ibm.com:2181, baseZNode=/hbase Transitioned node 510634b895ee706ff306728824babf96 from M_ZK_REGION_OFFLINE to RS_ZK_REGION_OPENING
2014-07-01 16:00:16,515 DEBUG [RS_OPEN_REGION-sceplus-vm48:60020-2] regionserver.HRegion: Opening region: {ENCODED => 510634b895ee706ff306728824babf96, NAME => 'usertable,,1404255616194.510634b895ee706ff306728824babf96.', STARTKEY => '', ENDKEY => 'user2'}
2014-07-01 16:00:16,516 DEBUG [RS_OPEN_REGION-sceplus-vm48:60020-2] regionserver.MetricsRegionSourceImpl: Creating new MetricsRegionSourceImpl for table usertable 510634b895ee706ff306728824babf96
2014-07-01 16:00:16,516 DEBUG [RS_OPEN_REGION-sceplus-vm48:60020-1] regionserver.MetricsRegionSourceImpl: Creating new MetricsRegionSourceImpl for table usertable 9e9a6a0fd8aef92a09081d29900a50f3
2014-07-01 16:00:16,516 DEBUG [RS_OPEN_REGION-sceplus-vm48:60020-0] zookeeper.ZKAssign: regionserver:60020-0x46f4224a300001, quorum=sceplus-vm49.almaden.ibm.com:2181,sceplus-vm48.almaden.ibm.com:2181, baseZNode=/hbase Transitioned node 431aad56a67499b48936eca121e3178e from M_ZK_REGION_OFFLINE to RS_ZK_REGION_OPENING
2014-07-01 16:00:16,516 DEBUG [RS_OPEN_REGION-sceplus-vm48:60020-2] regionserver.HRegion: Instantiated usertable,,1404255616194.510634b895ee706ff306728824babf96.
2014-07-01 16:00:16,516 DEBUG [RS_OPEN_REGION-sceplus-vm48:60020-1] regionserver.HRegion: Instantiated usertable,user8,1404255616195.9e9a6a0fd8aef92a09081d29900a50f3.
2014-07-01 16:00:16,516 DEBUG [RS_OPEN_REGION-sceplus-vm48:60020-0] regionserver.HRegion: Opening region: {ENCODED => 431aad56a67499b48936eca121e3178e, NAME => 'usertable,user7,1404255616194.431aad56a67499b48936eca121e3178e.', STARTKEY => 'user7', ENDKEY => 'user8'}
2014-07-01 16:00:16,517 DEBUG [RS_OPEN_REGION-sceplus-vm48:60020-0] regionserver.MetricsRegionSourceImpl: Creating new MetricsRegionSourceImpl for table usertable 431aad56a67499b48936eca121e3178e
2014-07-01 16:00:16,517 DEBUG [RS_OPEN_REGION-sceplus-vm48:60020-0] regionserver.HRegion: Instantiated usertable,user7,1404255616194.431aad56a67499b48936eca121e3178e.
2014-07-01 16:00:16,526 INFO  [StoreOpener-510634b895ee706ff306728824babf96-1] compactions.CompactionConfiguration: size [268435456, 9223372036854775807); files [3, 10); ratio 1.200000; off-peak ratio 5.000000; throttle point 5368709120; delete expired; major period 0, major jitter 0.500000
2014-07-01 16:00:16,526 INFO  [StoreOpener-9e9a6a0fd8aef92a09081d29900a50f3-1] compactions.CompactionConfiguration: size [268435456, 9223372036854775807); files [3, 10); ratio 1.200000; off-peak ratio 5.000000; throttle point 5368709120; delete expired; major period 0, major jitter 0.500000
2014-07-01 16:00:16,528 INFO  [StoreOpener-431aad56a67499b48936eca121e3178e-1] compactions.CompactionConfiguration: size [268435456, 9223372036854775807); files [3, 10); ratio 1.200000; off-peak ratio 5.000000; throttle point 5368709120; delete expired; major period 0, major jitter 0.500000
2014-07-01 16:00:16,529 DEBUG [RS_OPEN_REGION-sceplus-vm48:60020-2] regionserver.HRegion: Found 0 recovered edits file(s) under hdfs://master:54310/hbase/data/default/usertable/510634b895ee706ff306728824babf96
2014-07-01 16:00:16,529 DEBUG [RS_OPEN_REGION-sceplus-vm48:60020-1] regionserver.HRegion: Found 0 recovered edits file(s) under hdfs://master:54310/hbase/data/default/usertable/9e9a6a0fd8aef92a09081d29900a50f3
2014-07-01 16:00:16,530 DEBUG [RS_OPEN_REGION-sceplus-vm48:60020-0] regionserver.HRegion: Found 0 recovered edits file(s) under hdfs://master:54310/hbase/data/default/usertable/431aad56a67499b48936eca121e3178e
2014-07-01 16:00:16,531 INFO  [RS_OPEN_REGION-sceplus-vm48:60020-2] regionserver.HRegion: Onlined 510634b895ee706ff306728824babf96; next sequenceid=1
2014-07-01 16:00:16,531 DEBUG [RS_OPEN_REGION-sceplus-vm48:60020-2] zookeeper.ZKAssign: regionserver:60020-0x46f4224a300001, quorum=sceplus-vm49.almaden.ibm.com:2181,sceplus-vm48.almaden.ibm.com:2181, baseZNode=/hbase Attempting to retransition opening state of node 510634b895ee706ff306728824babf96
2014-07-01 16:00:16,531 INFO  [RS_OPEN_REGION-sceplus-vm48:60020-1] regionserver.HRegion: Onlined 9e9a6a0fd8aef92a09081d29900a50f3; next sequenceid=1
2014-07-01 16:00:16,531 DEBUG [RS_OPEN_REGION-sceplus-vm48:60020-1] zookeeper.ZKAssign: regionserver:60020-0x46f4224a300001, quorum=sceplus-vm49.almaden.ibm.com:2181,sceplus-vm48.almaden.ibm.com:2181, baseZNode=/hbase Attempting to retransition opening state of node 9e9a6a0fd8aef92a09081d29900a50f3
2014-07-01 16:00:16,533 INFO  [PostOpenDeployTasks:510634b895ee706ff306728824babf96] regionserver.HRegionServer: Post open deploy tasks for region=usertable,,1404255616194.510634b895ee706ff306728824babf96.
2014-07-01 16:00:16,533 INFO  [PostOpenDeployTasks:9e9a6a0fd8aef92a09081d29900a50f3] regionserver.HRegionServer: Post open deploy tasks for region=usertable,user8,1404255616195.9e9a6a0fd8aef92a09081d29900a50f3.
2014-07-01 16:00:16,545 INFO  [PostOpenDeployTasks:9e9a6a0fd8aef92a09081d29900a50f3] catalog.MetaEditor: Updated row usertable,user8,1404255616195.9e9a6a0fd8aef92a09081d29900a50f3. with server=sceplus-vm48.almaden.ibm.com,60020,1404255227840
2014-07-01 16:00:16,545 INFO  [PostOpenDeployTasks:9e9a6a0fd8aef92a09081d29900a50f3] regionserver.HRegionServer: Finished post open deploy task for usertable,user8,1404255616195.9e9a6a0fd8aef92a09081d29900a50f3.
2014-07-01 16:00:16,546 DEBUG [RS_OPEN_REGION-sceplus-vm48:60020-1] zookeeper.ZKAssign: regionserver:60020-0x46f4224a300001, quorum=sceplus-vm49.almaden.ibm.com:2181,sceplus-vm48.almaden.ibm.com:2181, baseZNode=/hbase Transitioning 9e9a6a0fd8aef92a09081d29900a50f3 from RS_ZK_REGION_OPENING to RS_ZK_REGION_OPENED
2014-07-01 16:00:16,547 INFO  [PostOpenDeployTasks:510634b895ee706ff306728824babf96] catalog.MetaEditor: Updated row usertable,,1404255616194.510634b895ee706ff306728824babf96. with server=sceplus-vm48.almaden.ibm.com,60020,1404255227840
2014-07-01 16:00:16,547 INFO  [PostOpenDeployTasks:510634b895ee706ff306728824babf96] regionserver.HRegionServer: Finished post open deploy task for usertable,,1404255616194.510634b895ee706ff306728824babf96.
2014-07-01 16:00:16,547 DEBUG [RS_OPEN_REGION-sceplus-vm48:60020-2] zookeeper.ZKAssign: regionserver:60020-0x46f4224a300001, quorum=sceplus-vm49.almaden.ibm.com:2181,sceplus-vm48.almaden.ibm.com:2181, baseZNode=/hbase Transitioning 510634b895ee706ff306728824babf96 from RS_ZK_REGION_OPENING to RS_ZK_REGION_OPENED
2014-07-01 16:00:16,565 DEBUG [RS_OPEN_REGION-sceplus-vm48:60020-1] zookeeper.ZKAssign: regionserver:60020-0x46f4224a300001, quorum=sceplus-vm49.almaden.ibm.com:2181,sceplus-vm48.almaden.ibm.com:2181, baseZNode=/hbase Transitioned node 9e9a6a0fd8aef92a09081d29900a50f3 from RS_ZK_REGION_OPENING to RS_ZK_REGION_OPENED
2014-07-01 16:00:16,565 DEBUG [RS_OPEN_REGION-sceplus-vm48:60020-1] handler.OpenRegionHandler: Transitioned 9e9a6a0fd8aef92a09081d29900a50f3 to OPENED in zk on sceplus-vm48.almaden.ibm.com,60020,1404255227840
2014-07-01 16:00:16,565 DEBUG [RS_OPEN_REGION-sceplus-vm48:60020-1] handler.OpenRegionHandler: Opened usertable,user8,1404255616195.9e9a6a0fd8aef92a09081d29900a50f3. on sceplus-vm48.almaden.ibm.com,60020,1404255227840
2014-07-01 16:00:16,565 DEBUG [RS_OPEN_REGION-sceplus-vm48:60020-1] zookeeper.ZKAssign: regionserver:60020-0x46f4224a300001, quorum=sceplus-vm49.almaden.ibm.com:2181,sceplus-vm48.almaden.ibm.com:2181, baseZNode=/hbase Transitioning 3e76136f0a8be94ccf3637676c064402 from M_ZK_REGION_OFFLINE to RS_ZK_REGION_OPENING
2014-07-01 16:00:16,570 DEBUG [RS_OPEN_REGION-sceplus-vm48:60020-2] zookeeper.ZKAssign: regionserver:60020-0x46f4224a300001, quorum=sceplus-vm49.almaden.ibm.com:2181,sceplus-vm48.almaden.ibm.com:2181, baseZNode=/hbase Transitioned node 510634b895ee706ff306728824babf96 from RS_ZK_REGION_OPENING to RS_ZK_REGION_OPENED
2014-07-01 16:00:16,570 DEBUG [RS_OPEN_REGION-sceplus-vm48:60020-2] handler.OpenRegionHandler: Transitioned 510634b895ee706ff306728824babf96 to OPENED in zk on sceplus-vm48.almaden.ibm.com,60020,1404255227840
2014-07-01 16:00:16,570 DEBUG [RS_OPEN_REGION-sceplus-vm48:60020-2] handler.OpenRegionHandler: Opened usertable,,1404255616194.510634b895ee706ff306728824babf96. on sceplus-vm48.almaden.ibm.com,60020,1404255227840
2014-07-01 16:00:16,571 DEBUG [RS_OPEN_REGION-sceplus-vm48:60020-2] zookeeper.ZKAssign: regionserver:60020-0x46f4224a300001, quorum=sceplus-vm49.almaden.ibm.com:2181,sceplus-vm48.almaden.ibm.com:2181, baseZNode=/hbase Transitioning 09108ac86cf3c374e79d1bb78f787b49 from M_ZK_REGION_OFFLINE to RS_ZK_REGION_OPENING
2014-07-01 16:00:16,572 INFO  [RS_OPEN_REGION-sceplus-vm48:60020-0] regionserver.HRegion: Onlined 431aad56a67499b48936eca121e3178e; next sequenceid=1
2014-07-01 16:00:16,572 DEBUG [RS_OPEN_REGION-sceplus-vm48:60020-0] zookeeper.ZKAssign: regionserver:60020-0x46f4224a300001, quorum=sceplus-vm49.almaden.ibm.com:2181,sceplus-vm48.almaden.ibm.com:2181, baseZNode=/hbase Attempting to retransition opening state of node 431aad56a67499b48936eca121e3178e
2014-07-01 16:00:16,577 DEBUG [RS_OPEN_REGION-sceplus-vm48:60020-1] zookeeper.ZKAssign: regionserver:60020-0x46f4224a300001, quorum=sceplus-vm49.almaden.ibm.com:2181,sceplus-vm48.almaden.ibm.com:2181, baseZNode=/hbase Transitioned node 3e76136f0a8be94ccf3637676c064402 from M_ZK_REGION_OFFLINE to RS_ZK_REGION_OPENING
2014-07-01 16:00:16,578 DEBUG [RS_OPEN_REGION-sceplus-vm48:60020-1] regionserver.HRegion: Opening region: {ENCODED => 3e76136f0a8be94ccf3637676c064402, NAME => 'usertable,user5,1404255616194.3e76136f0a8be94ccf3637676c064402.', STARTKEY => 'user5', ENDKEY => 'user6'}
2014-07-01 16:00:16,579 INFO  [PostOpenDeployTasks:431aad56a67499b48936eca121e3178e] regionserver.HRegionServer: Post open deploy tasks for region=usertable,user7,1404255616194.431aad56a67499b48936eca121e3178e.
2014-07-01 16:00:16,579 DEBUG [RS_OPEN_REGION-sceplus-vm48:60020-1] regionserver.MetricsRegionSourceImpl: Creating new MetricsRegionSourceImpl for table usertable 3e76136f0a8be94ccf3637676c064402
2014-07-01 16:00:16,579 DEBUG [RS_OPEN_REGION-sceplus-vm48:60020-1] regionserver.HRegion: Instantiated usertable,user5,1404255616194.3e76136f0a8be94ccf3637676c064402.
2014-07-01 16:00:16,579 DEBUG [RS_OPEN_REGION-sceplus-vm48:60020-2] zookeeper.ZKAssign: regionserver:60020-0x46f4224a300001, quorum=sceplus-vm49.almaden.ibm.com:2181,sceplus-vm48.almaden.ibm.com:2181, baseZNode=/hbase Transitioned node 09108ac86cf3c374e79d1bb78f787b49 from M_ZK_REGION_OFFLINE to RS_ZK_REGION_OPENING
2014-07-01 16:00:16,579 DEBUG [RS_OPEN_REGION-sceplus-vm48:60020-2] regionserver.HRegion: Opening region: {ENCODED => 09108ac86cf3c374e79d1bb78f787b49, NAME => 'usertable,user3,1404255616194.09108ac86cf3c374e79d1bb78f787b49.', STARTKEY => 'user3', ENDKEY => 'user4'}
2014-07-01 16:00:16,580 DEBUG [RS_OPEN_REGION-sceplus-vm48:60020-2] regionserver.MetricsRegionSourceImpl: Creating new MetricsRegionSourceImpl for table usertable 09108ac86cf3c374e79d1bb78f787b49
2014-07-01 16:00:16,580 DEBUG [RS_OPEN_REGION-sceplus-vm48:60020-2] regionserver.HRegion: Instantiated usertable,user3,1404255616194.09108ac86cf3c374e79d1bb78f787b49.
2014-07-01 16:00:16,585 INFO  [StoreOpener-3e76136f0a8be94ccf3637676c064402-1] compactions.CompactionConfiguration: size [268435456, 9223372036854775807); files [3, 10); ratio 1.200000; off-peak ratio 5.000000; throttle point 5368709120; delete expired; major period 0, major jitter 0.500000
2014-07-01 16:00:16,587 INFO  [StoreOpener-09108ac86cf3c374e79d1bb78f787b49-1] compactions.CompactionConfiguration: size [268435456, 9223372036854775807); files [3, 10); ratio 1.200000; off-peak ratio 5.000000; throttle point 5368709120; delete expired; major period 0, major jitter 0.500000
2014-07-01 16:00:16,588 DEBUG [RS_OPEN_REGION-sceplus-vm48:60020-1] regionserver.HRegion: Found 0 recovered edits file(s) under hdfs://master:54310/hbase/data/default/usertable/3e76136f0a8be94ccf3637676c064402
2014-07-01 16:00:16,588 INFO  [PostOpenDeployTasks:431aad56a67499b48936eca121e3178e] catalog.MetaEditor: Updated row usertable,user7,1404255616194.431aad56a67499b48936eca121e3178e. with server=sceplus-vm48.almaden.ibm.com,60020,1404255227840
2014-07-01 16:00:16,589 INFO  [PostOpenDeployTasks:431aad56a67499b48936eca121e3178e] regionserver.HRegionServer: Finished post open deploy task for usertable,user7,1404255616194.431aad56a67499b48936eca121e3178e.
2014-07-01 16:00:16,589 DEBUG [RS_OPEN_REGION-sceplus-vm48:60020-0] zookeeper.ZKAssign: regionserver:60020-0x46f4224a300001, quorum=sceplus-vm49.almaden.ibm.com:2181,sceplus-vm48.almaden.ibm.com:2181, baseZNode=/hbase Transitioning 431aad56a67499b48936eca121e3178e from RS_ZK_REGION_OPENING to RS_ZK_REGION_OPENED
2014-07-01 16:00:16,590 DEBUG [RS_OPEN_REGION-sceplus-vm48:60020-2] regionserver.HRegion: Found 0 recovered edits file(s) under hdfs://master:54310/hbase/data/default/usertable/09108ac86cf3c374e79d1bb78f787b49
2014-07-01 16:00:16,590 INFO  [RS_OPEN_REGION-sceplus-vm48:60020-1] regionserver.HRegion: Onlined 3e76136f0a8be94ccf3637676c064402; next sequenceid=1
2014-07-01 16:00:16,590 DEBUG [RS_OPEN_REGION-sceplus-vm48:60020-1] zookeeper.ZKAssign: regionserver:60020-0x46f4224a300001, quorum=sceplus-vm49.almaden.ibm.com:2181,sceplus-vm48.almaden.ibm.com:2181, baseZNode=/hbase Attempting to retransition opening state of node 3e76136f0a8be94ccf3637676c064402
2014-07-01 16:00:16,592 INFO  [RS_OPEN_REGION-sceplus-vm48:60020-2] regionserver.HRegion: Onlined 09108ac86cf3c374e79d1bb78f787b49; next sequenceid=1
2014-07-01 16:00:16,592 DEBUG [RS_OPEN_REGION-sceplus-vm48:60020-2] zookeeper.ZKAssign: regionserver:60020-0x46f4224a300001, quorum=sceplus-vm49.almaden.ibm.com:2181,sceplus-vm48.almaden.ibm.com:2181, baseZNode=/hbase Attempting to retransition opening state of node 09108ac86cf3c374e79d1bb78f787b49
2014-07-01 16:00:16,592 INFO  [PostOpenDeployTasks:3e76136f0a8be94ccf3637676c064402] regionserver.HRegionServer: Post open deploy tasks for region=usertable,user5,1404255616194.3e76136f0a8be94ccf3637676c064402.
2014-07-01 16:00:16,594 DEBUG [RS_OPEN_REGION-sceplus-vm48:60020-0] zookeeper.ZKAssign: regionserver:60020-0x46f4224a300001, quorum=sceplus-vm49.almaden.ibm.com:2181,sceplus-vm48.almaden.ibm.com:2181, baseZNode=/hbase Transitioned node 431aad56a67499b48936eca121e3178e from RS_ZK_REGION_OPENING to RS_ZK_REGION_OPENED
2014-07-01 16:00:16,594 DEBUG [RS_OPEN_REGION-sceplus-vm48:60020-0] handler.OpenRegionHandler: Transitioned 431aad56a67499b48936eca121e3178e to OPENED in zk on sceplus-vm48.almaden.ibm.com,60020,1404255227840
2014-07-01 16:00:16,594 DEBUG [RS_OPEN_REGION-sceplus-vm48:60020-0] handler.OpenRegionHandler: Opened usertable,user7,1404255616194.431aad56a67499b48936eca121e3178e. on sceplus-vm48.almaden.ibm.com,60020,1404255227840
2014-07-01 16:00:16,597 INFO  [PostOpenDeployTasks:09108ac86cf3c374e79d1bb78f787b49] regionserver.HRegionServer: Post open deploy tasks for region=usertable,user3,1404255616194.09108ac86cf3c374e79d1bb78f787b49.
2014-07-01 16:00:16,601 INFO  [PostOpenDeployTasks:3e76136f0a8be94ccf3637676c064402] catalog.MetaEditor: Updated row usertable,user5,1404255616194.3e76136f0a8be94ccf3637676c064402. with server=sceplus-vm48.almaden.ibm.com,60020,1404255227840
2014-07-01 16:00:16,601 INFO  [PostOpenDeployTasks:3e76136f0a8be94ccf3637676c064402] regionserver.HRegionServer: Finished post open deploy task for usertable,user5,1404255616194.3e76136f0a8be94ccf3637676c064402.
2014-07-01 16:00:16,602 DEBUG [RS_OPEN_REGION-sceplus-vm48:60020-1] zookeeper.ZKAssign: regionserver:60020-0x46f4224a300001, quorum=sceplus-vm49.almaden.ibm.com:2181,sceplus-vm48.almaden.ibm.com:2181, baseZNode=/hbase Transitioning 3e76136f0a8be94ccf3637676c064402 from RS_ZK_REGION_OPENING to RS_ZK_REGION_OPENED
2014-07-01 16:00:16,618 DEBUG [RS_OPEN_REGION-sceplus-vm48:60020-1] zookeeper.ZKAssign: regionserver:60020-0x46f4224a300001, quorum=sceplus-vm49.almaden.ibm.com:2181,sceplus-vm48.almaden.ibm.com:2181, baseZNode=/hbase Transitioned node 3e76136f0a8be94ccf3637676c064402 from RS_ZK_REGION_OPENING to RS_ZK_REGION_OPENED
2014-07-01 16:00:16,619 DEBUG [RS_OPEN_REGION-sceplus-vm48:60020-1] handler.OpenRegionHandler: Transitioned 3e76136f0a8be94ccf3637676c064402 to OPENED in zk on sceplus-vm48.almaden.ibm.com,60020,1404255227840
2014-07-01 16:00:16,619 DEBUG [RS_OPEN_REGION-sceplus-vm48:60020-1] handler.OpenRegionHandler: Opened usertable,user5,1404255616194.3e76136f0a8be94ccf3637676c064402. on sceplus-vm48.almaden.ibm.com,60020,1404255227840
2014-07-01 16:00:16,639 INFO  [PostOpenDeployTasks:09108ac86cf3c374e79d1bb78f787b49] catalog.MetaEditor: Updated row usertable,user3,1404255616194.09108ac86cf3c374e79d1bb78f787b49. with server=sceplus-vm48.almaden.ibm.com,60020,1404255227840
2014-07-01 16:00:16,639 INFO  [PostOpenDeployTasks:09108ac86cf3c374e79d1bb78f787b49] regionserver.HRegionServer: Finished post open deploy task for usertable,user3,1404255616194.09108ac86cf3c374e79d1bb78f787b49.
2014-07-01 16:00:16,639 DEBUG [RS_OPEN_REGION-sceplus-vm48:60020-2] zookeeper.ZKAssign: regionserver:60020-0x46f4224a300001, quorum=sceplus-vm49.almaden.ibm.com:2181,sceplus-vm48.almaden.ibm.com:2181, baseZNode=/hbase Transitioning 09108ac86cf3c374e79d1bb78f787b49 from RS_ZK_REGION_OPENING to RS_ZK_REGION_OPENED
2014-07-01 16:00:16,648 DEBUG [RS_OPEN_REGION-sceplus-vm48:60020-2] zookeeper.ZKAssign: regionserver:60020-0x46f4224a300001, quorum=sceplus-vm49.almaden.ibm.com:2181,sceplus-vm48.almaden.ibm.com:2181, baseZNode=/hbase Transitioned node 09108ac86cf3c374e79d1bb78f787b49 from RS_ZK_REGION_OPENING to RS_ZK_REGION_OPENED
2014-07-01 16:00:16,649 DEBUG [RS_OPEN_REGION-sceplus-vm48:60020-2] handler.OpenRegionHandler: Transitioned 09108ac86cf3c374e79d1bb78f787b49 to OPENED in zk on sceplus-vm48.almaden.ibm.com,60020,1404255227840
2014-07-01 16:00:16,649 DEBUG [RS_OPEN_REGION-sceplus-vm48:60020-2] handler.OpenRegionHandler: Opened usertable,user3,1404255616194.09108ac86cf3c374e79d1bb78f787b49. on sceplus-vm48.almaden.ibm.com,60020,1404255227840
2014-07-01 16:00:35,948 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-01 16:00:36,141 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 129 synced till here 99
2014-07-01 16:00:36,543 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1404255227840/sceplus-vm48.almaden.ibm.com%2C60020%2C1404255227840.1404255259008 with entries=129, filesize=108.9m; new WAL /hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1404255227840/sceplus-vm48.almaden.ibm.com%2C60020%2C1404255227840.1404255635949
2014-07-01 16:00:39,016 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-01 16:00:39,067 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 219 synced till here 205
2014-07-01 16:00:39,374 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1404255227840/sceplus-vm48.almaden.ibm.com%2C60020%2C1404255227840.1404255635949 with entries=90, filesize=76.8m; new WAL /hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1404255227840/sceplus-vm48.almaden.ibm.com%2C60020%2C1404255227840.1404255639017
2014-07-01 16:00:41,905 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-01 16:00:42,129 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 340 synced till here 311
2014-07-01 16:00:42,820 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1404255227840/sceplus-vm48.almaden.ibm.com%2C60020%2C1404255227840.1404255639017 with entries=121, filesize=103.4m; new WAL /hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1404255227840/sceplus-vm48.almaden.ibm.com%2C60020%2C1404255227840.1404255641905
2014-07-01 16:00:45,662 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-01 16:00:46,018 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1404255227840/sceplus-vm48.almaden.ibm.com%2C60020%2C1404255227840.1404255641905 with entries=108, filesize=91.5m; new WAL /hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1404255227840/sceplus-vm48.almaden.ibm.com%2C60020%2C1404255227840.1404255645663
2014-07-01 16:00:48,734 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-01 16:00:48,814 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 561 synced till here 543
2014-07-01 16:00:48,927 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1404255227840/sceplus-vm48.almaden.ibm.com%2C60020%2C1404255227840.1404255645663 with entries=113, filesize=95.3m; new WAL /hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1404255227840/sceplus-vm48.almaden.ibm.com%2C60020%2C1404255227840.1404255648735
2014-07-01 16:00:51,497 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-01 16:00:51,676 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 682 synced till here 657
2014-07-01 16:00:52,041 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1404255227840/sceplus-vm48.almaden.ibm.com%2C60020%2C1404255227840.1404255648735 with entries=121, filesize=103.2m; new WAL /hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1404255227840/sceplus-vm48.almaden.ibm.com%2C60020%2C1404255227840.1404255651497
2014-07-01 16:00:53,000 DEBUG [RpcServer.handler=11,port=60020] regionserver.HRegion: Flush requested on usertable,,1404255616194.510634b895ee706ff306728824babf96.
2014-07-01 16:00:53,235 DEBUG [MemStoreFlusher.0] regionserver.HRegion: Started memstore flush for usertable,,1404255616194.510634b895ee706ff306728824babf96., current region memstore size 258.5m
2014-07-01 16:00:53,482 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-01 16:00:53,496 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 786 synced till here 760
2014-07-01 16:00:53,685 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1404255227840/sceplus-vm48.almaden.ibm.com%2C60020%2C1404255227840.1404255651497 with entries=104, filesize=88.7m; new WAL /hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1404255227840/sceplus-vm48.almaden.ibm.com%2C60020%2C1404255227840.1404255653482
2014-07-01 16:00:53,692 DEBUG [RpcServer.handler=11,port=60020] regionserver.HRegion: Flush requested on usertable,user3,1404255616194.09108ac86cf3c374e79d1bb78f787b49.
2014-07-01 16:00:53,693 DEBUG [MemStoreFlusher.1] regionserver.HRegion: Started memstore flush for usertable,user3,1404255616194.09108ac86cf3c374e79d1bb78f787b49., current region memstore size 258.6m
2014-07-01 16:00:54,390 DEBUG [MemStoreFlusher.0] util.FSUtils: DFS Client does not support most favored nodes create; using default create
2014-07-01 16:00:54,417 DEBUG [MemStoreFlusher.1] util.FSUtils: DFS Client does not support most favored nodes create; using default create
2014-07-01 16:00:54,649 DEBUG [RpcServer.handler=11,port=60020] regionserver.HRegion: Flush requested on usertable,user5,1404255616194.3e76136f0a8be94ccf3637676c064402.
2014-07-01 16:01:03,989 WARN  [RpcServer.reader=4,port=60020] ipc.RpcServer: RpcServer.listener,port=60020: count of bytes read: 0
java.io.IOException: Connection reset by peer
	at sun.nio.ch.FileDispatcher.read0(Native Method)
	at sun.nio.ch.SocketDispatcher.read(SocketDispatcher.java:39)
	at sun.nio.ch.IOUtil.readIntoNativeBuffer(IOUtil.java:251)
	at sun.nio.ch.IOUtil.read(IOUtil.java:224)
	at sun.nio.ch.SocketChannelImpl.read(SocketChannelImpl.java:254)
	at org.apache.hadoop.hbase.ipc.RpcServer.channelRead(RpcServer.java:2229)
	at org.apache.hadoop.hbase.ipc.RpcServer$Connection.readAndProcess(RpcServer.java:1415)
	at org.apache.hadoop.hbase.ipc.RpcServer$Listener.doRead(RpcServer.java:790)
	at org.apache.hadoop.hbase.ipc.RpcServer$Listener$Reader.doRunLoop(RpcServer.java:581)
	at org.apache.hadoop.hbase.ipc.RpcServer$Listener$Reader.run(RpcServer.java:556)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1146)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:701)
2014-07-01 16:01:09,042 INFO  [regionserver60020-SendThread(sceplus-vm48.almaden.ibm.com:2181)] zookeeper.ClientCnxn: Unable to read additional data from server sessionid 0x46f4224a300001, likely server has closed socket, closing socket connection and attempting reconnect
2014-07-01 16:01:09,042 INFO  [regionserver60020-SendThread(sceplus-vm48.almaden.ibm.com:2181)] zookeeper.ClientCnxn: Unable to read additional data from server sessionid 0x46f4224a300004, likely server has closed socket, closing socket connection and attempting reconnect
2014-07-01 16:01:09,491 INFO  [regionserver60020-SendThread(slave1:2181)] zookeeper.ClientCnxn: Opening socket connection to server slave1/9.1.143.59:2181. Will not attempt to authenticate using SASL (unknown error)
2014-07-01 16:01:10,118 INFO  [regionserver60020-SendThread(slave1:2181)] zookeeper.ClientCnxn: Opening socket connection to server slave1/9.1.143.59:2181. Will not attempt to authenticate using SASL (unknown error)
2014-07-01 16:01:16,507 INFO  [regionserver60020-SendThread(slave1:2181)] zookeeper.ClientCnxn: Socket connection established to slave1/9.1.143.59:2181, initiating session
2014-07-01 16:01:16,509 INFO  [regionserver60020-SendThread(slave1:2181)] zookeeper.ClientCnxn: Session establishment complete on server slave1/9.1.143.59:2181, sessionid = 0x46f4224a300004, negotiated timeout = 90000
2014-07-01 16:01:17,131 INFO  [regionserver60020-SendThread(slave1:2181)] zookeeper.ClientCnxn: Socket connection established to slave1/9.1.143.59:2181, initiating session
2014-07-01 16:01:17,139 INFO  [regionserver60020-SendThread(slave1:2181)] zookeeper.ClientCnxn: Session establishment complete on server slave1/9.1.143.59:2181, sessionid = 0x46f4224a300001, negotiated timeout = 90000
2014-07-01 16:01:17,817 INFO  [regionserver60020-SendThread(slave1:2181)] zookeeper.ClientCnxn: Unable to read additional data from server sessionid 0x46f4224a300004, likely server has closed socket, closing socket connection and attempting reconnect
2014-07-01 16:01:17,819 INFO  [regionserver60020-SendThread(slave1:2181)] zookeeper.ClientCnxn: Unable to read additional data from server sessionid 0x46f4224a300001, likely server has closed socket, closing socket connection and attempting reconnect
2014-07-01 16:01:17,820 INFO  [regionserver60020-SendThread(slave1:2181)] zookeeper.ClientCnxn: Unable to read additional data from server sessionid 0x146f42242d40001, likely server has closed socket, closing socket connection and attempting reconnect
2014-07-01 16:01:18,304 INFO  [regionserver60020-SendThread(sceplus-vm48.almaden.ibm.com:2181)] zookeeper.ClientCnxn: Opening socket connection to server sceplus-vm48.almaden.ibm.com/9.1.143.58:2181. Will not attempt to authenticate using SASL (unknown error)
2014-07-01 16:01:18,304 INFO  [regionserver60020-SendThread(sceplus-vm48.almaden.ibm.com:2181)] zookeeper.ClientCnxn: Socket connection established to sceplus-vm48.almaden.ibm.com/9.1.143.58:2181, initiating session
2014-07-01 16:01:18,305 INFO  [regionserver60020-SendThread(sceplus-vm48.almaden.ibm.com:2181)] zookeeper.ClientCnxn: Unable to read additional data from server sessionid 0x146f42242d40001, likely server has closed socket, closing socket connection and attempting reconnect
2014-07-01 16:01:18,521 INFO  [regionserver60020-SendThread(sceplus-vm48.almaden.ibm.com:2181)] zookeeper.ClientCnxn: Opening socket connection to server sceplus-vm48.almaden.ibm.com/9.1.143.58:2181. Will not attempt to authenticate using SASL (unknown error)
2014-07-01 16:01:18,521 INFO  [regionserver60020-SendThread(sceplus-vm48.almaden.ibm.com:2181)] zookeeper.ClientCnxn: Socket connection established to sceplus-vm48.almaden.ibm.com/9.1.143.58:2181, initiating session
2014-07-01 16:01:18,526 INFO  [regionserver60020-SendThread(sceplus-vm48.almaden.ibm.com:2181)] zookeeper.ClientCnxn: Session establishment complete on server sceplus-vm48.almaden.ibm.com/9.1.143.58:2181, sessionid = 0x46f4224a300004, negotiated timeout = 90000
2014-07-01 16:01:18,584 INFO  [regionserver60020-SendThread(sceplus-vm48.almaden.ibm.com:2181)] zookeeper.ClientCnxn: Opening socket connection to server sceplus-vm48.almaden.ibm.com/9.1.143.58:2181. Will not attempt to authenticate using SASL (unknown error)
2014-07-01 16:01:18,584 INFO  [regionserver60020-SendThread(sceplus-vm48.almaden.ibm.com:2181)] zookeeper.ClientCnxn: Socket connection established to sceplus-vm48.almaden.ibm.com/9.1.143.58:2181, initiating session
2014-07-01 16:01:18,587 INFO  [regionserver60020-SendThread(sceplus-vm48.almaden.ibm.com:2181)] zookeeper.ClientCnxn: Session establishment complete on server sceplus-vm48.almaden.ibm.com/9.1.143.58:2181, sessionid = 0x46f4224a300001, negotiated timeout = 90000
2014-07-01 16:01:20,372 INFO  [regionserver60020-SendThread(slave1:2181)] zookeeper.ClientCnxn: Opening socket connection to server slave1/9.1.143.59:2181. Will not attempt to authenticate using SASL (unknown error)
2014-07-01 16:01:20,373 INFO  [regionserver60020-SendThread(slave1:2181)] zookeeper.ClientCnxn: Socket connection established to slave1/9.1.143.59:2181, initiating session
2014-07-01 16:01:20,374 INFO  [regionserver60020-SendThread(slave1:2181)] zookeeper.ClientCnxn: Session establishment complete on server slave1/9.1.143.59:2181, sessionid = 0x146f42242d40001, negotiated timeout = 90000
2014-07-01 16:01:23,775 WARN  [RpcServer.handler=43,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":33500,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.58:33844","starttimems":1404255650116,"queuetimems":3248,"class":"HRegionServer","responsesize":19727,"method":"Multi"}
2014-07-01 16:01:23,775 WARN  [RpcServer.handler=49,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":33499,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.58:33844","starttimems":1404255650117,"queuetimems":3237,"class":"HRegionServer","responsesize":19953,"method":"Multi"}
2014-07-01 16:01:23,775 WARN  [RpcServer.handler=25,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":33500,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.58:33844","starttimems":1404255650116,"queuetimems":3261,"class":"HRegionServer","responsesize":19475,"method":"Multi"}
2014-07-01 16:01:23,776 WARN  [RpcServer.handler=43,port=60020] ipc.RpcServer: RpcServer.respondercallId: 209 service: ClientService methodName: Multi size: 3.4m connection: 9.1.143.58:33844: output error
2014-07-01 16:01:23,777 WARN  [RpcServer.handler=43,port=60020] ipc.RpcServer: RpcServer.handler=43,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-07-01 16:01:23,777 WARN  [RpcServer.handler=25,port=60020] ipc.RpcServer: RpcServer.respondercallId: 210 service: ClientService methodName: Multi size: 3.4m connection: 9.1.143.58:33844: output error
2014-07-01 16:01:23,777 WARN  [RpcServer.handler=25,port=60020] ipc.RpcServer: RpcServer.handler=25,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-07-01 16:01:23,778 WARN  [RpcServer.handler=49,port=60020] ipc.RpcServer: RpcServer.respondercallId: 208 service: ClientService methodName: Multi size: 3.5m connection: 9.1.143.58:33844: output error
2014-07-01 16:01:23,778 WARN  [RpcServer.handler=49,port=60020] ipc.RpcServer: RpcServer.handler=49,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-07-01 16:01:27,229 INFO  [MemStoreFlusher.0] regionserver.DefaultStoreFlusher: Flushed, sequenceid=176, memsize=43.4m, hasBloomFilter=true, into tmp file hdfs://master:54310/hbase/data/default/usertable/510634b895ee706ff306728824babf96/.tmp/d1e1ce9c8f9947b7b1578eca934a9913
2014-07-01 16:01:27,247 DEBUG [MemStoreFlusher.0] regionserver.HRegionFileSystem: Committing store file hdfs://master:54310/hbase/data/default/usertable/510634b895ee706ff306728824babf96/.tmp/d1e1ce9c8f9947b7b1578eca934a9913 as hdfs://master:54310/hbase/data/default/usertable/510634b895ee706ff306728824babf96/family/d1e1ce9c8f9947b7b1578eca934a9913
2014-07-01 16:01:27,261 INFO  [MemStoreFlusher.0] regionserver.HStore: Added hdfs://master:54310/hbase/data/default/usertable/510634b895ee706ff306728824babf96/family/d1e1ce9c8f9947b7b1578eca934a9913, entries=157870, sequenceid=176, filesize=11.3m
2014-07-01 16:01:27,262 INFO  [MemStoreFlusher.0] regionserver.HRegion: Finished memstore flush of ~268.1m/281109360, currentsize=29.5m/30890000 for region usertable,,1404255616194.510634b895ee706ff306728824babf96. in 34027ms, sequenceid=176, compaction requested=false
2014-07-01 16:01:27,263 DEBUG [MemStoreFlusher.0] regionserver.HRegion: Started memstore flush for usertable,user5,1404255616194.3e76136f0a8be94ccf3637676c064402., current region memstore size 267.6m
2014-07-01 16:01:27,344 INFO  [MemStoreFlusher.1] regionserver.DefaultStoreFlusher: Flushed, sequenceid=171, memsize=43.4m, hasBloomFilter=true, into tmp file hdfs://master:54310/hbase/data/default/usertable/09108ac86cf3c374e79d1bb78f787b49/.tmp/d1420b6bb97b43e9908c941864032435
2014-07-01 16:01:27,354 DEBUG [MemStoreFlusher.1] regionserver.HRegionFileSystem: Committing store file hdfs://master:54310/hbase/data/default/usertable/09108ac86cf3c374e79d1bb78f787b49/.tmp/d1420b6bb97b43e9908c941864032435 as hdfs://master:54310/hbase/data/default/usertable/09108ac86cf3c374e79d1bb78f787b49/family/d1420b6bb97b43e9908c941864032435
2014-07-01 16:01:27,364 INFO  [MemStoreFlusher.1] regionserver.HStore: Added hdfs://master:54310/hbase/data/default/usertable/09108ac86cf3c374e79d1bb78f787b49/family/d1420b6bb97b43e9908c941864032435, entries=157980, sequenceid=171, filesize=11.3m
2014-07-01 16:01:27,365 INFO  [MemStoreFlusher.1] regionserver.HRegion: Finished memstore flush of ~261.9m/274574400, currentsize=17.1m/17907200 for region usertable,user3,1404255616194.09108ac86cf3c374e79d1bb78f787b49. in 33673ms, sequenceid=171, compaction requested=false
2014-07-01 16:01:27,471 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-01 16:01:27,533 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 903 synced till here 869
2014-07-01 16:01:28,629 WARN  [RpcServer.handler=48,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":38508,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.58:33844","starttimems":1404255650119,"queuetimems":2982,"class":"HRegionServer","responsesize":19650,"method":"Multi"}
2014-07-01 16:01:28,629 WARN  [RpcServer.handler=23,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":38507,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.58:33844","starttimems":1404255650120,"queuetimems":2971,"class":"HRegionServer","responsesize":20008,"method":"Multi"}
2014-07-01 16:01:28,630 WARN  [RpcServer.handler=48,port=60020] ipc.RpcServer: RpcServer.respondercallId: 204 service: ClientService methodName: Multi size: 3.4m connection: 9.1.143.58:33844: output error
2014-07-01 16:01:28,630 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1404255227840/sceplus-vm48.almaden.ibm.com%2C60020%2C1404255227840.1404255653482 with entries=117, filesize=99.4m; new WAL /hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1404255227840/sceplus-vm48.almaden.ibm.com%2C60020%2C1404255227840.1404255687472
2014-07-01 16:01:28,630 WARN  [RpcServer.handler=48,port=60020] ipc.RpcServer: RpcServer.handler=48,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-07-01 16:01:28,631 WARN  [RpcServer.handler=23,port=60020] ipc.RpcServer: RpcServer.respondercallId: 203 service: ClientService methodName: Multi size: 3.5m connection: 9.1.143.58:33844: output error
2014-07-01 16:01:28,631 WARN  [RpcServer.handler=23,port=60020] ipc.RpcServer: RpcServer.handler=23,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-07-01 16:01:28,631 WARN  [RpcServer.handler=26,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":38038,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.58:33844","starttimems":1404255650593,"queuetimems":2449,"class":"HRegionServer","responsesize":19969,"method":"Multi"}
2014-07-01 16:01:28,631 WARN  [RpcServer.handler=27,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":38038,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.58:33844","starttimems":1404255650593,"queuetimems":3245,"class":"HRegionServer","responsesize":19933,"method":"Multi"}
2014-07-01 16:01:28,632 WARN  [RpcServer.handler=29,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":38514,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.58:33844","starttimems":1404255650117,"queuetimems":3118,"class":"HRegionServer","responsesize":19815,"method":"Multi"}
2014-07-01 16:01:28,632 WARN  [RpcServer.handler=39,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":38037,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.58:33844","starttimems":1404255650594,"queuetimems":2249,"class":"HRegionServer","responsesize":19611,"method":"Multi"}
2014-07-01 16:01:28,632 WARN  [RpcServer.handler=9,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":38039,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.58:33844","starttimems":1404255650592,"queuetimems":3272,"class":"HRegionServer","responsesize":19799,"method":"Multi"}
2014-07-01 16:01:28,633 WARN  [RpcServer.handler=44,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":38040,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.58:33844","starttimems":1404255650592,"queuetimems":3431,"class":"HRegionServer","responsesize":19450,"method":"Multi"}
2014-07-01 16:01:28,633 WARN  [RpcServer.handler=3,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":38040,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.58:33844","starttimems":1404255650592,"queuetimems":3419,"class":"HRegionServer","responsesize":19490,"method":"Multi"}
2014-07-01 16:01:28,633 WARN  [RpcServer.handler=33,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":38040,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.58:33844","starttimems":1404255650593,"queuetimems":3258,"class":"HRegionServer","responsesize":20034,"method":"Multi"}
2014-07-01 16:01:28,632 WARN  [RpcServer.handler=26,port=60020] ipc.RpcServer: RpcServer.respondercallId: 227 service: ClientService methodName: Multi size: 3.5m connection: 9.1.143.58:33844: output error
2014-07-01 16:01:28,633 WARN  [RpcServer.handler=26,port=60020] ipc.RpcServer: RpcServer.handler=26,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-07-01 16:01:28,633 WARN  [RpcServer.handler=33,port=60020] ipc.RpcServer: RpcServer.respondercallId: 199 service: ClientService methodName: Multi size: 3.5m connection: 9.1.143.58:33844: output error
2014-07-01 16:01:28,634 WARN  [RpcServer.handler=33,port=60020] ipc.RpcServer: RpcServer.handler=33,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-07-01 16:01:28,634 WARN  [RpcServer.handler=27,port=60020] ipc.RpcServer: RpcServer.respondercallId: 217 service: ClientService methodName: Multi size: 3.5m connection: 9.1.143.58:33844: output error
2014-07-01 16:01:28,634 WARN  [RpcServer.handler=27,port=60020] ipc.RpcServer: RpcServer.handler=27,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-07-01 16:01:28,635 WARN  [RpcServer.handler=3,port=60020] ipc.RpcServer: RpcServer.respondercallId: 201 service: ClientService methodName: Multi size: 3.4m connection: 9.1.143.58:33844: output error
2014-07-01 16:01:28,635 WARN  [RpcServer.handler=3,port=60020] ipc.RpcServer: RpcServer.handler=3,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-07-01 16:01:28,635 WARN  [RpcServer.handler=44,port=60020] ipc.RpcServer: RpcServer.respondercallId: 202 service: ClientService methodName: Multi size: 3.4m connection: 9.1.143.58:33844: output error
2014-07-01 16:01:28,635 WARN  [RpcServer.handler=44,port=60020] ipc.RpcServer: RpcServer.handler=44,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-07-01 16:01:28,650 WARN  [RpcServer.handler=9,port=60020] ipc.RpcServer: RpcServer.respondercallId: 200 service: ClientService methodName: Multi size: 3.5m connection: 9.1.143.58:33844: output error
2014-07-01 16:01:28,651 WARN  [RpcServer.handler=9,port=60020] ipc.RpcServer: RpcServer.handler=9,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-07-01 16:01:28,651 WARN  [RpcServer.handler=39,port=60020] ipc.RpcServer: RpcServer.respondercallId: 226 service: ClientService methodName: Multi size: 3.4m connection: 9.1.143.58:33844: output error
2014-07-01 16:01:28,651 WARN  [RpcServer.handler=39,port=60020] ipc.RpcServer: RpcServer.handler=39,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-07-01 16:01:28,748 DEBUG [RpcServer.handler=16,port=60020] regionserver.HRegion: Flush requested on usertable,user7,1404255616194.431aad56a67499b48936eca121e3178e.
2014-07-01 16:01:28,749 DEBUG [MemStoreFlusher.1] regionserver.HRegion: Started memstore flush for usertable,user7,1404255616194.431aad56a67499b48936eca121e3178e., current region memstore size 266.8m
2014-07-01 16:01:28,750 WARN  [RpcServer.handler=16,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":36026,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.58:33844","starttimems":1404255652723,"queuetimems":4156,"class":"HRegionServer","responsesize":15908,"method":"Multi"}
2014-07-01 16:01:28,750 WARN  [RpcServer.handler=38,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":36827,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.58:33844","starttimems":1404255651922,"queuetimems":3493,"class":"HRegionServer","responsesize":15524,"method":"Multi"}
2014-07-01 16:01:28,754 WARN  [RpcServer.handler=29,port=60020] ipc.RpcServer: RpcServer.respondercallId: 206 service: ClientService methodName: Multi size: 3.5m connection: 9.1.143.58:33844: output error
2014-07-01 16:01:28,754 WARN  [RpcServer.handler=29,port=60020] ipc.RpcServer: RpcServer.handler=29,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-07-01 16:01:28,758 WARN  [RpcServer.handler=16,port=60020] ipc.RpcServer: RpcServer.respondercallId: 222 service: ClientService methodName: Multi size: 2.8m connection: 9.1.143.58:33844: output error
2014-07-01 16:01:28,758 WARN  [RpcServer.handler=16,port=60020] ipc.RpcServer: RpcServer.handler=16,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-07-01 16:01:28,762 WARN  [RpcServer.handler=38,port=60020] ipc.RpcServer: RpcServer.respondercallId: 225 service: ClientService methodName: Multi size: 2.7m connection: 9.1.143.58:33844: output error
2014-07-01 16:01:28,763 WARN  [RpcServer.handler=38,port=60020] ipc.RpcServer: RpcServer.handler=38,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-07-01 16:01:29,185 DEBUG [MemStoreFlusher.0] util.FSUtils: DFS Client does not support most favored nodes create; using default create
2014-07-01 16:01:30,285 WARN  [RpcServer.handler=37,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":37557,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.58:33844","starttimems":1404255652726,"queuetimems":4145,"class":"HRegionServer","responsesize":20010,"method":"Multi"}
2014-07-01 16:01:30,285 WARN  [RpcServer.handler=37,port=60020] ipc.RpcServer: RpcServer.respondercallId: 221 service: ClientService methodName: Multi size: 3.5m connection: 9.1.143.58:33844: output error
2014-07-01 16:01:30,285 WARN  [RpcServer.handler=37,port=60020] ipc.RpcServer: RpcServer.handler=37,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-07-01 16:01:30,296 WARN  [RpcServer.handler=24,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":37218,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.58:33844","starttimems":1404255653076,"queuetimems":2513,"class":"HRegionServer","responsesize":19815,"method":"Multi"}
2014-07-01 16:01:30,297 WARN  [RpcServer.handler=24,port=60020] ipc.RpcServer: RpcServer.respondercallId: 249 service: ClientService methodName: Multi size: 3.5m connection: 9.1.143.58:33844: output error
2014-07-01 16:01:30,297 WARN  [RpcServer.handler=24,port=60020] ipc.RpcServer: RpcServer.handler=24,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-07-01 16:01:30,298 WARN  [RpcServer.handler=35,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":37221,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.58:33844","starttimems":1404255653069,"queuetimems":3210,"class":"HRegionServer","responsesize":20034,"method":"Multi"}
2014-07-01 16:01:30,298 WARN  [RpcServer.handler=35,port=60020] ipc.RpcServer: RpcServer.respondercallId: 231 service: ClientService methodName: Multi size: 3.5m connection: 9.1.143.58:33844: output error
2014-07-01 16:01:30,299 WARN  [RpcServer.handler=35,port=60020] ipc.RpcServer: RpcServer.handler=35,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-07-01 16:01:30,299 WARN  [RpcServer.handler=30,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":38367,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.58:33844","starttimems":1404255651932,"queuetimems":3459,"class":"HRegionServer","responsesize":19749,"method":"Multi"}
2014-07-01 16:01:30,299 WARN  [RpcServer.handler=30,port=60020] ipc.RpcServer: RpcServer.respondercallId: 223 service: ClientService methodName: Multi size: 3.5m connection: 9.1.143.58:33844: output error
2014-07-01 16:01:30,299 WARN  [RpcServer.handler=30,port=60020] ipc.RpcServer: RpcServer.handler=30,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-07-01 16:01:30,300 WARN  [RpcServer.handler=18,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":37229,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.58:33844","starttimems":1404255653070,"queuetimems":2643,"class":"HRegionServer","responsesize":19953,"method":"Multi"}
2014-07-01 16:01:30,301 WARN  [RpcServer.handler=11,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":38377,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.58:33844","starttimems":1404255651923,"queuetimems":3467,"class":"HRegionServer","responsesize":20074,"method":"Multi"}
2014-07-01 16:01:30,302 WARN  [RpcServer.handler=15,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":37227,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.58:33844","starttimems":1404255653075,"queuetimems":2527,"class":"HRegionServer","responsesize":19650,"method":"Multi"}
2014-07-01 16:01:30,302 WARN  [RpcServer.handler=4,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":37555,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.58:33844","starttimems":1404255652744,"queuetimems":2988,"class":"HRegionServer","responsesize":20175,"method":"Multi"}
2014-07-01 16:01:30,303 WARN  [RpcServer.handler=18,port=60020] ipc.RpcServer: RpcServer.respondercallId: 234 service: ClientService methodName: Multi size: 3.5m connection: 9.1.143.58:33844: output error
2014-07-01 16:01:30,303 WARN  [RpcServer.handler=18,port=60020] ipc.RpcServer: RpcServer.handler=18,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-07-01 16:01:30,306 WARN  [RpcServer.handler=4,port=60020] ipc.RpcServer: RpcServer.respondercallId: 230 service: ClientService methodName: Multi size: 3.5m connection: 9.1.143.58:33844: output error
2014-07-01 16:01:30,306 WARN  [RpcServer.handler=4,port=60020] ipc.RpcServer: RpcServer.handler=4,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-07-01 16:01:30,435 WARN  [RpcServer.handler=15,port=60020] ipc.RpcServer: RpcServer.respondercallId: 250 service: ClientService methodName: Multi size: 3.4m connection: 9.1.143.58:33844: output error
2014-07-01 16:01:30,435 WARN  [RpcServer.handler=15,port=60020] ipc.RpcServer: RpcServer.handler=15,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-07-01 16:01:30,439 WARN  [RpcServer.handler=11,port=60020] ipc.RpcServer: RpcServer.respondercallId: 224 service: ClientService methodName: Multi size: 3.5m connection: 9.1.143.58:33844: output error
2014-07-01 16:01:30,439 WARN  [RpcServer.handler=11,port=60020] ipc.RpcServer: RpcServer.handler=11,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-07-01 16:01:30,965 DEBUG [MemStoreFlusher.1] util.FSUtils: DFS Client does not support most favored nodes create; using default create
2014-07-01 16:01:31,302 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-01 16:01:31,304 WARN  [RpcServer.handler=22,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":37346,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.58:33844","starttimems":1404255653957,"queuetimems":2706,"class":"HRegionServer","responsesize":19630,"method":"Multi"}
2014-07-01 16:01:31,304 WARN  [RpcServer.handler=34,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":37496,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.58:33844","starttimems":1404255653807,"queuetimems":2915,"class":"HRegionServer","responsesize":19490,"method":"Multi"}
2014-07-01 16:01:31,305 WARN  [RpcServer.handler=34,port=60020] ipc.RpcServer: RpcServer.respondercallId: 245 service: ClientService methodName: Multi size: 3.4m connection: 9.1.143.58:33844: output error
2014-07-01 16:01:31,305 WARN  [RpcServer.handler=34,port=60020] ipc.RpcServer: RpcServer.handler=34,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-07-01 16:01:31,305 WARN  [RpcServer.handler=22,port=60020] ipc.RpcServer: RpcServer.respondercallId: 239 service: ClientService methodName: Multi size: 3.4m connection: 9.1.143.58:33844: output error
2014-07-01 16:01:31,305 WARN  [RpcServer.handler=22,port=60020] ipc.RpcServer: RpcServer.handler=22,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-07-01 16:01:31,308 WARN  [RpcServer.handler=12,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":37500,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.58:33844","starttimems":1404255653807,"queuetimems":2885,"class":"HRegionServer","responsesize":19475,"method":"Multi"}
2014-07-01 16:01:31,308 WARN  [RpcServer.handler=13,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":37499,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.58:33844","starttimems":1404255653808,"queuetimems":2729,"class":"HRegionServer","responsesize":19734,"method":"Multi"}
2014-07-01 16:01:31,309 WARN  [RpcServer.handler=12,port=60020] ipc.RpcServer: RpcServer.respondercallId: 244 service: ClientService methodName: Multi size: 3.4m connection: 9.1.143.58:33844: output error
2014-07-01 16:01:31,309 WARN  [RpcServer.handler=12,port=60020] ipc.RpcServer: RpcServer.handler=12,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-07-01 16:01:31,309 WARN  [RpcServer.handler=13,port=60020] ipc.RpcServer: RpcServer.respondercallId: 243 service: ClientService methodName: Multi size: 3.4m connection: 9.1.143.58:33844: output error
2014-07-01 16:01:31,309 WARN  [RpcServer.handler=13,port=60020] ipc.RpcServer: RpcServer.handler=13,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-07-01 16:01:31,313 WARN  [RpcServer.handler=31,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":36697,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.58:33844","starttimems":1404255654615,"queuetimems":2126,"class":"HRegionServer","responsesize":15908,"method":"Multi"}
2014-07-01 16:01:31,313 WARN  [RpcServer.handler=31,port=60020] ipc.RpcServer: RpcServer.respondercallId: 255 service: ClientService methodName: Multi size: 2.8m connection: 9.1.143.58:33844: output error
2014-07-01 16:01:31,313 WARN  [RpcServer.handler=31,port=60020] ipc.RpcServer: RpcServer.handler=31,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-07-01 16:01:31,315 WARN  [RpcServer.handler=41,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":37342,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.58:33844","starttimems":1404255653972,"queuetimems":1806,"class":"HRegionServer","responsesize":15524,"method":"Multi"}
2014-07-01 16:01:31,315 WARN  [RpcServer.handler=41,port=60020] ipc.RpcServer: RpcServer.respondercallId: 254 service: ClientService methodName: Multi size: 2.7m connection: 9.1.143.58:33844: output error
2014-07-01 16:01:31,315 WARN  [RpcServer.handler=41,port=60020] ipc.RpcServer: RpcServer.handler=41,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-07-01 16:01:31,316 WARN  [RpcServer.handler=46,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":37533,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.58:33844","starttimems":1404255653779,"queuetimems":3054,"class":"HRegionServer","responsesize":19727,"method":"Multi"}
2014-07-01 16:01:31,316 WARN  [RpcServer.handler=46,port=60020] ipc.RpcServer: RpcServer.respondercallId: 247 service: ClientService methodName: Multi size: 3.4m connection: 9.1.143.58:33844: output error
2014-07-01 16:01:31,316 WARN  [RpcServer.handler=46,port=60020] ipc.RpcServer: RpcServer.handler=46,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-07-01 16:01:31,430 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 1021 synced till here 1005
2014-07-01 16:01:31,700 WARN  [RpcServer.handler=28,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":37098,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.58:33844","starttimems":1404255654601,"queuetimems":2139,"class":"HRegionServer","responsesize":13005,"method":"Multi"}
2014-07-01 16:01:31,700 DEBUG [RpcServer.handler=19,port=60020] regionserver.HRegion: Flush requested on usertable,user8,1404255616195.9e9a6a0fd8aef92a09081d29900a50f3.
2014-07-01 16:01:31,700 WARN  [RpcServer.handler=42,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":37919,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.58:33844","starttimems":1404255653780,"queuetimems":3030,"class":"HRegionServer","responsesize":20056,"method":"Multi"}
2014-07-01 16:01:31,701 WARN  [RpcServer.handler=28,port=60020] ipc.RpcServer: RpcServer.respondercallId: 257 service: ClientService methodName: Multi size: 2.3m connection: 9.1.143.58:33844: output error
2014-07-01 16:01:31,701 WARN  [RpcServer.handler=28,port=60020] ipc.RpcServer: RpcServer.handler=28,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-07-01 16:01:31,701 WARN  [RpcServer.handler=42,port=60020] ipc.RpcServer: RpcServer.respondercallId: 246 service: ClientService methodName: Multi size: 3.5m connection: 9.1.143.58:33844: output error
2014-07-01 16:01:31,701 WARN  [RpcServer.handler=42,port=60020] ipc.RpcServer: RpcServer.handler=42,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-07-01 16:01:31,701 WARN  [RpcServer.handler=19,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":37714,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.58:33844","starttimems":1404255653987,"queuetimems":1658,"class":"HRegionServer","responsesize":19749,"method":"Multi"}
2014-07-01 16:01:31,702 WARN  [RpcServer.handler=19,port=60020] ipc.RpcServer: RpcServer.respondercallId: 259 service: ClientService methodName: Multi size: 3.5m connection: 9.1.143.58:33844: output error
2014-07-01 16:01:31,702 WARN  [RpcServer.handler=19,port=60020] ipc.RpcServer: RpcServer.handler=19,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-07-01 16:01:31,712 WARN  [RpcServer.handler=17,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":37751,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.58:33844","starttimems":1404255653960,"queuetimems":2693,"class":"HRegionServer","responsesize":19465,"method":"Multi"}
2014-07-01 16:01:31,713 WARN  [RpcServer.handler=17,port=60020] ipc.RpcServer: RpcServer.respondercallId: 238 service: ClientService methodName: Multi size: 3.4m connection: 9.1.143.58:33844: output error
2014-07-01 16:01:31,713 WARN  [RpcServer.handler=17,port=60020] ipc.RpcServer: RpcServer.handler=17,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-07-01 16:01:31,712 WARN  [RpcServer.handler=20,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":37933,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.58:33844","starttimems":1404255653779,"queuetimems":3202,"class":"HRegionServer","responsesize":19450,"method":"Multi"}
2014-07-01 16:01:31,715 WARN  [RpcServer.handler=20,port=60020] ipc.RpcServer: RpcServer.respondercallId: 248 service: ClientService methodName: Multi size: 3.4m connection: 9.1.143.58:33844: output error
2014-07-01 16:01:31,715 WARN  [RpcServer.handler=20,port=60020] ipc.RpcServer: RpcServer.handler=20,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-07-01 16:01:31,950 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1404255227840/sceplus-vm48.almaden.ibm.com%2C60020%2C1404255227840.1404255687472 with entries=118, filesize=100.7m; new WAL /hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1404255227840/sceplus-vm48.almaden.ibm.com%2C60020%2C1404255227840.1404255691302
2014-07-01 16:01:32,310 INFO  [MemStoreFlusher.0] regionserver.DefaultStoreFlusher: Flushed, sequenceid=182, memsize=43.3m, hasBloomFilter=true, into tmp file hdfs://master:54310/hbase/data/default/usertable/3e76136f0a8be94ccf3637676c064402/.tmp/88cb14649c44424d8d6a3607fb5c6f82
2014-07-01 16:01:32,325 DEBUG [MemStoreFlusher.0] regionserver.HRegionFileSystem: Committing store file hdfs://master:54310/hbase/data/default/usertable/3e76136f0a8be94ccf3637676c064402/.tmp/88cb14649c44424d8d6a3607fb5c6f82 as hdfs://master:54310/hbase/data/default/usertable/3e76136f0a8be94ccf3637676c064402/family/88cb14649c44424d8d6a3607fb5c6f82
2014-07-01 16:01:32,335 INFO  [MemStoreFlusher.0] regionserver.HStore: Added hdfs://master:54310/hbase/data/default/usertable/3e76136f0a8be94ccf3637676c064402/family/88cb14649c44424d8d6a3607fb5c6f82, entries=157680, sequenceid=182, filesize=11.2m
2014-07-01 16:01:32,335 INFO  [MemStoreFlusher.0] regionserver.HRegion: Finished memstore flush of ~278.7m/292217920, currentsize=41.5m/43566640 for region usertable,user5,1404255616194.3e76136f0a8be94ccf3637676c064402. in 5072ms, sequenceid=182, compaction requested=false
2014-07-01 16:01:32,336 DEBUG [MemStoreFlusher.0] regionserver.HRegion: Started memstore flush for usertable,user8,1404255616195.9e9a6a0fd8aef92a09081d29900a50f3., current region memstore size 259.5m
2014-07-01 16:01:32,784 WARN  [RpcServer.handler=2,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":38804,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.58:33844","starttimems":1404255653979,"queuetimems":1665,"class":"HRegionServer","responsesize":20074,"method":"Multi"}
2014-07-01 16:01:32,785 WARN  [RpcServer.handler=2,port=60020] ipc.RpcServer: RpcServer.respondercallId: 260 service: ClientService methodName: Multi size: 3.5m connection: 9.1.143.58:33844: output error
2014-07-01 16:01:32,785 WARN  [RpcServer.handler=2,port=60020] ipc.RpcServer: RpcServer.handler=2,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-07-01 16:01:32,787 WARN  [RpcServer.handler=5,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":10583,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.58:33899","starttimems":1404255682204,"queuetimems":0,"class":"HRegionServer","responsesize":20034,"method":"Multi"}
2014-07-01 16:01:32,787 WARN  [RpcServer.handler=1,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":10456,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.58:33899","starttimems":1404255682331,"queuetimems":0,"class":"HRegionServer","responsesize":19734,"method":"Multi"}
2014-07-01 16:01:32,787 WARN  [RpcServer.handler=0,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":10476,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.58:33899","starttimems":1404255682311,"queuetimems":1,"class":"HRegionServer","responsesize":19475,"method":"Multi"}
2014-07-01 16:01:32,788 WARN  [RpcServer.handler=10,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":38809,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.58:33844","starttimems":1404255653979,"queuetimems":1781,"class":"HRegionServer","responsesize":19611,"method":"Multi"}
2014-07-01 16:01:32,788 WARN  [RpcServer.handler=8,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":38817,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.58:33844","starttimems":1404255653971,"queuetimems":2514,"class":"HRegionServer","responsesize":19799,"method":"Multi"}
2014-07-01 16:01:32,787 WARN  [RpcServer.handler=14,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":38816,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.58:33844","starttimems":1404255653971,"queuetimems":2528,"class":"HRegionServer","responsesize":19933,"method":"Multi"}
2014-07-01 16:01:32,789 WARN  [RpcServer.handler=10,port=60020] ipc.RpcServer: RpcServer.respondercallId: 261 service: ClientService methodName: Multi size: 3.4m connection: 9.1.143.58:33844: output error
2014-07-01 16:01:32,789 WARN  [RpcServer.handler=10,port=60020] ipc.RpcServer: RpcServer.handler=10,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-07-01 16:01:32,789 WARN  [RpcServer.handler=8,port=60020] ipc.RpcServer: RpcServer.respondercallId: 251 service: ClientService methodName: Multi size: 3.5m connection: 9.1.143.58:33844: output error
2014-07-01 16:01:32,789 WARN  [RpcServer.handler=8,port=60020] ipc.RpcServer: RpcServer.handler=8,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-07-01 16:01:32,789 WARN  [RpcServer.handler=6,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":38977,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.58:33844","starttimems":1404255653811,"queuetimems":2703,"class":"HRegionServer","responsesize":19471,"method":"Multi"}
2014-07-01 16:01:32,789 WARN  [RpcServer.handler=32,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":38819,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.58:33844","starttimems":1404255653969,"queuetimems":2542,"class":"HRegionServer","responsesize":19623,"method":"Multi"}
2014-07-01 16:01:32,787 WARN  [RpcServer.handler=40,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":38975,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.58:33844","starttimems":1404255653812,"queuetimems":2688,"class":"HRegionServer","responsesize":20008,"method":"Multi"}
2014-07-01 16:01:32,787 WARN  [RpcServer.handler=36,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":38827,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.58:33844","starttimems":1404255653960,"queuetimems":2678,"class":"HRegionServer","responsesize":19597,"method":"Multi"}
2014-07-01 16:01:32,789 WARN  [RpcServer.handler=14,port=60020] ipc.RpcServer: RpcServer.respondercallId: 235 service: ClientService methodName: Multi size: 3.5m connection: 9.1.143.58:33844: output error
2014-07-01 16:01:32,799 WARN  [RpcServer.handler=14,port=60020] ipc.RpcServer: RpcServer.handler=14,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-07-01 16:01:32,788 WARN  [RpcServer.handler=7,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":38831,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.58:33844","starttimems":1404255653957,"queuetimems":2722,"class":"HRegionServer","responsesize":19829,"method":"Multi"}
2014-07-01 16:01:32,788 WARN  [RpcServer.handler=21,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":10494,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.58:33899","starttimems":1404255682294,"queuetimems":2,"class":"HRegionServer","responsesize":19650,"method":"Multi"}
2014-07-01 16:01:32,799 WARN  [RpcServer.handler=32,port=60020] ipc.RpcServer: RpcServer.respondercallId: 236 service: ClientService methodName: Multi size: 3.4m connection: 9.1.143.58:33844: output error
2014-07-01 16:01:32,799 WARN  [RpcServer.handler=32,port=60020] ipc.RpcServer: RpcServer.handler=32,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-07-01 16:01:32,787 WARN  [RpcServer.handler=47,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":38181,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.58:33844","starttimems":1404255654606,"queuetimems":2129,"class":"HRegionServer","responsesize":20010,"method":"Multi"}
2014-07-01 16:01:32,800 WARN  [RpcServer.handler=47,port=60020] ipc.RpcServer: RpcServer.respondercallId: 256 service: ClientService methodName: Multi size: 3.5m connection: 9.1.143.58:33844: output error
2014-07-01 16:01:32,800 WARN  [RpcServer.handler=47,port=60020] ipc.RpcServer: RpcServer.handler=47,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-07-01 16:01:32,800 WARN  [RpcServer.handler=6,port=60020] ipc.RpcServer: RpcServer.respondercallId: 242 service: ClientService methodName: Multi size: 3.4m connection: 9.1.143.58:33844: output error
2014-07-01 16:01:32,800 WARN  [RpcServer.handler=6,port=60020] ipc.RpcServer: RpcServer.handler=6,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-07-01 16:01:32,800 WARN  [RpcServer.handler=7,port=60020] ipc.RpcServer: RpcServer.respondercallId: 240 service: ClientService methodName: Multi size: 3.5m connection: 9.1.143.58:33844: output error
2014-07-01 16:01:32,801 WARN  [RpcServer.handler=7,port=60020] ipc.RpcServer: RpcServer.handler=7,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-07-01 16:01:32,801 WARN  [RpcServer.handler=36,port=60020] ipc.RpcServer: RpcServer.respondercallId: 237 service: ClientService methodName: Multi size: 3.4m connection: 9.1.143.58:33844: output error
2014-07-01 16:01:32,801 WARN  [RpcServer.handler=36,port=60020] ipc.RpcServer: RpcServer.handler=36,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-07-01 16:01:32,801 WARN  [RpcServer.handler=40,port=60020] ipc.RpcServer: RpcServer.respondercallId: 241 service: ClientService methodName: Multi size: 3.5m connection: 9.1.143.58:33844: output error
2014-07-01 16:01:32,801 WARN  [RpcServer.handler=40,port=60020] ipc.RpcServer: RpcServer.handler=40,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-07-01 16:01:32,880 WARN  [RpcServer.handler=45,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":38278,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.58:33844","starttimems":1404255654601,"queuetimems":2256,"class":"HRegionServer","responsesize":19969,"method":"Multi"}
2014-07-01 16:01:32,880 WARN  [RpcServer.handler=45,port=60020] ipc.RpcServer: RpcServer.respondercallId: 258 service: ClientService methodName: Multi size: 3.5m connection: 9.1.143.58:33844: output error
2014-07-01 16:01:32,880 WARN  [RpcServer.handler=45,port=60020] ipc.RpcServer: RpcServer.handler=45,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-07-01 16:01:33,201 DEBUG [MemStoreFlusher.0] util.FSUtils: DFS Client does not support most favored nodes create; using default create
2014-07-01 16:01:33,577 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-01 16:01:33,596 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1404255227840/sceplus-vm48.almaden.ibm.com%2C60020%2C1404255227840.1404255691302 with entries=79, filesize=67.3m; new WAL /hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1404255227840/sceplus-vm48.almaden.ibm.com%2C60020%2C1404255227840.1404255693578
2014-07-01 16:01:33,968 INFO  [MemStoreFlusher.1] regionserver.DefaultStoreFlusher: Flushed, sequenceid=179, memsize=43.2m, hasBloomFilter=true, into tmp file hdfs://master:54310/hbase/data/default/usertable/431aad56a67499b48936eca121e3178e/.tmp/b2008e2170394a60a1326573a3a2f568
2014-07-01 16:01:33,982 DEBUG [MemStoreFlusher.1] regionserver.HRegionFileSystem: Committing store file hdfs://master:54310/hbase/data/default/usertable/431aad56a67499b48936eca121e3178e/.tmp/b2008e2170394a60a1326573a3a2f568 as hdfs://master:54310/hbase/data/default/usertable/431aad56a67499b48936eca121e3178e/family/b2008e2170394a60a1326573a3a2f568
2014-07-01 16:01:33,992 INFO  [MemStoreFlusher.1] regionserver.HStore: Added hdfs://master:54310/hbase/data/default/usertable/431aad56a67499b48936eca121e3178e/family/b2008e2170394a60a1326573a3a2f568, entries=157160, sequenceid=179, filesize=11.2m
2014-07-01 16:01:33,993 INFO  [MemStoreFlusher.1] regionserver.HRegion: Finished memstore flush of ~273.0m/286213600, currentsize=72.4m/75874720 for region usertable,user7,1404255616194.431aad56a67499b48936eca121e3178e. in 5244ms, sequenceid=179, compaction requested=false
2014-07-01 16:01:34,166 WARN  [RpcServer.reader=5,port=60020] ipc.RpcServer: RpcServer.listener,port=60020: count of bytes read: 0
java.io.IOException: Connection reset by peer
	at sun.nio.ch.FileDispatcher.read0(Native Method)
	at sun.nio.ch.SocketDispatcher.read(SocketDispatcher.java:39)
	at sun.nio.ch.IOUtil.readIntoNativeBuffer(IOUtil.java:251)
	at sun.nio.ch.IOUtil.read(IOUtil.java:224)
	at sun.nio.ch.SocketChannelImpl.read(SocketChannelImpl.java:254)
	at org.apache.hadoop.hbase.ipc.RpcServer.channelRead(RpcServer.java:2229)
	at org.apache.hadoop.hbase.ipc.RpcServer$Connection.readAndProcess(RpcServer.java:1415)
	at org.apache.hadoop.hbase.ipc.RpcServer$Listener.doRead(RpcServer.java:790)
	at org.apache.hadoop.hbase.ipc.RpcServer$Listener$Reader.doRunLoop(RpcServer.java:581)
	at org.apache.hadoop.hbase.ipc.RpcServer$Listener$Reader.run(RpcServer.java:556)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1146)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:701)
2014-07-01 16:01:35,662 INFO  [MemStoreFlusher.0] regionserver.DefaultStoreFlusher: Flushed, sequenceid=190, memsize=43.5m, hasBloomFilter=true, into tmp file hdfs://master:54310/hbase/data/default/usertable/9e9a6a0fd8aef92a09081d29900a50f3/.tmp/8a963e9c9b4c494aac7d936388246b2b
2014-07-01 16:01:35,675 DEBUG [MemStoreFlusher.0] regionserver.HRegionFileSystem: Committing store file hdfs://master:54310/hbase/data/default/usertable/9e9a6a0fd8aef92a09081d29900a50f3/.tmp/8a963e9c9b4c494aac7d936388246b2b as hdfs://master:54310/hbase/data/default/usertable/9e9a6a0fd8aef92a09081d29900a50f3/family/8a963e9c9b4c494aac7d936388246b2b
2014-07-01 16:01:35,689 INFO  [MemStoreFlusher.0] regionserver.HStore: Added hdfs://master:54310/hbase/data/default/usertable/9e9a6a0fd8aef92a09081d29900a50f3/family/8a963e9c9b4c494aac7d936388246b2b, entries=158430, sequenceid=190, filesize=11.3m
2014-07-01 16:01:35,689 INFO  [MemStoreFlusher.0] regionserver.HRegion: Finished memstore flush of ~285.9m/299807440, currentsize=38.9m/40751440 for region usertable,user8,1404255616195.9e9a6a0fd8aef92a09081d29900a50f3. in 3353ms, sequenceid=190, compaction requested=false
2014-07-01 16:01:37,884 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-01 16:01:37,974 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 1190 synced till here 1180
2014-07-01 16:01:38,138 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1404255227840/sceplus-vm48.almaden.ibm.com%2C60020%2C1404255227840.1404255693578 with entries=90, filesize=76.6m; new WAL /hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1404255227840/sceplus-vm48.almaden.ibm.com%2C60020%2C1404255227840.1404255697885
2014-07-01 16:01:38,139 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1404255227840/sceplus-vm48.almaden.ibm.com%2C60020%2C1404255227840.1404255259008
2014-07-01 16:01:38,139 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1404255227840/sceplus-vm48.almaden.ibm.com%2C60020%2C1404255227840.1404255635949
2014-07-01 16:01:38,139 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1404255227840/sceplus-vm48.almaden.ibm.com%2C60020%2C1404255227840.1404255639017
2014-07-01 16:01:38,139 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1404255227840/sceplus-vm48.almaden.ibm.com%2C60020%2C1404255227840.1404255641905
2014-07-01 16:01:38,139 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1404255227840/sceplus-vm48.almaden.ibm.com%2C60020%2C1404255227840.1404255645663
2014-07-01 16:01:38,139 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1404255227840/sceplus-vm48.almaden.ibm.com%2C60020%2C1404255227840.1404255648735
2014-07-01 16:01:38,139 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1404255227840/sceplus-vm48.almaden.ibm.com%2C60020%2C1404255227840.1404255651497
2014-07-01 16:01:40,061 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-01 16:01:40,093 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 1264 synced till here 1262
2014-07-01 16:01:40,146 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1404255227840/sceplus-vm48.almaden.ibm.com%2C60020%2C1404255227840.1404255697885 with entries=74, filesize=63.3m; new WAL /hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1404255227840/sceplus-vm48.almaden.ibm.com%2C60020%2C1404255227840.1404255700061
2014-07-01 16:01:43,305 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-01 16:01:43,325 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 1345 synced till here 1342
2014-07-01 16:01:43,331 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1404255227840/sceplus-vm48.almaden.ibm.com%2C60020%2C1404255227840.1404255700061 with entries=81, filesize=62.1m; new WAL /hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1404255227840/sceplus-vm48.almaden.ibm.com%2C60020%2C1404255227840.1404255703305
2014-07-01 16:01:45,856 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-01 16:01:46,505 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1404255227840/sceplus-vm48.almaden.ibm.com%2C60020%2C1404255227840.1404255703305 with entries=110, filesize=71.0m; new WAL /hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1404255227840/sceplus-vm48.almaden.ibm.com%2C60020%2C1404255227840.1404255705856
2014-07-01 16:01:50,133 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-01 16:01:50,154 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 1570 synced till here 1550
2014-07-01 16:01:50,517 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1404255227840/sceplus-vm48.almaden.ibm.com%2C60020%2C1404255227840.1404255705856 with entries=115, filesize=79.2m; new WAL /hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1404255227840/sceplus-vm48.almaden.ibm.com%2C60020%2C1404255227840.1404255710134
2014-07-01 16:01:52,283 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-01 16:01:52,302 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 1666 synced till here 1644
2014-07-01 16:01:52,831 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1404255227840/sceplus-vm48.almaden.ibm.com%2C60020%2C1404255227840.1404255710134 with entries=96, filesize=81.0m; new WAL /hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1404255227840/sceplus-vm48.almaden.ibm.com%2C60020%2C1404255227840.1404255712285
2014-07-01 16:01:53,917 DEBUG [RpcServer.handler=8,port=60020] regionserver.HRegion: Flush requested on usertable,user3,1404255616194.09108ac86cf3c374e79d1bb78f787b49.
2014-07-01 16:01:53,917 DEBUG [MemStoreFlusher.1] regionserver.HRegion: Started memstore flush for usertable,user3,1404255616194.09108ac86cf3c374e79d1bb78f787b49., current region memstore size 256.9m
2014-07-01 16:01:54,187 DEBUG [MemStoreFlusher.1] util.FSUtils: DFS Client does not support most favored nodes create; using default create
2014-07-01 16:01:54,778 DEBUG [RpcServer.handler=31,port=60020] regionserver.HRegion: Flush requested on usertable,,1404255616194.510634b895ee706ff306728824babf96.
2014-07-01 16:01:54,779 DEBUG [MemStoreFlusher.0] regionserver.HRegion: Started memstore flush for usertable,,1404255616194.510634b895ee706ff306728824babf96., current region memstore size 257.1m
2014-07-01 16:01:54,984 DEBUG [MemStoreFlusher.0] util.FSUtils: DFS Client does not support most favored nodes create; using default create
2014-07-01 16:01:55,265 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-01 16:01:55,282 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 1752 synced till here 1751
2014-07-01 16:01:55,294 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1404255227840/sceplus-vm48.almaden.ibm.com%2C60020%2C1404255227840.1404255712285 with entries=86, filesize=62.1m; new WAL /hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1404255227840/sceplus-vm48.almaden.ibm.com%2C60020%2C1404255227840.1404255715265
2014-07-01 16:01:55,820 DEBUG [RpcServer.handler=29,port=60020] regionserver.HRegion: Flush requested on usertable,user7,1404255616194.431aad56a67499b48936eca121e3178e.
2014-07-01 16:01:56,161 DEBUG [RpcServer.handler=38,port=60020] regionserver.HRegion: Flush requested on usertable,user5,1404255616194.3e76136f0a8be94ccf3637676c064402.
2014-07-01 16:01:57,771 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-01 16:01:57,793 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1404255227840/sceplus-vm48.almaden.ibm.com%2C60020%2C1404255227840.1404255715265 with entries=96, filesize=60.9m; new WAL /hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1404255227840/sceplus-vm48.almaden.ibm.com%2C60020%2C1404255227840.1404255717771
2014-07-01 16:02:00,080 INFO  [MemStoreFlusher.1] regionserver.DefaultStoreFlusher: Flushed, sequenceid=330, memsize=126.6m, hasBloomFilter=true, into tmp file hdfs://master:54310/hbase/data/default/usertable/09108ac86cf3c374e79d1bb78f787b49/.tmp/73a46d767521496888b1d46dba01e4c5
2014-07-01 16:02:00,093 DEBUG [MemStoreFlusher.1] regionserver.HRegionFileSystem: Committing store file hdfs://master:54310/hbase/data/default/usertable/09108ac86cf3c374e79d1bb78f787b49/.tmp/73a46d767521496888b1d46dba01e4c5 as hdfs://master:54310/hbase/data/default/usertable/09108ac86cf3c374e79d1bb78f787b49/family/73a46d767521496888b1d46dba01e4c5
2014-07-01 16:02:00,105 INFO  [MemStoreFlusher.1] regionserver.HStore: Added hdfs://master:54310/hbase/data/default/usertable/09108ac86cf3c374e79d1bb78f787b49/family/73a46d767521496888b1d46dba01e4c5, entries=460910, sequenceid=330, filesize=32.9m
2014-07-01 16:02:00,105 INFO  [MemStoreFlusher.1] regionserver.HRegion: Finished memstore flush of ~256.9m/269349600, currentsize=48.4m/50770160 for region usertable,user3,1404255616194.09108ac86cf3c374e79d1bb78f787b49. in 6188ms, sequenceid=330, compaction requested=false
2014-07-01 16:02:00,106 DEBUG [MemStoreFlusher.1] regionserver.HRegion: Started memstore flush for usertable,user7,1404255616194.431aad56a67499b48936eca121e3178e., current region memstore size 291.0m
2014-07-01 16:02:00,504 DEBUG [MemStoreFlusher.1] util.FSUtils: DFS Client does not support most favored nodes create; using default create
2014-07-01 16:02:01,387 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-01 16:02:01,721 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 1960 synced till here 1941
2014-07-01 16:02:01,991 INFO  [MemStoreFlusher.0] regionserver.DefaultStoreFlusher: Flushed, sequenceid=335, memsize=133.4m, hasBloomFilter=true, into tmp file hdfs://master:54310/hbase/data/default/usertable/510634b895ee706ff306728824babf96/.tmp/ff8f4d4163fd49e896015eb86b75b319
2014-07-01 16:02:02,003 DEBUG [MemStoreFlusher.0] regionserver.HRegionFileSystem: Committing store file hdfs://master:54310/hbase/data/default/usertable/510634b895ee706ff306728824babf96/.tmp/ff8f4d4163fd49e896015eb86b75b319 as hdfs://master:54310/hbase/data/default/usertable/510634b895ee706ff306728824babf96/family/ff8f4d4163fd49e896015eb86b75b319
2014-07-01 16:02:02,014 INFO  [MemStoreFlusher.0] regionserver.HStore: Added hdfs://master:54310/hbase/data/default/usertable/510634b895ee706ff306728824babf96/family/ff8f4d4163fd49e896015eb86b75b319, entries=485820, sequenceid=335, filesize=34.6m
2014-07-01 16:02:02,014 INFO  [MemStoreFlusher.0] regionserver.HRegion: Finished memstore flush of ~257.1m/269618720, currentsize=50.8m/53255840 for region usertable,,1404255616194.510634b895ee706ff306728824babf96. in 7235ms, sequenceid=335, compaction requested=false
2014-07-01 16:02:02,015 DEBUG [MemStoreFlusher.0] regionserver.HRegion: Started memstore flush for usertable,user5,1404255616194.3e76136f0a8be94ccf3637676c064402., current region memstore size 291.2m
2014-07-01 16:02:03,028 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1404255227840/sceplus-vm48.almaden.ibm.com%2C60020%2C1404255227840.1404255717771 with entries=112, filesize=74.7m; new WAL /hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1404255227840/sceplus-vm48.almaden.ibm.com%2C60020%2C1404255227840.1404255721387
2014-07-01 16:02:03,028 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1404255227840/sceplus-vm48.almaden.ibm.com%2C60020%2C1404255227840.1404255653482
2014-07-01 16:02:03,405 DEBUG [MemStoreFlusher.0] util.FSUtils: DFS Client does not support most favored nodes create; using default create
2014-07-01 16:02:05,282 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-01 16:02:05,347 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 2048 synced till here 2035
2014-07-01 16:02:05,515 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1404255227840/sceplus-vm48.almaden.ibm.com%2C60020%2C1404255227840.1404255721387 with entries=88, filesize=73.9m; new WAL /hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1404255227840/sceplus-vm48.almaden.ibm.com%2C60020%2C1404255227840.1404255725283
2014-07-01 16:02:06,470 DEBUG [RpcServer.handler=34,port=60020] regionserver.HRegion: Flush requested on usertable,user8,1404255616195.9e9a6a0fd8aef92a09081d29900a50f3.
2014-07-01 16:02:08,167 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-01 16:02:08,182 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 2126 synced till here 2125
2014-07-01 16:02:08,195 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1404255227840/sceplus-vm48.almaden.ibm.com%2C60020%2C1404255227840.1404255725283 with entries=78, filesize=62.1m; new WAL /hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1404255227840/sceplus-vm48.almaden.ibm.com%2C60020%2C1404255227840.1404255728167
2014-07-01 16:02:10,861 INFO  [MemStoreFlusher.1] regionserver.DefaultStoreFlusher: Flushed, sequenceid=356, memsize=174.0m, hasBloomFilter=true, into tmp file hdfs://master:54310/hbase/data/default/usertable/431aad56a67499b48936eca121e3178e/.tmp/3132c21bc7b14fcfa090d003d916750e
2014-07-01 16:02:10,873 DEBUG [MemStoreFlusher.1] regionserver.HRegionFileSystem: Committing store file hdfs://master:54310/hbase/data/default/usertable/431aad56a67499b48936eca121e3178e/.tmp/3132c21bc7b14fcfa090d003d916750e as hdfs://master:54310/hbase/data/default/usertable/431aad56a67499b48936eca121e3178e/family/3132c21bc7b14fcfa090d003d916750e
2014-07-01 16:02:10,965 INFO  [MemStoreFlusher.1] regionserver.HStore: Added hdfs://master:54310/hbase/data/default/usertable/431aad56a67499b48936eca121e3178e/family/3132c21bc7b14fcfa090d003d916750e, entries=633640, sequenceid=356, filesize=45.2m
2014-07-01 16:02:10,966 INFO  [MemStoreFlusher.1] regionserver.HRegion: Finished memstore flush of ~291.0m/305152720, currentsize=82.1m/86093440 for region usertable,user7,1404255616194.431aad56a67499b48936eca121e3178e. in 10859ms, sequenceid=356, compaction requested=false
2014-07-01 16:02:10,966 DEBUG [MemStoreFlusher.1] regionserver.HRegion: Started memstore flush for usertable,user8,1404255616195.9e9a6a0fd8aef92a09081d29900a50f3., current region memstore size 304.4m
2014-07-01 16:02:11,064 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-01 16:02:11,084 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1404255227840/sceplus-vm48.almaden.ibm.com%2C60020%2C1404255227840.1404255728167 with entries=100, filesize=61.9m; new WAL /hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1404255227840/sceplus-vm48.almaden.ibm.com%2C60020%2C1404255227840.1404255731065
2014-07-01 16:02:11,282 DEBUG [MemStoreFlusher.1] util.FSUtils: DFS Client does not support most favored nodes create; using default create
2014-07-01 16:02:11,577 INFO  [MemStoreFlusher.0] regionserver.DefaultStoreFlusher: Flushed, sequenceid=359, memsize=179.9m, hasBloomFilter=true, into tmp file hdfs://master:54310/hbase/data/default/usertable/3e76136f0a8be94ccf3637676c064402/.tmp/f46a6a2ccf4b4eab8b2f30fc7c5671ef
2014-07-01 16:02:11,588 DEBUG [MemStoreFlusher.0] regionserver.HRegionFileSystem: Committing store file hdfs://master:54310/hbase/data/default/usertable/3e76136f0a8be94ccf3637676c064402/.tmp/f46a6a2ccf4b4eab8b2f30fc7c5671ef as hdfs://master:54310/hbase/data/default/usertable/3e76136f0a8be94ccf3637676c064402/family/f46a6a2ccf4b4eab8b2f30fc7c5671ef
2014-07-01 16:02:11,597 INFO  [MemStoreFlusher.0] regionserver.HStore: Added hdfs://master:54310/hbase/data/default/usertable/3e76136f0a8be94ccf3637676c064402/family/f46a6a2ccf4b4eab8b2f30fc7c5671ef, entries=655090, sequenceid=359, filesize=46.7m
2014-07-01 16:02:11,597 INFO  [MemStoreFlusher.0] regionserver.HRegion: Finished memstore flush of ~292.9m/307149520, currentsize=82.6m/86627040 for region usertable,user5,1404255616194.3e76136f0a8be94ccf3637676c064402. in 9582ms, sequenceid=359, compaction requested=false
2014-07-01 16:02:13,952 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-01 16:02:13,974 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 2323 synced till here 2318
2014-07-01 16:02:14,221 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1404255227840/sceplus-vm48.almaden.ibm.com%2C60020%2C1404255227840.1404255731065 with entries=97, filesize=66.7m; new WAL /hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1404255227840/sceplus-vm48.almaden.ibm.com%2C60020%2C1404255227840.1404255733953
2014-07-01 16:02:14,222 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1404255227840/sceplus-vm48.almaden.ibm.com%2C60020%2C1404255227840.1404255687472
2014-07-01 16:02:18,224 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-01 16:02:18,302 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 2426 synced till here 2409
2014-07-01 16:02:18,789 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1404255227840/sceplus-vm48.almaden.ibm.com%2C60020%2C1404255227840.1404255733953 with entries=103, filesize=89.3m; new WAL /hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1404255227840/sceplus-vm48.almaden.ibm.com%2C60020%2C1404255227840.1404255738224
2014-07-01 16:02:21,521 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-01 16:02:21,675 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 2536 synced till here 2519
2014-07-01 16:02:21,811 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1404255227840/sceplus-vm48.almaden.ibm.com%2C60020%2C1404255227840.1404255738224 with entries=110, filesize=90.8m; new WAL /hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1404255227840/sceplus-vm48.almaden.ibm.com%2C60020%2C1404255227840.1404255741521
2014-07-01 16:02:23,205 DEBUG [RpcServer.handler=7,port=60020] regionserver.HRegion: Flush requested on usertable,,1404255616194.510634b895ee706ff306728824babf96.
2014-07-01 16:02:23,205 DEBUG [MemStoreFlusher.0] regionserver.HRegion: Started memstore flush for usertable,,1404255616194.510634b895ee706ff306728824babf96., current region memstore size 256.4m
2014-07-01 16:02:23,225 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-01 16:02:23,239 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 2613 synced till here 2606
2014-07-01 16:02:23,338 DEBUG [RpcServer.handler=44,port=60020] regionserver.HRegion: Flush requested on usertable,user3,1404255616194.09108ac86cf3c374e79d1bb78f787b49.
2014-07-01 16:02:23,354 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1404255227840/sceplus-vm48.almaden.ibm.com%2C60020%2C1404255227840.1404255741521 with entries=77, filesize=65.1m; new WAL /hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1404255227840/sceplus-vm48.almaden.ibm.com%2C60020%2C1404255227840.1404255743225
2014-07-01 16:02:23,550 DEBUG [MemStoreFlusher.0] util.FSUtils: DFS Client does not support most favored nodes create; using default create
2014-07-01 16:02:24,580 INFO  [MemStoreFlusher.1] regionserver.DefaultStoreFlusher: Flushed, sequenceid=611, memsize=213.2m, hasBloomFilter=true, into tmp file hdfs://master:54310/hbase/data/default/usertable/9e9a6a0fd8aef92a09081d29900a50f3/.tmp/ac57ec120e374baab0221e35e5bfb625
2014-07-01 16:02:24,626 DEBUG [MemStoreFlusher.1] regionserver.HRegionFileSystem: Committing store file hdfs://master:54310/hbase/data/default/usertable/9e9a6a0fd8aef92a09081d29900a50f3/.tmp/ac57ec120e374baab0221e35e5bfb625 as hdfs://master:54310/hbase/data/default/usertable/9e9a6a0fd8aef92a09081d29900a50f3/family/ac57ec120e374baab0221e35e5bfb625
2014-07-01 16:02:24,644 INFO  [MemStoreFlusher.1] regionserver.HStore: Added hdfs://master:54310/hbase/data/default/usertable/9e9a6a0fd8aef92a09081d29900a50f3/family/ac57ec120e374baab0221e35e5bfb625, entries=776330, sequenceid=611, filesize=55.4m
2014-07-01 16:02:24,645 INFO  [MemStoreFlusher.1] regionserver.HRegion: Finished memstore flush of ~304.6m/319377600, currentsize=90.6m/95026480 for region usertable,user8,1404255616195.9e9a6a0fd8aef92a09081d29900a50f3. in 13679ms, sequenceid=611, compaction requested=false
2014-07-01 16:02:24,645 DEBUG [MemStoreFlusher.1] regionserver.HRegion: Started memstore flush for usertable,user3,1404255616194.09108ac86cf3c374e79d1bb78f787b49., current region memstore size 271.9m
2014-07-01 16:02:25,265 DEBUG [MemStoreFlusher.1] util.FSUtils: DFS Client does not support most favored nodes create; using default create
2014-07-01 16:02:25,670 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-01 16:02:25,691 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 2702 synced till here 2694
2014-07-01 16:02:25,944 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1404255227840/sceplus-vm48.almaden.ibm.com%2C60020%2C1404255227840.1404255743225 with entries=89, filesize=68.9m; new WAL /hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1404255227840/sceplus-vm48.almaden.ibm.com%2C60020%2C1404255227840.1404255745671
2014-07-01 16:02:25,945 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1404255227840/sceplus-vm48.almaden.ibm.com%2C60020%2C1404255227840.1404255691302
2014-07-01 16:02:25,945 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1404255227840/sceplus-vm48.almaden.ibm.com%2C60020%2C1404255227840.1404255693578
2014-07-01 16:02:25,946 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1404255227840/sceplus-vm48.almaden.ibm.com%2C60020%2C1404255227840.1404255697885
2014-07-01 16:02:25,946 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1404255227840/sceplus-vm48.almaden.ibm.com%2C60020%2C1404255227840.1404255700061
2014-07-01 16:02:25,946 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1404255227840/sceplus-vm48.almaden.ibm.com%2C60020%2C1404255227840.1404255703305
2014-07-01 16:02:25,947 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1404255227840/sceplus-vm48.almaden.ibm.com%2C60020%2C1404255227840.1404255705856
2014-07-01 16:02:25,947 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1404255227840/sceplus-vm48.almaden.ibm.com%2C60020%2C1404255227840.1404255710134
2014-07-01 16:02:28,671 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-01 16:02:28,745 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 2803 synced till here 2795
2014-07-01 16:02:28,915 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1404255227840/sceplus-vm48.almaden.ibm.com%2C60020%2C1404255227840.1404255745671 with entries=101, filesize=66.7m; new WAL /hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1404255227840/sceplus-vm48.almaden.ibm.com%2C60020%2C1404255227840.1404255748671
2014-07-01 16:02:29,400 DEBUG [RpcServer.handler=28,port=60020] regionserver.HRegion: Flush requested on usertable,user7,1404255616194.431aad56a67499b48936eca121e3178e.
2014-07-01 16:02:29,667 DEBUG [RpcServer.handler=6,port=60020] regionserver.HRegion: Flush requested on usertable,user5,1404255616194.3e76136f0a8be94ccf3637676c064402.
2014-07-01 16:02:31,186 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-01 16:02:31,218 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 2900 synced till here 2896
2014-07-01 16:02:31,379 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1404255227840/sceplus-vm48.almaden.ibm.com%2C60020%2C1404255227840.1404255748671 with entries=97, filesize=65.8m; new WAL /hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1404255227840/sceplus-vm48.almaden.ibm.com%2C60020%2C1404255227840.1404255751186
2014-07-01 16:02:31,933 INFO  [MemStoreFlusher.0] regionserver.DefaultStoreFlusher: Flushed, sequenceid=488, memsize=151.2m, hasBloomFilter=true, into tmp file hdfs://master:54310/hbase/data/default/usertable/510634b895ee706ff306728824babf96/.tmp/545ce41f474d45c68c265c8ac4716903
2014-07-01 16:02:31,944 DEBUG [MemStoreFlusher.0] regionserver.HRegionFileSystem: Committing store file hdfs://master:54310/hbase/data/default/usertable/510634b895ee706ff306728824babf96/.tmp/545ce41f474d45c68c265c8ac4716903 as hdfs://master:54310/hbase/data/default/usertable/510634b895ee706ff306728824babf96/family/545ce41f474d45c68c265c8ac4716903
2014-07-01 16:02:31,953 INFO  [MemStoreFlusher.0] regionserver.HStore: Added hdfs://master:54310/hbase/data/default/usertable/510634b895ee706ff306728824babf96/family/545ce41f474d45c68c265c8ac4716903, entries=550390, sequenceid=488, filesize=39.3m
2014-07-01 16:02:31,954 INFO  [MemStoreFlusher.0] regionserver.HRegion: Finished memstore flush of ~256.4m/268819600, currentsize=72.1m/75626640 for region usertable,,1404255616194.510634b895ee706ff306728824babf96. in 8749ms, sequenceid=488, compaction requested=true
2014-07-01 16:02:31,957 DEBUG [MemStoreFlusher.0] regionserver.CompactSplitThread: Small Compaction requested: system; Because: MemStoreFlusher.0; compaction_queue=(0:0), split_queue=0, merge_queue=0
2014-07-01 16:02:31,957 DEBUG [MemStoreFlusher.0] regionserver.HRegion: Started memstore flush for usertable,user7,1404255616194.431aad56a67499b48936eca121e3178e., current region memstore size 281.5m
2014-07-01 16:02:31,957 DEBUG [regionserver60020-smallCompactions-1404255751957] compactions.RatioBasedCompactionPolicy: Selecting compaction from 3 store files, 0 compacting, 3 eligible, 20 blocking
2014-07-01 16:02:31,959 DEBUG [regionserver60020-smallCompactions-1404255751957] compactions.ExploringCompactionPolicy: Exploring compaction algorithm has selected 3 files of size 89279875 starting at candidate #0 after considering 1 permutations with 1 in ratio
2014-07-01 16:02:31,961 DEBUG [regionserver60020-smallCompactions-1404255751957] regionserver.HStore: 510634b895ee706ff306728824babf96 - family: Initiating major compaction
2014-07-01 16:02:31,961 INFO  [regionserver60020-smallCompactions-1404255751957] regionserver.HRegion: Starting compaction on family in region usertable,,1404255616194.510634b895ee706ff306728824babf96.
2014-07-01 16:02:31,962 INFO  [regionserver60020-smallCompactions-1404255751957] regionserver.HStore: Starting compaction of 3 file(s) in family of usertable,,1404255616194.510634b895ee706ff306728824babf96. into tmpdir=hdfs://master:54310/hbase/data/default/usertable/510634b895ee706ff306728824babf96/.tmp, totalSize=85.1m
2014-07-01 16:02:31,963 DEBUG [regionserver60020-smallCompactions-1404255751957] compactions.Compactor: Compacting hdfs://master:54310/hbase/data/default/usertable/510634b895ee706ff306728824babf96/family/d1e1ce9c8f9947b7b1578eca934a9913, keycount=15787, bloomtype=ROW, size=11.3m, encoding=NONE, seqNum=176, earliestPutTs=1404255648930
2014-07-01 16:02:31,963 DEBUG [regionserver60020-smallCompactions-1404255751957] compactions.Compactor: Compacting hdfs://master:54310/hbase/data/default/usertable/510634b895ee706ff306728824babf96/family/ff8f4d4163fd49e896015eb86b75b319, keycount=48582, bloomtype=ROW, size=34.6m, encoding=NONE, seqNum=335, earliestPutTs=1404255694239
2014-07-01 16:02:31,963 DEBUG [regionserver60020-smallCompactions-1404255751957] compactions.Compactor: Compacting hdfs://master:54310/hbase/data/default/usertable/510634b895ee706ff306728824babf96/family/545ce41f474d45c68c265c8ac4716903, keycount=55039, bloomtype=ROW, size=39.3m, encoding=NONE, seqNum=488, earliestPutTs=1404255715075
2014-07-01 16:02:31,982 DEBUG [regionserver60020-smallCompactions-1404255751957] util.FSUtils: DFS Client does not support most favored nodes create; using default create
2014-07-01 16:02:32,416 DEBUG [MemStoreFlusher.0] util.FSUtils: DFS Client does not support most favored nodes create; using default create
2014-07-01 16:02:32,420 INFO  [MemStoreFlusher.0] compress.CodecPool: Got brand-new compressor
2014-07-01 16:02:32,420 INFO  [MemStoreFlusher.0] compress.CodecPool: Got brand-new compressor
2014-07-01 16:02:33,871 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-01 16:02:35,125 INFO  [MemStoreFlusher.1] regionserver.DefaultStoreFlusher: Flushed, sequenceid=494, memsize=169.8m, hasBloomFilter=true, into tmp file hdfs://master:54310/hbase/data/default/usertable/09108ac86cf3c374e79d1bb78f787b49/.tmp/9f756d9305f542bdbee9096c3c9a577e
2014-07-01 16:02:35,216 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1404255227840/sceplus-vm48.almaden.ibm.com%2C60020%2C1404255227840.1404255751186 with entries=131, filesize=84.9m; new WAL /hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1404255227840/sceplus-vm48.almaden.ibm.com%2C60020%2C1404255227840.1404255753872
2014-07-01 16:02:35,256 DEBUG [MemStoreFlusher.1] regionserver.HRegionFileSystem: Committing store file hdfs://master:54310/hbase/data/default/usertable/09108ac86cf3c374e79d1bb78f787b49/.tmp/9f756d9305f542bdbee9096c3c9a577e as hdfs://master:54310/hbase/data/default/usertable/09108ac86cf3c374e79d1bb78f787b49/family/9f756d9305f542bdbee9096c3c9a577e
2014-07-01 16:02:35,351 INFO  [MemStoreFlusher.1] regionserver.HStore: Added hdfs://master:54310/hbase/data/default/usertable/09108ac86cf3c374e79d1bb78f787b49/family/9f756d9305f542bdbee9096c3c9a577e, entries=618240, sequenceid=494, filesize=44.1m
2014-07-01 16:02:35,352 INFO  [MemStoreFlusher.1] regionserver.HRegion: Finished memstore flush of ~274.5m/287797520, currentsize=86.1m/90310800 for region usertable,user3,1404255616194.09108ac86cf3c374e79d1bb78f787b49. in 10707ms, sequenceid=494, compaction requested=true
2014-07-01 16:02:35,353 DEBUG [MemStoreFlusher.1] regionserver.CompactSplitThread: Small Compaction requested: system; Because: MemStoreFlusher.1; compaction_queue=(0:1), split_queue=0, merge_queue=0
2014-07-01 16:02:35,354 DEBUG [MemStoreFlusher.1] regionserver.HRegion: Started memstore flush for usertable,user5,1404255616194.3e76136f0a8be94ccf3637676c064402., current region memstore size 307.1m
2014-07-01 16:02:35,614 DEBUG [MemStoreFlusher.1] util.FSUtils: DFS Client does not support most favored nodes create; using default create
2014-07-01 16:02:38,000 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-01 16:02:38,193 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 3123 synced till here 3114
2014-07-01 16:02:38,584 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1404255227840/sceplus-vm48.almaden.ibm.com%2C60020%2C1404255227840.1404255753872 with entries=92, filesize=69.3m; new WAL /hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1404255227840/sceplus-vm48.almaden.ibm.com%2C60020%2C1404255227840.1404255758000
2014-07-01 16:02:38,584 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1404255227840/sceplus-vm48.almaden.ibm.com%2C60020%2C1404255227840.1404255712285
2014-07-01 16:02:38,585 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1404255227840/sceplus-vm48.almaden.ibm.com%2C60020%2C1404255227840.1404255715265
2014-07-01 16:02:39,949 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-01 16:02:40,140 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 3223 synced till here 3204
2014-07-01 16:02:40,304 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1404255227840/sceplus-vm48.almaden.ibm.com%2C60020%2C1404255227840.1404255758000 with entries=100, filesize=79.2m; new WAL /hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1404255227840/sceplus-vm48.almaden.ibm.com%2C60020%2C1404255227840.1404255759949
2014-07-01 16:02:42,149 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-01 16:02:42,162 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 3310 synced till here 3302
2014-07-01 16:02:42,411 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1404255227840/sceplus-vm48.almaden.ibm.com%2C60020%2C1404255227840.1404255759949 with entries=87, filesize=71.2m; new WAL /hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1404255227840/sceplus-vm48.almaden.ibm.com%2C60020%2C1404255227840.1404255762149
2014-07-01 16:02:42,559 DEBUG [RpcServer.handler=9,port=60020] regionserver.HRegion: Flush requested on usertable,user8,1404255616195.9e9a6a0fd8aef92a09081d29900a50f3.
2014-07-01 16:02:44,504 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-01 16:02:44,528 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 3410 synced till here 3403
2014-07-01 16:02:44,710 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1404255227840/sceplus-vm48.almaden.ibm.com%2C60020%2C1404255227840.1404255762149 with entries=100, filesize=65.4m; new WAL /hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1404255227840/sceplus-vm48.almaden.ibm.com%2C60020%2C1404255227840.1404255764505
2014-07-01 16:02:44,910 INFO  [MemStoreFlusher.0] regionserver.DefaultStoreFlusher: Flushed, sequenceid=528, memsize=180.3m, hasBloomFilter=true, into tmp file hdfs://master:54310/hbase/data/default/usertable/431aad56a67499b48936eca121e3178e/.tmp/abb653cbf3de4706aa1e0d30fbc523ac
2014-07-01 16:02:45,039 DEBUG [MemStoreFlusher.0] regionserver.HRegionFileSystem: Committing store file hdfs://master:54310/hbase/data/default/usertable/431aad56a67499b48936eca121e3178e/.tmp/abb653cbf3de4706aa1e0d30fbc523ac as hdfs://master:54310/hbase/data/default/usertable/431aad56a67499b48936eca121e3178e/family/abb653cbf3de4706aa1e0d30fbc523ac
2014-07-01 16:02:45,052 INFO  [MemStoreFlusher.0] regionserver.HStore: Added hdfs://master:54310/hbase/data/default/usertable/431aad56a67499b48936eca121e3178e/family/abb653cbf3de4706aa1e0d30fbc523ac, entries=656350, sequenceid=528, filesize=46.8m
2014-07-01 16:02:45,053 INFO  [MemStoreFlusher.0] regionserver.HRegion: Finished memstore flush of ~287.0m/300928800, currentsize=126.3m/132397920 for region usertable,user7,1404255616194.431aad56a67499b48936eca121e3178e. in 13095ms, sequenceid=528, compaction requested=true
2014-07-01 16:02:45,053 DEBUG [MemStoreFlusher.0] regionserver.CompactSplitThread: Small Compaction requested: system; Because: MemStoreFlusher.0; compaction_queue=(0:2), split_queue=0, merge_queue=0
2014-07-01 16:02:45,053 DEBUG [MemStoreFlusher.0] regionserver.HRegion: Started memstore flush for usertable,user8,1404255616195.9e9a6a0fd8aef92a09081d29900a50f3., current region memstore size 283.3m
2014-07-01 16:02:45,666 DEBUG [MemStoreFlusher.0] util.FSUtils: DFS Client does not support most favored nodes create; using default create
2014-07-01 16:02:46,780 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-01 16:02:47,227 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 3529 synced till here 3522
2014-07-01 16:02:47,355 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1404255227840/sceplus-vm48.almaden.ibm.com%2C60020%2C1404255227840.1404255764505 with entries=119, filesize=80.7m; new WAL /hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1404255227840/sceplus-vm48.almaden.ibm.com%2C60020%2C1404255227840.1404255766780
2014-07-01 16:02:47,356 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1404255227840/sceplus-vm48.almaden.ibm.com%2C60020%2C1404255227840.1404255717771
2014-07-01 16:02:49,890 INFO  [MemStoreFlusher.1] regionserver.DefaultStoreFlusher: Flushed, sequenceid=542, memsize=202.1m, hasBloomFilter=true, into tmp file hdfs://master:54310/hbase/data/default/usertable/3e76136f0a8be94ccf3637676c064402/.tmp/6bfb236c21454a8d97b9df0faa54f12f
2014-07-01 16:02:49,901 DEBUG [MemStoreFlusher.1] regionserver.HRegionFileSystem: Committing store file hdfs://master:54310/hbase/data/default/usertable/3e76136f0a8be94ccf3637676c064402/.tmp/6bfb236c21454a8d97b9df0faa54f12f as hdfs://master:54310/hbase/data/default/usertable/3e76136f0a8be94ccf3637676c064402/family/6bfb236c21454a8d97b9df0faa54f12f
2014-07-01 16:02:49,910 INFO  [MemStoreFlusher.1] regionserver.HStore: Added hdfs://master:54310/hbase/data/default/usertable/3e76136f0a8be94ccf3637676c064402/family/6bfb236c21454a8d97b9df0faa54f12f, entries=735690, sequenceid=542, filesize=52.4m
2014-07-01 16:02:49,910 INFO  [MemStoreFlusher.1] regionserver.HRegion: Finished memstore flush of ~307.1m/322054080, currentsize=153.6m/161099040 for region usertable,user5,1404255616194.3e76136f0a8be94ccf3637676c064402. in 14556ms, sequenceid=542, compaction requested=true
2014-07-01 16:02:49,911 DEBUG [MemStoreFlusher.1] regionserver.CompactSplitThread: Small Compaction requested: system; Because: MemStoreFlusher.1; compaction_queue=(0:3), split_queue=0, merge_queue=0
2014-07-01 16:02:49,915 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-01 16:02:49,917 DEBUG [RpcServer.handler=36,port=60020] regionserver.HRegion: Flush requested on usertable,,1404255616194.510634b895ee706ff306728824babf96.
2014-07-01 16:02:49,917 DEBUG [MemStoreFlusher.1] regionserver.HRegion: Started memstore flush for usertable,,1404255616194.510634b895ee706ff306728824babf96., current region memstore size 257.9m
2014-07-01 16:02:49,931 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 3634 synced till here 3627
2014-07-01 16:02:50,084 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1404255227840/sceplus-vm48.almaden.ibm.com%2C60020%2C1404255227840.1404255766780 with entries=105, filesize=67.0m; new WAL /hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1404255227840/sceplus-vm48.almaden.ibm.com%2C60020%2C1404255227840.1404255769916
2014-07-01 16:02:50,084 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1404255227840/sceplus-vm48.almaden.ibm.com%2C60020%2C1404255227840.1404255721387
2014-07-01 16:02:50,084 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1404255227840/sceplus-vm48.almaden.ibm.com%2C60020%2C1404255227840.1404255725283
2014-07-01 16:02:50,574 DEBUG [MemStoreFlusher.1] util.FSUtils: DFS Client does not support most favored nodes create; using default create
2014-07-01 16:02:50,960 DEBUG [RpcServer.handler=20,port=60020] regionserver.HRegion: Flush requested on usertable,user3,1404255616194.09108ac86cf3c374e79d1bb78f787b49.
2014-07-01 16:02:51,898 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-01 16:02:51,920 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 3740 synced till here 3734
2014-07-01 16:02:51,942 DEBUG [regionserver60020-smallCompactions-1404255751957] regionserver.HRegionFileSystem: Committing store file hdfs://master:54310/hbase/data/default/usertable/510634b895ee706ff306728824babf96/.tmp/f66b6ec5f49d4d8fb94e77677a7c9532 as hdfs://master:54310/hbase/data/default/usertable/510634b895ee706ff306728824babf96/family/f66b6ec5f49d4d8fb94e77677a7c9532
2014-07-01 16:02:52,034 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1404255227840/sceplus-vm48.almaden.ibm.com%2C60020%2C1404255227840.1404255769916 with entries=106, filesize=67.2m; new WAL /hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1404255227840/sceplus-vm48.almaden.ibm.com%2C60020%2C1404255227840.1404255771898
2014-07-01 16:02:52,105 DEBUG [regionserver60020-smallCompactions-1404255751957] regionserver.HStore: Removing store files after compaction...
2014-07-01 16:02:52,118 DEBUG [regionserver60020-smallCompactions-1404255751957] backup.HFileArchiver: Finished archiving from class org.apache.hadoop.hbase.backup.HFileArchiver$FileableStoreFile, file:hdfs://master:54310/hbase/data/default/usertable/510634b895ee706ff306728824babf96/family/d1e1ce9c8f9947b7b1578eca934a9913, to hdfs://master:54310/hbase/archive/data/default/usertable/510634b895ee706ff306728824babf96/family/d1e1ce9c8f9947b7b1578eca934a9913
2014-07-01 16:02:52,120 DEBUG [regionserver60020-smallCompactions-1404255751957] backup.HFileArchiver: Finished archiving from class org.apache.hadoop.hbase.backup.HFileArchiver$FileableStoreFile, file:hdfs://master:54310/hbase/data/default/usertable/510634b895ee706ff306728824babf96/family/ff8f4d4163fd49e896015eb86b75b319, to hdfs://master:54310/hbase/archive/data/default/usertable/510634b895ee706ff306728824babf96/family/ff8f4d4163fd49e896015eb86b75b319
2014-07-01 16:02:52,123 DEBUG [regionserver60020-smallCompactions-1404255751957] backup.HFileArchiver: Finished archiving from class org.apache.hadoop.hbase.backup.HFileArchiver$FileableStoreFile, file:hdfs://master:54310/hbase/data/default/usertable/510634b895ee706ff306728824babf96/family/545ce41f474d45c68c265c8ac4716903, to hdfs://master:54310/hbase/archive/data/default/usertable/510634b895ee706ff306728824babf96/family/545ce41f474d45c68c265c8ac4716903
2014-07-01 16:02:52,123 INFO  [regionserver60020-smallCompactions-1404255751957] regionserver.HStore: Completed major compaction of 3 file(s) in family of usertable,,1404255616194.510634b895ee706ff306728824babf96. into f66b6ec5f49d4d8fb94e77677a7c9532(size=65.7m), total size for store is 65.7m. This selection was in queue for 0sec, and took 20sec to execute.
2014-07-01 16:02:52,125 INFO  [regionserver60020-smallCompactions-1404255751957] regionserver.CompactSplitThread: Completed compaction: Request = regionName=usertable,,1404255616194.510634b895ee706ff306728824babf96., storeName=family, fileCount=3, fileSize=85.1m, priority=17, time=75894318827412; duration=20sec
2014-07-01 16:02:52,126 DEBUG [regionserver60020-smallCompactions-1404255751957] regionserver.CompactSplitThread: CompactSplitThread Status: compaction_queue=(0:3), split_queue=0, merge_queue=0
2014-07-01 16:02:52,126 DEBUG [regionserver60020-smallCompactions-1404255751957] compactions.RatioBasedCompactionPolicy: Selecting compaction from 3 store files, 0 compacting, 3 eligible, 20 blocking
2014-07-01 16:02:52,126 DEBUG [regionserver60020-smallCompactions-1404255751957] compactions.ExploringCompactionPolicy: Exploring compaction algorithm has selected 3 files of size 92495936 starting at candidate #0 after considering 1 permutations with 1 in ratio
2014-07-01 16:02:52,126 DEBUG [regionserver60020-smallCompactions-1404255751957] regionserver.HStore: 09108ac86cf3c374e79d1bb78f787b49 - family: Initiating major compaction
2014-07-01 16:02:52,126 INFO  [regionserver60020-smallCompactions-1404255751957] regionserver.HRegion: Starting compaction on family in region usertable,user3,1404255616194.09108ac86cf3c374e79d1bb78f787b49.
2014-07-01 16:02:52,127 INFO  [regionserver60020-smallCompactions-1404255751957] regionserver.HStore: Starting compaction of 3 file(s) in family of usertable,user3,1404255616194.09108ac86cf3c374e79d1bb78f787b49. into tmpdir=hdfs://master:54310/hbase/data/default/usertable/09108ac86cf3c374e79d1bb78f787b49/.tmp, totalSize=88.2m
2014-07-01 16:02:52,127 DEBUG [regionserver60020-smallCompactions-1404255751957] compactions.Compactor: Compacting hdfs://master:54310/hbase/data/default/usertable/09108ac86cf3c374e79d1bb78f787b49/family/d1420b6bb97b43e9908c941864032435, keycount=15798, bloomtype=ROW, size=11.3m, encoding=NONE, seqNum=171, earliestPutTs=1404255649919
2014-07-01 16:02:52,127 DEBUG [regionserver60020-smallCompactions-1404255751957] compactions.Compactor: Compacting hdfs://master:54310/hbase/data/default/usertable/09108ac86cf3c374e79d1bb78f787b49/family/73a46d767521496888b1d46dba01e4c5, keycount=46091, bloomtype=ROW, size=32.9m, encoding=NONE, seqNum=330, earliestPutTs=1404255694579
2014-07-01 16:02:52,127 DEBUG [regionserver60020-smallCompactions-1404255751957] compactions.Compactor: Compacting hdfs://master:54310/hbase/data/default/usertable/09108ac86cf3c374e79d1bb78f787b49/family/9f756d9305f542bdbee9096c3c9a577e, keycount=61824, bloomtype=ROW, size=44.1m, encoding=NONE, seqNum=494, earliestPutTs=1404255714200
2014-07-01 16:02:52,144 DEBUG [regionserver60020-smallCompactions-1404255751957] util.FSUtils: DFS Client does not support most favored nodes create; using default create
2014-07-01 16:02:54,699 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-01 16:02:54,876 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 3852 synced till here 3845
2014-07-01 16:02:55,083 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1404255227840/sceplus-vm48.almaden.ibm.com%2C60020%2C1404255227840.1404255771898 with entries=112, filesize=70.0m; new WAL /hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1404255227840/sceplus-vm48.almaden.ibm.com%2C60020%2C1404255227840.1404255774699
2014-07-01 16:02:57,213 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-01 16:02:57,408 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 3952 synced till here 3943
2014-07-01 16:02:57,684 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1404255227840/sceplus-vm48.almaden.ibm.com%2C60020%2C1404255227840.1404255774699 with entries=100, filesize=72.0m; new WAL /hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1404255227840/sceplus-vm48.almaden.ibm.com%2C60020%2C1404255227840.1404255777213
2014-07-01 16:02:58,260 DEBUG [RpcServer.handler=4,port=60020] regionserver.HRegion: Flush requested on usertable,user7,1404255616194.431aad56a67499b48936eca121e3178e.
2014-07-01 16:03:00,431 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-01 16:03:00,674 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 4065 synced till here 4050
2014-07-01 16:03:00,950 DEBUG [RpcServer.handler=6,port=60020] regionserver.HRegion: Flush requested on usertable,user5,1404255616194.3e76136f0a8be94ccf3637676c064402.
2014-07-01 16:03:00,981 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1404255227840/sceplus-vm48.almaden.ibm.com%2C60020%2C1404255227840.1404255777213 with entries=113, filesize=87.6m; new WAL /hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1404255227840/sceplus-vm48.almaden.ibm.com%2C60020%2C1404255227840.1404255780431
2014-07-01 16:03:01,160 INFO  [MemStoreFlusher.0] regionserver.DefaultStoreFlusher: Flushed, sequenceid=1030, memsize=250.8m, hasBloomFilter=true, into tmp file hdfs://master:54310/hbase/data/default/usertable/9e9a6a0fd8aef92a09081d29900a50f3/.tmp/8e15e7b24e714917923038d3a8a41661
2014-07-01 16:03:01,173 DEBUG [MemStoreFlusher.0] regionserver.HRegionFileSystem: Committing store file hdfs://master:54310/hbase/data/default/usertable/9e9a6a0fd8aef92a09081d29900a50f3/.tmp/8e15e7b24e714917923038d3a8a41661 as hdfs://master:54310/hbase/data/default/usertable/9e9a6a0fd8aef92a09081d29900a50f3/family/8e15e7b24e714917923038d3a8a41661
2014-07-01 16:03:01,183 INFO  [MemStoreFlusher.0] regionserver.HStore: Added hdfs://master:54310/hbase/data/default/usertable/9e9a6a0fd8aef92a09081d29900a50f3/family/8e15e7b24e714917923038d3a8a41661, entries=913110, sequenceid=1030, filesize=65.0m
2014-07-01 16:03:01,183 INFO  [MemStoreFlusher.0] regionserver.HRegion: Finished memstore flush of ~288.7m/302731840, currentsize=138.7m/145470240 for region usertable,user8,1404255616195.9e9a6a0fd8aef92a09081d29900a50f3. in 16130ms, sequenceid=1030, compaction requested=true
2014-07-01 16:03:01,184 DEBUG [MemStoreFlusher.0] regionserver.CompactSplitThread: Small Compaction requested: system; Because: MemStoreFlusher.0; compaction_queue=(0:3), split_queue=0, merge_queue=0
2014-07-01 16:03:01,184 DEBUG [MemStoreFlusher.0] regionserver.HRegion: Started memstore flush for usertable,user3,1404255616194.09108ac86cf3c374e79d1bb78f787b49., current region memstore size 360.0m
2014-07-01 16:03:01,757 DEBUG [MemStoreFlusher.0] util.FSUtils: DFS Client does not support most favored nodes create; using default create
2014-07-01 16:03:02,249 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-01 16:03:02,341 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 4145 synced till here 4137
2014-07-01 16:03:02,398 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1404255227840/sceplus-vm48.almaden.ibm.com%2C60020%2C1404255227840.1404255780431 with entries=80, filesize=66.7m; new WAL /hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1404255227840/sceplus-vm48.almaden.ibm.com%2C60020%2C1404255227840.1404255782249
2014-07-01 16:03:02,398 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1404255227840/sceplus-vm48.almaden.ibm.com%2C60020%2C1404255227840.1404255728167
2014-07-01 16:03:02,399 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1404255227840/sceplus-vm48.almaden.ibm.com%2C60020%2C1404255227840.1404255731065
2014-07-01 16:03:02,399 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1404255227840/sceplus-vm48.almaden.ibm.com%2C60020%2C1404255227840.1404255733953
2014-07-01 16:03:02,399 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1404255227840/sceplus-vm48.almaden.ibm.com%2C60020%2C1404255227840.1404255738224
2014-07-01 16:03:02,399 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1404255227840/sceplus-vm48.almaden.ibm.com%2C60020%2C1404255227840.1404255741521
2014-07-01 16:03:04,634 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-01 16:03:04,661 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 4237 synced till here 4228
2014-07-01 16:03:04,982 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1404255227840/sceplus-vm48.almaden.ibm.com%2C60020%2C1404255227840.1404255782249 with entries=92, filesize=66.5m; new WAL /hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1404255227840/sceplus-vm48.almaden.ibm.com%2C60020%2C1404255227840.1404255784635
2014-07-01 16:03:05,444 INFO  [MemStoreFlusher.1] regionserver.DefaultStoreFlusher: Flushed, sequenceid=637, memsize=243.6m, hasBloomFilter=true, into tmp file hdfs://master:54310/hbase/data/default/usertable/510634b895ee706ff306728824babf96/.tmp/4eead3f5ffef4c37bd398c66ece498b7
2014-07-01 16:03:05,460 DEBUG [MemStoreFlusher.1] regionserver.HRegionFileSystem: Committing store file hdfs://master:54310/hbase/data/default/usertable/510634b895ee706ff306728824babf96/.tmp/4eead3f5ffef4c37bd398c66ece498b7 as hdfs://master:54310/hbase/data/default/usertable/510634b895ee706ff306728824babf96/family/4eead3f5ffef4c37bd398c66ece498b7
2014-07-01 16:03:05,471 INFO  [MemStoreFlusher.1] regionserver.HStore: Added hdfs://master:54310/hbase/data/default/usertable/510634b895ee706ff306728824babf96/family/4eead3f5ffef4c37bd398c66ece498b7, entries=887080, sequenceid=637, filesize=63.2m
2014-07-01 16:03:05,471 INFO  [MemStoreFlusher.1] regionserver.HRegion: Finished memstore flush of ~261.7m/274454400, currentsize=161.2m/168979840 for region usertable,,1404255616194.510634b895ee706ff306728824babf96. in 15554ms, sequenceid=637, compaction requested=false
2014-07-01 16:03:05,471 DEBUG [MemStoreFlusher.1] regionserver.HRegion: Started memstore flush for usertable,user7,1404255616194.431aad56a67499b48936eca121e3178e., current region memstore size 334.9m
2014-07-01 16:03:06,300 DEBUG [MemStoreFlusher.1] util.FSUtils: DFS Client does not support most favored nodes create; using default create
2014-07-01 16:03:07,220 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-01 16:03:07,238 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 4329 synced till here 4324
2014-07-01 16:03:07,405 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1404255227840/sceplus-vm48.almaden.ibm.com%2C60020%2C1404255227840.1404255784635 with entries=92, filesize=65.1m; new WAL /hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1404255227840/sceplus-vm48.almaden.ibm.com%2C60020%2C1404255227840.1404255787221
2014-07-01 16:03:09,252 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-01 16:03:09,275 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 4428 synced till here 4426
2014-07-01 16:03:09,298 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1404255227840/sceplus-vm48.almaden.ibm.com%2C60020%2C1404255227840.1404255787221 with entries=99, filesize=62.9m; new WAL /hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1404255227840/sceplus-vm48.almaden.ibm.com%2C60020%2C1404255227840.1404255789253
2014-07-01 16:03:11,484 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-01 16:03:11,655 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 4522 synced till here 4516
2014-07-01 16:03:11,925 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1404255227840/sceplus-vm48.almaden.ibm.com%2C60020%2C1404255227840.1404255789253 with entries=94, filesize=66.6m; new WAL /hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1404255227840/sceplus-vm48.almaden.ibm.com%2C60020%2C1404255227840.1404255791484
2014-07-01 16:03:12,298 DEBUG [RpcServer.handler=37,port=60020] regionserver.HRegion: Flush requested on usertable,user8,1404255616195.9e9a6a0fd8aef92a09081d29900a50f3.
2014-07-01 16:03:14,089 DEBUG [regionserver60020-smallCompactions-1404255751957] regionserver.HRegionFileSystem: Committing store file hdfs://master:54310/hbase/data/default/usertable/09108ac86cf3c374e79d1bb78f787b49/.tmp/b77ae91bf1934355ae9e1b0c228c9263 as hdfs://master:54310/hbase/data/default/usertable/09108ac86cf3c374e79d1bb78f787b49/family/b77ae91bf1934355ae9e1b0c228c9263
2014-07-01 16:03:14,142 DEBUG [regionserver60020-smallCompactions-1404255751957] regionserver.HStore: Removing store files after compaction...
2014-07-01 16:03:14,766 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-01 16:03:14,827 DEBUG [regionserver60020-smallCompactions-1404255751957] backup.HFileArchiver: Finished archiving from class org.apache.hadoop.hbase.backup.HFileArchiver$FileableStoreFile, file:hdfs://master:54310/hbase/data/default/usertable/09108ac86cf3c374e79d1bb78f787b49/family/d1420b6bb97b43e9908c941864032435, to hdfs://master:54310/hbase/archive/data/default/usertable/09108ac86cf3c374e79d1bb78f787b49/family/d1420b6bb97b43e9908c941864032435
2014-07-01 16:03:15,579 DEBUG [regionserver60020-smallCompactions-1404255751957] backup.HFileArchiver: Finished archiving from class org.apache.hadoop.hbase.backup.HFileArchiver$FileableStoreFile, file:hdfs://master:54310/hbase/data/default/usertable/09108ac86cf3c374e79d1bb78f787b49/family/73a46d767521496888b1d46dba01e4c5, to hdfs://master:54310/hbase/archive/data/default/usertable/09108ac86cf3c374e79d1bb78f787b49/family/73a46d767521496888b1d46dba01e4c5
2014-07-01 16:03:15,627 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 4635 synced till here 4634
2014-07-01 16:03:15,627 DEBUG [regionserver60020-smallCompactions-1404255751957] backup.HFileArchiver: Finished archiving from class org.apache.hadoop.hbase.backup.HFileArchiver$FileableStoreFile, file:hdfs://master:54310/hbase/data/default/usertable/09108ac86cf3c374e79d1bb78f787b49/family/9f756d9305f542bdbee9096c3c9a577e, to hdfs://master:54310/hbase/archive/data/default/usertable/09108ac86cf3c374e79d1bb78f787b49/family/9f756d9305f542bdbee9096c3c9a577e
2014-07-01 16:03:15,627 INFO  [regionserver60020-smallCompactions-1404255751957] regionserver.HStore: Completed major compaction of 3 file(s) in family of usertable,user3,1404255616194.09108ac86cf3c374e79d1bb78f787b49. into b77ae91bf1934355ae9e1b0c228c9263(size=68.8m), total size for store is 68.8m. This selection was in queue for 0sec, and took 23sec to execute.
2014-07-01 16:03:15,628 INFO  [regionserver60020-smallCompactions-1404255751957] regionserver.CompactSplitThread: Completed compaction: Request = regionName=usertable,user3,1404255616194.09108ac86cf3c374e79d1bb78f787b49., storeName=family, fileCount=3, fileSize=88.2m, priority=17, time=75914483933503; duration=23sec
2014-07-01 16:03:15,628 DEBUG [regionserver60020-smallCompactions-1404255751957] regionserver.CompactSplitThread: CompactSplitThread Status: compaction_queue=(0:3), split_queue=0, merge_queue=0
2014-07-01 16:03:15,628 DEBUG [regionserver60020-smallCompactions-1404255751957] compactions.RatioBasedCompactionPolicy: Selecting compaction from 3 store files, 0 compacting, 3 eligible, 20 blocking
2014-07-01 16:03:15,629 DEBUG [regionserver60020-smallCompactions-1404255751957] compactions.ExploringCompactionPolicy: Exploring compaction algorithm has selected 3 files of size 115741030 starting at candidate #0 after considering 1 permutations with 1 in ratio
2014-07-01 16:03:15,629 DEBUG [regionserver60020-smallCompactions-1404255751957] regionserver.HStore: 3e76136f0a8be94ccf3637676c064402 - family: Initiating major compaction
2014-07-01 16:03:15,629 INFO  [regionserver60020-smallCompactions-1404255751957] regionserver.HRegion: Starting compaction on family in region usertable,user5,1404255616194.3e76136f0a8be94ccf3637676c064402.
2014-07-01 16:03:15,629 INFO  [regionserver60020-smallCompactions-1404255751957] regionserver.HStore: Starting compaction of 3 file(s) in family of usertable,user5,1404255616194.3e76136f0a8be94ccf3637676c064402. into tmpdir=hdfs://master:54310/hbase/data/default/usertable/3e76136f0a8be94ccf3637676c064402/.tmp, totalSize=110.4m
2014-07-01 16:03:15,630 DEBUG [regionserver60020-smallCompactions-1404255751957] compactions.Compactor: Compacting hdfs://master:54310/hbase/data/default/usertable/3e76136f0a8be94ccf3637676c064402/family/88cb14649c44424d8d6a3607fb5c6f82, keycount=15768, bloomtype=ROW, size=11.2m, encoding=NONE, seqNum=182, earliestPutTs=1404255650730
2014-07-01 16:03:15,630 DEBUG [regionserver60020-smallCompactions-1404255751957] compactions.Compactor: Compacting hdfs://master:54310/hbase/data/default/usertable/3e76136f0a8be94ccf3637676c064402/family/f46a6a2ccf4b4eab8b2f30fc7c5671ef, keycount=65509, bloomtype=ROW, size=46.7m, encoding=NONE, seqNum=359, earliestPutTs=1404255694787
2014-07-01 16:03:15,630 DEBUG [regionserver60020-smallCompactions-1404255751957] compactions.Compactor: Compacting hdfs://master:54310/hbase/data/default/usertable/3e76136f0a8be94ccf3637676c064402/family/6bfb236c21454a8d97b9df0faa54f12f, keycount=73569, bloomtype=ROW, size=52.4m, encoding=NONE, seqNum=542, earliestPutTs=1404255726995
2014-07-01 16:03:15,649 DEBUG [regionserver60020-smallCompactions-1404255751957] util.FSUtils: DFS Client does not support most favored nodes create; using default create
2014-07-01 16:03:15,661 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1404255227840/sceplus-vm48.almaden.ibm.com%2C60020%2C1404255227840.1404255791484 with entries=113, filesize=70.6m; new WAL /hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1404255227840/sceplus-vm48.almaden.ibm.com%2C60020%2C1404255227840.1404255794766
2014-07-01 16:03:16,779 DEBUG [RpcServer.handler=20,port=60020] regionserver.HRegion: Flush requested on usertable,,1404255616194.510634b895ee706ff306728824babf96.
2014-07-01 16:03:19,028 INFO  [MemStoreFlusher.0] regionserver.DefaultStoreFlusher: Flushed, sequenceid=703, memsize=324.7m, hasBloomFilter=true, into tmp file hdfs://master:54310/hbase/data/default/usertable/09108ac86cf3c374e79d1bb78f787b49/.tmp/d5ccec31c1f64850baaee00b94bd5a8a
2014-07-01 16:03:19,047 DEBUG [MemStoreFlusher.0] regionserver.HRegionFileSystem: Committing store file hdfs://master:54310/hbase/data/default/usertable/09108ac86cf3c374e79d1bb78f787b49/.tmp/d5ccec31c1f64850baaee00b94bd5a8a as hdfs://master:54310/hbase/data/default/usertable/09108ac86cf3c374e79d1bb78f787b49/family/d5ccec31c1f64850baaee00b94bd5a8a
2014-07-01 16:03:19,057 INFO  [MemStoreFlusher.0] regionserver.HStore: Added hdfs://master:54310/hbase/data/default/usertable/09108ac86cf3c374e79d1bb78f787b49/family/d5ccec31c1f64850baaee00b94bd5a8a, entries=1182260, sequenceid=703, filesize=84.2m
2014-07-01 16:03:19,057 INFO  [MemStoreFlusher.0] regionserver.HRegion: Finished memstore flush of ~368.2m/386059600, currentsize=149.8m/157056640 for region usertable,user3,1404255616194.09108ac86cf3c374e79d1bb78f787b49. in 17873ms, sequenceid=703, compaction requested=false
2014-07-01 16:03:19,057 DEBUG [MemStoreFlusher.0] regionserver.HRegion: Started memstore flush for usertable,user5,1404255616194.3e76136f0a8be94ccf3637676c064402., current region memstore size 433.2m
2014-07-01 16:03:19,465 DEBUG [MemStoreFlusher.0] util.FSUtils: DFS Client does not support most favored nodes create; using default create
2014-07-01 16:03:20,130 INFO  [MemStoreFlusher.1] regionserver.DefaultStoreFlusher: Flushed, sequenceid=721, memsize=290.1m, hasBloomFilter=true, into tmp file hdfs://master:54310/hbase/data/default/usertable/431aad56a67499b48936eca121e3178e/.tmp/f214477da2c544918532b903aef19a2a
2014-07-01 16:03:20,140 DEBUG [MemStoreFlusher.1] regionserver.HRegionFileSystem: Committing store file hdfs://master:54310/hbase/data/default/usertable/431aad56a67499b48936eca121e3178e/.tmp/f214477da2c544918532b903aef19a2a as hdfs://master:54310/hbase/data/default/usertable/431aad56a67499b48936eca121e3178e/family/f214477da2c544918532b903aef19a2a
2014-07-01 16:03:20,148 INFO  [MemStoreFlusher.1] regionserver.HStore: Added hdfs://master:54310/hbase/data/default/usertable/431aad56a67499b48936eca121e3178e/family/f214477da2c544918532b903aef19a2a, entries=1056300, sequenceid=721, filesize=75.2m
2014-07-01 16:03:20,149 INFO  [MemStoreFlusher.1] regionserver.HRegion: Finished memstore flush of ~340.5m/357029600, currentsize=120.7m/126575600 for region usertable,user7,1404255616194.431aad56a67499b48936eca121e3178e. in 14678ms, sequenceid=721, compaction requested=true
2014-07-01 16:03:20,149 DEBUG [MemStoreFlusher.1] regionserver.CompactSplitThread: Small Compaction requested: system; Because: MemStoreFlusher.1; compaction_queue=(0:3), split_queue=0, merge_queue=0
2014-07-01 16:03:20,149 DEBUG [MemStoreFlusher.1] regionserver.HRegion: Started memstore flush for usertable,user8,1404255616195.9e9a6a0fd8aef92a09081d29900a50f3., current region memstore size 292.5m
2014-07-01 16:03:20,443 DEBUG [MemStoreFlusher.1] util.FSUtils: DFS Client does not support most favored nodes create; using default create
2014-07-01 16:03:21,048 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-01 16:03:21,071 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 4714 synced till here 4713
2014-07-01 16:03:21,083 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1404255227840/sceplus-vm48.almaden.ibm.com%2C60020%2C1404255227840.1404255794766 with entries=79, filesize=61.2m; new WAL /hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1404255227840/sceplus-vm48.almaden.ibm.com%2C60020%2C1404255227840.1404255801048
2014-07-01 16:03:21,083 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1404255227840/sceplus-vm48.almaden.ibm.com%2C60020%2C1404255227840.1404255743225
2014-07-01 16:03:21,083 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1404255227840/sceplus-vm48.almaden.ibm.com%2C60020%2C1404255227840.1404255745671
2014-07-01 16:03:21,083 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1404255227840/sceplus-vm48.almaden.ibm.com%2C60020%2C1404255227840.1404255748671
2014-07-01 16:03:21,083 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1404255227840/sceplus-vm48.almaden.ibm.com%2C60020%2C1404255227840.1404255751186
2014-07-01 16:03:23,620 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-01 16:03:23,788 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 4830 synced till here 4811
2014-07-01 16:03:23,967 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1404255227840/sceplus-vm48.almaden.ibm.com%2C60020%2C1404255227840.1404255801048 with entries=116, filesize=85.1m; new WAL /hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1404255227840/sceplus-vm48.almaden.ibm.com%2C60020%2C1404255227840.1404255803621
2014-07-01 16:03:25,566 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-01 16:03:25,584 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 4921 synced till here 4917
2014-07-01 16:03:25,742 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1404255227840/sceplus-vm48.almaden.ibm.com%2C60020%2C1404255227840.1404255803621 with entries=91, filesize=65.3m; new WAL /hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1404255227840/sceplus-vm48.almaden.ibm.com%2C60020%2C1404255227840.1404255805567
2014-07-01 16:03:27,665 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-01 16:03:27,687 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 5013 synced till here 5005
2014-07-01 16:03:27,817 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1404255227840/sceplus-vm48.almaden.ibm.com%2C60020%2C1404255227840.1404255805567 with entries=92, filesize=66.7m; new WAL /hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1404255227840/sceplus-vm48.almaden.ibm.com%2C60020%2C1404255227840.1404255807666
2014-07-01 16:03:29,795 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-01 16:03:29,833 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 5116 synced till here 5112
2014-07-01 16:03:29,864 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1404255227840/sceplus-vm48.almaden.ibm.com%2C60020%2C1404255227840.1404255807666 with entries=103, filesize=63.4m; new WAL /hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1404255227840/sceplus-vm48.almaden.ibm.com%2C60020%2C1404255227840.1404255809796
2014-07-01 16:03:30,017 DEBUG [RpcServer.handler=17,port=60020] regionserver.HRegion: Flush requested on usertable,user3,1404255616194.09108ac86cf3c374e79d1bb78f787b49.
2014-07-01 16:03:31,840 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-01 16:03:31,871 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 5217 synced till here 5210
2014-07-01 16:03:31,920 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1404255227840/sceplus-vm48.almaden.ibm.com%2C60020%2C1404255227840.1404255809796 with entries=101, filesize=64.0m; new WAL /hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1404255227840/sceplus-vm48.almaden.ibm.com%2C60020%2C1404255227840.1404255811841
2014-07-01 16:03:33,544 DEBUG [RpcServer.handler=35,port=60020] regionserver.HRegion: Flush requested on usertable,user7,1404255616194.431aad56a67499b48936eca121e3178e.
2014-07-01 16:03:34,214 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-01 16:03:34,463 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1404255227840/sceplus-vm48.almaden.ibm.com%2C60020%2C1404255227840.1404255811841 with entries=95, filesize=66.0m; new WAL /hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1404255227840/sceplus-vm48.almaden.ibm.com%2C60020%2C1404255227840.1404255814214
2014-07-01 16:03:37,009 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-01 16:03:37,143 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 5398 synced till here 5397
2014-07-01 16:03:37,168 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1404255227840/sceplus-vm48.almaden.ibm.com%2C60020%2C1404255227840.1404255814214 with entries=86, filesize=64.2m; new WAL /hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1404255227840/sceplus-vm48.almaden.ibm.com%2C60020%2C1404255227840.1404255817010
2014-07-01 16:03:38,838 INFO  [MemStoreFlusher.1] regionserver.DefaultStoreFlusher: Flushed, sequenceid=1573, memsize=296.6m, hasBloomFilter=true, into tmp file hdfs://master:54310/hbase/data/default/usertable/9e9a6a0fd8aef92a09081d29900a50f3/.tmp/74710d70237240e5ae756722f1520961
2014-07-01 16:03:38,849 DEBUG [MemStoreFlusher.1] regionserver.HRegionFileSystem: Committing store file hdfs://master:54310/hbase/data/default/usertable/9e9a6a0fd8aef92a09081d29900a50f3/.tmp/74710d70237240e5ae756722f1520961 as hdfs://master:54310/hbase/data/default/usertable/9e9a6a0fd8aef92a09081d29900a50f3/family/74710d70237240e5ae756722f1520961
2014-07-01 16:03:38,857 INFO  [MemStoreFlusher.1] regionserver.HStore: Added hdfs://master:54310/hbase/data/default/usertable/9e9a6a0fd8aef92a09081d29900a50f3/family/74710d70237240e5ae756722f1520961, entries=1079980, sequenceid=1573, filesize=76.9m
2014-07-01 16:03:38,857 INFO  [MemStoreFlusher.1] regionserver.HRegion: Finished memstore flush of ~296.6m/311025360, currentsize=195.6m/205099040 for region usertable,user8,1404255616195.9e9a6a0fd8aef92a09081d29900a50f3. in 18708ms, sequenceid=1573, compaction requested=true
2014-07-01 16:03:38,858 DEBUG [MemStoreFlusher.1] regionserver.CompactSplitThread: Small Compaction requested: system; Because: MemStoreFlusher.1; compaction_queue=(0:4), split_queue=0, merge_queue=0
2014-07-01 16:03:38,858 DEBUG [MemStoreFlusher.1] regionserver.HRegion: Started memstore flush for usertable,,1404255616194.510634b895ee706ff306728824babf96., current region memstore size 466.4m
2014-07-01 16:03:39,059 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-01 16:03:39,072 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 5491 synced till here 5486
2014-07-01 16:03:39,173 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1404255227840/sceplus-vm48.almaden.ibm.com%2C60020%2C1404255227840.1404255817010 with entries=93, filesize=66.6m; new WAL /hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1404255227840/sceplus-vm48.almaden.ibm.com%2C60020%2C1404255227840.1404255819059
2014-07-01 16:03:39,185 DEBUG [regionserver60020-smallCompactions-1404255751957] regionserver.HRegionFileSystem: Committing store file hdfs://master:54310/hbase/data/default/usertable/3e76136f0a8be94ccf3637676c064402/.tmp/e49b6c77370249f388139db102764e09 as hdfs://master:54310/hbase/data/default/usertable/3e76136f0a8be94ccf3637676c064402/family/e49b6c77370249f388139db102764e09
2014-07-01 16:03:39,352 DEBUG [regionserver60020-smallCompactions-1404255751957] regionserver.HStore: Removing store files after compaction...
2014-07-01 16:03:39,692 DEBUG [regionserver60020-smallCompactions-1404255751957] backup.HFileArchiver: Finished archiving from class org.apache.hadoop.hbase.backup.HFileArchiver$FileableStoreFile, file:hdfs://master:54310/hbase/data/default/usertable/3e76136f0a8be94ccf3637676c064402/family/88cb14649c44424d8d6a3607fb5c6f82, to hdfs://master:54310/hbase/archive/data/default/usertable/3e76136f0a8be94ccf3637676c064402/family/88cb14649c44424d8d6a3607fb5c6f82
2014-07-01 16:03:39,695 DEBUG [regionserver60020-smallCompactions-1404255751957] backup.HFileArchiver: Finished archiving from class org.apache.hadoop.hbase.backup.HFileArchiver$FileableStoreFile, file:hdfs://master:54310/hbase/data/default/usertable/3e76136f0a8be94ccf3637676c064402/family/f46a6a2ccf4b4eab8b2f30fc7c5671ef, to hdfs://master:54310/hbase/archive/data/default/usertable/3e76136f0a8be94ccf3637676c064402/family/f46a6a2ccf4b4eab8b2f30fc7c5671ef
2014-07-01 16:03:39,697 DEBUG [regionserver60020-smallCompactions-1404255751957] backup.HFileArchiver: Finished archiving from class org.apache.hadoop.hbase.backup.HFileArchiver$FileableStoreFile, file:hdfs://master:54310/hbase/data/default/usertable/3e76136f0a8be94ccf3637676c064402/family/6bfb236c21454a8d97b9df0faa54f12f, to hdfs://master:54310/hbase/archive/data/default/usertable/3e76136f0a8be94ccf3637676c064402/family/6bfb236c21454a8d97b9df0faa54f12f
2014-07-01 16:03:39,697 INFO  [regionserver60020-smallCompactions-1404255751957] regionserver.HStore: Completed major compaction of 3 file(s) in family of usertable,user5,1404255616194.3e76136f0a8be94ccf3637676c064402. into e49b6c77370249f388139db102764e09(size=90.5m), total size for store is 90.5m. This selection was in queue for 0sec, and took 24sec to execute.
2014-07-01 16:03:39,697 INFO  [regionserver60020-smallCompactions-1404255751957] regionserver.CompactSplitThread: Completed compaction: Request = regionName=usertable,user5,1404255616194.3e76136f0a8be94ccf3637676c064402., storeName=family, fileCount=3, fileSize=110.4m, priority=17, time=75937986508969; duration=24sec
2014-07-01 16:03:39,698 DEBUG [regionserver60020-smallCompactions-1404255751957] regionserver.CompactSplitThread: CompactSplitThread Status: compaction_queue=(0:4), split_queue=0, merge_queue=0
2014-07-01 16:03:39,698 DEBUG [regionserver60020-smallCompactions-1404255751957] compactions.RatioBasedCompactionPolicy: Selecting compaction from 4 store files, 0 compacting, 4 eligible, 20 blocking
2014-07-01 16:03:39,698 DEBUG [regionserver60020-smallCompactions-1404255751957] compactions.ExploringCompactionPolicy: Exploring compaction algorithm has selected 4 files of size 187071073 starting at candidate #0 after considering 3 permutations with 3 in ratio
2014-07-01 16:03:39,698 DEBUG [regionserver60020-smallCompactions-1404255751957] regionserver.HStore: 431aad56a67499b48936eca121e3178e - family: Initiating major compaction
2014-07-01 16:03:39,699 INFO  [regionserver60020-smallCompactions-1404255751957] regionserver.HRegion: Starting compaction on family in region usertable,user7,1404255616194.431aad56a67499b48936eca121e3178e.
2014-07-01 16:03:39,699 INFO  [regionserver60020-smallCompactions-1404255751957] regionserver.HStore: Starting compaction of 4 file(s) in family of usertable,user7,1404255616194.431aad56a67499b48936eca121e3178e. into tmpdir=hdfs://master:54310/hbase/data/default/usertable/431aad56a67499b48936eca121e3178e/.tmp, totalSize=178.4m
2014-07-01 16:03:39,699 DEBUG [regionserver60020-smallCompactions-1404255751957] compactions.Compactor: Compacting hdfs://master:54310/hbase/data/default/usertable/431aad56a67499b48936eca121e3178e/family/b2008e2170394a60a1326573a3a2f568, keycount=15716, bloomtype=ROW, size=11.2m, encoding=NONE, seqNum=179, earliestPutTs=1404255652073
2014-07-01 16:03:39,699 DEBUG [regionserver60020-smallCompactions-1404255751957] compactions.Compactor: Compacting hdfs://master:54310/hbase/data/default/usertable/431aad56a67499b48936eca121e3178e/family/3132c21bc7b14fcfa090d003d916750e, keycount=63364, bloomtype=ROW, size=45.2m, encoding=NONE, seqNum=356, earliestPutTs=1404255694911
2014-07-01 16:03:39,700 DEBUG [regionserver60020-smallCompactions-1404255751957] compactions.Compactor: Compacting hdfs://master:54310/hbase/data/default/usertable/431aad56a67499b48936eca121e3178e/family/abb653cbf3de4706aa1e0d30fbc523ac, keycount=65635, bloomtype=ROW, size=46.8m, encoding=NONE, seqNum=528, earliestPutTs=1404255720370
2014-07-01 16:03:39,700 DEBUG [regionserver60020-smallCompactions-1404255751957] compactions.Compactor: Compacting hdfs://master:54310/hbase/data/default/usertable/431aad56a67499b48936eca121e3178e/family/f214477da2c544918532b903aef19a2a, keycount=105630, bloomtype=ROW, size=75.2m, encoding=NONE, seqNum=721, earliestPutTs=1404255752578
2014-07-01 16:03:39,735 DEBUG [regionserver60020-smallCompactions-1404255751957] util.FSUtils: DFS Client does not support most favored nodes create; using default create
2014-07-01 16:03:39,762 DEBUG [MemStoreFlusher.1] util.FSUtils: DFS Client does not support most favored nodes create; using default create
2014-07-01 16:03:40,995 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-01 16:03:41,026 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 5580 synced till here 5578
2014-07-01 16:03:41,047 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1404255227840/sceplus-vm48.almaden.ibm.com%2C60020%2C1404255227840.1404255819059 with entries=89, filesize=63.5m; new WAL /hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1404255227840/sceplus-vm48.almaden.ibm.com%2C60020%2C1404255227840.1404255820996
2014-07-01 16:03:42,749 INFO  [MemStoreFlusher.0] regionserver.DefaultStoreFlusher: Flushed, sequenceid=786, memsize=380.1m, hasBloomFilter=true, into tmp file hdfs://master:54310/hbase/data/default/usertable/3e76136f0a8be94ccf3637676c064402/.tmp/b3616e237843458abd24c08525f4e0b5
2014-07-01 16:03:42,759 DEBUG [MemStoreFlusher.0] regionserver.HRegionFileSystem: Committing store file hdfs://master:54310/hbase/data/default/usertable/3e76136f0a8be94ccf3637676c064402/.tmp/b3616e237843458abd24c08525f4e0b5 as hdfs://master:54310/hbase/data/default/usertable/3e76136f0a8be94ccf3637676c064402/family/b3616e237843458abd24c08525f4e0b5
2014-07-01 16:03:42,767 INFO  [MemStoreFlusher.0] regionserver.HStore: Added hdfs://master:54310/hbase/data/default/usertable/3e76136f0a8be94ccf3637676c064402/family/b3616e237843458abd24c08525f4e0b5, entries=1383790, sequenceid=786, filesize=98.6m
2014-07-01 16:03:42,768 INFO  [MemStoreFlusher.0] regionserver.HRegion: Finished memstore flush of ~433.2m/454210560, currentsize=234.2m/245538480 for region usertable,user5,1404255616194.3e76136f0a8be94ccf3637676c064402. in 23710ms, sequenceid=786, compaction requested=false
2014-07-01 16:03:42,768 DEBUG [MemStoreFlusher.0] regionserver.HRegion: Started memstore flush for usertable,user3,1404255616194.09108ac86cf3c374e79d1bb78f787b49., current region memstore size 386.0m
2014-07-01 16:03:43,534 DEBUG [MemStoreFlusher.0] util.FSUtils: DFS Client does not support most favored nodes create; using default create
2014-07-01 16:03:43,901 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-01 16:03:44,059 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 5674 synced till here 5665
2014-07-01 16:03:44,446 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1404255227840/sceplus-vm48.almaden.ibm.com%2C60020%2C1404255227840.1404255820996 with entries=94, filesize=70.4m; new WAL /hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1404255227840/sceplus-vm48.almaden.ibm.com%2C60020%2C1404255227840.1404255823907
2014-07-01 16:03:44,446 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1404255227840/sceplus-vm48.almaden.ibm.com%2C60020%2C1404255227840.1404255753872
2014-07-01 16:03:44,446 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1404255227840/sceplus-vm48.almaden.ibm.com%2C60020%2C1404255227840.1404255758000
2014-07-01 16:03:44,447 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1404255227840/sceplus-vm48.almaden.ibm.com%2C60020%2C1404255227840.1404255759949
2014-07-01 16:03:44,447 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1404255227840/sceplus-vm48.almaden.ibm.com%2C60020%2C1404255227840.1404255762149
2014-07-01 16:03:44,447 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1404255227840/sceplus-vm48.almaden.ibm.com%2C60020%2C1404255227840.1404255764505
2014-07-01 16:03:44,447 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1404255227840/sceplus-vm48.almaden.ibm.com%2C60020%2C1404255227840.1404255766780
2014-07-01 16:03:45,477 DEBUG [RpcServer.handler=42,port=60020] regionserver.HRegion: Flush requested on usertable,user5,1404255616194.3e76136f0a8be94ccf3637676c064402.
2014-07-01 16:03:45,960 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-01 16:03:45,962 DEBUG [RpcServer.handler=11,port=60020] regionserver.HRegion: Flush requested on usertable,user8,1404255616195.9e9a6a0fd8aef92a09081d29900a50f3.
2014-07-01 16:03:46,439 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1404255227840/sceplus-vm48.almaden.ibm.com%2C60020%2C1404255227840.1404255823907 with entries=101, filesize=80.9m; new WAL /hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1404255227840/sceplus-vm48.almaden.ibm.com%2C60020%2C1404255227840.1404255825960
2014-07-01 16:03:48,012 DEBUG [LruStats #0] hfile.LruBlockCache: Total=4.16 MB, free=3.95 GB, max=3.96 GB, blocks=0, accesses=10590, hits=0, hitRatio=0, cachingAccesses=0, cachingHits=0, cachingHitsRatio=0,evictions=0, evicted=0, evictedPerRun=NaN
2014-07-01 16:03:48,505 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-01 16:03:48,558 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 5869 synced till here 5867
2014-07-01 16:03:48,717 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1404255227840/sceplus-vm48.almaden.ibm.com%2C60020%2C1404255227840.1404255825960 with entries=94, filesize=66.0m; new WAL /hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1404255227840/sceplus-vm48.almaden.ibm.com%2C60020%2C1404255227840.1404255828506
2014-07-01 16:03:50,620 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-01 16:03:50,678 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 5965 synced till here 5961
2014-07-01 16:03:50,861 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1404255227840/sceplus-vm48.almaden.ibm.com%2C60020%2C1404255227840.1404255828506 with entries=96, filesize=64.6m; new WAL /hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1404255227840/sceplus-vm48.almaden.ibm.com%2C60020%2C1404255227840.1404255830620
2014-07-01 16:03:53,342 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-01 16:03:53,366 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 6056 synced till here 6055
2014-07-01 16:03:53,402 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1404255227840/sceplus-vm48.almaden.ibm.com%2C60020%2C1404255227840.1404255830620 with entries=91, filesize=64.3m; new WAL /hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1404255227840/sceplus-vm48.almaden.ibm.com%2C60020%2C1404255227840.1404255833342
2014-07-01 16:03:59,976 INFO  [MemStoreFlusher.1] regionserver.DefaultStoreFlusher: Flushed, sequenceid=903, memsize=434.5m, hasBloomFilter=true, into tmp file hdfs://master:54310/hbase/data/default/usertable/510634b895ee706ff306728824babf96/.tmp/af73009e14a640c3a4f5b5a099fc66af
2014-07-01 16:03:59,987 DEBUG [MemStoreFlusher.1] regionserver.HRegionFileSystem: Committing store file hdfs://master:54310/hbase/data/default/usertable/510634b895ee706ff306728824babf96/.tmp/af73009e14a640c3a4f5b5a099fc66af as hdfs://master:54310/hbase/data/default/usertable/510634b895ee706ff306728824babf96/family/af73009e14a640c3a4f5b5a099fc66af
2014-07-01 16:03:59,995 INFO  [MemStoreFlusher.1] regionserver.HStore: Added hdfs://master:54310/hbase/data/default/usertable/510634b895ee706ff306728824babf96/family/af73009e14a640c3a4f5b5a099fc66af, entries=1582070, sequenceid=903, filesize=112.6m
2014-07-01 16:03:59,995 INFO  [MemStoreFlusher.1] regionserver.HRegion: Finished memstore flush of ~471.4m/494277200, currentsize=164.2m/172214160 for region usertable,,1404255616194.510634b895ee706ff306728824babf96. in 21137ms, sequenceid=903, compaction requested=true
2014-07-01 16:03:59,995 DEBUG [MemStoreFlusher.1] regionserver.CompactSplitThread: Small Compaction requested: system; Because: MemStoreFlusher.1; compaction_queue=(0:4), split_queue=0, merge_queue=0
2014-07-01 16:03:59,996 DEBUG [MemStoreFlusher.1] regionserver.HRegion: Started memstore flush for usertable,user7,1404255616194.431aad56a67499b48936eca121e3178e., current region memstore size 482.8m
2014-07-01 16:04:00,402 DEBUG [MemStoreFlusher.1] util.FSUtils: DFS Client does not support most favored nodes create; using default create
2014-07-01 16:04:00,693 INFO  [MemStoreFlusher.0] regionserver.DefaultStoreFlusher: Flushed, sequenceid=923, memsize=389.3m, hasBloomFilter=true, into tmp file hdfs://master:54310/hbase/data/default/usertable/09108ac86cf3c374e79d1bb78f787b49/.tmp/3e086712d9304c5f873e794c8e5e93aa
2014-07-01 16:04:00,707 DEBUG [MemStoreFlusher.0] regionserver.HRegionFileSystem: Committing store file hdfs://master:54310/hbase/data/default/usertable/09108ac86cf3c374e79d1bb78f787b49/.tmp/3e086712d9304c5f873e794c8e5e93aa as hdfs://master:54310/hbase/data/default/usertable/09108ac86cf3c374e79d1bb78f787b49/family/3e086712d9304c5f873e794c8e5e93aa
2014-07-01 16:04:00,719 INFO  [MemStoreFlusher.0] regionserver.HStore: Added hdfs://master:54310/hbase/data/default/usertable/09108ac86cf3c374e79d1bb78f787b49/family/3e086712d9304c5f873e794c8e5e93aa, entries=1417550, sequenceid=923, filesize=100.9m
2014-07-01 16:04:00,720 INFO  [MemStoreFlusher.0] regionserver.HRegion: Finished memstore flush of ~391.2m/410237920, currentsize=125.9m/132001760 for region usertable,user3,1404255616194.09108ac86cf3c374e79d1bb78f787b49. in 17951ms, sequenceid=923, compaction requested=true
2014-07-01 16:04:00,720 DEBUG [MemStoreFlusher.0] regionserver.CompactSplitThread: Small Compaction requested: system; Because: MemStoreFlusher.0; compaction_queue=(0:5), split_queue=0, merge_queue=0
2014-07-01 16:04:00,736 DEBUG [MemStoreFlusher.0] regionserver.HRegion: Started memstore flush for usertable,user5,1404255616194.3e76136f0a8be94ccf3637676c064402., current region memstore size 367.3m
2014-07-01 16:04:01,057 DEBUG [MemStoreFlusher.0] util.FSUtils: DFS Client does not support most favored nodes create; using default create
2014-07-01 16:04:06,527 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-01 16:04:07,651 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 6183 synced till here 6181
2014-07-01 16:04:07,696 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1404255227840/sceplus-vm48.almaden.ibm.com%2C60020%2C1404255227840.1404255833342 with entries=127, filesize=96.5m; new WAL /hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1404255227840/sceplus-vm48.almaden.ibm.com%2C60020%2C1404255227840.1404255846528
2014-07-01 16:04:07,696 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1404255227840/sceplus-vm48.almaden.ibm.com%2C60020%2C1404255227840.1404255769916
2014-07-01 16:04:07,696 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1404255227840/sceplus-vm48.almaden.ibm.com%2C60020%2C1404255227840.1404255771898
2014-07-01 16:04:07,696 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1404255227840/sceplus-vm48.almaden.ibm.com%2C60020%2C1404255227840.1404255774699
2014-07-01 16:04:07,696 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1404255227840/sceplus-vm48.almaden.ibm.com%2C60020%2C1404255227840.1404255777213
2014-07-01 16:04:07,696 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1404255227840/sceplus-vm48.almaden.ibm.com%2C60020%2C1404255227840.1404255780431
2014-07-01 16:04:07,696 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1404255227840/sceplus-vm48.almaden.ibm.com%2C60020%2C1404255227840.1404255782249
2014-07-01 16:04:09,513 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-01 16:04:09,697 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 6267 synced till here 6260
2014-07-01 16:04:09,868 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1404255227840/sceplus-vm48.almaden.ibm.com%2C60020%2C1404255227840.1404255846528 with entries=84, filesize=68.7m; new WAL /hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1404255227840/sceplus-vm48.almaden.ibm.com%2C60020%2C1404255227840.1404255849514
2014-07-01 16:04:11,841 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-01 16:04:11,944 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 6348 synced till here 6342
2014-07-01 16:04:11,993 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1404255227840/sceplus-vm48.almaden.ibm.com%2C60020%2C1404255227840.1404255849514 with entries=81, filesize=69.0m; new WAL /hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1404255227840/sceplus-vm48.almaden.ibm.com%2C60020%2C1404255227840.1404255851841
2014-07-01 16:04:13,479 DEBUG [RpcServer.handler=26,port=60020] regionserver.HRegion: Flush requested on usertable,,1404255616194.510634b895ee706ff306728824babf96.
2014-07-01 16:04:14,043 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-01 16:04:14,321 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 6428 synced till here 6416
2014-07-01 16:04:14,334 DEBUG [regionserver60020-smallCompactions-1404255751957] regionserver.HRegionFileSystem: Committing store file hdfs://master:54310/hbase/data/default/usertable/431aad56a67499b48936eca121e3178e/.tmp/29207a3e75a446f28386936686268649 as hdfs://master:54310/hbase/data/default/usertable/431aad56a67499b48936eca121e3178e/family/29207a3e75a446f28386936686268649
2014-07-01 16:04:14,617 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1404255227840/sceplus-vm48.almaden.ibm.com%2C60020%2C1404255227840.1404255851841 with entries=80, filesize=68.9m; new WAL /hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1404255227840/sceplus-vm48.almaden.ibm.com%2C60020%2C1404255227840.1404255854044
2014-07-01 16:04:14,756 DEBUG [regionserver60020-smallCompactions-1404255751957] regionserver.HStore: Removing store files after compaction...
2014-07-01 16:04:16,350 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-01 16:04:16,715 DEBUG [RpcServer.handler=45,port=60020] regionserver.HRegion: Flush requested on usertable,user3,1404255616194.09108ac86cf3c374e79d1bb78f787b49.
2014-07-01 16:04:17,444 DEBUG [regionserver60020-smallCompactions-1404255751957] backup.HFileArchiver: Finished archiving from class org.apache.hadoop.hbase.backup.HFileArchiver$FileableStoreFile, file:hdfs://master:54310/hbase/data/default/usertable/431aad56a67499b48936eca121e3178e/family/b2008e2170394a60a1326573a3a2f568, to hdfs://master:54310/hbase/archive/data/default/usertable/431aad56a67499b48936eca121e3178e/family/b2008e2170394a60a1326573a3a2f568
2014-07-01 16:04:17,451 DEBUG [regionserver60020-smallCompactions-1404255751957] backup.HFileArchiver: Finished archiving from class org.apache.hadoop.hbase.backup.HFileArchiver$FileableStoreFile, file:hdfs://master:54310/hbase/data/default/usertable/431aad56a67499b48936eca121e3178e/family/3132c21bc7b14fcfa090d003d916750e, to hdfs://master:54310/hbase/archive/data/default/usertable/431aad56a67499b48936eca121e3178e/family/3132c21bc7b14fcfa090d003d916750e
2014-07-01 16:04:17,571 DEBUG [regionserver60020-smallCompactions-1404255751957] backup.HFileArchiver: Finished archiving from class org.apache.hadoop.hbase.backup.HFileArchiver$FileableStoreFile, file:hdfs://master:54310/hbase/data/default/usertable/431aad56a67499b48936eca121e3178e/family/abb653cbf3de4706aa1e0d30fbc523ac, to hdfs://master:54310/hbase/archive/data/default/usertable/431aad56a67499b48936eca121e3178e/family/abb653cbf3de4706aa1e0d30fbc523ac
2014-07-01 16:04:17,572 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 6556 synced till here 6551
2014-07-01 16:04:17,574 DEBUG [regionserver60020-smallCompactions-1404255751957] backup.HFileArchiver: Finished archiving from class org.apache.hadoop.hbase.backup.HFileArchiver$FileableStoreFile, file:hdfs://master:54310/hbase/data/default/usertable/431aad56a67499b48936eca121e3178e/family/f214477da2c544918532b903aef19a2a, to hdfs://master:54310/hbase/archive/data/default/usertable/431aad56a67499b48936eca121e3178e/family/f214477da2c544918532b903aef19a2a
2014-07-01 16:04:17,575 INFO  [regionserver60020-smallCompactions-1404255751957] regionserver.HStore: Completed major compaction of 4 file(s) in family of usertable,user7,1404255616194.431aad56a67499b48936eca121e3178e. into 29207a3e75a446f28386936686268649(size=153.4m), total size for store is 153.4m. This selection was in queue for 0sec, and took 37sec to execute.
2014-07-01 16:04:17,575 INFO  [regionserver60020-smallCompactions-1404255751957] regionserver.CompactSplitThread: Completed compaction: Request = regionName=usertable,user7,1404255616194.431aad56a67499b48936eca121e3178e., storeName=family, fileCount=4, fileSize=178.4m, priority=16, time=75962055976146; duration=37sec
2014-07-01 16:04:17,576 DEBUG [regionserver60020-smallCompactions-1404255751957] regionserver.CompactSplitThread: CompactSplitThread Status: compaction_queue=(0:5), split_queue=0, merge_queue=0
2014-07-01 16:04:17,576 DEBUG [regionserver60020-smallCompactions-1404255751957] compactions.RatioBasedCompactionPolicy: Selecting compaction from 4 store files, 0 compacting, 4 eligible, 20 blocking
2014-07-01 16:04:17,576 DEBUG [regionserver60020-smallCompactions-1404255751957] compactions.ExploringCompactionPolicy: Exploring compaction algorithm has selected 4 files of size 218713123 starting at candidate #0 after considering 3 permutations with 3 in ratio
2014-07-01 16:04:17,576 DEBUG [regionserver60020-smallCompactions-1404255751957] regionserver.HStore: 9e9a6a0fd8aef92a09081d29900a50f3 - family: Initiating major compaction
2014-07-01 16:04:17,576 INFO  [regionserver60020-smallCompactions-1404255751957] regionserver.HRegion: Starting compaction on family in region usertable,user8,1404255616195.9e9a6a0fd8aef92a09081d29900a50f3.
2014-07-01 16:04:17,577 INFO  [regionserver60020-smallCompactions-1404255751957] regionserver.HStore: Starting compaction of 4 file(s) in family of usertable,user8,1404255616195.9e9a6a0fd8aef92a09081d29900a50f3. into tmpdir=hdfs://master:54310/hbase/data/default/usertable/9e9a6a0fd8aef92a09081d29900a50f3/.tmp, totalSize=208.6m
2014-07-01 16:04:17,577 DEBUG [regionserver60020-smallCompactions-1404255751957] compactions.Compactor: Compacting hdfs://master:54310/hbase/data/default/usertable/9e9a6a0fd8aef92a09081d29900a50f3/family/8a963e9c9b4c494aac7d936388246b2b, keycount=15843, bloomtype=ROW, size=11.3m, encoding=NONE, seqNum=190, earliestPutTs=1404255633975
2014-07-01 16:04:17,577 DEBUG [regionserver60020-smallCompactions-1404255751957] compactions.Compactor: Compacting hdfs://master:54310/hbase/data/default/usertable/9e9a6a0fd8aef92a09081d29900a50f3/family/ac57ec120e374baab0221e35e5bfb625, keycount=77633, bloomtype=ROW, size=55.4m, encoding=NONE, seqNum=611, earliestPutTs=1404255695088
2014-07-01 16:04:17,577 DEBUG [regionserver60020-smallCompactions-1404255751957] compactions.Compactor: Compacting hdfs://master:54310/hbase/data/default/usertable/9e9a6a0fd8aef92a09081d29900a50f3/family/8e15e7b24e714917923038d3a8a41661, keycount=91311, bloomtype=ROW, size=65.0m, encoding=NONE, seqNum=1030, earliestPutTs=1404255730970
2014-07-01 16:04:17,577 DEBUG [regionserver60020-smallCompactions-1404255751957] compactions.Compactor: Compacting hdfs://master:54310/hbase/data/default/usertable/9e9a6a0fd8aef92a09081d29900a50f3/family/74710d70237240e5ae756722f1520961, keycount=107998, bloomtype=ROW, size=76.9m, encoding=NONE, seqNum=1573, earliestPutTs=1404255765207
2014-07-01 16:04:17,605 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1404255227840/sceplus-vm48.almaden.ibm.com%2C60020%2C1404255227840.1404255854044 with entries=128, filesize=106.2m; new WAL /hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1404255227840/sceplus-vm48.almaden.ibm.com%2C60020%2C1404255227840.1404255856351
2014-07-01 16:04:17,752 DEBUG [regionserver60020-smallCompactions-1404255751957] util.FSUtils: DFS Client does not support most favored nodes create; using default create
2014-07-01 16:04:19,279 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-01 16:04:19,315 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 6637 synced till here 6632
2014-07-01 16:04:19,401 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1404255227840/sceplus-vm48.almaden.ibm.com%2C60020%2C1404255227840.1404255856351 with entries=81, filesize=68.5m; new WAL /hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1404255227840/sceplus-vm48.almaden.ibm.com%2C60020%2C1404255227840.1404255859279
2014-07-01 16:04:20,547 INFO  [MemStoreFlusher.0] regionserver.DefaultStoreFlusher: Flushed, sequenceid=995, memsize=340.0m, hasBloomFilter=true, into tmp file hdfs://master:54310/hbase/data/default/usertable/3e76136f0a8be94ccf3637676c064402/.tmp/5b970a751db84b4ea94c5a41d59e7f96
2014-07-01 16:04:20,697 DEBUG [MemStoreFlusher.0] regionserver.HRegionFileSystem: Committing store file hdfs://master:54310/hbase/data/default/usertable/3e76136f0a8be94ccf3637676c064402/.tmp/5b970a751db84b4ea94c5a41d59e7f96 as hdfs://master:54310/hbase/data/default/usertable/3e76136f0a8be94ccf3637676c064402/family/5b970a751db84b4ea94c5a41d59e7f96
2014-07-01 16:04:20,710 INFO  [MemStoreFlusher.0] regionserver.HStore: Added hdfs://master:54310/hbase/data/default/usertable/3e76136f0a8be94ccf3637676c064402/family/5b970a751db84b4ea94c5a41d59e7f96, entries=1238030, sequenceid=995, filesize=88.1m
2014-07-01 16:04:20,710 INFO  [MemStoreFlusher.0] regionserver.HRegion: Finished memstore flush of ~367.3m/385127360, currentsize=168.0m/176170240 for region usertable,user5,1404255616194.3e76136f0a8be94ccf3637676c064402. in 19974ms, sequenceid=995, compaction requested=true
2014-07-01 16:04:20,710 DEBUG [MemStoreFlusher.0] regionserver.CompactSplitThread: Small Compaction requested: system; Because: MemStoreFlusher.0; compaction_queue=(0:5), split_queue=0, merge_queue=0
2014-07-01 16:04:20,710 DEBUG [MemStoreFlusher.0] regionserver.HRegion: Started memstore flush for usertable,user8,1404255616195.9e9a6a0fd8aef92a09081d29900a50f3., current region memstore size 501.4m
2014-07-01 16:04:21,630 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-01 16:04:21,778 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 6715 synced till here 6709
2014-07-01 16:04:21,844 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1404255227840/sceplus-vm48.almaden.ibm.com%2C60020%2C1404255227840.1404255859279 with entries=78, filesize=67.1m; new WAL /hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1404255227840/sceplus-vm48.almaden.ibm.com%2C60020%2C1404255227840.1404255861631
2014-07-01 16:04:22,409 DEBUG [MemStoreFlusher.0] util.FSUtils: DFS Client does not support most favored nodes create; using default create
2014-07-01 16:04:24,175 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-01 16:04:24,355 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 6813 synced till here 6799
2014-07-01 16:04:24,608 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1404255227840/sceplus-vm48.almaden.ibm.com%2C60020%2C1404255227840.1404255861631 with entries=98, filesize=74.9m; new WAL /hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1404255227840/sceplus-vm48.almaden.ibm.com%2C60020%2C1404255227840.1404255864176
2014-07-01 16:04:26,671 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-01 16:04:26,698 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 6903 synced till here 6887
2014-07-01 16:04:26,978 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1404255227840/sceplus-vm48.almaden.ibm.com%2C60020%2C1404255227840.1404255864176 with entries=90, filesize=76.4m; new WAL /hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1404255227840/sceplus-vm48.almaden.ibm.com%2C60020%2C1404255227840.1404255866671
2014-07-01 16:04:27,299 INFO  [MemStoreFlusher.1] regionserver.DefaultStoreFlusher: Flushed, sequenceid=994, memsize=455.7m, hasBloomFilter=true, into tmp file hdfs://master:54310/hbase/data/default/usertable/431aad56a67499b48936eca121e3178e/.tmp/e20b07cf980149b4af089656e5ab685a
2014-07-01 16:04:27,313 DEBUG [MemStoreFlusher.1] regionserver.HRegionFileSystem: Committing store file hdfs://master:54310/hbase/data/default/usertable/431aad56a67499b48936eca121e3178e/.tmp/e20b07cf980149b4af089656e5ab685a as hdfs://master:54310/hbase/data/default/usertable/431aad56a67499b48936eca121e3178e/family/e20b07cf980149b4af089656e5ab685a
2014-07-01 16:04:27,324 INFO  [MemStoreFlusher.1] regionserver.HStore: Added hdfs://master:54310/hbase/data/default/usertable/431aad56a67499b48936eca121e3178e/family/e20b07cf980149b4af089656e5ab685a, entries=1659180, sequenceid=994, filesize=118.2m
2014-07-01 16:04:27,324 INFO  [MemStoreFlusher.1] regionserver.HRegion: Finished memstore flush of ~482.8m/506279920, currentsize=237.7m/249221040 for region usertable,user7,1404255616194.431aad56a67499b48936eca121e3178e. in 27328ms, sequenceid=994, compaction requested=false
2014-07-01 16:04:27,324 DEBUG [MemStoreFlusher.1] regionserver.HRegion: Started memstore flush for usertable,,1404255616194.510634b895ee706ff306728824babf96., current region memstore size 422.5m
2014-07-01 16:04:28,221 DEBUG [RpcServer.handler=17,port=60020] regionserver.HRegion: Flush requested on usertable,user5,1404255616194.3e76136f0a8be94ccf3637676c064402.
2014-07-01 16:04:28,234 DEBUG [RpcServer.handler=18,port=60020] regionserver.HRegion: Flush requested on usertable,user7,1404255616194.431aad56a67499b48936eca121e3178e.
2014-07-01 16:04:28,378 DEBUG [MemStoreFlusher.1] util.FSUtils: DFS Client does not support most favored nodes create; using default create
2014-07-01 16:04:28,698 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-01 16:04:28,863 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 6983 synced till here 6975
2014-07-01 16:04:29,087 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1404255227840/sceplus-vm48.almaden.ibm.com%2C60020%2C1404255227840.1404255866671 with entries=80, filesize=69.5m; new WAL /hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1404255227840/sceplus-vm48.almaden.ibm.com%2C60020%2C1404255227840.1404255868699
2014-07-01 16:04:29,087 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1404255227840/sceplus-vm48.almaden.ibm.com%2C60020%2C1404255227840.1404255784635
2014-07-01 16:04:29,087 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1404255227840/sceplus-vm48.almaden.ibm.com%2C60020%2C1404255227840.1404255787221
2014-07-01 16:04:29,087 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1404255227840/sceplus-vm48.almaden.ibm.com%2C60020%2C1404255227840.1404255789253
2014-07-01 16:04:29,088 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1404255227840/sceplus-vm48.almaden.ibm.com%2C60020%2C1404255227840.1404255791484
2014-07-01 16:04:31,047 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-01 16:04:31,537 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 7084 synced till here 7077
2014-07-01 16:04:32,019 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1404255227840/sceplus-vm48.almaden.ibm.com%2C60020%2C1404255227840.1404255868699 with entries=101, filesize=83.5m; new WAL /hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1404255227840/sceplus-vm48.almaden.ibm.com%2C60020%2C1404255227840.1404255871048
2014-07-01 16:04:34,263 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-01 16:04:34,277 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 7165 synced till here 7159
2014-07-01 16:04:34,321 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1404255227840/sceplus-vm48.almaden.ibm.com%2C60020%2C1404255227840.1404255871048 with entries=81, filesize=66.2m; new WAL /hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1404255227840/sceplus-vm48.almaden.ibm.com%2C60020%2C1404255227840.1404255874263
2014-07-01 16:04:36,542 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-01 16:04:36,636 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 7249 synced till here 7242
2014-07-01 16:04:36,874 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1404255227840/sceplus-vm48.almaden.ibm.com%2C60020%2C1404255227840.1404255874263 with entries=84, filesize=68.0m; new WAL /hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1404255227840/sceplus-vm48.almaden.ibm.com%2C60020%2C1404255227840.1404255876543
2014-07-01 16:04:39,062 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-01 16:04:39,078 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 7340 synced till here 7329
2014-07-01 16:04:39,391 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1404255227840/sceplus-vm48.almaden.ibm.com%2C60020%2C1404255227840.1404255876543 with entries=91, filesize=73.3m; new WAL /hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1404255227840/sceplus-vm48.almaden.ibm.com%2C60020%2C1404255227840.1404255879063
2014-07-01 16:04:41,566 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-01 16:04:41,593 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 7437 synced till here 7429
2014-07-01 16:04:41,739 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1404255227840/sceplus-vm48.almaden.ibm.com%2C60020%2C1404255227840.1404255879063 with entries=97, filesize=69.0m; new WAL /hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1404255227840/sceplus-vm48.almaden.ibm.com%2C60020%2C1404255227840.1404255881567
2014-07-01 16:04:44,350 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-01 16:04:44,412 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 7531 synced till here 7513
2014-07-01 16:04:44,780 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1404255227840/sceplus-vm48.almaden.ibm.com%2C60020%2C1404255227840.1404255881567 with entries=94, filesize=79.2m; new WAL /hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1404255227840/sceplus-vm48.almaden.ibm.com%2C60020%2C1404255227840.1404255884351
2014-07-01 16:04:46,641 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-01 16:04:46,702 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 7625 synced till here 7611
2014-07-01 16:04:46,863 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1404255227840/sceplus-vm48.almaden.ibm.com%2C60020%2C1404255227840.1404255884351 with entries=94, filesize=78.5m; new WAL /hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1404255227840/sceplus-vm48.almaden.ibm.com%2C60020%2C1404255227840.1404255886642
2014-07-01 16:04:48,924 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-01 16:04:48,956 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 7699 synced till here 7693
2014-07-01 16:04:49,097 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1404255227840/sceplus-vm48.almaden.ibm.com%2C60020%2C1404255227840.1404255886642 with entries=74, filesize=70.9m; new WAL /hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1404255227840/sceplus-vm48.almaden.ibm.com%2C60020%2C1404255227840.1404255888925
2014-07-01 16:04:51,334 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-01 16:04:51,772 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1404255227840/sceplus-vm48.almaden.ibm.com%2C60020%2C1404255227840.1404255888925 with entries=66, filesize=63.3m; new WAL /hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1404255227840/sceplus-vm48.almaden.ibm.com%2C60020%2C1404255227840.1404255891335
2014-07-01 16:04:54,761 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-01 16:04:54,790 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 7839 synced till here 7838
2014-07-01 16:04:55,346 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1404255227840/sceplus-vm48.almaden.ibm.com%2C60020%2C1404255227840.1404255891335 with entries=74, filesize=64.6m; new WAL /hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1404255227840/sceplus-vm48.almaden.ibm.com%2C60020%2C1404255227840.1404255894762
2014-07-01 16:04:56,495 INFO  [MemStoreFlusher.1] regionserver.DefaultStoreFlusher: Flushed, sequenceid=1161, memsize=317.1m, hasBloomFilter=true, into tmp file hdfs://master:54310/hbase/data/default/usertable/510634b895ee706ff306728824babf96/.tmp/7aafde38cd1d49a79297ea4ca205a621
2014-07-01 16:04:56,510 DEBUG [MemStoreFlusher.1] regionserver.HRegionFileSystem: Committing store file hdfs://master:54310/hbase/data/default/usertable/510634b895ee706ff306728824babf96/.tmp/7aafde38cd1d49a79297ea4ca205a621 as hdfs://master:54310/hbase/data/default/usertable/510634b895ee706ff306728824babf96/family/7aafde38cd1d49a79297ea4ca205a621
2014-07-01 16:04:57,293 INFO  [MemStoreFlusher.1] regionserver.HStore: Added hdfs://master:54310/hbase/data/default/usertable/510634b895ee706ff306728824babf96/family/7aafde38cd1d49a79297ea4ca205a621, entries=1154480, sequenceid=1161, filesize=82.2m
2014-07-01 16:04:57,294 INFO  [MemStoreFlusher.1] regionserver.HRegion: Finished memstore flush of ~424.4m/445025840, currentsize=293.1m/307355600 for region usertable,,1404255616194.510634b895ee706ff306728824babf96. in 29970ms, sequenceid=1161, compaction requested=true
2014-07-01 16:04:57,295 DEBUG [MemStoreFlusher.1] regionserver.CompactSplitThread: Small Compaction requested: system; Because: MemStoreFlusher.1; compaction_queue=(0:6), split_queue=0, merge_queue=0
2014-07-01 16:04:57,296 DEBUG [MemStoreFlusher.1] regionserver.HRegion: Started memstore flush for usertable,user3,1404255616194.09108ac86cf3c374e79d1bb78f787b49., current region memstore size 675.0m
2014-07-01 16:04:58,474 DEBUG [RpcServer.handler=10,port=60020] regionserver.HRegion: Flush requested on usertable,,1404255616194.510634b895ee706ff306728824babf96.
2014-07-01 16:05:00,278 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-01 16:05:00,312 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 7910 synced till here 7905
2014-07-01 16:05:00,464 DEBUG [MemStoreFlusher.1] util.FSUtils: DFS Client does not support most favored nodes create; using default create
2014-07-01 16:05:00,479 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1404255227840/sceplus-vm48.almaden.ibm.com%2C60020%2C1404255227840.1404255894762 with entries=71, filesize=65.0m; new WAL /hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1404255227840/sceplus-vm48.almaden.ibm.com%2C60020%2C1404255227840.1404255900279
2014-07-01 16:05:01,140 INFO  [MemStoreFlusher.0] regionserver.DefaultStoreFlusher: Flushed, sequenceid=2285, memsize=440.4m, hasBloomFilter=true, into tmp file hdfs://master:54310/hbase/data/default/usertable/9e9a6a0fd8aef92a09081d29900a50f3/.tmp/7aba3925a1bb4009a59ff08ef6dc8b77
2014-07-01 16:05:01,153 DEBUG [MemStoreFlusher.0] regionserver.HRegionFileSystem: Committing store file hdfs://master:54310/hbase/data/default/usertable/9e9a6a0fd8aef92a09081d29900a50f3/.tmp/7aba3925a1bb4009a59ff08ef6dc8b77 as hdfs://master:54310/hbase/data/default/usertable/9e9a6a0fd8aef92a09081d29900a50f3/family/7aba3925a1bb4009a59ff08ef6dc8b77
2014-07-01 16:05:01,167 INFO  [MemStoreFlusher.0] regionserver.HStore: Added hdfs://master:54310/hbase/data/default/usertable/9e9a6a0fd8aef92a09081d29900a50f3/family/7aba3925a1bb4009a59ff08ef6dc8b77, entries=1603510, sequenceid=2285, filesize=114.1m
2014-07-01 16:05:01,167 INFO  [MemStoreFlusher.0] regionserver.HRegion: Finished memstore flush of ~512.3m/537136400, currentsize=273.6m/286857440 for region usertable,user8,1404255616195.9e9a6a0fd8aef92a09081d29900a50f3. in 40457ms, sequenceid=2285, compaction requested=false
2014-07-01 16:05:01,168 DEBUG [MemStoreFlusher.0] regionserver.HRegion: Started memstore flush for usertable,user5,1404255616194.3e76136f0a8be94ccf3637676c064402., current region memstore size 576.8m
2014-07-01 16:05:01,729 DEBUG [RpcServer.handler=13,port=60020] regionserver.HRegion: Flush requested on usertable,user8,1404255616195.9e9a6a0fd8aef92a09081d29900a50f3.
2014-07-01 16:05:03,620 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-01 16:05:03,671 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 7983 synced till here 7975
2014-07-01 16:05:03,811 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1404255227840/sceplus-vm48.almaden.ibm.com%2C60020%2C1404255227840.1404255900279 with entries=73, filesize=67.1m; new WAL /hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1404255227840/sceplus-vm48.almaden.ibm.com%2C60020%2C1404255227840.1404255903620
2014-07-01 16:05:03,811 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1404255227840/sceplus-vm48.almaden.ibm.com%2C60020%2C1404255227840.1404255794766
2014-07-01 16:05:03,811 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1404255227840/sceplus-vm48.almaden.ibm.com%2C60020%2C1404255227840.1404255801048
2014-07-01 16:05:03,812 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1404255227840/sceplus-vm48.almaden.ibm.com%2C60020%2C1404255227840.1404255803621
2014-07-01 16:05:03,812 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1404255227840/sceplus-vm48.almaden.ibm.com%2C60020%2C1404255227840.1404255805567
2014-07-01 16:05:03,812 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1404255227840/sceplus-vm48.almaden.ibm.com%2C60020%2C1404255227840.1404255807666
2014-07-01 16:05:03,812 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1404255227840/sceplus-vm48.almaden.ibm.com%2C60020%2C1404255227840.1404255809796
2014-07-01 16:05:03,812 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1404255227840/sceplus-vm48.almaden.ibm.com%2C60020%2C1404255227840.1404255811841
2014-07-01 16:05:03,812 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1404255227840/sceplus-vm48.almaden.ibm.com%2C60020%2C1404255227840.1404255814214
2014-07-01 16:05:03,812 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1404255227840/sceplus-vm48.almaden.ibm.com%2C60020%2C1404255227840.1404255817010
2014-07-01 16:05:03,812 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1404255227840/sceplus-vm48.almaden.ibm.com%2C60020%2C1404255227840.1404255819059
2014-07-01 16:05:03,831 DEBUG [MemStoreFlusher.0] util.FSUtils: DFS Client does not support most favored nodes create; using default create
2014-07-01 16:05:05,901 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-01 16:05:06,013 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 8075 synced till here 8062
2014-07-01 16:05:06,131 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1404255227840/sceplus-vm48.almaden.ibm.com%2C60020%2C1404255227840.1404255903620 with entries=92, filesize=72.3m; new WAL /hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1404255227840/sceplus-vm48.almaden.ibm.com%2C60020%2C1404255227840.1404255905902
2014-07-01 16:05:08,041 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-01 16:05:08,084 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1404255227840/sceplus-vm48.almaden.ibm.com%2C60020%2C1404255227840.1404255905902 with entries=81, filesize=61.9m; new WAL /hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1404255227840/sceplus-vm48.almaden.ibm.com%2C60020%2C1404255227840.1404255908042
2014-07-01 16:05:10,232 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-01 16:05:10,612 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 8260 synced till here 8250
2014-07-01 16:05:11,156 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1404255227840/sceplus-vm48.almaden.ibm.com%2C60020%2C1404255227840.1404255908042 with entries=104, filesize=75.6m; new WAL /hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1404255227840/sceplus-vm48.almaden.ibm.com%2C60020%2C1404255227840.1404255910232
2014-07-01 16:05:13,420 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-01 16:05:13,596 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 8342 synced till here 8334
2014-07-01 16:05:13,795 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1404255227840/sceplus-vm48.almaden.ibm.com%2C60020%2C1404255227840.1404255910232 with entries=82, filesize=67.7m; new WAL /hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1404255227840/sceplus-vm48.almaden.ibm.com%2C60020%2C1404255227840.1404255913421
2014-07-01 16:05:16,401 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-01 16:05:16,599 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 8433 synced till here 8417
2014-07-01 16:05:16,827 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1404255227840/sceplus-vm48.almaden.ibm.com%2C60020%2C1404255227840.1404255913421 with entries=91, filesize=78.4m; new WAL /hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1404255227840/sceplus-vm48.almaden.ibm.com%2C60020%2C1404255227840.1404255916401
2014-07-01 16:05:33,524 WARN  [regionserver60020.periodicFlusher] util.Sleeper: We slept 23964ms instead of 10000ms, this is likely due to a long garbage collecting pause and it's usually bad, see http://hbase.apache.org/book.html#trouble.rs.runtime.zkexpired
2014-07-01 16:05:33,525 WARN  [regionserver60020.compactionChecker] util.Sleeper: We slept 23965ms instead of 10000ms, this is likely due to a long garbage collecting pause and it's usually bad, see http://hbase.apache.org/book.html#trouble.rs.runtime.zkexpired
2014-07-01 16:05:33,525 WARN  [regionserver60020] util.Sleeper: We slept 17291ms instead of 3000ms, this is likely due to a long garbage collecting pause and it's usually bad, see http://hbase.apache.org/book.html#trouble.rs.runtime.zkexpired
2014-07-01 16:05:33,607 WARN  [JvmPauseMonitor] util.JvmPauseMonitor: Detected pause in JVM or host machine (eg GC): pause of approximately 16466ms
GC pool 'ParNew' had collection(s): count=3 time=142ms
GC pool 'ConcurrentMarkSweep' had collection(s): count=1 time=16538ms
2014-07-01 16:05:33,626 WARN  [RpcServer.handler=20,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":19846,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.58:34288","starttimems":1404255913779,"queuetimems":1,"class":"HRegionServer","responsesize":18524,"method":"Multi"}
2014-07-01 16:05:33,626 WARN  [RpcServer.handler=15,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":19188,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.58:34288","starttimems":1404255914438,"queuetimems":9,"class":"HRegionServer","responsesize":15522,"method":"Multi"}
2014-07-01 16:05:33,626 WARN  [RpcServer.handler=20,port=60020] ipc.RpcServer: RpcServer.respondercallId: 6528 service: ClientService methodName: Multi size: 3.2m connection: 9.1.143.58:34288: output error
2014-07-01 16:05:33,627 WARN  [RpcServer.handler=35,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":19389,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.58:34288","starttimems":1404255914238,"queuetimems":0,"class":"HRegionServer","responsesize":18856,"method":"Multi"}
2014-07-01 16:05:33,632 WARN  [RpcServer.handler=33,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":19631,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.58:34288","starttimems":1404255914001,"queuetimems":0,"class":"HRegionServer","responsesize":18211,"method":"Multi"}
2014-07-01 16:05:33,631 WARN  [RpcServer.handler=12,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":19647,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.58:34288","starttimems":1404255913984,"queuetimems":168,"class":"HRegionServer","responsesize":18250,"method":"Multi"}
2014-07-01 16:05:33,630 WARN  [RpcServer.handler=15,port=60020] ipc.RpcServer: RpcServer.respondercallId: 6541 service: ClientService methodName: Multi size: 2.7m connection: 9.1.143.58:34288: output error
2014-07-01 16:05:33,643 WARN  [RpcServer.handler=15,port=60020] ipc.RpcServer: RpcServer.handler=15,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-07-01 16:05:33,630 WARN  [RpcServer.handler=20,port=60020] ipc.RpcServer: RpcServer.handler=20,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-07-01 16:05:33,646 WARN  [RpcServer.handler=12,port=60020] ipc.RpcServer: RpcServer.respondercallId: 6525 service: ClientService methodName: Multi size: 3.2m connection: 9.1.143.58:34288: output error
2014-07-01 16:05:33,646 WARN  [RpcServer.handler=12,port=60020] ipc.RpcServer: RpcServer.handler=12,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-07-01 16:05:33,647 WARN  [RpcServer.handler=35,port=60020] ipc.RpcServer: RpcServer.respondercallId: 6538 service: ClientService methodName: Multi size: 3.3m connection: 9.1.143.58:34288: output error
2014-07-01 16:05:33,647 WARN  [RpcServer.handler=35,port=60020] ipc.RpcServer: RpcServer.handler=35,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-07-01 16:05:33,647 WARN  [RpcServer.handler=33,port=60020] ipc.RpcServer: RpcServer.respondercallId: 6522 service: ClientService methodName: Multi size: 3.2m connection: 9.1.143.58:34288: output error
2014-07-01 16:05:33,647 WARN  [RpcServer.handler=33,port=60020] ipc.RpcServer: RpcServer.handler=33,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-07-01 16:05:33,752 WARN  [RpcServer.handler=30,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":20155,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.58:34288","starttimems":1404255913597,"queuetimems":0,"class":"HRegionServer","responsesize":19953,"method":"Multi"}
2014-07-01 16:05:33,752 WARN  [RpcServer.handler=41,port=60020] ipc.RpcServer: RpcServer.respondercallId: 6687 service: ClientService methodName: Multi size: 11.3k connection: 9.1.143.58:34445: output error
2014-07-01 16:05:33,752 WARN  [RpcServer.handler=44,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":20517,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.58:34288","starttimems":1404255913234,"queuetimems":0,"class":"HRegionServer","responsesize":19734,"method":"Multi"}
2014-07-01 16:05:33,754 WARN  [RpcServer.handler=30,port=60020] ipc.RpcServer: RpcServer.respondercallId: 6530 service: ClientService methodName: Multi size: 3.5m connection: 9.1.143.58:34288: output error
2014-07-01 16:05:33,754 WARN  [RpcServer.handler=22,port=60020] ipc.RpcServer: RpcServer.respondercallId: 6669 service: ClientService methodName: Multi size: 11.3k connection: 9.1.143.58:34444: output error
2014-07-01 16:05:33,754 WARN  [RpcServer.handler=30,port=60020] ipc.RpcServer: RpcServer.handler=30,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-07-01 16:05:33,755 WARN  [RpcServer.handler=22,port=60020] ipc.RpcServer: RpcServer.handler=22,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-07-01 16:05:33,755 WARN  [RpcServer.handler=37,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":16927,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.58:34288","starttimems":1404255916828,"queuetimems":0,"class":"HRegionServer","responsesize":56,"method":"Multi"}
2014-07-01 16:05:33,755 WARN  [RpcServer.handler=39,port=60020] ipc.RpcServer: RpcServer.respondercallId: 6595 service: ClientService methodName: Multi size: 11.3k connection: 9.1.143.58:34288: output error
2014-07-01 16:05:33,755 WARN  [RpcServer.handler=39,port=60020] ipc.RpcServer: RpcServer.handler=39,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-07-01 16:05:33,755 WARN  [RpcServer.handler=37,port=60020] ipc.RpcServer: RpcServer.respondercallId: 6590 service: ClientService methodName: Multi size: 11.3k connection: 9.1.143.58:34288: output error
2014-07-01 16:05:33,755 WARN  [RpcServer.handler=41,port=60020] ipc.RpcServer: RpcServer.handler=41,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-07-01 16:05:33,755 WARN  [RpcServer.handler=37,port=60020] ipc.RpcServer: RpcServer.handler=37,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-07-01 16:05:33,755 WARN  [RpcServer.handler=44,port=60020] ipc.RpcServer: RpcServer.respondercallId: 6511 service: ClientService methodName: Multi size: 3.4m connection: 9.1.143.58:34288: output error
2014-07-01 16:05:33,755 WARN  [RpcServer.handler=44,port=60020] ipc.RpcServer: RpcServer.handler=44,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-07-01 16:05:33,929 WARN  [RpcServer.handler=7,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":19665,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.58:34288","starttimems":1404255914264,"queuetimems":0,"class":"HRegionServer","responsesize":19650,"method":"Multi"}
2014-07-01 16:05:33,930 WARN  [RpcServer.handler=7,port=60020] ipc.RpcServer: RpcServer.respondercallId: 6533 service: ClientService methodName: Multi size: 3.4m connection: 9.1.143.58:34288: output error
2014-07-01 16:05:33,930 WARN  [RpcServer.handler=7,port=60020] ipc.RpcServer: RpcServer.handler=7,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-07-01 16:05:34,068 WARN  [RpcServer.handler=42,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":19428,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.58:34288","starttimems":1404255914639,"queuetimems":0,"class":"HRegionServer","responsesize":18687,"method":"Multi"}
2014-07-01 16:05:34,068 WARN  [RpcServer.handler=34,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":19443,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.58:34288","starttimems":1404255914624,"queuetimems":0,"class":"HRegionServer","responsesize":18598,"method":"Multi"}
2014-07-01 16:05:34,068 WARN  [RpcServer.handler=42,port=60020] ipc.RpcServer: RpcServer.respondercallId: 6546 service: ClientService methodName: Multi size: 3.3m connection: 9.1.143.58:34288: output error
2014-07-01 16:05:34,068 WARN  [RpcServer.handler=42,port=60020] ipc.RpcServer: RpcServer.handler=42,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-07-01 16:05:34,068 WARN  [RpcServer.handler=34,port=60020] ipc.RpcServer: RpcServer.respondercallId: 6545 service: ClientService methodName: Multi size: 3.3m connection: 9.1.143.58:34288: output error
2014-07-01 16:05:34,068 WARN  [RpcServer.handler=34,port=60020] ipc.RpcServer: RpcServer.handler=34,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-07-01 16:05:34,575 WARN  [RpcServer.handler=8,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":18544,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.58:34288","starttimems":1404255916030,"queuetimems":0,"class":"HRegionServer","responsesize":18032,"method":"Multi"}
2014-07-01 16:05:34,576 WARN  [RpcServer.handler=8,port=60020] ipc.RpcServer: RpcServer.respondercallId: 6573 service: ClientService methodName: Multi size: 3.2m connection: 9.1.143.58:34288: output error
2014-07-01 16:05:34,576 WARN  [RpcServer.handler=8,port=60020] ipc.RpcServer: RpcServer.handler=8,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-07-01 16:05:34,575 WARN  [RpcServer.handler=46,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":18964,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.58:34288","starttimems":1404255915610,"queuetimems":0,"class":"HRegionServer","responsesize":18820,"method":"Multi"}
2014-07-01 16:05:34,577 WARN  [RpcServer.handler=46,port=60020] ipc.RpcServer: RpcServer.respondercallId: 6566 service: ClientService methodName: Multi size: 3.3m connection: 9.1.143.58:34288: output error
2014-07-01 16:05:34,577 WARN  [RpcServer.handler=46,port=60020] ipc.RpcServer: RpcServer.handler=46,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-07-01 16:05:34,664 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-01 16:05:34,712 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 8510 synced till here 8505
2014-07-01 16:05:34,741 WARN  [RpcServer.handler=16,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":18724,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.58:34288","starttimems":1404255916016,"queuetimems":0,"class":"HRegionServer","responsesize":18390,"method":"Multi"}
2014-07-01 16:05:34,741 WARN  [RpcServer.handler=2,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":18347,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.58:34288","starttimems":1404255916393,"queuetimems":1,"class":"HRegionServer","responsesize":18974,"method":"Multi"}
2014-07-01 16:05:34,742 WARN  [RpcServer.handler=16,port=60020] ipc.RpcServer: RpcServer.respondercallId: 6570 service: ClientService methodName: Multi size: 3.2m connection: 9.1.143.58:34288: output error
2014-07-01 16:05:34,742 WARN  [RpcServer.handler=16,port=60020] ipc.RpcServer: RpcServer.handler=16,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-07-01 16:05:34,741 WARN  [RpcServer.handler=29,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":18334,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.58:34288","starttimems":1404255916406,"queuetimems":0,"class":"HRegionServer","responsesize":18856,"method":"Multi"}
2014-07-01 16:05:34,743 WARN  [RpcServer.handler=10,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":18164,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.58:34288","starttimems":1404255916578,"queuetimems":0,"class":"HRegionServer","responsesize":15522,"method":"Multi"}
2014-07-01 16:05:34,744 WARN  [RpcServer.handler=31,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":18766,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.58:34288","starttimems":1404255915978,"queuetimems":342,"class":"HRegionServer","responsesize":18211,"method":"Multi"}
2014-07-01 16:05:34,744 WARN  [RpcServer.handler=40,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":19146,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.58:34288","starttimems":1404255915598,"queuetimems":2,"class":"HRegionServer","responsesize":18966,"method":"Multi"}
2014-07-01 16:05:34,741 WARN  [RpcServer.handler=21,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":18312,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.58:34288","starttimems":1404255916428,"queuetimems":0,"class":"HRegionServer","responsesize":12493,"method":"Multi"}
2014-07-01 16:05:34,742 WARN  [RpcServer.handler=2,port=60020] ipc.RpcServer: RpcServer.respondercallId: 6574 service: ClientService methodName: Multi size: 3.3m connection: 9.1.143.58:34288: output error
2014-07-01 16:05:34,746 WARN  [RpcServer.handler=2,port=60020] ipc.RpcServer: RpcServer.handler=2,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-07-01 16:05:34,746 WARN  [RpcServer.handler=21,port=60020] ipc.RpcServer: RpcServer.respondercallId: 6583 service: ClientService methodName: Multi size: 2.2m connection: 9.1.143.58:34288: output error
2014-07-01 16:05:34,746 WARN  [RpcServer.handler=21,port=60020] ipc.RpcServer: RpcServer.handler=21,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-07-01 16:05:34,746 WARN  [RpcServer.handler=40,port=60020] ipc.RpcServer: RpcServer.respondercallId: 6563 service: ClientService methodName: Multi size: 3.3m connection: 9.1.143.58:34288: output error
2014-07-01 16:05:34,746 WARN  [RpcServer.handler=40,port=60020] ipc.RpcServer: RpcServer.handler=40,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-07-01 16:05:34,747 WARN  [RpcServer.handler=31,port=60020] ipc.RpcServer: RpcServer.respondercallId: 6565 service: ClientService methodName: Multi size: 3.2m connection: 9.1.143.58:34288: output error
2014-07-01 16:05:34,747 WARN  [RpcServer.handler=31,port=60020] ipc.RpcServer: RpcServer.handler=31,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-07-01 16:05:34,747 WARN  [RpcServer.handler=10,port=60020] ipc.RpcServer: RpcServer.respondercallId: 6586 service: ClientService methodName: Multi size: 2.7m connection: 9.1.143.58:34288: output error
2014-07-01 16:05:34,747 WARN  [RpcServer.handler=10,port=60020] ipc.RpcServer: RpcServer.handler=10,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-07-01 16:05:34,747 WARN  [RpcServer.handler=29,port=60020] ipc.RpcServer: RpcServer.respondercallId: 6582 service: ClientService methodName: Multi size: 3.3m connection: 9.1.143.58:34288: output error
2014-07-01 16:05:34,747 WARN  [RpcServer.handler=29,port=60020] ipc.RpcServer: RpcServer.handler=29,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-07-01 16:05:34,748 WARN  [RpcServer.handler=28,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":18746,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.58:34288","starttimems":1404255916001,"queuetimems":0,"class":"HRegionServer","responsesize":18238,"method":"Multi"}
2014-07-01 16:05:34,748 WARN  [RpcServer.handler=28,port=60020] ipc.RpcServer: RpcServer.respondercallId: 6564 service: ClientService methodName: Multi size: 3.2m connection: 9.1.143.58:34288: output error
2014-07-01 16:05:34,749 WARN  [RpcServer.handler=28,port=60020] ipc.RpcServer: RpcServer.handler=28,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-07-01 16:05:34,769 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1404255227840/sceplus-vm48.almaden.ibm.com%2C60020%2C1404255227840.1404255916401 with entries=77, filesize=66.7m; new WAL /hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1404255227840/sceplus-vm48.almaden.ibm.com%2C60020%2C1404255227840.1404255934664
2014-07-01 16:05:34,830 WARN  [RpcServer.handler=18,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":18619,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.58:34288","starttimems":1404255916210,"queuetimems":0,"class":"HRegionServer","responsesize":18524,"method":"Multi"}
2014-07-01 16:05:34,830 WARN  [RpcServer.handler=36,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":18578,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.58:34288","starttimems":1404255916251,"queuetimems":1,"class":"HRegionServer","responsesize":18250,"method":"Multi"}
2014-07-01 16:05:34,831 WARN  [RpcServer.handler=18,port=60020] ipc.RpcServer: RpcServer.respondercallId: 6572 service: ClientService methodName: Multi size: 3.2m connection: 9.1.143.58:34288: output error
2014-07-01 16:05:34,831 WARN  [RpcServer.handler=18,port=60020] ipc.RpcServer: RpcServer.handler=18,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-07-01 16:05:34,831 WARN  [RpcServer.handler=36,port=60020] ipc.RpcServer: RpcServer.respondercallId: 6571 service: ClientService methodName: Multi size: 3.2m connection: 9.1.143.58:34288: output error
2014-07-01 16:05:34,831 WARN  [RpcServer.handler=36,port=60020] ipc.RpcServer: RpcServer.handler=36,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-07-01 16:05:34,876 WARN  [RpcServer.handler=5,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":17897,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.58:34288","starttimems":1404255916978,"queuetimems":0,"class":"HRegionServer","responsesize":18598,"method":"Multi"}
2014-07-01 16:05:34,876 WARN  [RpcServer.handler=1,port=60020] ipc.RpcServer: RpcServer.respondercallId: 6670 service: ClientService methodName: Multi size: 3.3m connection: 9.1.143.58:34444: output error
2014-07-01 16:05:34,876 WARN  [RpcServer.handler=1,port=60020] ipc.RpcServer: RpcServer.handler=1,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-07-01 16:05:34,877 WARN  [RpcServer.handler=5,port=60020] ipc.RpcServer: RpcServer.respondercallId: 6591 service: ClientService methodName: Multi size: 3.3m connection: 9.1.143.58:34288: output error
2014-07-01 16:05:34,877 WARN  [RpcServer.handler=5,port=60020] ipc.RpcServer: RpcServer.handler=5,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-07-01 16:05:34,913 WARN  [RpcServer.handler=23,port=60020] ipc.RpcServer: RpcServer.respondercallId: 6690 service: ClientService methodName: Multi size: 2.7m connection: 9.1.143.58:34445: output error
2014-07-01 16:05:34,914 WARN  [RpcServer.handler=23,port=60020] ipc.RpcServer: RpcServer.handler=23,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-07-01 16:05:35,012 WARN  [RpcServer.handler=11,port=60020] ipc.RpcServer: RpcServer.respondercallId: 6592 service: ClientService methodName: Multi size: 3.3m connection: 9.1.143.58:34288: output error
2014-07-01 16:05:35,013 WARN  [RpcServer.handler=11,port=60020] ipc.RpcServer: RpcServer.handler=11,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-07-01 16:05:35,110 WARN  [RpcServer.handler=6,port=60020] ipc.RpcServer: RpcServer.respondercallId: 6598 service: ClientService methodName: Multi size: 2.7m connection: 9.1.143.58:34288: output error
2014-07-01 16:05:35,111 WARN  [RpcServer.handler=6,port=60020] ipc.RpcServer: RpcServer.handler=6,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-07-01 16:05:37,346 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-01 16:05:37,363 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 8583 synced till here 8580
2014-07-01 16:05:37,393 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1404255227840/sceplus-vm48.almaden.ibm.com%2C60020%2C1404255227840.1404255934664 with entries=73, filesize=63.4m; new WAL /hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1404255227840/sceplus-vm48.almaden.ibm.com%2C60020%2C1404255227840.1404255937347
2014-07-01 16:05:39,779 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-01 16:05:39,834 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 8671 synced till here 8668
2014-07-01 16:05:39,916 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1404255227840/sceplus-vm48.almaden.ibm.com%2C60020%2C1404255227840.1404255937347 with entries=88, filesize=66.8m; new WAL /hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1404255227840/sceplus-vm48.almaden.ibm.com%2C60020%2C1404255227840.1404255939780
2014-07-01 16:05:42,543 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-01 16:05:42,570 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 8759 synced till here 8747
2014-07-01 16:05:42,699 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1404255227840/sceplus-vm48.almaden.ibm.com%2C60020%2C1404255227840.1404255939780 with entries=88, filesize=81.7m; new WAL /hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1404255227840/sceplus-vm48.almaden.ibm.com%2C60020%2C1404255227840.1404255942543
2014-07-01 16:05:44,542 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-01 16:05:44,566 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 8841 synced till here 8836
2014-07-01 16:05:44,601 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1404255227840/sceplus-vm48.almaden.ibm.com%2C60020%2C1404255227840.1404255942543 with entries=82, filesize=65.8m; new WAL /hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1404255227840/sceplus-vm48.almaden.ibm.com%2C60020%2C1404255227840.1404255944543
2014-07-01 16:05:44,888 DEBUG [regionserver60020-smallCompactions-1404255751957] regionserver.HRegionFileSystem: Committing store file hdfs://master:54310/hbase/data/default/usertable/9e9a6a0fd8aef92a09081d29900a50f3/.tmp/41114ac488ec418c96706a7d6dbfda7b as hdfs://master:54310/hbase/data/default/usertable/9e9a6a0fd8aef92a09081d29900a50f3/family/41114ac488ec418c96706a7d6dbfda7b
2014-07-01 16:05:45,881 DEBUG [regionserver60020-smallCompactions-1404255751957] regionserver.HStore: Removing store files after compaction...
2014-07-01 16:05:45,895 DEBUG [regionserver60020-smallCompactions-1404255751957] backup.HFileArchiver: Finished archiving from class org.apache.hadoop.hbase.backup.HFileArchiver$FileableStoreFile, file:hdfs://master:54310/hbase/data/default/usertable/9e9a6a0fd8aef92a09081d29900a50f3/family/8a963e9c9b4c494aac7d936388246b2b, to hdfs://master:54310/hbase/archive/data/default/usertable/9e9a6a0fd8aef92a09081d29900a50f3/family/8a963e9c9b4c494aac7d936388246b2b
2014-07-01 16:05:45,898 DEBUG [regionserver60020-smallCompactions-1404255751957] backup.HFileArchiver: Finished archiving from class org.apache.hadoop.hbase.backup.HFileArchiver$FileableStoreFile, file:hdfs://master:54310/hbase/data/default/usertable/9e9a6a0fd8aef92a09081d29900a50f3/family/ac57ec120e374baab0221e35e5bfb625, to hdfs://master:54310/hbase/archive/data/default/usertable/9e9a6a0fd8aef92a09081d29900a50f3/family/ac57ec120e374baab0221e35e5bfb625
2014-07-01 16:05:45,902 DEBUG [regionserver60020-smallCompactions-1404255751957] backup.HFileArchiver: Finished archiving from class org.apache.hadoop.hbase.backup.HFileArchiver$FileableStoreFile, file:hdfs://master:54310/hbase/data/default/usertable/9e9a6a0fd8aef92a09081d29900a50f3/family/8e15e7b24e714917923038d3a8a41661, to hdfs://master:54310/hbase/archive/data/default/usertable/9e9a6a0fd8aef92a09081d29900a50f3/family/8e15e7b24e714917923038d3a8a41661
2014-07-01 16:05:45,910 DEBUG [regionserver60020-smallCompactions-1404255751957] backup.HFileArchiver: Finished archiving from class org.apache.hadoop.hbase.backup.HFileArchiver$FileableStoreFile, file:hdfs://master:54310/hbase/data/default/usertable/9e9a6a0fd8aef92a09081d29900a50f3/family/74710d70237240e5ae756722f1520961, to hdfs://master:54310/hbase/archive/data/default/usertable/9e9a6a0fd8aef92a09081d29900a50f3/family/74710d70237240e5ae756722f1520961
2014-07-01 16:05:45,911 INFO  [regionserver60020-smallCompactions-1404255751957] regionserver.HStore: Completed major compaction of 4 file(s) in family of usertable,user8,1404255616195.9e9a6a0fd8aef92a09081d29900a50f3. into 41114ac488ec418c96706a7d6dbfda7b(size=185.1m), total size for store is 299.2m. This selection was in queue for 0sec, and took 1mins, 28sec to execute.
2014-07-01 16:05:45,911 INFO  [regionserver60020-smallCompactions-1404255751957] regionserver.CompactSplitThread: Completed compaction: Request = regionName=usertable,user8,1404255616195.9e9a6a0fd8aef92a09081d29900a50f3., storeName=family, fileCount=4, fileSize=208.6m, priority=16, time=75999933968678; duration=1mins, 28sec
2014-07-01 16:05:45,911 DEBUG [regionserver60020-smallCompactions-1404255751957] regionserver.CompactSplitThread: CompactSplitThread Status: compaction_queue=(0:6), split_queue=0, merge_queue=0
2014-07-01 16:05:45,911 DEBUG [regionserver60020-smallCompactions-1404255751957] compactions.RatioBasedCompactionPolicy: Selecting compaction from 4 store files, 0 compacting, 4 eligible, 20 blocking
2014-07-01 16:05:45,912 DEBUG [regionserver60020-smallCompactions-1404255751957] compactions.ExploringCompactionPolicy: Exploring compaction algorithm has selected 4 files of size 339495688 starting at candidate #0 after considering 3 permutations with 3 in ratio
2014-07-01 16:05:45,912 DEBUG [regionserver60020-smallCompactions-1404255751957] regionserver.HStore: 510634b895ee706ff306728824babf96 - family: Initiating major compaction
2014-07-01 16:05:45,912 INFO  [regionserver60020-smallCompactions-1404255751957] regionserver.HRegion: Starting compaction on family in region usertable,,1404255616194.510634b895ee706ff306728824babf96.
2014-07-01 16:05:45,912 INFO  [regionserver60020-smallCompactions-1404255751957] regionserver.HStore: Starting compaction of 4 file(s) in family of usertable,,1404255616194.510634b895ee706ff306728824babf96. into tmpdir=hdfs://master:54310/hbase/data/default/usertable/510634b895ee706ff306728824babf96/.tmp, totalSize=323.8m
2014-07-01 16:05:45,913 DEBUG [regionserver60020-smallCompactions-1404255751957] compactions.Compactor: Compacting hdfs://master:54310/hbase/data/default/usertable/510634b895ee706ff306728824babf96/family/f66b6ec5f49d4d8fb94e77677a7c9532, keycount=92231, bloomtype=ROW, size=65.7m, encoding=NONE, seqNum=488, earliestPutTs=1404255694239
2014-07-01 16:05:45,913 DEBUG [regionserver60020-smallCompactions-1404255751957] compactions.Compactor: Compacting hdfs://master:54310/hbase/data/default/usertable/510634b895ee706ff306728824babf96/family/4eead3f5ffef4c37bd398c66ece498b7, keycount=88708, bloomtype=ROW, size=63.2m, encoding=NONE, seqNum=637, earliestPutTs=1404255743329
2014-07-01 16:05:45,913 DEBUG [regionserver60020-smallCompactions-1404255751957] compactions.Compactor: Compacting hdfs://master:54310/hbase/data/default/usertable/510634b895ee706ff306728824babf96/family/af73009e14a640c3a4f5b5a099fc66af, keycount=158207, bloomtype=ROW, size=112.6m, encoding=NONE, seqNum=903, earliestPutTs=1404255770235
2014-07-01 16:05:45,913 DEBUG [regionserver60020-smallCompactions-1404255751957] compactions.Compactor: Compacting hdfs://master:54310/hbase/data/default/usertable/510634b895ee706ff306728824babf96/family/7aafde38cd1d49a79297ea4ca205a621, keycount=115448, bloomtype=ROW, size=82.2m, encoding=NONE, seqNum=1161, earliestPutTs=1404255818872
2014-07-01 16:05:46,004 DEBUG [regionserver60020-smallCompactions-1404255751957] util.FSUtils: DFS Client does not support most favored nodes create; using default create
2014-07-01 16:05:46,457 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-01 16:05:46,539 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 8922 synced till here 8913
2014-07-01 16:05:46,647 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1404255227840/sceplus-vm48.almaden.ibm.com%2C60020%2C1404255227840.1404255944543 with entries=81, filesize=71.6m; new WAL /hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1404255227840/sceplus-vm48.almaden.ibm.com%2C60020%2C1404255227840.1404255946458
2014-07-01 16:05:46,886 INFO  [MemStoreFlusher.0] regionserver.DefaultStoreFlusher: Flushed, sequenceid=1350, memsize=297.6m, hasBloomFilter=true, into tmp file hdfs://master:54310/hbase/data/default/usertable/3e76136f0a8be94ccf3637676c064402/.tmp/c6582863f548409586f782a8d6933006
2014-07-01 16:05:46,905 DEBUG [MemStoreFlusher.0] regionserver.HRegionFileSystem: Committing store file hdfs://master:54310/hbase/data/default/usertable/3e76136f0a8be94ccf3637676c064402/.tmp/c6582863f548409586f782a8d6933006 as hdfs://master:54310/hbase/data/default/usertable/3e76136f0a8be94ccf3637676c064402/family/c6582863f548409586f782a8d6933006
2014-07-01 16:05:46,921 INFO  [MemStoreFlusher.0] regionserver.HStore: Added hdfs://master:54310/hbase/data/default/usertable/3e76136f0a8be94ccf3637676c064402/family/c6582863f548409586f782a8d6933006, entries=1083430, sequenceid=1350, filesize=77.1m
2014-07-01 16:05:46,921 INFO  [MemStoreFlusher.0] regionserver.HRegion: Finished memstore flush of ~578.5m/606640720, currentsize=327.9m/343788720 for region usertable,user5,1404255616194.3e76136f0a8be94ccf3637676c064402. in 45753ms, sequenceid=1350, compaction requested=true
2014-07-01 16:05:46,921 DEBUG [MemStoreFlusher.0] regionserver.CompactSplitThread: Small Compaction requested: system; Because: MemStoreFlusher.0; compaction_queue=(0:6), split_queue=0, merge_queue=0
2014-07-01 16:05:46,921 DEBUG [MemStoreFlusher.0] regionserver.HRegion: Started memstore flush for usertable,user7,1404255616194.431aad56a67499b48936eca121e3178e., current region memstore size 897.7m
2014-07-01 16:05:46,928 DEBUG [RpcServer.handler=33,port=60020] regionserver.HRegion: Flush requested on usertable,user5,1404255616194.3e76136f0a8be94ccf3637676c064402.
2014-07-01 16:05:48,913 DEBUG [MemStoreFlusher.0] util.FSUtils: DFS Client does not support most favored nodes create; using default create
2014-07-01 16:05:50,049 INFO  [MemStoreFlusher.1] regionserver.DefaultStoreFlusher: Flushed, sequenceid=1333, memsize=395.8m, hasBloomFilter=true, into tmp file hdfs://master:54310/hbase/data/default/usertable/09108ac86cf3c374e79d1bb78f787b49/.tmp/1aaaed3484934af4972b2511e5e1a501
2014-07-01 16:05:50,061 DEBUG [MemStoreFlusher.1] regionserver.HRegionFileSystem: Committing store file hdfs://master:54310/hbase/data/default/usertable/09108ac86cf3c374e79d1bb78f787b49/.tmp/1aaaed3484934af4972b2511e5e1a501 as hdfs://master:54310/hbase/data/default/usertable/09108ac86cf3c374e79d1bb78f787b49/family/1aaaed3484934af4972b2511e5e1a501
2014-07-01 16:05:50,069 INFO  [MemStoreFlusher.1] regionserver.HStore: Added hdfs://master:54310/hbase/data/default/usertable/09108ac86cf3c374e79d1bb78f787b49/family/1aaaed3484934af4972b2511e5e1a501, entries=1440930, sequenceid=1333, filesize=102.6m
2014-07-01 16:05:50,070 INFO  [MemStoreFlusher.1] regionserver.HRegion: Finished memstore flush of ~675.0m/707790080, currentsize=357.8m/375201760 for region usertable,user3,1404255616194.09108ac86cf3c374e79d1bb78f787b49. in 52773ms, sequenceid=1333, compaction requested=true
2014-07-01 16:05:50,070 DEBUG [MemStoreFlusher.1] regionserver.CompactSplitThread: Small Compaction requested: system; Because: MemStoreFlusher.1; compaction_queue=(0:7), split_queue=0, merge_queue=0
2014-07-01 16:05:50,070 DEBUG [MemStoreFlusher.1] regionserver.HRegion: Started memstore flush for usertable,,1404255616194.510634b895ee706ff306728824babf96., current region memstore size 654.0m
2014-07-01 16:05:50,581 DEBUG [MemStoreFlusher.1] util.FSUtils: DFS Client does not support most favored nodes create; using default create
2014-07-01 16:05:52,309 DEBUG [RpcServer.handler=26,port=60020] regionserver.HRegion: Flush requested on usertable,user3,1404255616194.09108ac86cf3c374e79d1bb78f787b49.
2014-07-01 16:05:52,992 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-01 16:05:53,024 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 9005 synced till here 8993
2014-07-01 16:05:53,136 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1404255227840/sceplus-vm48.almaden.ibm.com%2C60020%2C1404255227840.1404255946458 with entries=83, filesize=72.9m; new WAL /hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1404255227840/sceplus-vm48.almaden.ibm.com%2C60020%2C1404255227840.1404255952992
2014-07-01 16:05:53,137 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1404255227840/sceplus-vm48.almaden.ibm.com%2C60020%2C1404255227840.1404255820996
2014-07-01 16:05:53,137 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1404255227840/sceplus-vm48.almaden.ibm.com%2C60020%2C1404255227840.1404255823907
2014-07-01 16:05:53,137 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1404255227840/sceplus-vm48.almaden.ibm.com%2C60020%2C1404255227840.1404255825960
2014-07-01 16:05:53,137 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1404255227840/sceplus-vm48.almaden.ibm.com%2C60020%2C1404255227840.1404255828506
2014-07-01 16:05:53,137 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1404255227840/sceplus-vm48.almaden.ibm.com%2C60020%2C1404255227840.1404255830620
2014-07-01 16:05:54,525 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-01 16:05:54,555 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 9082 synced till here 9074
2014-07-01 16:05:54,583 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1404255227840/sceplus-vm48.almaden.ibm.com%2C60020%2C1404255227840.1404255952992 with entries=77, filesize=66.3m; new WAL /hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1404255227840/sceplus-vm48.almaden.ibm.com%2C60020%2C1404255227840.1404255954526
2014-07-01 16:05:56,315 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-01 16:05:56,372 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 9178 synced till here 9161
2014-07-01 16:05:56,454 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1404255227840/sceplus-vm48.almaden.ibm.com%2C60020%2C1404255227840.1404255954526 with entries=96, filesize=81.1m; new WAL /hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1404255227840/sceplus-vm48.almaden.ibm.com%2C60020%2C1404255227840.1404255956316
2014-07-01 16:05:58,067 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-01 16:05:58,131 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 9265 synced till here 9254
2014-07-01 16:05:58,235 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1404255227840/sceplus-vm48.almaden.ibm.com%2C60020%2C1404255227840.1404255956316 with entries=87, filesize=73.2m; new WAL /hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1404255227840/sceplus-vm48.almaden.ibm.com%2C60020%2C1404255227840.1404255958068
2014-07-01 16:05:59,894 INFO  [JvmPauseMonitor] util.JvmPauseMonitor: Detected pause in JVM or host machine (eg GC): pause of approximately 1023ms
GC pool 'ParNew' had collection(s): count=1 time=1055ms
2014-07-01 16:06:00,029 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-01 16:06:00,046 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 9358 synced till here 9350
2014-07-01 16:06:00,177 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1404255227840/sceplus-vm48.almaden.ibm.com%2C60020%2C1404255227840.1404255958068 with entries=93, filesize=70.2m; new WAL /hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1404255227840/sceplus-vm48.almaden.ibm.com%2C60020%2C1404255227840.1404255960029
2014-07-01 16:06:01,491 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-01 16:06:01,554 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 9447 synced till here 9441
2014-07-01 16:06:01,627 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1404255227840/sceplus-vm48.almaden.ibm.com%2C60020%2C1404255227840.1404255960029 with entries=89, filesize=68.1m; new WAL /hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1404255227840/sceplus-vm48.almaden.ibm.com%2C60020%2C1404255227840.1404255961492
2014-07-01 16:06:03,092 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-01 16:06:03,115 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 9536 synced till here 9533
2014-07-01 16:06:03,221 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1404255227840/sceplus-vm48.almaden.ibm.com%2C60020%2C1404255227840.1404255961492 with entries=89, filesize=66.8m; new WAL /hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1404255227840/sceplus-vm48.almaden.ibm.com%2C60020%2C1404255227840.1404255963093
2014-07-01 16:06:05,070 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-01 16:06:05,141 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 9615 synced till here 9604
2014-07-01 16:06:05,638 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1404255227840/sceplus-vm48.almaden.ibm.com%2C60020%2C1404255227840.1404255963093 with entries=79, filesize=71.0m; new WAL /hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1404255227840/sceplus-vm48.almaden.ibm.com%2C60020%2C1404255227840.1404255965071
2014-07-01 16:06:07,086 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-01 16:06:07,147 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 9705 synced till here 9699
2014-07-01 16:06:07,271 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1404255227840/sceplus-vm48.almaden.ibm.com%2C60020%2C1404255227840.1404255965071 with entries=90, filesize=69.6m; new WAL /hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1404255227840/sceplus-vm48.almaden.ibm.com%2C60020%2C1404255227840.1404255967086
2014-07-01 16:06:08,887 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-01 16:06:08,930 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 9782 synced till here 9773
2014-07-01 16:06:09,018 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1404255227840/sceplus-vm48.almaden.ibm.com%2C60020%2C1404255227840.1404255967086 with entries=77, filesize=68.3m; new WAL /hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1404255227840/sceplus-vm48.almaden.ibm.com%2C60020%2C1404255227840.1404255968888
2014-07-01 16:06:09,577 INFO  [RpcServer.handler=29,port=60020] regionserver.MemStoreFlusher: Blocking updates on sceplus-vm48.almaden.ibm.com,60020,1404255227840: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-01 16:06:09,577 INFO  [RpcServer.handler=5,port=60020] regionserver.MemStoreFlusher: Blocking updates on sceplus-vm48.almaden.ibm.com,60020,1404255227840: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-01 16:06:09,588 INFO  [RpcServer.handler=21,port=60020] regionserver.MemStoreFlusher: Blocking updates on sceplus-vm48.almaden.ibm.com,60020,1404255227840: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-01 16:06:09,601 INFO  [RpcServer.handler=10,port=60020] regionserver.MemStoreFlusher: Blocking updates on sceplus-vm48.almaden.ibm.com,60020,1404255227840: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-01 16:06:09,625 INFO  [RpcServer.handler=36,port=60020] regionserver.MemStoreFlusher: Blocking updates on sceplus-vm48.almaden.ibm.com,60020,1404255227840: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-01 16:06:09,625 INFO  [RpcServer.handler=1,port=60020] regionserver.MemStoreFlusher: Blocking updates on sceplus-vm48.almaden.ibm.com,60020,1404255227840: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-01 16:06:09,628 INFO  [RpcServer.handler=13,port=60020] regionserver.MemStoreFlusher: Blocking updates on sceplus-vm48.almaden.ibm.com,60020,1404255227840: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-01 16:06:09,645 INFO  [RpcServer.handler=22,port=60020] regionserver.MemStoreFlusher: Blocking updates on sceplus-vm48.almaden.ibm.com,60020,1404255227840: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-01 16:06:09,647 INFO  [RpcServer.handler=30,port=60020] regionserver.MemStoreFlusher: Blocking updates on sceplus-vm48.almaden.ibm.com,60020,1404255227840: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-01 16:06:09,647 INFO  [RpcServer.handler=12,port=60020] regionserver.MemStoreFlusher: Blocking updates on sceplus-vm48.almaden.ibm.com,60020,1404255227840: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-01 16:06:09,648 INFO  [RpcServer.handler=33,port=60020] regionserver.MemStoreFlusher: Blocking updates on sceplus-vm48.almaden.ibm.com,60020,1404255227840: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-01 16:06:09,653 INFO  [RpcServer.handler=0,port=60020] regionserver.MemStoreFlusher: Blocking updates on sceplus-vm48.almaden.ibm.com,60020,1404255227840: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-01 16:06:09,661 INFO  [RpcServer.handler=4,port=60020] regionserver.MemStoreFlusher: Blocking updates on sceplus-vm48.almaden.ibm.com,60020,1404255227840: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-01 16:06:09,663 INFO  [RpcServer.handler=39,port=60020] regionserver.MemStoreFlusher: Blocking updates on sceplus-vm48.almaden.ibm.com,60020,1404255227840: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-01 16:06:09,665 INFO  [RpcServer.handler=38,port=60020] regionserver.MemStoreFlusher: Blocking updates on sceplus-vm48.almaden.ibm.com,60020,1404255227840: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-01 16:06:09,667 INFO  [RpcServer.handler=35,port=60020] regionserver.MemStoreFlusher: Blocking updates on sceplus-vm48.almaden.ibm.com,60020,1404255227840: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-01 16:06:09,668 INFO  [RpcServer.handler=37,port=60020] regionserver.MemStoreFlusher: Blocking updates on sceplus-vm48.almaden.ibm.com,60020,1404255227840: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-01 16:06:09,669 INFO  [RpcServer.handler=49,port=60020] regionserver.MemStoreFlusher: Blocking updates on sceplus-vm48.almaden.ibm.com,60020,1404255227840: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-01 16:06:09,693 INFO  [RpcServer.handler=40,port=60020] regionserver.MemStoreFlusher: Blocking updates on sceplus-vm48.almaden.ibm.com,60020,1404255227840: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-01 16:06:09,698 INFO  [RpcServer.handler=41,port=60020] regionserver.MemStoreFlusher: Blocking updates on sceplus-vm48.almaden.ibm.com,60020,1404255227840: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-01 16:06:09,736 INFO  [RpcServer.handler=16,port=60020] regionserver.MemStoreFlusher: Blocking updates on sceplus-vm48.almaden.ibm.com,60020,1404255227840: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-01 16:06:09,872 INFO  [RpcServer.handler=15,port=60020] regionserver.MemStoreFlusher: Blocking updates on sceplus-vm48.almaden.ibm.com,60020,1404255227840: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-01 16:06:10,379 INFO  [MemStoreFlusher.1] regionserver.DefaultStoreFlusher: Flushed, sequenceid=1548, memsize=330.4m, hasBloomFilter=true, into tmp file hdfs://master:54310/hbase/data/default/usertable/510634b895ee706ff306728824babf96/.tmp/b4f220a200b543afba21ccef263dc906
2014-07-01 16:06:10,399 DEBUG [MemStoreFlusher.1] regionserver.HRegionFileSystem: Committing store file hdfs://master:54310/hbase/data/default/usertable/510634b895ee706ff306728824babf96/.tmp/b4f220a200b543afba21ccef263dc906 as hdfs://master:54310/hbase/data/default/usertable/510634b895ee706ff306728824babf96/family/b4f220a200b543afba21ccef263dc906
2014-07-01 16:06:10,416 INFO  [MemStoreFlusher.1] regionserver.HStore: Added hdfs://master:54310/hbase/data/default/usertable/510634b895ee706ff306728824babf96/family/b4f220a200b543afba21ccef263dc906, entries=1202940, sequenceid=1548, filesize=85.6m
2014-07-01 16:06:10,417 INFO  [MemStoreFlusher.1] regionserver.HRegion: Finished memstore flush of ~654.0m/685774720, currentsize=281.8m/295486160 for region usertable,,1404255616194.510634b895ee706ff306728824babf96. in 20347ms, sequenceid=1548, compaction requested=false
2014-07-01 16:06:10,417 WARN  [RpcServer.handler=15,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 545ms
2014-07-01 16:06:10,417 INFO  [RpcServer.handler=15,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server sceplus-vm48.almaden.ibm.com,60020,1404255227840
2014-07-01 16:06:10,418 WARN  [RpcServer.handler=16,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 682ms
2014-07-01 16:06:10,418 INFO  [RpcServer.handler=16,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server sceplus-vm48.almaden.ibm.com,60020,1404255227840
2014-07-01 16:06:10,418 WARN  [RpcServer.handler=41,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 720ms
2014-07-01 16:06:10,418 INFO  [RpcServer.handler=41,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server sceplus-vm48.almaden.ibm.com,60020,1404255227840
2014-07-01 16:06:10,422 WARN  [RpcServer.handler=40,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 729ms
2014-07-01 16:06:10,422 INFO  [RpcServer.handler=40,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server sceplus-vm48.almaden.ibm.com,60020,1404255227840
2014-07-01 16:06:10,426 WARN  [RpcServer.handler=49,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 757ms
2014-07-01 16:06:10,426 INFO  [RpcServer.handler=49,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server sceplus-vm48.almaden.ibm.com,60020,1404255227840
2014-07-01 16:06:10,432 WARN  [RpcServer.handler=37,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 764ms
2014-07-01 16:06:10,432 INFO  [RpcServer.handler=37,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server sceplus-vm48.almaden.ibm.com,60020,1404255227840
2014-07-01 16:06:10,434 WARN  [RpcServer.handler=35,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 767ms
2014-07-01 16:06:10,434 INFO  [RpcServer.handler=35,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server sceplus-vm48.almaden.ibm.com,60020,1404255227840
2014-07-01 16:06:10,435 DEBUG [MemStoreFlusher.1] regionserver.HRegion: Started memstore flush for usertable,user8,1404255616195.9e9a6a0fd8aef92a09081d29900a50f3., current region memstore size 710.6m
2014-07-01 16:06:10,438 WARN  [RpcServer.handler=38,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 773ms
2014-07-01 16:06:10,438 INFO  [RpcServer.handler=38,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server sceplus-vm48.almaden.ibm.com,60020,1404255227840
2014-07-01 16:06:10,442 WARN  [RpcServer.handler=39,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 779ms
2014-07-01 16:06:10,442 INFO  [RpcServer.handler=39,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server sceplus-vm48.almaden.ibm.com,60020,1404255227840
2014-07-01 16:06:10,446 WARN  [RpcServer.handler=4,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 785ms
2014-07-01 16:06:10,446 INFO  [RpcServer.handler=4,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server sceplus-vm48.almaden.ibm.com,60020,1404255227840
2014-07-01 16:06:10,447 WARN  [RpcServer.handler=0,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 794ms
2014-07-01 16:06:10,447 INFO  [RpcServer.handler=0,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server sceplus-vm48.almaden.ibm.com,60020,1404255227840
2014-07-01 16:06:10,447 WARN  [RpcServer.handler=33,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 799ms
2014-07-01 16:06:10,447 INFO  [RpcServer.handler=33,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server sceplus-vm48.almaden.ibm.com,60020,1404255227840
2014-07-01 16:06:10,450 WARN  [RpcServer.handler=12,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 803ms
2014-07-01 16:06:10,450 INFO  [RpcServer.handler=12,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server sceplus-vm48.almaden.ibm.com,60020,1404255227840
2014-07-01 16:06:10,450 WARN  [RpcServer.handler=30,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 803ms
2014-07-01 16:06:10,451 INFO  [RpcServer.handler=30,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server sceplus-vm48.almaden.ibm.com,60020,1404255227840
2014-07-01 16:06:10,454 WARN  [RpcServer.handler=22,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 809ms
2014-07-01 16:06:10,454 INFO  [RpcServer.handler=22,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server sceplus-vm48.almaden.ibm.com,60020,1404255227840
2014-07-01 16:06:10,462 WARN  [RpcServer.handler=13,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 834ms
2014-07-01 16:06:10,462 INFO  [RpcServer.handler=13,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server sceplus-vm48.almaden.ibm.com,60020,1404255227840
2014-07-01 16:06:10,466 WARN  [RpcServer.handler=1,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 841ms
2014-07-01 16:06:10,466 INFO  [RpcServer.handler=1,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server sceplus-vm48.almaden.ibm.com,60020,1404255227840
2014-07-01 16:06:10,470 WARN  [RpcServer.handler=36,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 845ms
2014-07-01 16:06:10,470 INFO  [RpcServer.handler=36,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server sceplus-vm48.almaden.ibm.com,60020,1404255227840
2014-07-01 16:06:10,470 WARN  [RpcServer.handler=10,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 869ms
2014-07-01 16:06:10,471 INFO  [RpcServer.handler=10,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server sceplus-vm48.almaden.ibm.com,60020,1404255227840
2014-07-01 16:06:10,474 WARN  [RpcServer.handler=21,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 886ms
2014-07-01 16:06:10,474 INFO  [RpcServer.handler=21,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server sceplus-vm48.almaden.ibm.com,60020,1404255227840
2014-07-01 16:06:10,476 WARN  [RpcServer.handler=5,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 899ms
2014-07-01 16:06:10,476 INFO  [RpcServer.handler=5,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server sceplus-vm48.almaden.ibm.com,60020,1404255227840
2014-07-01 16:06:10,486 WARN  [RpcServer.handler=29,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 909ms
2014-07-01 16:06:10,486 INFO  [RpcServer.handler=29,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server sceplus-vm48.almaden.ibm.com,60020,1404255227840
2014-07-01 16:06:11,228 DEBUG [RpcServer.handler=16,port=60020] regionserver.HRegion: Flush requested on usertable,,1404255616194.510634b895ee706ff306728824babf96.
2014-07-01 16:06:11,399 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-01 16:06:11,529 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 9883 synced till here 9870
2014-07-01 16:06:11,788 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1404255227840/sceplus-vm48.almaden.ibm.com%2C60020%2C1404255227840.1404255968888 with entries=101, filesize=82.1m; new WAL /hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1404255227840/sceplus-vm48.almaden.ibm.com%2C60020%2C1404255227840.1404255971400
2014-07-01 16:06:12,099 DEBUG [MemStoreFlusher.1] util.FSUtils: DFS Client does not support most favored nodes create; using default create
2014-07-01 16:06:13,441 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-01 16:06:13,486 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 9970 synced till here 9958
2014-07-01 16:06:13,583 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1404255227840/sceplus-vm48.almaden.ibm.com%2C60020%2C1404255227840.1404255971400 with entries=87, filesize=72.4m; new WAL /hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1404255227840/sceplus-vm48.almaden.ibm.com%2C60020%2C1404255227840.1404255973441
2014-07-01 16:06:15,281 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-01 16:06:15,333 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 10058 synced till here 10055
2014-07-01 16:06:15,375 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1404255227840/sceplus-vm48.almaden.ibm.com%2C60020%2C1404255227840.1404255973441 with entries=88, filesize=78.4m; new WAL /hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1404255227840/sceplus-vm48.almaden.ibm.com%2C60020%2C1404255227840.1404255975282
2014-07-01 16:06:16,947 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-01 16:06:17,382 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 10174 synced till here 10161
2014-07-01 16:06:17,500 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1404255227840/sceplus-vm48.almaden.ibm.com%2C60020%2C1404255227840.1404255975282 with entries=116, filesize=97.7m; new WAL /hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1404255227840/sceplus-vm48.almaden.ibm.com%2C60020%2C1404255227840.1404255976948
2014-07-01 16:06:20,241 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-01 16:06:20,246 INFO  [RpcServer.handler=15,port=60020] regionserver.MemStoreFlusher: Blocking updates on sceplus-vm48.almaden.ibm.com,60020,1404255227840: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-01 16:06:20,247 INFO  [RpcServer.handler=19,port=60020] regionserver.MemStoreFlusher: Blocking updates on sceplus-vm48.almaden.ibm.com,60020,1404255227840: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-01 16:06:20,249 INFO  [RpcServer.handler=1,port=60020] regionserver.MemStoreFlusher: Blocking updates on sceplus-vm48.almaden.ibm.com,60020,1404255227840: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-01 16:06:20,249 INFO  [RpcServer.handler=24,port=60020] regionserver.MemStoreFlusher: Blocking updates on sceplus-vm48.almaden.ibm.com,60020,1404255227840: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-01 16:06:20,249 INFO  [RpcServer.handler=32,port=60020] regionserver.MemStoreFlusher: Blocking updates on sceplus-vm48.almaden.ibm.com,60020,1404255227840: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-01 16:06:20,249 INFO  [RpcServer.handler=41,port=60020] regionserver.MemStoreFlusher: Blocking updates on sceplus-vm48.almaden.ibm.com,60020,1404255227840: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-01 16:06:20,249 INFO  [RpcServer.handler=34,port=60020] regionserver.MemStoreFlusher: Blocking updates on sceplus-vm48.almaden.ibm.com,60020,1404255227840: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-01 16:06:20,250 INFO  [RpcServer.handler=43,port=60020] regionserver.MemStoreFlusher: Blocking updates on sceplus-vm48.almaden.ibm.com,60020,1404255227840: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-01 16:06:20,250 INFO  [RpcServer.handler=45,port=60020] regionserver.MemStoreFlusher: Blocking updates on sceplus-vm48.almaden.ibm.com,60020,1404255227840: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-01 16:06:20,251 INFO  [RpcServer.handler=35,port=60020] regionserver.MemStoreFlusher: Blocking updates on sceplus-vm48.almaden.ibm.com,60020,1404255227840: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-01 16:06:20,251 INFO  [RpcServer.handler=9,port=60020] regionserver.MemStoreFlusher: Blocking updates on sceplus-vm48.almaden.ibm.com,60020,1404255227840: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-01 16:06:20,263 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 10280 synced till here 10264
2014-07-01 16:06:21,395 INFO  [RpcServer.handler=0,port=60020] regionserver.MemStoreFlusher: Blocking updates on sceplus-vm48.almaden.ibm.com,60020,1404255227840: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-01 16:06:21,397 INFO  [RpcServer.handler=17,port=60020] regionserver.MemStoreFlusher: Blocking updates on sceplus-vm48.almaden.ibm.com,60020,1404255227840: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-01 16:06:21,398 INFO  [RpcServer.handler=2,port=60020] regionserver.MemStoreFlusher: Blocking updates on sceplus-vm48.almaden.ibm.com,60020,1404255227840: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-01 16:06:21,399 INFO  [RpcServer.handler=13,port=60020] regionserver.MemStoreFlusher: Blocking updates on sceplus-vm48.almaden.ibm.com,60020,1404255227840: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-01 16:06:21,400 INFO  [RpcServer.handler=33,port=60020] regionserver.MemStoreFlusher: Blocking updates on sceplus-vm48.almaden.ibm.com,60020,1404255227840: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-01 16:06:21,401 INFO  [RpcServer.handler=46,port=60020] regionserver.MemStoreFlusher: Blocking updates on sceplus-vm48.almaden.ibm.com,60020,1404255227840: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-01 16:06:21,401 INFO  [RpcServer.handler=4,port=60020] regionserver.MemStoreFlusher: Blocking updates on sceplus-vm48.almaden.ibm.com,60020,1404255227840: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-01 16:06:21,401 INFO  [RpcServer.handler=38,port=60020] regionserver.MemStoreFlusher: Blocking updates on sceplus-vm48.almaden.ibm.com,60020,1404255227840: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-01 16:06:21,401 INFO  [RpcServer.handler=28,port=60020] regionserver.MemStoreFlusher: Blocking updates on sceplus-vm48.almaden.ibm.com,60020,1404255227840: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-01 16:06:21,402 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1404255227840/sceplus-vm48.almaden.ibm.com%2C60020%2C1404255227840.1404255976948 with entries=106, filesize=86.2m; new WAL /hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1404255227840/sceplus-vm48.almaden.ibm.com%2C60020%2C1404255227840.1404255980241
2014-07-01 16:06:21,402 INFO  [RpcServer.handler=6,port=60020] regionserver.MemStoreFlusher: Blocking updates on sceplus-vm48.almaden.ibm.com,60020,1404255227840: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-01 16:06:21,402 INFO  [RpcServer.handler=36,port=60020] regionserver.MemStoreFlusher: Blocking updates on sceplus-vm48.almaden.ibm.com,60020,1404255227840: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-01 16:06:21,403 INFO  [RpcServer.handler=37,port=60020] regionserver.MemStoreFlusher: Blocking updates on sceplus-vm48.almaden.ibm.com,60020,1404255227840: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-01 16:06:21,413 INFO  [RpcServer.handler=20,port=60020] regionserver.MemStoreFlusher: Blocking updates on sceplus-vm48.almaden.ibm.com,60020,1404255227840: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-01 16:06:21,845 INFO  [MemStoreFlusher.0] regionserver.DefaultStoreFlusher: Flushed, sequenceid=1545, memsize=484.7m, hasBloomFilter=true, into tmp file hdfs://master:54310/hbase/data/default/usertable/431aad56a67499b48936eca121e3178e/.tmp/86e5e86be9ec4cb4b3651e20e4b1f05b
2014-07-01 16:06:21,880 DEBUG [MemStoreFlusher.0] regionserver.HRegionFileSystem: Committing store file hdfs://master:54310/hbase/data/default/usertable/431aad56a67499b48936eca121e3178e/.tmp/86e5e86be9ec4cb4b3651e20e4b1f05b as hdfs://master:54310/hbase/data/default/usertable/431aad56a67499b48936eca121e3178e/family/86e5e86be9ec4cb4b3651e20e4b1f05b
2014-07-01 16:06:21,889 INFO  [MemStoreFlusher.0] regionserver.HStore: Added hdfs://master:54310/hbase/data/default/usertable/431aad56a67499b48936eca121e3178e/family/86e5e86be9ec4cb4b3651e20e4b1f05b, entries=1764880, sequenceid=1545, filesize=125.6m
2014-07-01 16:06:21,889 INFO  [MemStoreFlusher.0] regionserver.HRegion: Finished memstore flush of ~908.9m/953017520, currentsize=408.9m/428713280 for region usertable,user7,1404255616194.431aad56a67499b48936eca121e3178e. in 34968ms, sequenceid=1545, compaction requested=true
2014-07-01 16:06:21,890 DEBUG [MemStoreFlusher.0] regionserver.CompactSplitThread: Small Compaction requested: system; Because: MemStoreFlusher.0; compaction_queue=(0:8), split_queue=0, merge_queue=0
2014-07-01 16:06:21,890 WARN  [RpcServer.handler=20,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 477ms
2014-07-01 16:06:21,890 INFO  [RpcServer.handler=20,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server sceplus-vm48.almaden.ibm.com,60020,1404255227840
2014-07-01 16:06:21,890 WARN  [RpcServer.handler=37,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 487ms
2014-07-01 16:06:21,890 INFO  [RpcServer.handler=37,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server sceplus-vm48.almaden.ibm.com,60020,1404255227840
2014-07-01 16:06:21,890 DEBUG [MemStoreFlusher.0] regionserver.HRegion: Started memstore flush for usertable,user5,1404255616194.3e76136f0a8be94ccf3637676c064402., current region memstore size 744.2m
2014-07-01 16:06:21,890 WARN  [RpcServer.handler=36,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 488ms
2014-07-01 16:06:21,890 INFO  [RpcServer.handler=36,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server sceplus-vm48.almaden.ibm.com,60020,1404255227840
2014-07-01 16:06:21,890 WARN  [RpcServer.handler=6,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 488ms
2014-07-01 16:06:21,891 INFO  [RpcServer.handler=6,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server sceplus-vm48.almaden.ibm.com,60020,1404255227840
2014-07-01 16:06:21,891 WARN  [RpcServer.handler=28,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 490ms
2014-07-01 16:06:21,891 INFO  [RpcServer.handler=28,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server sceplus-vm48.almaden.ibm.com,60020,1404255227840
2014-07-01 16:06:21,891 WARN  [RpcServer.handler=38,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 490ms
2014-07-01 16:06:21,891 INFO  [RpcServer.handler=38,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server sceplus-vm48.almaden.ibm.com,60020,1404255227840
2014-07-01 16:06:21,891 WARN  [RpcServer.handler=4,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 490ms
2014-07-01 16:06:21,891 INFO  [RpcServer.handler=4,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server sceplus-vm48.almaden.ibm.com,60020,1404255227840
2014-07-01 16:06:21,892 WARN  [RpcServer.handler=46,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 491ms
2014-07-01 16:06:21,893 INFO  [RpcServer.handler=46,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server sceplus-vm48.almaden.ibm.com,60020,1404255227840
2014-07-01 16:06:21,893 WARN  [RpcServer.handler=33,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 493ms
2014-07-01 16:06:21,893 INFO  [RpcServer.handler=33,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server sceplus-vm48.almaden.ibm.com,60020,1404255227840
2014-07-01 16:06:21,893 WARN  [RpcServer.handler=13,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 494ms
2014-07-01 16:06:21,893 INFO  [RpcServer.handler=13,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server sceplus-vm48.almaden.ibm.com,60020,1404255227840
2014-07-01 16:06:21,893 WARN  [RpcServer.handler=2,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 496ms
2014-07-01 16:06:21,893 INFO  [RpcServer.handler=2,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server sceplus-vm48.almaden.ibm.com,60020,1404255227840
2014-07-01 16:06:21,894 WARN  [RpcServer.handler=17,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 497ms
2014-07-01 16:06:21,894 INFO  [RpcServer.handler=17,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server sceplus-vm48.almaden.ibm.com,60020,1404255227840
2014-07-01 16:06:21,895 WARN  [RpcServer.handler=0,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 500ms
2014-07-01 16:06:21,895 INFO  [RpcServer.handler=0,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server sceplus-vm48.almaden.ibm.com,60020,1404255227840
2014-07-01 16:06:21,909 WARN  [RpcServer.handler=9,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 1658ms
2014-07-01 16:06:21,909 INFO  [RpcServer.handler=9,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server sceplus-vm48.almaden.ibm.com,60020,1404255227840
2014-07-01 16:06:21,911 WARN  [RpcServer.handler=35,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 1660ms
2014-07-01 16:06:21,911 INFO  [RpcServer.handler=35,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server sceplus-vm48.almaden.ibm.com,60020,1404255227840
2014-07-01 16:06:21,911 WARN  [RpcServer.handler=45,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 1661ms
2014-07-01 16:06:21,911 INFO  [RpcServer.handler=45,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server sceplus-vm48.almaden.ibm.com,60020,1404255227840
2014-07-01 16:06:21,911 WARN  [RpcServer.handler=43,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 1662ms
2014-07-01 16:06:21,911 INFO  [RpcServer.handler=43,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server sceplus-vm48.almaden.ibm.com,60020,1404255227840
2014-07-01 16:06:21,911 WARN  [RpcServer.handler=34,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 1662ms
2014-07-01 16:06:21,911 INFO  [RpcServer.handler=34,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server sceplus-vm48.almaden.ibm.com,60020,1404255227840
2014-07-01 16:06:21,911 WARN  [RpcServer.handler=41,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 1662ms
2014-07-01 16:06:21,912 INFO  [RpcServer.handler=41,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server sceplus-vm48.almaden.ibm.com,60020,1404255227840
2014-07-01 16:06:21,913 WARN  [RpcServer.handler=32,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 1664ms
2014-07-01 16:06:21,913 INFO  [RpcServer.handler=32,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server sceplus-vm48.almaden.ibm.com,60020,1404255227840
2014-07-01 16:06:21,914 WARN  [RpcServer.handler=24,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 1665ms
2014-07-01 16:06:21,914 INFO  [RpcServer.handler=24,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server sceplus-vm48.almaden.ibm.com,60020,1404255227840
2014-07-01 16:06:21,914 WARN  [RpcServer.handler=1,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 1666ms
2014-07-01 16:06:21,914 INFO  [RpcServer.handler=1,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server sceplus-vm48.almaden.ibm.com,60020,1404255227840
2014-07-01 16:06:21,915 WARN  [RpcServer.handler=19,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 1669ms
2014-07-01 16:06:21,915 INFO  [RpcServer.handler=19,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server sceplus-vm48.almaden.ibm.com,60020,1404255227840
2014-07-01 16:06:21,915 WARN  [RpcServer.handler=15,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 1669ms
2014-07-01 16:06:21,915 INFO  [RpcServer.handler=15,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server sceplus-vm48.almaden.ibm.com,60020,1404255227840
2014-07-01 16:06:22,017 DEBUG [RpcServer.handler=9,port=60020] regionserver.HRegion: Flush requested on usertable,user7,1404255616194.431aad56a67499b48936eca121e3178e.
2014-07-01 16:06:22,607 DEBUG [MemStoreFlusher.0] util.FSUtils: DFS Client does not support most favored nodes create; using default create
2014-07-01 16:06:22,670 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-01 16:06:22,708 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 10361 synced till here 10349
2014-07-01 16:06:23,852 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1404255227840/sceplus-vm48.almaden.ibm.com%2C60020%2C1404255227840.1404255980241 with entries=81, filesize=73.0m; new WAL /hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1404255227840/sceplus-vm48.almaden.ibm.com%2C60020%2C1404255227840.1404255982670
2014-07-01 16:06:23,853 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1404255227840/sceplus-vm48.almaden.ibm.com%2C60020%2C1404255227840.1404255833342
2014-07-01 16:06:23,853 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1404255227840/sceplus-vm48.almaden.ibm.com%2C60020%2C1404255227840.1404255846528
2014-07-01 16:06:23,853 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1404255227840/sceplus-vm48.almaden.ibm.com%2C60020%2C1404255227840.1404255849514
2014-07-01 16:06:23,853 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1404255227840/sceplus-vm48.almaden.ibm.com%2C60020%2C1404255227840.1404255851841
2014-07-01 16:06:23,853 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1404255227840/sceplus-vm48.almaden.ibm.com%2C60020%2C1404255227840.1404255854044
2014-07-01 16:06:23,853 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1404255227840/sceplus-vm48.almaden.ibm.com%2C60020%2C1404255227840.1404255856351
2014-07-01 16:06:24,623 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-01 16:06:24,643 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 10433 synced till here 10432
2014-07-01 16:06:24,674 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1404255227840/sceplus-vm48.almaden.ibm.com%2C60020%2C1404255227840.1404255982670 with entries=72, filesize=62.1m; new WAL /hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1404255227840/sceplus-vm48.almaden.ibm.com%2C60020%2C1404255227840.1404255984624
2014-07-01 16:06:26,436 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-01 16:06:27,482 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 10558 synced till here 10553
2014-07-01 16:06:27,540 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1404255227840/sceplus-vm48.almaden.ibm.com%2C60020%2C1404255227840.1404255984624 with entries=125, filesize=105.0m; new WAL /hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1404255227840/sceplus-vm48.almaden.ibm.com%2C60020%2C1404255227840.1404255986436
2014-07-01 16:06:28,906 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-01 16:06:29,079 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 10654 synced till here 10639
2014-07-01 16:06:29,295 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1404255227840/sceplus-vm48.almaden.ibm.com%2C60020%2C1404255227840.1404255986436 with entries=96, filesize=75.6m; new WAL /hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1404255227840/sceplus-vm48.almaden.ibm.com%2C60020%2C1404255227840.1404255988907
2014-07-01 16:06:30,710 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-01 16:06:30,763 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 10746 synced till here 10742
2014-07-01 16:06:31,039 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1404255227840/sceplus-vm48.almaden.ibm.com%2C60020%2C1404255227840.1404255988907 with entries=92, filesize=66.2m; new WAL /hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1404255227840/sceplus-vm48.almaden.ibm.com%2C60020%2C1404255227840.1404255990711
2014-07-01 16:06:32,832 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-01 16:06:32,855 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 10828 synced till here 10824
2014-07-01 16:06:32,891 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1404255227840/sceplus-vm48.almaden.ibm.com%2C60020%2C1404255227840.1404255990711 with entries=82, filesize=65.0m; new WAL /hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1404255227840/sceplus-vm48.almaden.ibm.com%2C60020%2C1404255227840.1404255992832
2014-07-01 16:06:33,356 INFO  [RpcServer.handler=23,port=60020] regionserver.MemStoreFlusher: Blocking updates on sceplus-vm48.almaden.ibm.com,60020,1404255227840: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-01 16:06:33,359 INFO  [RpcServer.handler=16,port=60020] regionserver.MemStoreFlusher: Blocking updates on sceplus-vm48.almaden.ibm.com,60020,1404255227840: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-01 16:06:33,362 INFO  [RpcServer.handler=45,port=60020] regionserver.MemStoreFlusher: Blocking updates on sceplus-vm48.almaden.ibm.com,60020,1404255227840: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-01 16:06:33,363 INFO  [RpcServer.handler=7,port=60020] regionserver.MemStoreFlusher: Blocking updates on sceplus-vm48.almaden.ibm.com,60020,1404255227840: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-01 16:06:33,365 INFO  [RpcServer.handler=12,port=60020] regionserver.MemStoreFlusher: Blocking updates on sceplus-vm48.almaden.ibm.com,60020,1404255227840: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-01 16:06:33,366 INFO  [RpcServer.handler=9,port=60020] regionserver.MemStoreFlusher: Blocking updates on sceplus-vm48.almaden.ibm.com,60020,1404255227840: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-01 16:06:33,372 INFO  [RpcServer.handler=2,port=60020] regionserver.MemStoreFlusher: Blocking updates on sceplus-vm48.almaden.ibm.com,60020,1404255227840: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-01 16:06:33,396 INFO  [RpcServer.handler=21,port=60020] regionserver.MemStoreFlusher: Blocking updates on sceplus-vm48.almaden.ibm.com,60020,1404255227840: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-01 16:06:33,397 INFO  [RpcServer.handler=17,port=60020] regionserver.MemStoreFlusher: Blocking updates on sceplus-vm48.almaden.ibm.com,60020,1404255227840: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-01 16:06:33,423 INFO  [RpcServer.handler=35,port=60020] regionserver.MemStoreFlusher: Blocking updates on sceplus-vm48.almaden.ibm.com,60020,1404255227840: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-01 16:06:33,449 INFO  [RpcServer.handler=41,port=60020] regionserver.MemStoreFlusher: Blocking updates on sceplus-vm48.almaden.ibm.com,60020,1404255227840: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-01 16:06:33,453 INFO  [RpcServer.handler=1,port=60020] regionserver.MemStoreFlusher: Blocking updates on sceplus-vm48.almaden.ibm.com,60020,1404255227840: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-01 16:06:33,460 INFO  [RpcServer.handler=28,port=60020] regionserver.MemStoreFlusher: Blocking updates on sceplus-vm48.almaden.ibm.com,60020,1404255227840: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-01 16:06:33,553 INFO  [RpcServer.handler=18,port=60020] regionserver.MemStoreFlusher: Blocking updates on sceplus-vm48.almaden.ibm.com,60020,1404255227840: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-01 16:06:33,576 INFO  [RpcServer.handler=26,port=60020] regionserver.MemStoreFlusher: Blocking updates on sceplus-vm48.almaden.ibm.com,60020,1404255227840: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-01 16:06:33,603 INFO  [RpcServer.handler=48,port=60020] regionserver.MemStoreFlusher: Blocking updates on sceplus-vm48.almaden.ibm.com,60020,1404255227840: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-01 16:06:33,605 INFO  [RpcServer.handler=15,port=60020] regionserver.MemStoreFlusher: Blocking updates on sceplus-vm48.almaden.ibm.com,60020,1404255227840: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-01 16:06:33,609 INFO  [RpcServer.handler=0,port=60020] regionserver.MemStoreFlusher: Blocking updates on sceplus-vm48.almaden.ibm.com,60020,1404255227840: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-01 16:06:33,611 INFO  [RpcServer.handler=32,port=60020] regionserver.MemStoreFlusher: Blocking updates on sceplus-vm48.almaden.ibm.com,60020,1404255227840: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-01 16:06:33,773 INFO  [RpcServer.handler=11,port=60020] regionserver.MemStoreFlusher: Blocking updates on sceplus-vm48.almaden.ibm.com,60020,1404255227840: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-01 16:06:33,774 INFO  [RpcServer.handler=19,port=60020] regionserver.MemStoreFlusher: Blocking updates on sceplus-vm48.almaden.ibm.com,60020,1404255227840: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-01 16:06:33,924 INFO  [RpcServer.handler=49,port=60020] regionserver.MemStoreFlusher: Blocking updates on sceplus-vm48.almaden.ibm.com,60020,1404255227840: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-01 16:06:34,040 INFO  [RpcServer.handler=38,port=60020] regionserver.MemStoreFlusher: Blocking updates on sceplus-vm48.almaden.ibm.com,60020,1404255227840: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-01 16:06:34,860 INFO  [RpcServer.handler=10,port=60020] regionserver.MemStoreFlusher: Blocking updates on sceplus-vm48.almaden.ibm.com,60020,1404255227840: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-01 16:06:34,869 INFO  [RpcServer.handler=3,port=60020] regionserver.MemStoreFlusher: Blocking updates on sceplus-vm48.almaden.ibm.com,60020,1404255227840: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-01 16:06:34,882 INFO  [RpcServer.handler=13,port=60020] regionserver.MemStoreFlusher: Blocking updates on sceplus-vm48.almaden.ibm.com,60020,1404255227840: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-01 16:06:35,618 INFO  [RpcServer.handler=46,port=60020] regionserver.MemStoreFlusher: Blocking updates on sceplus-vm48.almaden.ibm.com,60020,1404255227840: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-01 16:06:35,624 INFO  [RpcServer.handler=42,port=60020] regionserver.MemStoreFlusher: Blocking updates on sceplus-vm48.almaden.ibm.com,60020,1404255227840: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-01 16:06:35,639 INFO  [RpcServer.handler=27,port=60020] regionserver.MemStoreFlusher: Blocking updates on sceplus-vm48.almaden.ibm.com,60020,1404255227840: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-01 16:06:35,655 INFO  [RpcServer.handler=8,port=60020] regionserver.MemStoreFlusher: Blocking updates on sceplus-vm48.almaden.ibm.com,60020,1404255227840: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-01 16:06:35,677 INFO  [RpcServer.handler=25,port=60020] regionserver.MemStoreFlusher: Blocking updates on sceplus-vm48.almaden.ibm.com,60020,1404255227840: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-01 16:06:35,707 INFO  [RpcServer.handler=43,port=60020] regionserver.MemStoreFlusher: Blocking updates on sceplus-vm48.almaden.ibm.com,60020,1404255227840: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-01 16:06:35,739 INFO  [RpcServer.handler=40,port=60020] regionserver.MemStoreFlusher: Blocking updates on sceplus-vm48.almaden.ibm.com,60020,1404255227840: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-01 16:06:35,757 INFO  [RpcServer.handler=5,port=60020] regionserver.MemStoreFlusher: Blocking updates on sceplus-vm48.almaden.ibm.com,60020,1404255227840: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-01 16:06:35,778 INFO  [RpcServer.handler=44,port=60020] regionserver.MemStoreFlusher: Blocking updates on sceplus-vm48.almaden.ibm.com,60020,1404255227840: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-01 16:06:35,796 INFO  [RpcServer.handler=30,port=60020] regionserver.MemStoreFlusher: Blocking updates on sceplus-vm48.almaden.ibm.com,60020,1404255227840: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-01 16:06:35,819 INFO  [RpcServer.handler=22,port=60020] regionserver.MemStoreFlusher: Blocking updates on sceplus-vm48.almaden.ibm.com,60020,1404255227840: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-01 16:06:35,837 INFO  [RpcServer.handler=4,port=60020] regionserver.MemStoreFlusher: Blocking updates on sceplus-vm48.almaden.ibm.com,60020,1404255227840: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-01 16:06:35,854 INFO  [RpcServer.handler=31,port=60020] regionserver.MemStoreFlusher: Blocking updates on sceplus-vm48.almaden.ibm.com,60020,1404255227840: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-01 16:06:37,618 INFO  [RpcServer.handler=33,port=60020] regionserver.MemStoreFlusher: Blocking updates on sceplus-vm48.almaden.ibm.com,60020,1404255227840: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-01 16:06:37,624 INFO  [RpcServer.handler=37,port=60020] regionserver.MemStoreFlusher: Blocking updates on sceplus-vm48.almaden.ibm.com,60020,1404255227840: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-01 16:06:37,626 INFO  [RpcServer.handler=34,port=60020] regionserver.MemStoreFlusher: Blocking updates on sceplus-vm48.almaden.ibm.com,60020,1404255227840: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-01 16:06:37,730 INFO  [RpcServer.handler=36,port=60020] regionserver.MemStoreFlusher: Blocking updates on sceplus-vm48.almaden.ibm.com,60020,1404255227840: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-01 16:06:37,764 INFO  [RpcServer.handler=20,port=60020] regionserver.MemStoreFlusher: Blocking updates on sceplus-vm48.almaden.ibm.com,60020,1404255227840: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-01 16:06:37,770 INFO  [RpcServer.handler=39,port=60020] regionserver.MemStoreFlusher: Blocking updates on sceplus-vm48.almaden.ibm.com,60020,1404255227840: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-01 16:06:37,792 INFO  [RpcServer.handler=29,port=60020] regionserver.MemStoreFlusher: Blocking updates on sceplus-vm48.almaden.ibm.com,60020,1404255227840: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-01 16:06:37,813 INFO  [RpcServer.handler=47,port=60020] regionserver.MemStoreFlusher: Blocking updates on sceplus-vm48.almaden.ibm.com,60020,1404255227840: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-01 16:06:37,835 INFO  [RpcServer.handler=14,port=60020] regionserver.MemStoreFlusher: Blocking updates on sceplus-vm48.almaden.ibm.com,60020,1404255227840: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-01 16:06:37,852 INFO  [RpcServer.handler=24,port=60020] regionserver.MemStoreFlusher: Blocking updates on sceplus-vm48.almaden.ibm.com,60020,1404255227840: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-01 16:06:37,891 INFO  [RpcServer.handler=6,port=60020] regionserver.MemStoreFlusher: Blocking updates on sceplus-vm48.almaden.ibm.com,60020,1404255227840: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-01 16:06:38,356 WARN  [RpcServer.handler=23,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-01 16:06:38,360 WARN  [RpcServer.handler=16,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5001ms
2014-07-01 16:06:38,362 WARN  [RpcServer.handler=45,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-01 16:06:38,363 WARN  [RpcServer.handler=7,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-01 16:06:38,365 WARN  [RpcServer.handler=12,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-01 16:06:38,367 WARN  [RpcServer.handler=9,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5001ms
2014-07-01 16:06:38,372 WARN  [RpcServer.handler=2,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-01 16:06:38,397 WARN  [RpcServer.handler=21,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5001ms
2014-07-01 16:06:38,397 WARN  [RpcServer.handler=17,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-01 16:06:38,423 WARN  [RpcServer.handler=35,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-01 16:06:38,449 WARN  [RpcServer.handler=41,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-01 16:06:38,453 WARN  [RpcServer.handler=1,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-01 16:06:38,460 WARN  [RpcServer.handler=28,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5001ms
2014-07-01 16:06:38,554 WARN  [RpcServer.handler=18,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5001ms
2014-07-01 16:06:38,576 WARN  [RpcServer.handler=26,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-01 16:06:38,604 WARN  [RpcServer.handler=48,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5001ms
2014-07-01 16:06:38,606 WARN  [RpcServer.handler=15,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5001ms
2014-07-01 16:06:38,609 WARN  [RpcServer.handler=0,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-01 16:06:38,611 WARN  [RpcServer.handler=32,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-01 16:06:39,269 WARN  [RpcServer.handler=38,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5229ms
2014-07-01 16:06:39,270 WARN  [RpcServer.handler=11,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5497ms
2014-07-01 16:06:39,270 WARN  [RpcServer.handler=19,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5496ms
2014-07-01 16:06:39,271 WARN  [RpcServer.handler=49,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5346ms
2014-07-01 16:06:39,861 WARN  [RpcServer.handler=10,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5001ms
2014-07-01 16:06:39,869 WARN  [RpcServer.handler=3,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-01 16:06:39,882 WARN  [RpcServer.handler=13,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-01 16:06:40,618 WARN  [RpcServer.handler=46,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-01 16:06:40,624 WARN  [RpcServer.handler=42,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-01 16:06:40,639 WARN  [RpcServer.handler=27,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-01 16:06:40,656 WARN  [RpcServer.handler=8,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5001ms
2014-07-01 16:06:40,677 WARN  [RpcServer.handler=25,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5001ms
2014-07-01 16:06:40,707 WARN  [RpcServer.handler=43,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-01 16:06:40,739 WARN  [RpcServer.handler=40,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-01 16:06:40,757 WARN  [RpcServer.handler=5,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5001ms
2014-07-01 16:06:40,778 WARN  [RpcServer.handler=44,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-01 16:06:40,796 WARN  [RpcServer.handler=30,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-01 16:06:40,819 WARN  [RpcServer.handler=22,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-01 16:06:40,837 WARN  [RpcServer.handler=4,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-01 16:06:40,854 WARN  [RpcServer.handler=31,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-01 16:06:42,477 INFO  [MemStoreFlusher.1] regionserver.DefaultStoreFlusher: Flushed, sequenceid=3052, memsize=546.3m, hasBloomFilter=true, into tmp file hdfs://master:54310/hbase/data/default/usertable/9e9a6a0fd8aef92a09081d29900a50f3/.tmp/26459e61df9c4e4c843ed5735437f446
2014-07-01 16:06:42,524 DEBUG [MemStoreFlusher.1] regionserver.HRegionFileSystem: Committing store file hdfs://master:54310/hbase/data/default/usertable/9e9a6a0fd8aef92a09081d29900a50f3/.tmp/26459e61df9c4e4c843ed5735437f446 as hdfs://master:54310/hbase/data/default/usertable/9e9a6a0fd8aef92a09081d29900a50f3/family/26459e61df9c4e4c843ed5735437f446
2014-07-01 16:06:42,539 INFO  [MemStoreFlusher.1] regionserver.HStore: Added hdfs://master:54310/hbase/data/default/usertable/9e9a6a0fd8aef92a09081d29900a50f3/family/26459e61df9c4e4c843ed5735437f446, entries=1988930, sequenceid=3052, filesize=141.6m
2014-07-01 16:06:42,539 INFO  [MemStoreFlusher.1] regionserver.HRegion: Finished memstore flush of ~713.8m/748446800, currentsize=202.0m/211791040 for region usertable,user8,1404255616195.9e9a6a0fd8aef92a09081d29900a50f3. in 32104ms, sequenceid=3052, compaction requested=true
2014-07-01 16:06:42,539 DEBUG [MemStoreFlusher.1] regionserver.CompactSplitThread: Small Compaction requested: system; Because: MemStoreFlusher.1; compaction_queue=(0:9), split_queue=0, merge_queue=0
2014-07-01 16:06:42,540 WARN  [RpcServer.handler=31,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 6686ms
2014-07-01 16:06:42,540 INFO  [RpcServer.handler=31,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server sceplus-vm48.almaden.ibm.com,60020,1404255227840
2014-07-01 16:06:42,540 DEBUG [MemStoreFlusher.1] regionserver.HRegion: Started memstore flush for usertable,user3,1404255616194.09108ac86cf3c374e79d1bb78f787b49., current region memstore size 978.9m
2014-07-01 16:06:42,540 WARN  [RpcServer.handler=4,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 6703ms
2014-07-01 16:06:42,540 INFO  [RpcServer.handler=4,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server sceplus-vm48.almaden.ibm.com,60020,1404255227840
2014-07-01 16:06:42,540 WARN  [RpcServer.handler=22,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 6721ms
2014-07-01 16:06:42,540 INFO  [RpcServer.handler=22,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server sceplus-vm48.almaden.ibm.com,60020,1404255227840
2014-07-01 16:06:42,540 WARN  [RpcServer.handler=30,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 6744ms
2014-07-01 16:06:42,541 INFO  [RpcServer.handler=30,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server sceplus-vm48.almaden.ibm.com,60020,1404255227840
2014-07-01 16:06:42,541 WARN  [RpcServer.handler=44,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 6763ms
2014-07-01 16:06:42,541 INFO  [RpcServer.handler=44,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server sceplus-vm48.almaden.ibm.com,60020,1404255227840
2014-07-01 16:06:42,541 WARN  [RpcServer.handler=5,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 6785ms
2014-07-01 16:06:42,541 INFO  [RpcServer.handler=5,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server sceplus-vm48.almaden.ibm.com,60020,1404255227840
2014-07-01 16:06:42,544 WARN  [RpcServer.handler=40,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 6805ms
2014-07-01 16:06:42,544 INFO  [RpcServer.handler=40,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server sceplus-vm48.almaden.ibm.com,60020,1404255227840
2014-07-01 16:06:42,545 WARN  [RpcServer.handler=43,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 6837ms
2014-07-01 16:06:42,545 INFO  [RpcServer.handler=43,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server sceplus-vm48.almaden.ibm.com,60020,1404255227840
2014-07-01 16:06:42,545 WARN  [RpcServer.handler=25,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 6869ms
2014-07-01 16:06:42,545 INFO  [RpcServer.handler=25,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server sceplus-vm48.almaden.ibm.com,60020,1404255227840
2014-07-01 16:06:42,545 WARN  [RpcServer.handler=8,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 6890ms
2014-07-01 16:06:42,545 INFO  [RpcServer.handler=8,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server sceplus-vm48.almaden.ibm.com,60020,1404255227840
2014-07-01 16:06:42,558 WARN  [RpcServer.handler=27,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 6919ms
2014-07-01 16:06:42,558 INFO  [RpcServer.handler=27,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server sceplus-vm48.almaden.ibm.com,60020,1404255227840
2014-07-01 16:06:42,559 WARN  [RpcServer.handler=42,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 6935ms
2014-07-01 16:06:42,559 INFO  [RpcServer.handler=42,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server sceplus-vm48.almaden.ibm.com,60020,1404255227840
2014-07-01 16:06:42,559 WARN  [RpcServer.handler=46,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 6941ms
2014-07-01 16:06:42,559 INFO  [RpcServer.handler=46,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server sceplus-vm48.almaden.ibm.com,60020,1404255227840
2014-07-01 16:06:42,559 WARN  [RpcServer.handler=13,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 7677ms
2014-07-01 16:06:42,559 INFO  [RpcServer.handler=13,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server sceplus-vm48.almaden.ibm.com,60020,1404255227840
2014-07-01 16:06:42,562 WARN  [RpcServer.handler=3,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 7693ms
2014-07-01 16:06:42,562 INFO  [RpcServer.handler=3,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server sceplus-vm48.almaden.ibm.com,60020,1404255227840
2014-07-01 16:06:42,563 WARN  [RpcServer.handler=10,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 7702ms
2014-07-01 16:06:42,563 INFO  [RpcServer.handler=10,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server sceplus-vm48.almaden.ibm.com,60020,1404255227840
2014-07-01 16:06:42,566 WARN  [RpcServer.handler=49,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 8642ms
2014-07-01 16:06:42,567 INFO  [RpcServer.handler=49,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server sceplus-vm48.almaden.ibm.com,60020,1404255227840
2014-07-01 16:06:42,570 WARN  [RpcServer.handler=19,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 8796ms
2014-07-01 16:06:42,570 INFO  [RpcServer.handler=19,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server sceplus-vm48.almaden.ibm.com,60020,1404255227840
2014-07-01 16:06:42,571 WARN  [RpcServer.handler=11,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 8798ms
2014-07-01 16:06:42,571 INFO  [RpcServer.handler=11,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server sceplus-vm48.almaden.ibm.com,60020,1404255227840
2014-07-01 16:06:42,571 WARN  [RpcServer.handler=38,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 8531ms
2014-07-01 16:06:42,571 INFO  [RpcServer.handler=38,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server sceplus-vm48.almaden.ibm.com,60020,1404255227840
2014-07-01 16:06:42,571 WARN  [RpcServer.handler=32,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 8960ms
2014-07-01 16:06:42,571 INFO  [RpcServer.handler=32,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server sceplus-vm48.almaden.ibm.com,60020,1404255227840
2014-07-01 16:06:42,577 WARN  [RpcServer.handler=0,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 8967ms
2014-07-01 16:06:42,577 INFO  [RpcServer.handler=0,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server sceplus-vm48.almaden.ibm.com,60020,1404255227840
2014-07-01 16:06:42,577 WARN  [RpcServer.handler=15,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 8972ms
2014-07-01 16:06:42,577 INFO  [RpcServer.handler=15,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server sceplus-vm48.almaden.ibm.com,60020,1404255227840
2014-07-01 16:06:42,577 WARN  [RpcServer.handler=48,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 8974ms
2014-07-01 16:06:42,577 INFO  [RpcServer.handler=48,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server sceplus-vm48.almaden.ibm.com,60020,1404255227840
2014-07-01 16:06:42,580 WARN  [RpcServer.handler=26,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 9004ms
2014-07-01 16:06:42,580 INFO  [RpcServer.handler=26,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server sceplus-vm48.almaden.ibm.com,60020,1404255227840
2014-07-01 16:06:42,580 WARN  [RpcServer.handler=18,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 9027ms
2014-07-01 16:06:42,580 INFO  [RpcServer.handler=18,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server sceplus-vm48.almaden.ibm.com,60020,1404255227840
2014-07-01 16:06:42,580 WARN  [RpcServer.handler=28,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 9121ms
2014-07-01 16:06:42,580 INFO  [RpcServer.handler=28,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server sceplus-vm48.almaden.ibm.com,60020,1404255227840
2014-07-01 16:06:42,580 WARN  [RpcServer.handler=1,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 9127ms
2014-07-01 16:06:42,581 INFO  [RpcServer.handler=1,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server sceplus-vm48.almaden.ibm.com,60020,1404255227840
2014-07-01 16:06:42,581 WARN  [RpcServer.handler=41,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 9132ms
2014-07-01 16:06:42,581 INFO  [RpcServer.handler=41,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server sceplus-vm48.almaden.ibm.com,60020,1404255227840
2014-07-01 16:06:42,582 WARN  [RpcServer.handler=35,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 9158ms
2014-07-01 16:06:42,582 INFO  [RpcServer.handler=35,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server sceplus-vm48.almaden.ibm.com,60020,1404255227840
2014-07-01 16:06:42,590 WARN  [RpcServer.handler=17,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 9193ms
2014-07-01 16:06:42,590 INFO  [RpcServer.handler=17,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server sceplus-vm48.almaden.ibm.com,60020,1404255227840
2014-07-01 16:06:42,591 WARN  [RpcServer.handler=21,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 9194ms
2014-07-01 16:06:42,591 INFO  [RpcServer.handler=21,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server sceplus-vm48.almaden.ibm.com,60020,1404255227840
2014-07-01 16:06:42,598 WARN  [RpcServer.handler=2,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 9226ms
2014-07-01 16:06:42,598 INFO  [RpcServer.handler=2,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server sceplus-vm48.almaden.ibm.com,60020,1404255227840
2014-07-01 16:06:42,598 WARN  [RpcServer.handler=9,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 9232ms
2014-07-01 16:06:42,599 INFO  [RpcServer.handler=9,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server sceplus-vm48.almaden.ibm.com,60020,1404255227840
2014-07-01 16:06:42,599 WARN  [RpcServer.handler=12,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 9234ms
2014-07-01 16:06:42,599 INFO  [RpcServer.handler=12,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server sceplus-vm48.almaden.ibm.com,60020,1404255227840
2014-07-01 16:06:42,599 WARN  [RpcServer.handler=7,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 9236ms
2014-07-01 16:06:42,599 INFO  [RpcServer.handler=7,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server sceplus-vm48.almaden.ibm.com,60020,1404255227840
2014-07-01 16:06:42,599 WARN  [RpcServer.handler=45,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 9237ms
2014-07-01 16:06:42,599 INFO  [RpcServer.handler=45,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server sceplus-vm48.almaden.ibm.com,60020,1404255227840
2014-07-01 16:06:42,600 WARN  [RpcServer.handler=16,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 9241ms
2014-07-01 16:06:42,600 INFO  [RpcServer.handler=16,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server sceplus-vm48.almaden.ibm.com,60020,1404255227840
2014-07-01 16:06:42,601 WARN  [RpcServer.handler=23,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 9244ms
2014-07-01 16:06:42,601 INFO  [RpcServer.handler=23,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server sceplus-vm48.almaden.ibm.com,60020,1404255227840
2014-07-01 16:06:42,601 WARN  [RpcServer.handler=6,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 4710ms
2014-07-01 16:06:42,601 INFO  [RpcServer.handler=6,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server sceplus-vm48.almaden.ibm.com,60020,1404255227840
2014-07-01 16:06:42,601 WARN  [RpcServer.handler=24,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 4749ms
2014-07-01 16:06:42,601 INFO  [RpcServer.handler=24,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server sceplus-vm48.almaden.ibm.com,60020,1404255227840
2014-07-01 16:06:42,601 WARN  [RpcServer.handler=14,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 4766ms
2014-07-01 16:06:42,601 INFO  [RpcServer.handler=14,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server sceplus-vm48.almaden.ibm.com,60020,1404255227840
2014-07-01 16:06:42,606 WARN  [RpcServer.handler=47,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 4793ms
2014-07-01 16:06:42,606 INFO  [RpcServer.handler=47,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server sceplus-vm48.almaden.ibm.com,60020,1404255227840
2014-07-01 16:06:42,606 WARN  [RpcServer.handler=29,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 4814ms
2014-07-01 16:06:42,607 INFO  [RpcServer.handler=29,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server sceplus-vm48.almaden.ibm.com,60020,1404255227840
2014-07-01 16:06:42,607 WARN  [RpcServer.handler=39,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 4838ms
2014-07-01 16:06:42,607 INFO  [RpcServer.handler=39,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server sceplus-vm48.almaden.ibm.com,60020,1404255227840
2014-07-01 16:06:42,607 WARN  [RpcServer.handler=20,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 4843ms
2014-07-01 16:06:42,607 INFO  [RpcServer.handler=20,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server sceplus-vm48.almaden.ibm.com,60020,1404255227840
2014-07-01 16:06:42,607 WARN  [RpcServer.handler=36,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 4877ms
2014-07-01 16:06:42,607 INFO  [RpcServer.handler=36,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server sceplus-vm48.almaden.ibm.com,60020,1404255227840
2014-07-01 16:06:42,608 WARN  [RpcServer.handler=34,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 4983ms
2014-07-01 16:06:42,608 INFO  [RpcServer.handler=34,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server sceplus-vm48.almaden.ibm.com,60020,1404255227840
2014-07-01 16:06:42,608 WARN  [RpcServer.handler=37,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 4984ms
2014-07-01 16:06:42,608 INFO  [RpcServer.handler=37,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server sceplus-vm48.almaden.ibm.com,60020,1404255227840
2014-07-01 16:06:42,608 WARN  [RpcServer.handler=33,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 4990ms
2014-07-01 16:06:42,608 INFO  [RpcServer.handler=33,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server sceplus-vm48.almaden.ibm.com,60020,1404255227840
2014-07-01 16:06:43,624 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-01 16:06:43,625 WARN  [RpcServer.handler=15,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":10021,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.58:34446","starttimems":1404255993604,"queuetimems":0,"class":"HRegionServer","responsesize":3697,"method":"Multi"}
2014-07-01 16:06:43,626 WARN  [RpcServer.handler=0,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":10018,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.58:34446","starttimems":1404255993607,"queuetimems":0,"class":"HRegionServer","responsesize":3946,"method":"Multi"}
2014-07-01 16:06:43,626 WARN  [RpcServer.handler=32,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":10016,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.58:34446","starttimems":1404255993609,"queuetimems":0,"class":"HRegionServer","responsesize":3787,"method":"Multi"}
2014-07-01 16:06:43,626 WARN  [RpcServer.handler=41,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":10764,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.58:34446","starttimems":1404255992861,"queuetimems":1,"class":"HRegionServer","responsesize":19863,"method":"Multi"}
2014-07-01 16:06:43,650 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 10936 synced till here 10917
2014-07-01 16:06:43,798 WARN  [RpcServer.handler=1,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":10785,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.58:34446","starttimems":1404255993013,"queuetimems":1,"class":"HRegionServer","responsesize":18200,"method":"Multi"}
2014-07-01 16:06:43,798 WARN  [RpcServer.handler=12,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":10887,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.58:34446","starttimems":1404255992911,"queuetimems":0,"class":"HRegionServer","responsesize":17636,"method":"Multi"}
2014-07-01 16:06:43,798 WARN  [RpcServer.handler=16,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":10861,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.58:34446","starttimems":1404255992937,"queuetimems":1,"class":"HRegionServer","responsesize":18192,"method":"Multi"}
2014-07-01 16:06:43,798 WARN  [RpcServer.handler=2,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":10819,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.58:34446","starttimems":1404255992979,"queuetimems":0,"class":"HRegionServer","responsesize":18497,"method":"Multi"}
2014-07-01 16:06:43,802 WARN  [RpcServer.handler=17,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":10687,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.58:34446","starttimems":1404255993114,"queuetimems":0,"class":"HRegionServer","responsesize":18275,"method":"Multi"}
2014-07-01 16:06:43,807 WARN  [RpcServer.handler=9,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":10443,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.58:34446","starttimems":1404255993364,"queuetimems":0,"class":"HRegionServer","responsesize":4245,"method":"Multi"}
2014-07-01 16:06:43,807 WARN  [RpcServer.handler=7,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":10446,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.58:34446","starttimems":1404255993360,"queuetimems":0,"class":"HRegionServer","responsesize":4574,"method":"Multi"}
2014-07-01 16:06:43,815 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1404255227840/sceplus-vm48.almaden.ibm.com%2C60020%2C1404255227840.1404255992832 with entries=108, filesize=94.6m; new WAL /hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1404255227840/sceplus-vm48.almaden.ibm.com%2C60020%2C1404255227840.1404256003624
2014-07-01 16:06:43,815 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1404255227840/sceplus-vm48.almaden.ibm.com%2C60020%2C1404255227840.1404255859279
2014-07-01 16:06:43,816 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1404255227840/sceplus-vm48.almaden.ibm.com%2C60020%2C1404255227840.1404255861631
2014-07-01 16:06:43,816 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1404255227840/sceplus-vm48.almaden.ibm.com%2C60020%2C1404255227840.1404255864176
2014-07-01 16:06:43,816 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1404255227840/sceplus-vm48.almaden.ibm.com%2C60020%2C1404255227840.1404255866671
2014-07-01 16:06:43,816 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1404255227840/sceplus-vm48.almaden.ibm.com%2C60020%2C1404255227840.1404255868699
2014-07-01 16:06:43,816 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1404255227840/sceplus-vm48.almaden.ibm.com%2C60020%2C1404255227840.1404255871048
2014-07-01 16:06:43,816 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1404255227840/sceplus-vm48.almaden.ibm.com%2C60020%2C1404255227840.1404255874263
2014-07-01 16:06:43,816 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1404255227840/sceplus-vm48.almaden.ibm.com%2C60020%2C1404255227840.1404255876543
2014-07-01 16:06:43,816 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1404255227840/sceplus-vm48.almaden.ibm.com%2C60020%2C1404255227840.1404255879063
2014-07-01 16:06:43,817 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1404255227840/sceplus-vm48.almaden.ibm.com%2C60020%2C1404255227840.1404255881567
2014-07-01 16:06:43,817 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1404255227840/sceplus-vm48.almaden.ibm.com%2C60020%2C1404255227840.1404255884351
2014-07-01 16:06:43,817 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1404255227840/sceplus-vm48.almaden.ibm.com%2C60020%2C1404255227840.1404255886642
2014-07-01 16:06:43,817 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1404255227840/sceplus-vm48.almaden.ibm.com%2C60020%2C1404255227840.1404255888925
2014-07-01 16:06:43,817 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1404255227840/sceplus-vm48.almaden.ibm.com%2C60020%2C1404255227840.1404255891335
2014-07-01 16:06:43,818 WARN  [RpcServer.handler=35,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":10398,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.58:34446","starttimems":1404255993419,"queuetimems":0,"class":"HRegionServer","responsesize":7344,"method":"Multi"}
2014-07-01 16:06:44,072 WARN  [RpcServer.handler=23,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":10829,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.58:34446","starttimems":1404255993242,"queuetimems":1,"class":"HRegionServer","responsesize":15795,"method":"Multi"}
2014-07-01 16:06:44,290 WARN  [RpcServer.handler=21,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":11198,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.58:34446","starttimems":1404255993092,"queuetimems":1,"class":"HRegionServer","responsesize":19961,"method":"Multi"}
2014-07-01 16:06:45,414 WARN  [RpcServer.handler=28,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":12261,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.58:34446","starttimems":1404255993153,"queuetimems":1,"class":"HRegionServer","responsesize":20634,"method":"Multi"}
2014-07-01 16:06:45,428 DEBUG [MemStoreFlusher.1] util.FSUtils: DFS Client does not support most favored nodes create; using default create
2014-07-01 16:06:45,572 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-01 16:06:45,689 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 11048 synced till here 11012
2014-07-01 16:06:45,919 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1404255227840/sceplus-vm48.almaden.ibm.com%2C60020%2C1404255227840.1404256003624 with entries=112, filesize=101.6m; new WAL /hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1404255227840/sceplus-vm48.almaden.ibm.com%2C60020%2C1404255227840.1404256005572
2014-07-01 16:06:46,066 WARN  [RpcServer.handler=22,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":10250,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.58:34446","starttimems":1404255995816,"queuetimems":1,"class":"HRegionServer","responsesize":19961,"method":"Multi"}
2014-07-01 16:06:46,170 INFO  [MemStoreFlusher.0] regionserver.DefaultStoreFlusher: Flushed, sequenceid=1789, memsize=438.5m, hasBloomFilter=true, into tmp file hdfs://master:54310/hbase/data/default/usertable/3e76136f0a8be94ccf3637676c064402/.tmp/70db44eba76b4511a00fbd62c8850daf
2014-07-01 16:06:46,180 DEBUG [MemStoreFlusher.0] regionserver.HRegionFileSystem: Committing store file hdfs://master:54310/hbase/data/default/usertable/3e76136f0a8be94ccf3637676c064402/.tmp/70db44eba76b4511a00fbd62c8850daf as hdfs://master:54310/hbase/data/default/usertable/3e76136f0a8be94ccf3637676c064402/family/70db44eba76b4511a00fbd62c8850daf
2014-07-01 16:06:46,188 INFO  [MemStoreFlusher.0] regionserver.HStore: Added hdfs://master:54310/hbase/data/default/usertable/3e76136f0a8be94ccf3637676c064402/family/70db44eba76b4511a00fbd62c8850daf, entries=1596750, sequenceid=1789, filesize=113.7m
2014-07-01 16:06:46,189 INFO  [MemStoreFlusher.0] regionserver.HRegion: Finished memstore flush of ~744.2m/780367920, currentsize=244.0m/255870560 for region usertable,user5,1404255616194.3e76136f0a8be94ccf3637676c064402. in 24298ms, sequenceid=1789, compaction requested=true
2014-07-01 16:06:46,189 DEBUG [MemStoreFlusher.0] regionserver.CompactSplitThread: Small Compaction requested: system; Because: MemStoreFlusher.0; compaction_queue=(0:10), split_queue=0, merge_queue=0
2014-07-01 16:06:46,189 DEBUG [MemStoreFlusher.0] regionserver.HRegion: Started memstore flush for usertable,,1404255616194.510634b895ee706ff306728824babf96., current region memstore size 706.8m
2014-07-01 16:06:46,217 DEBUG [RpcServer.handler=23,port=60020] regionserver.HRegion: Flush requested on usertable,user5,1404255616194.3e76136f0a8be94ccf3637676c064402.
2014-07-01 16:06:46,223 WARN  [RpcServer.handler=30,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":10430,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.58:34446","starttimems":1404255995793,"queuetimems":1,"class":"HRegionServer","responsesize":19863,"method":"Multi"}
2014-07-01 16:06:46,231 WARN  [RpcServer.handler=3,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":11363,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.58:34446","starttimems":1404255994867,"queuetimems":0,"class":"HRegionServer","responsesize":13356,"method":"Multi"}
2014-07-01 16:06:46,231 WARN  [RpcServer.handler=18,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":12683,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.58:34446","starttimems":1404255993548,"queuetimems":0,"class":"HRegionServer","responsesize":18899,"method":"Multi"}
2014-07-01 16:06:46,233 WARN  [RpcServer.handler=5,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":10480,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.58:34446","starttimems":1404255995753,"queuetimems":1,"class":"HRegionServer","responsesize":15795,"method":"Multi"}
2014-07-01 16:06:46,875 WARN  [RpcServer.handler=26,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":13303,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.58:34446","starttimems":1404255993572,"queuetimems":0,"class":"HRegionServer","responsesize":18801,"method":"Multi"}
2014-07-01 16:06:46,882 WARN  [RpcServer.handler=43,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":11181,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.58:34446","starttimems":1404255995701,"queuetimems":0,"class":"HRegionServer","responsesize":18275,"method":"Multi"}
2014-07-01 16:06:46,887 WARN  [RpcServer.handler=25,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":11213,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.58:34446","starttimems":1404255995674,"queuetimems":1,"class":"HRegionServer","responsesize":18192,"method":"Multi"}
2014-07-01 16:06:46,888 WARN  [RpcServer.handler=48,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":13287,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.58:34446","starttimems":1404255993600,"queuetimems":0,"class":"HRegionServer","responsesize":19436,"method":"Multi"}
2014-07-01 16:06:46,902 WARN  [RpcServer.handler=38,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":12864,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.58:34446","starttimems":1404255994037,"queuetimems":1,"class":"HRegionServer","responsesize":18301,"method":"Multi"}
2014-07-01 16:06:46,903 WARN  [RpcServer.handler=4,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":11070,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.58:34446","starttimems":1404255995833,"queuetimems":0,"class":"HRegionServer","responsesize":18497,"method":"Multi"}
2014-07-01 16:06:46,902 WARN  [RpcServer.handler=44,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":11129,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.58:34446","starttimems":1404255995772,"queuetimems":0,"class":"HRegionServer","responsesize":18200,"method":"Multi"}
2014-07-01 16:06:46,903 WARN  [RpcServer.handler=40,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":11170,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.58:34446","starttimems":1404255995733,"queuetimems":1,"class":"HRegionServer","responsesize":17636,"method":"Multi"}
2014-07-01 16:06:46,903 WARN  [RpcServer.handler=45,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":13546,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.58:34446","starttimems":1404255993357,"queuetimems":0,"class":"HRegionServer","responsesize":18247,"method":"Multi"}
2014-07-01 16:06:47,108 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-01 16:06:47,172 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 11150 synced till here 11122
2014-07-01 16:06:47,362 WARN  [RpcServer.handler=8,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":11710,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.58:34446","starttimems":1404255995652,"queuetimems":1,"class":"HRegionServer","responsesize":18247,"method":"Multi"}
2014-07-01 16:06:47,372 WARN  [RpcServer.handler=49,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":13454,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.58:34446","starttimems":1404255993918,"queuetimems":0,"class":"HRegionServer","responsesize":18579,"method":"Multi"}
2014-07-01 16:06:47,383 WARN  [RpcServer.handler=10,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":12524,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.58:34446","starttimems":1404255994859,"queuetimems":0,"class":"HRegionServer","responsesize":15772,"method":"Multi"}
2014-07-01 16:06:47,398 DEBUG [RpcServer.handler=13,port=60020] regionserver.HRegion: Flush requested on usertable,user8,1404255616195.9e9a6a0fd8aef92a09081d29900a50f3.
2014-07-01 16:06:47,399 WARN  [RpcServer.handler=13,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":12519,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.58:34446","starttimems":1404255994880,"queuetimems":1,"class":"HRegionServer","responsesize":19707,"method":"Multi"}
2014-07-01 16:06:47,438 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1404255227840/sceplus-vm48.almaden.ibm.com%2C60020%2C1404255227840.1404256005572 with entries=102, filesize=93.1m; new WAL /hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1404255227840/sceplus-vm48.almaden.ibm.com%2C60020%2C1404255227840.1404256007108
2014-07-01 16:06:47,482 DEBUG [MemStoreFlusher.0] util.FSUtils: DFS Client does not support most favored nodes create; using default create
2014-07-01 16:06:48,970 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-01 16:06:49,099 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 11254 synced till here 11230
2014-07-01 16:06:49,106 WARN  [RpcServer.handler=29,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":11318,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.58:34446","starttimems":1404255997788,"queuetimems":1,"class":"HRegionServer","responsesize":19436,"method":"Multi"}
2014-07-01 16:06:49,263 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1404255227840/sceplus-vm48.almaden.ibm.com%2C60020%2C1404255227840.1404256007108 with entries=104, filesize=84.7m; new WAL /hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1404255227840/sceplus-vm48.almaden.ibm.com%2C60020%2C1404255227840.1404256008971
2014-07-01 16:06:50,471 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-01 16:06:50,519 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 11323 synced till here 11322
2014-07-01 16:06:50,590 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1404255227840/sceplus-vm48.almaden.ibm.com%2C60020%2C1404255227840.1404256008971 with entries=69, filesize=65.5m; new WAL /hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1404255227840/sceplus-vm48.almaden.ibm.com%2C60020%2C1404255227840.1404256010471
2014-07-01 16:06:52,104 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-01 16:06:52,226 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 11422 synced till here 11412
2014-07-01 16:06:52,323 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1404255227840/sceplus-vm48.almaden.ibm.com%2C60020%2C1404255227840.1404256010471 with entries=99, filesize=79.9m; new WAL /hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1404255227840/sceplus-vm48.almaden.ibm.com%2C60020%2C1404255227840.1404256012104
2014-07-01 16:06:53,933 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-01 16:06:54,013 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 11523 synced till here 11521
2014-07-01 16:06:54,052 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1404255227840/sceplus-vm48.almaden.ibm.com%2C60020%2C1404255227840.1404256012104 with entries=101, filesize=74.0m; new WAL /hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1404255227840/sceplus-vm48.almaden.ibm.com%2C60020%2C1404255227840.1404256013948
2014-07-01 16:06:55,511 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-01 16:06:55,598 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 11618 synced till here 11612
2014-07-01 16:06:55,659 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1404255227840/sceplus-vm48.almaden.ibm.com%2C60020%2C1404255227840.1404256013948 with entries=95, filesize=68.7m; new WAL /hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1404255227840/sceplus-vm48.almaden.ibm.com%2C60020%2C1404255227840.1404256015511
2014-07-01 16:06:57,199 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-01 16:06:57,272 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 11721 synced till here 11703
2014-07-01 16:06:57,481 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1404255227840/sceplus-vm48.almaden.ibm.com%2C60020%2C1404255227840.1404256015511 with entries=103, filesize=80.1m; new WAL /hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1404255227840/sceplus-vm48.almaden.ibm.com%2C60020%2C1404255227840.1404256017199
2014-07-01 16:06:59,038 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-01 16:06:59,100 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 11807 synced till here 11796
2014-07-01 16:06:59,206 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1404255227840/sceplus-vm48.almaden.ibm.com%2C60020%2C1404255227840.1404256017199 with entries=86, filesize=69.5m; new WAL /hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1404255227840/sceplus-vm48.almaden.ibm.com%2C60020%2C1404255227840.1404256019038
2014-07-01 16:07:00,427 DEBUG [regionserver60020-smallCompactions-1404255751957] regionserver.HRegionFileSystem: Committing store file hdfs://master:54310/hbase/data/default/usertable/510634b895ee706ff306728824babf96/.tmp/91cc3ebeaa7946fa8d06d8dc5f4f4a7c as hdfs://master:54310/hbase/data/default/usertable/510634b895ee706ff306728824babf96/family/91cc3ebeaa7946fa8d06d8dc5f4f4a7c
2014-07-01 16:07:00,493 INFO  [RpcServer.handler=44,port=60020] regionserver.MemStoreFlusher: Blocking updates on sceplus-vm48.almaden.ibm.com,60020,1404255227840: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-01 16:07:00,495 INFO  [RpcServer.handler=28,port=60020] regionserver.MemStoreFlusher: Blocking updates on sceplus-vm48.almaden.ibm.com,60020,1404255227840: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-01 16:07:00,496 INFO  [RpcServer.handler=48,port=60020] regionserver.MemStoreFlusher: Blocking updates on sceplus-vm48.almaden.ibm.com,60020,1404255227840: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-01 16:07:00,500 INFO  [RpcServer.handler=0,port=60020] regionserver.MemStoreFlusher: Blocking updates on sceplus-vm48.almaden.ibm.com,60020,1404255227840: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-01 16:07:00,528 INFO  [RpcServer.handler=47,port=60020] regionserver.MemStoreFlusher: Blocking updates on sceplus-vm48.almaden.ibm.com,60020,1404255227840: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-01 16:07:00,532 DEBUG [regionserver60020-smallCompactions-1404255751957] regionserver.HStore: Removing store files after compaction...
2014-07-01 16:07:00,539 INFO  [RpcServer.handler=13,port=60020] regionserver.MemStoreFlusher: Blocking updates on sceplus-vm48.almaden.ibm.com,60020,1404255227840: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-01 16:07:00,539 INFO  [RpcServer.handler=49,port=60020] regionserver.MemStoreFlusher: Blocking updates on sceplus-vm48.almaden.ibm.com,60020,1404255227840: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-01 16:07:00,545 INFO  [RpcServer.handler=45,port=60020] regionserver.MemStoreFlusher: Blocking updates on sceplus-vm48.almaden.ibm.com,60020,1404255227840: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-01 16:07:00,546 DEBUG [regionserver60020-smallCompactions-1404255751957] backup.HFileArchiver: Finished archiving from class org.apache.hadoop.hbase.backup.HFileArchiver$FileableStoreFile, file:hdfs://master:54310/hbase/data/default/usertable/510634b895ee706ff306728824babf96/family/f66b6ec5f49d4d8fb94e77677a7c9532, to hdfs://master:54310/hbase/archive/data/default/usertable/510634b895ee706ff306728824babf96/family/f66b6ec5f49d4d8fb94e77677a7c9532
2014-07-01 16:07:00,549 DEBUG [regionserver60020-smallCompactions-1404255751957] backup.HFileArchiver: Finished archiving from class org.apache.hadoop.hbase.backup.HFileArchiver$FileableStoreFile, file:hdfs://master:54310/hbase/data/default/usertable/510634b895ee706ff306728824babf96/family/4eead3f5ffef4c37bd398c66ece498b7, to hdfs://master:54310/hbase/archive/data/default/usertable/510634b895ee706ff306728824babf96/family/4eead3f5ffef4c37bd398c66ece498b7
2014-07-01 16:07:00,550 INFO  [RpcServer.handler=19,port=60020] regionserver.MemStoreFlusher: Blocking updates on sceplus-vm48.almaden.ibm.com,60020,1404255227840: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-01 16:07:00,551 INFO  [RpcServer.handler=29,port=60020] regionserver.MemStoreFlusher: Blocking updates on sceplus-vm48.almaden.ibm.com,60020,1404255227840: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-01 16:07:00,552 DEBUG [regionserver60020-smallCompactions-1404255751957] backup.HFileArchiver: Finished archiving from class org.apache.hadoop.hbase.backup.HFileArchiver$FileableStoreFile, file:hdfs://master:54310/hbase/data/default/usertable/510634b895ee706ff306728824babf96/family/af73009e14a640c3a4f5b5a099fc66af, to hdfs://master:54310/hbase/archive/data/default/usertable/510634b895ee706ff306728824babf96/family/af73009e14a640c3a4f5b5a099fc66af
2014-07-01 16:07:00,554 DEBUG [regionserver60020-smallCompactions-1404255751957] backup.HFileArchiver: Finished archiving from class org.apache.hadoop.hbase.backup.HFileArchiver$FileableStoreFile, file:hdfs://master:54310/hbase/data/default/usertable/510634b895ee706ff306728824babf96/family/7aafde38cd1d49a79297ea4ca205a621, to hdfs://master:54310/hbase/archive/data/default/usertable/510634b895ee706ff306728824babf96/family/7aafde38cd1d49a79297ea4ca205a621
2014-07-01 16:07:00,555 INFO  [regionserver60020-smallCompactions-1404255751957] regionserver.HStore: Completed major compaction of 4 file(s) in family of usertable,,1404255616194.510634b895ee706ff306728824babf96. into 91cc3ebeaa7946fa8d06d8dc5f4f4a7c(size=306.2m), total size for store is 391.8m. This selection was in queue for 0sec, and took 1mins, 14sec to execute.
2014-07-01 16:07:00,555 INFO  [regionserver60020-smallCompactions-1404255751957] regionserver.CompactSplitThread: Completed compaction: Request = regionName=usertable,,1404255616194.510634b895ee706ff306728824babf96., storeName=family, fileCount=4, fileSize=323.8m, priority=16, time=76088269641339; duration=1mins, 14sec
2014-07-01 16:07:00,555 DEBUG [regionserver60020-smallCompactions-1404255751957] regionserver.CompactSplitThread: CompactSplitThread Status: compaction_queue=(0:10), split_queue=0, merge_queue=0
2014-07-01 16:07:00,556 DEBUG [regionserver60020-smallCompactions-1404255751957] compactions.RatioBasedCompactionPolicy: Selecting compaction from 5 store files, 0 compacting, 5 eligible, 20 blocking
2014-07-01 16:07:00,556 DEBUG [regionserver60020-smallCompactions-1404255751957] compactions.ExploringCompactionPolicy: Exploring compaction algorithm has selected 5 files of size 490742171 starting at candidate #0 after considering 6 permutations with 6 in ratio
2014-07-01 16:07:00,556 DEBUG [regionserver60020-smallCompactions-1404255751957] regionserver.HStore: 3e76136f0a8be94ccf3637676c064402 - family: Initiating major compaction
2014-07-01 16:07:00,557 INFO  [regionserver60020-smallCompactions-1404255751957] regionserver.HRegion: Starting compaction on family in region usertable,user5,1404255616194.3e76136f0a8be94ccf3637676c064402.
2014-07-01 16:07:00,558 INFO  [regionserver60020-smallCompactions-1404255751957] regionserver.HStore: Starting compaction of 5 file(s) in family of usertable,user5,1404255616194.3e76136f0a8be94ccf3637676c064402. into tmpdir=hdfs://master:54310/hbase/data/default/usertable/3e76136f0a8be94ccf3637676c064402/.tmp, totalSize=468.0m
2014-07-01 16:07:00,559 DEBUG [regionserver60020-smallCompactions-1404255751957] compactions.Compactor: Compacting hdfs://master:54310/hbase/data/default/usertable/3e76136f0a8be94ccf3637676c064402/family/e49b6c77370249f388139db102764e09, keycount=127055, bloomtype=ROW, size=90.5m, encoding=NONE, seqNum=542, earliestPutTs=1404255694787
2014-07-01 16:07:00,559 DEBUG [regionserver60020-smallCompactions-1404255751957] compactions.Compactor: Compacting hdfs://master:54310/hbase/data/default/usertable/3e76136f0a8be94ccf3637676c064402/family/b3616e237843458abd24c08525f4e0b5, keycount=138379, bloomtype=ROW, size=98.6m, encoding=NONE, seqNum=786, earliestPutTs=1404255755358
2014-07-01 16:07:00,559 DEBUG [regionserver60020-smallCompactions-1404255751957] compactions.Compactor: Compacting hdfs://master:54310/hbase/data/default/usertable/3e76136f0a8be94ccf3637676c064402/family/5b970a751db84b4ea94c5a41d59e7f96, keycount=123803, bloomtype=ROW, size=88.1m, encoding=NONE, seqNum=995, earliestPutTs=1404255799312
2014-07-01 16:07:00,559 DEBUG [regionserver60020-smallCompactions-1404255751957] compactions.Compactor: Compacting hdfs://master:54310/hbase/data/default/usertable/3e76136f0a8be94ccf3637676c064402/family/c6582863f548409586f782a8d6933006, keycount=108343, bloomtype=ROW, size=77.1m, encoding=NONE, seqNum=1350, earliestPutTs=1404255846066
2014-07-01 16:07:00,560 DEBUG [regionserver60020-smallCompactions-1404255751957] compactions.Compactor: Compacting hdfs://master:54310/hbase/data/default/usertable/3e76136f0a8be94ccf3637676c064402/family/70db44eba76b4511a00fbd62c8850daf, keycount=159675, bloomtype=ROW, size=113.7m, encoding=NONE, seqNum=1789, earliestPutTs=1404255901733
2014-07-01 16:07:00,578 INFO  [RpcServer.handler=32,port=60020] regionserver.MemStoreFlusher: Blocking updates on sceplus-vm48.almaden.ibm.com,60020,1404255227840: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-01 16:07:00,597 INFO  [RpcServer.handler=36,port=60020] regionserver.MemStoreFlusher: Blocking updates on sceplus-vm48.almaden.ibm.com,60020,1404255227840: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-01 16:07:00,619 INFO  [RpcServer.handler=20,port=60020] regionserver.MemStoreFlusher: Blocking updates on sceplus-vm48.almaden.ibm.com,60020,1404255227840: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-01 16:07:00,637 INFO  [RpcServer.handler=7,port=60020] regionserver.MemStoreFlusher: Blocking updates on sceplus-vm48.almaden.ibm.com,60020,1404255227840: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-01 16:07:00,659 INFO  [RpcServer.handler=1,port=60020] regionserver.MemStoreFlusher: Blocking updates on sceplus-vm48.almaden.ibm.com,60020,1404255227840: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-01 16:07:00,673 INFO  [RpcServer.handler=11,port=60020] regionserver.MemStoreFlusher: Blocking updates on sceplus-vm48.almaden.ibm.com,60020,1404255227840: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-01 16:07:00,675 INFO  [RpcServer.handler=25,port=60020] regionserver.MemStoreFlusher: Blocking updates on sceplus-vm48.almaden.ibm.com,60020,1404255227840: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-01 16:07:00,676 INFO  [RpcServer.handler=15,port=60020] regionserver.MemStoreFlusher: Blocking updates on sceplus-vm48.almaden.ibm.com,60020,1404255227840: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-01 16:07:00,677 INFO  [RpcServer.handler=34,port=60020] regionserver.MemStoreFlusher: Blocking updates on sceplus-vm48.almaden.ibm.com,60020,1404255227840: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-01 16:07:00,677 INFO  [RpcServer.handler=43,port=60020] regionserver.MemStoreFlusher: Blocking updates on sceplus-vm48.almaden.ibm.com,60020,1404255227840: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-01 16:07:00,698 INFO  [RpcServer.handler=10,port=60020] regionserver.MemStoreFlusher: Blocking updates on sceplus-vm48.almaden.ibm.com,60020,1404255227840: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-01 16:07:00,708 DEBUG [regionserver60020-smallCompactions-1404255751957] util.FSUtils: DFS Client does not support most favored nodes create; using default create
2014-07-01 16:07:00,713 INFO  [RpcServer.handler=35,port=60020] regionserver.MemStoreFlusher: Blocking updates on sceplus-vm48.almaden.ibm.com,60020,1404255227840: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-01 16:07:01,710 INFO  [RpcServer.handler=23,port=60020] regionserver.MemStoreFlusher: Blocking updates on sceplus-vm48.almaden.ibm.com,60020,1404255227840: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-01 16:07:01,743 INFO  [RpcServer.handler=46,port=60020] regionserver.MemStoreFlusher: Blocking updates on sceplus-vm48.almaden.ibm.com,60020,1404255227840: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-01 16:07:02,721 INFO  [RpcServer.handler=3,port=60020] regionserver.MemStoreFlusher: Blocking updates on sceplus-vm48.almaden.ibm.com,60020,1404255227840: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-01 16:07:02,844 INFO  [RpcServer.handler=33,port=60020] regionserver.MemStoreFlusher: Blocking updates on sceplus-vm48.almaden.ibm.com,60020,1404255227840: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-01 16:07:02,873 INFO  [RpcServer.handler=14,port=60020] regionserver.MemStoreFlusher: Blocking updates on sceplus-vm48.almaden.ibm.com,60020,1404255227840: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-01 16:07:02,890 INFO  [RpcServer.handler=8,port=60020] regionserver.MemStoreFlusher: Blocking updates on sceplus-vm48.almaden.ibm.com,60020,1404255227840: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-01 16:07:02,914 INFO  [RpcServer.handler=21,port=60020] regionserver.MemStoreFlusher: Blocking updates on sceplus-vm48.almaden.ibm.com,60020,1404255227840: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-01 16:07:02,937 INFO  [RpcServer.handler=41,port=60020] regionserver.MemStoreFlusher: Blocking updates on sceplus-vm48.almaden.ibm.com,60020,1404255227840: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-01 16:07:02,963 INFO  [RpcServer.handler=38,port=60020] regionserver.MemStoreFlusher: Blocking updates on sceplus-vm48.almaden.ibm.com,60020,1404255227840: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-01 16:07:02,987 INFO  [RpcServer.handler=24,port=60020] regionserver.MemStoreFlusher: Blocking updates on sceplus-vm48.almaden.ibm.com,60020,1404255227840: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-01 16:07:03,011 INFO  [RpcServer.handler=16,port=60020] regionserver.MemStoreFlusher: Blocking updates on sceplus-vm48.almaden.ibm.com,60020,1404255227840: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-01 16:07:03,035 INFO  [RpcServer.handler=37,port=60020] regionserver.MemStoreFlusher: Blocking updates on sceplus-vm48.almaden.ibm.com,60020,1404255227840: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-01 16:07:03,053 INFO  [RpcServer.handler=31,port=60020] regionserver.MemStoreFlusher: Blocking updates on sceplus-vm48.almaden.ibm.com,60020,1404255227840: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-01 16:07:03,071 INFO  [RpcServer.handler=12,port=60020] regionserver.MemStoreFlusher: Blocking updates on sceplus-vm48.almaden.ibm.com,60020,1404255227840: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-01 16:07:03,733 INFO  [RpcServer.handler=39,port=60020] regionserver.MemStoreFlusher: Blocking updates on sceplus-vm48.almaden.ibm.com,60020,1404255227840: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-01 16:07:03,744 INFO  [RpcServer.handler=26,port=60020] regionserver.MemStoreFlusher: Blocking updates on sceplus-vm48.almaden.ibm.com,60020,1404255227840: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-01 16:07:03,756 INFO  [RpcServer.handler=42,port=60020] regionserver.MemStoreFlusher: Blocking updates on sceplus-vm48.almaden.ibm.com,60020,1404255227840: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-01 16:07:03,769 INFO  [RpcServer.handler=40,port=60020] regionserver.MemStoreFlusher: Blocking updates on sceplus-vm48.almaden.ibm.com,60020,1404255227840: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-01 16:07:03,780 INFO  [RpcServer.handler=9,port=60020] regionserver.MemStoreFlusher: Blocking updates on sceplus-vm48.almaden.ibm.com,60020,1404255227840: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-01 16:07:03,793 INFO  [RpcServer.handler=5,port=60020] regionserver.MemStoreFlusher: Blocking updates on sceplus-vm48.almaden.ibm.com,60020,1404255227840: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-01 16:07:03,824 INFO  [RpcServer.handler=17,port=60020] regionserver.MemStoreFlusher: Blocking updates on sceplus-vm48.almaden.ibm.com,60020,1404255227840: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-01 16:07:03,842 INFO  [RpcServer.handler=6,port=60020] regionserver.MemStoreFlusher: Blocking updates on sceplus-vm48.almaden.ibm.com,60020,1404255227840: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-01 16:07:03,860 INFO  [RpcServer.handler=27,port=60020] regionserver.MemStoreFlusher: Blocking updates on sceplus-vm48.almaden.ibm.com,60020,1404255227840: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-01 16:07:04,718 INFO  [RpcServer.handler=18,port=60020] regionserver.MemStoreFlusher: Blocking updates on sceplus-vm48.almaden.ibm.com,60020,1404255227840: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-01 16:07:04,719 INFO  [RpcServer.handler=4,port=60020] regionserver.MemStoreFlusher: Blocking updates on sceplus-vm48.almaden.ibm.com,60020,1404255227840: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-01 16:07:04,721 INFO  [RpcServer.handler=22,port=60020] regionserver.MemStoreFlusher: Blocking updates on sceplus-vm48.almaden.ibm.com,60020,1404255227840: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-01 16:07:05,493 WARN  [RpcServer.handler=44,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5001ms
2014-07-01 16:07:05,495 WARN  [RpcServer.handler=28,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-01 16:07:05,496 WARN  [RpcServer.handler=48,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-01 16:07:05,500 WARN  [RpcServer.handler=0,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5001ms
2014-07-01 16:07:05,528 WARN  [RpcServer.handler=47,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5001ms
2014-07-01 16:07:05,539 WARN  [RpcServer.handler=13,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5001ms
2014-07-01 16:07:05,539 WARN  [RpcServer.handler=49,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5001ms
2014-07-01 16:07:05,545 WARN  [RpcServer.handler=45,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5001ms
2014-07-01 16:07:05,551 WARN  [RpcServer.handler=19,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5001ms
2014-07-01 16:07:05,551 WARN  [RpcServer.handler=29,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5001ms
2014-07-01 16:07:05,578 WARN  [RpcServer.handler=32,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-01 16:07:05,598 WARN  [RpcServer.handler=36,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-01 16:07:05,619 WARN  [RpcServer.handler=20,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-01 16:07:05,637 WARN  [RpcServer.handler=7,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-01 16:07:05,659 WARN  [RpcServer.handler=1,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-01 16:07:05,673 WARN  [RpcServer.handler=11,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-01 16:07:05,676 WARN  [RpcServer.handler=25,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5001ms
2014-07-01 16:07:05,676 WARN  [RpcServer.handler=15,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-01 16:07:05,677 WARN  [RpcServer.handler=34,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-01 16:07:05,678 WARN  [RpcServer.handler=43,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5001ms
2014-07-01 16:07:05,698 WARN  [RpcServer.handler=10,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-01 16:07:05,713 WARN  [RpcServer.handler=35,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-01 16:07:06,710 WARN  [RpcServer.handler=23,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-01 16:07:06,743 WARN  [RpcServer.handler=46,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5000ms
2014-07-01 16:07:06,819 INFO  [RpcServer.handler=30,port=60020] regionserver.MemStoreFlusher: Blocking updates on sceplus-vm48.almaden.ibm.com,60020,1404255227840: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-01 16:07:06,853 INFO  [RpcServer.handler=2,port=60020] regionserver.MemStoreFlusher: Blocking updates on sceplus-vm48.almaden.ibm.com,60020,1404255227840: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-01 16:07:06,943 INFO  [MemStoreFlusher.0] regionserver.DefaultStoreFlusher: Flushed, sequenceid=1966, memsize=382.9m, hasBloomFilter=true, into tmp file hdfs://master:54310/hbase/data/default/usertable/510634b895ee706ff306728824babf96/.tmp/f947715524054d1688a810aa255ef5d4
2014-07-01 16:07:06,955 DEBUG [MemStoreFlusher.0] regionserver.HRegionFileSystem: Committing store file hdfs://master:54310/hbase/data/default/usertable/510634b895ee706ff306728824babf96/.tmp/f947715524054d1688a810aa255ef5d4 as hdfs://master:54310/hbase/data/default/usertable/510634b895ee706ff306728824babf96/family/f947715524054d1688a810aa255ef5d4
2014-07-01 16:07:06,977 INFO  [MemStoreFlusher.0] regionserver.HStore: Added hdfs://master:54310/hbase/data/default/usertable/510634b895ee706ff306728824babf96/family/f947715524054d1688a810aa255ef5d4, entries=1394090, sequenceid=1966, filesize=99.3m
2014-07-01 16:07:06,977 INFO  [MemStoreFlusher.0] regionserver.HRegion: Finished memstore flush of ~710.1m/744559760, currentsize=220.2m/230941280 for region usertable,,1404255616194.510634b895ee706ff306728824babf96. in 20788ms, sequenceid=1966, compaction requested=true
2014-07-01 16:07:06,978 DEBUG [MemStoreFlusher.0] regionserver.CompactSplitThread: Small Compaction requested: system; Because: MemStoreFlusher.0; compaction_queue=(0:10), split_queue=0, merge_queue=0
2014-07-01 16:07:06,978 WARN  [RpcServer.handler=2,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 125ms
2014-07-01 16:07:06,978 INFO  [RpcServer.handler=2,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server sceplus-vm48.almaden.ibm.com,60020,1404255227840
2014-07-01 16:07:06,978 DEBUG [MemStoreFlusher.0] regionserver.HRegion: Started memstore flush for usertable,user7,1404255616194.431aad56a67499b48936eca121e3178e., current region memstore size 919.6m
2014-07-01 16:07:06,978 WARN  [RpcServer.handler=30,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 159ms
2014-07-01 16:07:06,978 INFO  [RpcServer.handler=30,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server sceplus-vm48.almaden.ibm.com,60020,1404255227840
2014-07-01 16:07:06,978 WARN  [RpcServer.handler=46,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5235ms
2014-07-01 16:07:06,978 INFO  [RpcServer.handler=46,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server sceplus-vm48.almaden.ibm.com,60020,1404255227840
2014-07-01 16:07:06,982 WARN  [RpcServer.handler=23,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5272ms
2014-07-01 16:07:06,982 INFO  [RpcServer.handler=23,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server sceplus-vm48.almaden.ibm.com,60020,1404255227840
2014-07-01 16:07:06,983 WARN  [RpcServer.handler=35,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 6269ms
2014-07-01 16:07:06,983 INFO  [RpcServer.handler=35,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server sceplus-vm48.almaden.ibm.com,60020,1404255227840
2014-07-01 16:07:06,983 WARN  [RpcServer.handler=10,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 6285ms
2014-07-01 16:07:06,983 INFO  [RpcServer.handler=10,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server sceplus-vm48.almaden.ibm.com,60020,1404255227840
2014-07-01 16:07:06,986 WARN  [RpcServer.handler=43,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 6309ms
2014-07-01 16:07:06,987 INFO  [RpcServer.handler=43,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server sceplus-vm48.almaden.ibm.com,60020,1404255227840
2014-07-01 16:07:06,987 WARN  [RpcServer.handler=34,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 6310ms
2014-07-01 16:07:06,987 INFO  [RpcServer.handler=34,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server sceplus-vm48.almaden.ibm.com,60020,1404255227840
2014-07-01 16:07:06,990 WARN  [RpcServer.handler=15,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 6314ms
2014-07-01 16:07:06,990 INFO  [RpcServer.handler=15,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server sceplus-vm48.almaden.ibm.com,60020,1404255227840
2014-07-01 16:07:06,991 WARN  [RpcServer.handler=25,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 6315ms
2014-07-01 16:07:06,991 INFO  [RpcServer.handler=25,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server sceplus-vm48.almaden.ibm.com,60020,1404255227840
2014-07-01 16:07:06,991 WARN  [RpcServer.handler=11,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 6318ms
2014-07-01 16:07:06,991 INFO  [RpcServer.handler=11,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server sceplus-vm48.almaden.ibm.com,60020,1404255227840
2014-07-01 16:07:06,991 WARN  [RpcServer.handler=1,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 6332ms
2014-07-01 16:07:06,991 INFO  [RpcServer.handler=1,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server sceplus-vm48.almaden.ibm.com,60020,1404255227840
2014-07-01 16:07:06,997 WARN  [RpcServer.handler=7,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 6360ms
2014-07-01 16:07:06,997 INFO  [RpcServer.handler=7,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server sceplus-vm48.almaden.ibm.com,60020,1404255227840
2014-07-01 16:07:06,997 WARN  [RpcServer.handler=20,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 6378ms
2014-07-01 16:07:06,997 INFO  [RpcServer.handler=20,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server sceplus-vm48.almaden.ibm.com,60020,1404255227840
2014-07-01 16:07:06,997 WARN  [RpcServer.handler=36,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 6400ms
2014-07-01 16:07:06,997 INFO  [RpcServer.handler=36,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server sceplus-vm48.almaden.ibm.com,60020,1404255227840
2014-07-01 16:07:06,997 WARN  [RpcServer.handler=32,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 6419ms
2014-07-01 16:07:06,997 INFO  [RpcServer.handler=32,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server sceplus-vm48.almaden.ibm.com,60020,1404255227840
2014-07-01 16:07:06,998 WARN  [RpcServer.handler=29,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 6447ms
2014-07-01 16:07:06,998 INFO  [RpcServer.handler=29,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server sceplus-vm48.almaden.ibm.com,60020,1404255227840
2014-07-01 16:07:07,003 WARN  [RpcServer.handler=19,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 6453ms
2014-07-01 16:07:07,003 INFO  [RpcServer.handler=19,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server sceplus-vm48.almaden.ibm.com,60020,1404255227840
2014-07-01 16:07:07,003 WARN  [RpcServer.handler=45,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 6459ms
2014-07-01 16:07:07,003 INFO  [RpcServer.handler=45,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server sceplus-vm48.almaden.ibm.com,60020,1404255227840
2014-07-01 16:07:07,003 WARN  [RpcServer.handler=49,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 6465ms
2014-07-01 16:07:07,003 INFO  [RpcServer.handler=49,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server sceplus-vm48.almaden.ibm.com,60020,1404255227840
2014-07-01 16:07:07,003 WARN  [RpcServer.handler=13,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 6465ms
2014-07-01 16:07:07,003 INFO  [RpcServer.handler=13,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server sceplus-vm48.almaden.ibm.com,60020,1404255227840
2014-07-01 16:07:07,004 WARN  [RpcServer.handler=47,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 6477ms
2014-07-01 16:07:07,004 INFO  [RpcServer.handler=47,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server sceplus-vm48.almaden.ibm.com,60020,1404255227840
2014-07-01 16:07:07,014 WARN  [RpcServer.handler=0,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 6515ms
2014-07-01 16:07:07,014 INFO  [RpcServer.handler=0,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server sceplus-vm48.almaden.ibm.com,60020,1404255227840
2014-07-01 16:07:07,014 WARN  [RpcServer.handler=48,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 6518ms
2014-07-01 16:07:07,014 INFO  [RpcServer.handler=48,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server sceplus-vm48.almaden.ibm.com,60020,1404255227840
2014-07-01 16:07:07,021 WARN  [RpcServer.handler=28,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 6526ms
2014-07-01 16:07:07,022 INFO  [RpcServer.handler=28,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server sceplus-vm48.almaden.ibm.com,60020,1404255227840
2014-07-01 16:07:07,022 WARN  [RpcServer.handler=44,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 6530ms
2014-07-01 16:07:07,022 INFO  [RpcServer.handler=44,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server sceplus-vm48.almaden.ibm.com,60020,1404255227840
2014-07-01 16:07:07,022 WARN  [RpcServer.handler=22,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 2301ms
2014-07-01 16:07:07,022 INFO  [RpcServer.handler=22,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server sceplus-vm48.almaden.ibm.com,60020,1404255227840
2014-07-01 16:07:07,022 WARN  [RpcServer.handler=4,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 2303ms
2014-07-01 16:07:07,022 INFO  [RpcServer.handler=4,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server sceplus-vm48.almaden.ibm.com,60020,1404255227840
2014-07-01 16:07:07,026 WARN  [RpcServer.handler=18,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 2308ms
2014-07-01 16:07:07,026 INFO  [RpcServer.handler=18,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server sceplus-vm48.almaden.ibm.com,60020,1404255227840
2014-07-01 16:07:07,026 WARN  [RpcServer.handler=27,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 3166ms
2014-07-01 16:07:07,027 INFO  [RpcServer.handler=27,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server sceplus-vm48.almaden.ibm.com,60020,1404255227840
2014-07-01 16:07:07,027 WARN  [RpcServer.handler=6,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 3185ms
2014-07-01 16:07:07,027 INFO  [RpcServer.handler=6,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server sceplus-vm48.almaden.ibm.com,60020,1404255227840
2014-07-01 16:07:07,027 WARN  [RpcServer.handler=17,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 3203ms
2014-07-01 16:07:07,027 INFO  [RpcServer.handler=17,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server sceplus-vm48.almaden.ibm.com,60020,1404255227840
2014-07-01 16:07:07,028 WARN  [RpcServer.handler=5,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 3236ms
2014-07-01 16:07:07,028 INFO  [RpcServer.handler=5,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server sceplus-vm48.almaden.ibm.com,60020,1404255227840
2014-07-01 16:07:07,028 WARN  [RpcServer.handler=9,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 3248ms
2014-07-01 16:07:07,028 INFO  [RpcServer.handler=9,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server sceplus-vm48.almaden.ibm.com,60020,1404255227840
2014-07-01 16:07:07,030 WARN  [RpcServer.handler=40,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 3261ms
2014-07-01 16:07:07,030 INFO  [RpcServer.handler=40,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server sceplus-vm48.almaden.ibm.com,60020,1404255227840
2014-07-01 16:07:07,030 WARN  [RpcServer.handler=42,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 3274ms
2014-07-01 16:07:07,031 INFO  [RpcServer.handler=42,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server sceplus-vm48.almaden.ibm.com,60020,1404255227840
2014-07-01 16:07:07,031 WARN  [RpcServer.handler=26,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 3287ms
2014-07-01 16:07:07,031 INFO  [RpcServer.handler=26,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server sceplus-vm48.almaden.ibm.com,60020,1404255227840
2014-07-01 16:07:07,031 WARN  [RpcServer.handler=39,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 3298ms
2014-07-01 16:07:07,031 INFO  [RpcServer.handler=39,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server sceplus-vm48.almaden.ibm.com,60020,1404255227840
2014-07-01 16:07:07,032 WARN  [RpcServer.handler=12,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 3960ms
2014-07-01 16:07:07,032 INFO  [RpcServer.handler=12,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server sceplus-vm48.almaden.ibm.com,60020,1404255227840
2014-07-01 16:07:07,038 WARN  [RpcServer.handler=31,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 3985ms
2014-07-01 16:07:07,038 INFO  [RpcServer.handler=31,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server sceplus-vm48.almaden.ibm.com,60020,1404255227840
2014-07-01 16:07:07,039 WARN  [RpcServer.handler=37,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 4004ms
2014-07-01 16:07:07,040 INFO  [RpcServer.handler=37,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server sceplus-vm48.almaden.ibm.com,60020,1404255227840
2014-07-01 16:07:07,040 WARN  [RpcServer.handler=16,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 4029ms
2014-07-01 16:07:07,040 INFO  [RpcServer.handler=16,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server sceplus-vm48.almaden.ibm.com,60020,1404255227840
2014-07-01 16:07:07,040 WARN  [RpcServer.handler=24,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 4054ms
2014-07-01 16:07:07,040 INFO  [RpcServer.handler=24,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server sceplus-vm48.almaden.ibm.com,60020,1404255227840
2014-07-01 16:07:07,040 WARN  [RpcServer.handler=38,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 4077ms
2014-07-01 16:07:07,040 INFO  [RpcServer.handler=38,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server sceplus-vm48.almaden.ibm.com,60020,1404255227840
2014-07-01 16:07:07,041 WARN  [RpcServer.handler=41,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 4104ms
2014-07-01 16:07:07,041 INFO  [RpcServer.handler=41,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server sceplus-vm48.almaden.ibm.com,60020,1404255227840
2014-07-01 16:07:07,049 WARN  [RpcServer.handler=21,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 4135ms
2014-07-01 16:07:07,049 INFO  [RpcServer.handler=21,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server sceplus-vm48.almaden.ibm.com,60020,1404255227840
2014-07-01 16:07:07,049 WARN  [RpcServer.handler=8,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 4159ms
2014-07-01 16:07:07,049 INFO  [RpcServer.handler=8,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server sceplus-vm48.almaden.ibm.com,60020,1404255227840
2014-07-01 16:07:07,050 WARN  [RpcServer.handler=14,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 4176ms
2014-07-01 16:07:07,050 INFO  [RpcServer.handler=14,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server sceplus-vm48.almaden.ibm.com,60020,1404255227840
2014-07-01 16:07:07,050 WARN  [RpcServer.handler=33,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 4206ms
2014-07-01 16:07:07,050 INFO  [RpcServer.handler=33,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server sceplus-vm48.almaden.ibm.com,60020,1404255227840
2014-07-01 16:07:07,050 WARN  [RpcServer.handler=3,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 4329ms
2014-07-01 16:07:07,050 INFO  [RpcServer.handler=3,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server sceplus-vm48.almaden.ibm.com,60020,1404255227840
2014-07-01 16:07:07,973 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-01 16:07:08,019 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 11934 synced till here 11901
2014-07-01 16:07:08,322 DEBUG [RpcServer.handler=32,port=60020] regionserver.HRegion: Flush requested on usertable,,1404255616194.510634b895ee706ff306728824babf96.
2014-07-01 16:07:08,513 DEBUG [MemStoreFlusher.0] util.FSUtils: DFS Client does not support most favored nodes create; using default create
2014-07-01 16:07:08,525 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1404255227840/sceplus-vm48.almaden.ibm.com%2C60020%2C1404255227840.1404256019038 with entries=127, filesize=96.7m; new WAL /hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1404255227840/sceplus-vm48.almaden.ibm.com%2C60020%2C1404255227840.1404256027974
2014-07-01 16:07:10,355 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-01 16:07:10,441 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 12046 synced till here 12029
2014-07-01 16:07:10,532 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1404255227840/sceplus-vm48.almaden.ibm.com%2C60020%2C1404255227840.1404256027974 with entries=112, filesize=101.9m; new WAL /hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1404255227840/sceplus-vm48.almaden.ibm.com%2C60020%2C1404255227840.1404256030355
2014-07-01 16:07:11,650 WARN  [RpcServer.handler=44,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":11162,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.58:34446","starttimems":1404256020487,"queuetimems":1,"class":"HRegionServer","responsesize":18037,"method":"Multi"}
2014-07-01 16:07:11,651 WARN  [RpcServer.handler=36,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":11057,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.58:34446","starttimems":1404256020593,"queuetimems":0,"class":"HRegionServer","responsesize":18548,"method":"Multi"}
2014-07-01 16:07:11,651 WARN  [RpcServer.handler=10,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":10958,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.58:34446","starttimems":1404256020693,"queuetimems":1,"class":"HRegionServer","responsesize":19263,"method":"Multi"}
2014-07-01 16:07:11,659 WARN  [RpcServer.handler=35,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":10949,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.58:34446","starttimems":1404256020710,"queuetimems":0,"class":"HRegionServer","responsesize":19745,"method":"Multi"}
2014-07-01 16:07:11,673 WARN  [RpcServer.handler=20,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":11057,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.58:34446","starttimems":1404256020616,"queuetimems":0,"class":"HRegionServer","responsesize":18448,"method":"Multi"}
2014-07-01 16:07:11,674 WARN  [RpcServer.handler=32,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":11099,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.58:34446","starttimems":1404256020574,"queuetimems":1,"class":"HRegionServer","responsesize":19175,"method":"Multi"}
2014-07-01 16:07:11,675 WARN  [RpcServer.handler=47,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":11157,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.58:34446","starttimems":1404256020518,"queuetimems":1,"class":"HRegionServer","responsesize":18624,"method":"Multi"}
2014-07-01 16:07:11,683 WARN  [RpcServer.handler=7,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":11049,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.58:34446","starttimems":1404256020633,"queuetimems":0,"class":"HRegionServer","responsesize":18602,"method":"Multi"}
2014-07-01 16:07:11,859 WARN  [RpcServer.handler=1,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":11205,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.58:34446","starttimems":1404256020653,"queuetimems":0,"class":"HRegionServer","responsesize":18776,"method":"Multi"}
2014-07-01 16:07:11,859 WARN  [RpcServer.handler=43,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":11186,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.58:34446","starttimems":1404256020672,"queuetimems":0,"class":"HRegionServer","responsesize":18638,"method":"Multi"}
2014-07-01 16:07:11,995 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-01 16:07:12,017 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 12143 synced till here 12127
2014-07-01 16:07:12,117 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1404255227840/sceplus-vm48.almaden.ibm.com%2C60020%2C1404255227840.1404256030355 with entries=97, filesize=81.8m; new WAL /hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1404255227840/sceplus-vm48.almaden.ibm.com%2C60020%2C1404255227840.1404256031995
2014-07-01 16:07:13,455 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-01 16:07:13,489 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 12226 synced till here 12213
2014-07-01 16:07:13,602 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1404255227840/sceplus-vm48.almaden.ibm.com%2C60020%2C1404255227840.1404256031995 with entries=83, filesize=74.8m; new WAL /hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1404255227840/sceplus-vm48.almaden.ibm.com%2C60020%2C1404255227840.1404256033455
2014-07-01 16:07:15,016 INFO  [MemStoreFlusher.1] regionserver.DefaultStoreFlusher: Flushed, sequenceid=1911, memsize=539.3m, hasBloomFilter=true, into tmp file hdfs://master:54310/hbase/data/default/usertable/09108ac86cf3c374e79d1bb78f787b49/.tmp/b1ace32190624cbf83bdb450c826b010
2014-07-01 16:07:15,029 DEBUG [MemStoreFlusher.1] regionserver.HRegionFileSystem: Committing store file hdfs://master:54310/hbase/data/default/usertable/09108ac86cf3c374e79d1bb78f787b49/.tmp/b1ace32190624cbf83bdb450c826b010 as hdfs://master:54310/hbase/data/default/usertable/09108ac86cf3c374e79d1bb78f787b49/family/b1ace32190624cbf83bdb450c826b010
2014-07-01 16:07:15,057 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-01 16:07:15,070 INFO  [MemStoreFlusher.1] regionserver.HStore: Added hdfs://master:54310/hbase/data/default/usertable/09108ac86cf3c374e79d1bb78f787b49/family/b1ace32190624cbf83bdb450c826b010, entries=1963670, sequenceid=1911, filesize=139.8m
2014-07-01 16:07:15,070 INFO  [MemStoreFlusher.1] regionserver.HRegion: Finished memstore flush of ~978.9m/1026410960, currentsize=465.3m/487889600 for region usertable,user3,1404255616194.09108ac86cf3c374e79d1bb78f787b49. in 32530ms, sequenceid=1911, compaction requested=true
2014-07-01 16:07:15,071 DEBUG [MemStoreFlusher.1] regionserver.CompactSplitThread: Small Compaction requested: system; Because: MemStoreFlusher.1; compaction_queue=(0:11), split_queue=0, merge_queue=0
2014-07-01 16:07:15,071 DEBUG [MemStoreFlusher.1] regionserver.HRegion: Started memstore flush for usertable,user5,1404255616194.3e76136f0a8be94ccf3637676c064402., current region memstore size 663.8m
2014-07-01 16:07:15,131 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 12317 synced till here 12299
2014-07-01 16:07:15,162 DEBUG [RpcServer.handler=28,port=60020] regionserver.HRegion: Flush requested on usertable,user3,1404255616194.09108ac86cf3c374e79d1bb78f787b49.
2014-07-01 16:07:15,208 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1404255227840/sceplus-vm48.almaden.ibm.com%2C60020%2C1404255227840.1404256033455 with entries=91, filesize=74.2m; new WAL /hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1404255227840/sceplus-vm48.almaden.ibm.com%2C60020%2C1404255227840.1404256035057
2014-07-01 16:07:15,208 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1404255227840/sceplus-vm48.almaden.ibm.com%2C60020%2C1404255227840.1404255894762
2014-07-01 16:07:15,208 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1404255227840/sceplus-vm48.almaden.ibm.com%2C60020%2C1404255227840.1404255900279
2014-07-01 16:07:15,208 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1404255227840/sceplus-vm48.almaden.ibm.com%2C60020%2C1404255227840.1404255903620
2014-07-01 16:07:15,208 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1404255227840/sceplus-vm48.almaden.ibm.com%2C60020%2C1404255227840.1404255905902
2014-07-01 16:07:15,209 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1404255227840/sceplus-vm48.almaden.ibm.com%2C60020%2C1404255227840.1404255908042
2014-07-01 16:07:15,209 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1404255227840/sceplus-vm48.almaden.ibm.com%2C60020%2C1404255227840.1404255910232
2014-07-01 16:07:15,209 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1404255227840/sceplus-vm48.almaden.ibm.com%2C60020%2C1404255227840.1404255913421
2014-07-01 16:07:15,209 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1404255227840/sceplus-vm48.almaden.ibm.com%2C60020%2C1404255227840.1404255916401
2014-07-01 16:07:15,209 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1404255227840/sceplus-vm48.almaden.ibm.com%2C60020%2C1404255227840.1404255934664
2014-07-01 16:07:15,209 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1404255227840/sceplus-vm48.almaden.ibm.com%2C60020%2C1404255227840.1404255937347
2014-07-01 16:07:15,209 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1404255227840/sceplus-vm48.almaden.ibm.com%2C60020%2C1404255227840.1404255939780
2014-07-01 16:07:15,209 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1404255227840/sceplus-vm48.almaden.ibm.com%2C60020%2C1404255227840.1404255942543
2014-07-01 16:07:15,209 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1404255227840/sceplus-vm48.almaden.ibm.com%2C60020%2C1404255227840.1404255944543
2014-07-01 16:07:15,936 DEBUG [MemStoreFlusher.1] util.FSUtils: DFS Client does not support most favored nodes create; using default create
2014-07-01 16:07:16,737 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-01 16:07:16,785 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 12411 synced till here 12402
2014-07-01 16:07:16,871 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1404255227840/sceplus-vm48.almaden.ibm.com%2C60020%2C1404255227840.1404256035057 with entries=94, filesize=68.8m; new WAL /hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1404255227840/sceplus-vm48.almaden.ibm.com%2C60020%2C1404255227840.1404256036738
2014-07-01 16:07:18,247 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-01 16:07:18,306 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 12501 synced till here 12488
2014-07-01 16:07:18,372 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1404255227840/sceplus-vm48.almaden.ibm.com%2C60020%2C1404255227840.1404256036738 with entries=90, filesize=66.3m; new WAL /hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1404255227840/sceplus-vm48.almaden.ibm.com%2C60020%2C1404255227840.1404256038248
2014-07-01 16:07:20,260 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-01 16:07:20,302 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 12592 synced till here 12575
2014-07-01 16:07:20,504 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1404255227840/sceplus-vm48.almaden.ibm.com%2C60020%2C1404255227840.1404256038248 with entries=91, filesize=77.2m; new WAL /hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1404255227840/sceplus-vm48.almaden.ibm.com%2C60020%2C1404255227840.1404256040260
2014-07-01 16:07:22,071 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-01 16:07:22,097 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 12687 synced till here 12674
2014-07-01 16:07:22,230 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1404255227840/sceplus-vm48.almaden.ibm.com%2C60020%2C1404255227840.1404256040260 with entries=95, filesize=80.5m; new WAL /hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1404255227840/sceplus-vm48.almaden.ibm.com%2C60020%2C1404255227840.1404256042072
2014-07-01 16:07:24,422 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-01 16:07:24,461 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 12785 synced till here 12780
2014-07-01 16:07:24,477 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1404255227840/sceplus-vm48.almaden.ibm.com%2C60020%2C1404255227840.1404256042072 with entries=98, filesize=69.1m; new WAL /hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1404255227840/sceplus-vm48.almaden.ibm.com%2C60020%2C1404255227840.1404256044422
2014-07-01 16:07:26,039 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-01 16:07:26,074 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 12882 synced till here 12866
2014-07-01 16:07:26,159 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1404255227840/sceplus-vm48.almaden.ibm.com%2C60020%2C1404255227840.1404256044422 with entries=97, filesize=74.9m; new WAL /hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1404255227840/sceplus-vm48.almaden.ibm.com%2C60020%2C1404255227840.1404256046039
2014-07-01 16:07:28,316 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-01 16:07:28,609 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 12992 synced till here 12977
2014-07-01 16:07:29,501 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1404255227840/sceplus-vm48.almaden.ibm.com%2C60020%2C1404255227840.1404256046039 with entries=110, filesize=90.9m; new WAL /hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1404255227840/sceplus-vm48.almaden.ibm.com%2C60020%2C1404255227840.1404256048316
2014-07-01 16:07:29,705 INFO  [RpcServer.handler=46,port=60020] regionserver.MemStoreFlusher: Blocking updates on sceplus-vm48.almaden.ibm.com,60020,1404255227840: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-01 16:07:29,705 INFO  [RpcServer.handler=6,port=60020] regionserver.MemStoreFlusher: Blocking updates on sceplus-vm48.almaden.ibm.com,60020,1404255227840: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-01 16:07:29,711 INFO  [RpcServer.handler=11,port=60020] regionserver.MemStoreFlusher: Blocking updates on sceplus-vm48.almaden.ibm.com,60020,1404255227840: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-01 16:07:29,712 INFO  [RpcServer.handler=39,port=60020] regionserver.MemStoreFlusher: Blocking updates on sceplus-vm48.almaden.ibm.com,60020,1404255227840: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-01 16:07:29,740 INFO  [RpcServer.handler=19,port=60020] regionserver.MemStoreFlusher: Blocking updates on sceplus-vm48.almaden.ibm.com,60020,1404255227840: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-01 16:07:29,742 INFO  [RpcServer.handler=0,port=60020] regionserver.MemStoreFlusher: Blocking updates on sceplus-vm48.almaden.ibm.com,60020,1404255227840: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-01 16:07:29,761 INFO  [RpcServer.handler=28,port=60020] regionserver.MemStoreFlusher: Blocking updates on sceplus-vm48.almaden.ibm.com,60020,1404255227840: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-01 16:07:29,767 INFO  [RpcServer.handler=9,port=60020] regionserver.MemStoreFlusher: Blocking updates on sceplus-vm48.almaden.ibm.com,60020,1404255227840: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-01 16:07:29,768 INFO  [RpcServer.handler=34,port=60020] regionserver.MemStoreFlusher: Blocking updates on sceplus-vm48.almaden.ibm.com,60020,1404255227840: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-01 16:07:29,780 INFO  [RpcServer.handler=16,port=60020] regionserver.MemStoreFlusher: Blocking updates on sceplus-vm48.almaden.ibm.com,60020,1404255227840: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-01 16:07:29,783 INFO  [RpcServer.handler=30,port=60020] regionserver.MemStoreFlusher: Blocking updates on sceplus-vm48.almaden.ibm.com,60020,1404255227840: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-01 16:07:29,783 INFO  [RpcServer.handler=13,port=60020] regionserver.MemStoreFlusher: Blocking updates on sceplus-vm48.almaden.ibm.com,60020,1404255227840: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-01 16:07:29,784 INFO  [RpcServer.handler=27,port=60020] regionserver.MemStoreFlusher: Blocking updates on sceplus-vm48.almaden.ibm.com,60020,1404255227840: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-01 16:07:29,784 INFO  [RpcServer.handler=26,port=60020] regionserver.MemStoreFlusher: Blocking updates on sceplus-vm48.almaden.ibm.com,60020,1404255227840: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-01 16:07:29,796 INFO  [RpcServer.handler=37,port=60020] regionserver.MemStoreFlusher: Blocking updates on sceplus-vm48.almaden.ibm.com,60020,1404255227840: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-01 16:07:29,817 INFO  [RpcServer.handler=25,port=60020] regionserver.MemStoreFlusher: Blocking updates on sceplus-vm48.almaden.ibm.com,60020,1404255227840: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-01 16:07:29,839 INFO  [RpcServer.handler=10,port=60020] regionserver.MemStoreFlusher: Blocking updates on sceplus-vm48.almaden.ibm.com,60020,1404255227840: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-01 16:07:29,864 INFO  [RpcServer.handler=42,port=60020] regionserver.MemStoreFlusher: Blocking updates on sceplus-vm48.almaden.ibm.com,60020,1404255227840: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-01 16:07:29,888 INFO  [RpcServer.handler=15,port=60020] regionserver.MemStoreFlusher: Blocking updates on sceplus-vm48.almaden.ibm.com,60020,1404255227840: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-01 16:07:29,915 INFO  [RpcServer.handler=7,port=60020] regionserver.MemStoreFlusher: Blocking updates on sceplus-vm48.almaden.ibm.com,60020,1404255227840: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-01 16:07:29,916 INFO  [RpcServer.handler=38,port=60020] regionserver.MemStoreFlusher: Blocking updates on sceplus-vm48.almaden.ibm.com,60020,1404255227840: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-01 16:07:29,917 INFO  [RpcServer.handler=29,port=60020] regionserver.MemStoreFlusher: Blocking updates on sceplus-vm48.almaden.ibm.com,60020,1404255227840: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-01 16:07:29,921 INFO  [RpcServer.handler=3,port=60020] regionserver.MemStoreFlusher: Blocking updates on sceplus-vm48.almaden.ibm.com,60020,1404255227840: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-01 16:07:29,963 INFO  [RpcServer.handler=22,port=60020] regionserver.MemStoreFlusher: Blocking updates on sceplus-vm48.almaden.ibm.com,60020,1404255227840: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-01 16:07:31,881 INFO  [RpcServer.handler=21,port=60020] regionserver.MemStoreFlusher: Blocking updates on sceplus-vm48.almaden.ibm.com,60020,1404255227840: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-01 16:07:31,887 INFO  [RpcServer.handler=40,port=60020] regionserver.MemStoreFlusher: Blocking updates on sceplus-vm48.almaden.ibm.com,60020,1404255227840: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-01 16:07:31,922 INFO  [RpcServer.handler=31,port=60020] regionserver.MemStoreFlusher: Blocking updates on sceplus-vm48.almaden.ibm.com,60020,1404255227840: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-01 16:07:31,941 INFO  [RpcServer.handler=32,port=60020] regionserver.MemStoreFlusher: Blocking updates on sceplus-vm48.almaden.ibm.com,60020,1404255227840: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-01 16:07:31,964 INFO  [RpcServer.handler=45,port=60020] regionserver.MemStoreFlusher: Blocking updates on sceplus-vm48.almaden.ibm.com,60020,1404255227840: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-01 16:07:31,989 INFO  [RpcServer.handler=12,port=60020] regionserver.MemStoreFlusher: Blocking updates on sceplus-vm48.almaden.ibm.com,60020,1404255227840: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-01 16:07:31,995 INFO  [RpcServer.handler=23,port=60020] regionserver.MemStoreFlusher: Blocking updates on sceplus-vm48.almaden.ibm.com,60020,1404255227840: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-01 16:07:32,024 INFO  [RpcServer.handler=1,port=60020] regionserver.MemStoreFlusher: Blocking updates on sceplus-vm48.almaden.ibm.com,60020,1404255227840: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-01 16:07:32,043 INFO  [RpcServer.handler=8,port=60020] regionserver.MemStoreFlusher: Blocking updates on sceplus-vm48.almaden.ibm.com,60020,1404255227840: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-01 16:07:32,049 INFO  [MemStoreFlusher.1] regionserver.DefaultStoreFlusher: Flushed, sequenceid=2181, memsize=302.6m, hasBloomFilter=true, into tmp file hdfs://master:54310/hbase/data/default/usertable/3e76136f0a8be94ccf3637676c064402/.tmp/792af824f9b14a9b94cf8593f0c25e14
2014-07-01 16:07:32,060 DEBUG [MemStoreFlusher.1] regionserver.HRegionFileSystem: Committing store file hdfs://master:54310/hbase/data/default/usertable/3e76136f0a8be94ccf3637676c064402/.tmp/792af824f9b14a9b94cf8593f0c25e14 as hdfs://master:54310/hbase/data/default/usertable/3e76136f0a8be94ccf3637676c064402/family/792af824f9b14a9b94cf8593f0c25e14
2014-07-01 16:07:32,060 INFO  [RpcServer.handler=5,port=60020] regionserver.MemStoreFlusher: Blocking updates on sceplus-vm48.almaden.ibm.com,60020,1404255227840: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-01 16:07:32,067 INFO  [MemStoreFlusher.1] regionserver.HStore: Added hdfs://master:54310/hbase/data/default/usertable/3e76136f0a8be94ccf3637676c064402/family/792af824f9b14a9b94cf8593f0c25e14, entries=1101710, sequenceid=2181, filesize=78.5m
2014-07-01 16:07:32,068 INFO  [MemStoreFlusher.1] regionserver.HRegion: Finished memstore flush of ~671.1m/703669680, currentsize=202.0m/211776640 for region usertable,user5,1404255616194.3e76136f0a8be94ccf3637676c064402. in 16997ms, sequenceid=2181, compaction requested=false
2014-07-01 16:07:32,068 WARN  [RpcServer.handler=5,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 8ms
2014-07-01 16:07:32,068 INFO  [RpcServer.handler=5,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server sceplus-vm48.almaden.ibm.com,60020,1404255227840
2014-07-01 16:07:32,068 DEBUG [MemStoreFlusher.1] regionserver.HRegion: Started memstore flush for usertable,user8,1404255616195.9e9a6a0fd8aef92a09081d29900a50f3., current region memstore size 657.2m
2014-07-01 16:07:32,068 WARN  [RpcServer.handler=8,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 25ms
2014-07-01 16:07:32,068 INFO  [RpcServer.handler=8,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server sceplus-vm48.almaden.ibm.com,60020,1404255227840
2014-07-01 16:07:32,068 WARN  [RpcServer.handler=1,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 44ms
2014-07-01 16:07:32,069 INFO  [RpcServer.handler=1,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server sceplus-vm48.almaden.ibm.com,60020,1404255227840
2014-07-01 16:07:32,069 WARN  [RpcServer.handler=23,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 74ms
2014-07-01 16:07:32,069 INFO  [RpcServer.handler=23,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server sceplus-vm48.almaden.ibm.com,60020,1404255227840
2014-07-01 16:07:32,071 WARN  [RpcServer.handler=12,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 82ms
2014-07-01 16:07:32,071 INFO  [RpcServer.handler=12,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server sceplus-vm48.almaden.ibm.com,60020,1404255227840
2014-07-01 16:07:32,072 WARN  [RpcServer.handler=45,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 108ms
2014-07-01 16:07:32,072 INFO  [RpcServer.handler=45,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server sceplus-vm48.almaden.ibm.com,60020,1404255227840
2014-07-01 16:07:32,073 WARN  [RpcServer.handler=32,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 132ms
2014-07-01 16:07:32,073 INFO  [RpcServer.handler=32,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server sceplus-vm48.almaden.ibm.com,60020,1404255227840
2014-07-01 16:07:32,074 WARN  [RpcServer.handler=31,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 153ms
2014-07-01 16:07:32,074 INFO  [RpcServer.handler=31,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server sceplus-vm48.almaden.ibm.com,60020,1404255227840
2014-07-01 16:07:32,075 WARN  [RpcServer.handler=40,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 188ms
2014-07-01 16:07:32,075 INFO  [RpcServer.handler=40,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server sceplus-vm48.almaden.ibm.com,60020,1404255227840
2014-07-01 16:07:32,075 WARN  [RpcServer.handler=21,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 194ms
2014-07-01 16:07:32,075 INFO  [RpcServer.handler=21,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server sceplus-vm48.almaden.ibm.com,60020,1404255227840
2014-07-01 16:07:32,075 WARN  [RpcServer.handler=22,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 2112ms
2014-07-01 16:07:32,076 INFO  [RpcServer.handler=22,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server sceplus-vm48.almaden.ibm.com,60020,1404255227840
2014-07-01 16:07:32,076 WARN  [RpcServer.handler=3,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 2155ms
2014-07-01 16:07:32,076 INFO  [RpcServer.handler=3,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server sceplus-vm48.almaden.ibm.com,60020,1404255227840
2014-07-01 16:07:32,077 WARN  [RpcServer.handler=29,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 2160ms
2014-07-01 16:07:32,077 INFO  [RpcServer.handler=29,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server sceplus-vm48.almaden.ibm.com,60020,1404255227840
2014-07-01 16:07:32,077 WARN  [RpcServer.handler=38,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 2161ms
2014-07-01 16:07:32,077 INFO  [RpcServer.handler=38,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server sceplus-vm48.almaden.ibm.com,60020,1404255227840
2014-07-01 16:07:32,077 WARN  [RpcServer.handler=7,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 2162ms
2014-07-01 16:07:32,077 INFO  [RpcServer.handler=7,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server sceplus-vm48.almaden.ibm.com,60020,1404255227840
2014-07-01 16:07:32,078 WARN  [RpcServer.handler=15,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 2190ms
2014-07-01 16:07:32,079 INFO  [RpcServer.handler=15,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server sceplus-vm48.almaden.ibm.com,60020,1404255227840
2014-07-01 16:07:32,079 WARN  [RpcServer.handler=42,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 2215ms
2014-07-01 16:07:32,079 INFO  [RpcServer.handler=42,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server sceplus-vm48.almaden.ibm.com,60020,1404255227840
2014-07-01 16:07:32,079 WARN  [RpcServer.handler=10,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 2240ms
2014-07-01 16:07:32,079 INFO  [RpcServer.handler=10,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server sceplus-vm48.almaden.ibm.com,60020,1404255227840
2014-07-01 16:07:32,088 WARN  [RpcServer.handler=25,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 2271ms
2014-07-01 16:07:32,808 INFO  [RpcServer.handler=25,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server sceplus-vm48.almaden.ibm.com,60020,1404255227840
2014-07-01 16:07:32,808 WARN  [RpcServer.handler=37,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 3012ms
2014-07-01 16:07:32,808 INFO  [RpcServer.handler=37,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server sceplus-vm48.almaden.ibm.com,60020,1404255227840
2014-07-01 16:07:32,814 WARN  [RpcServer.handler=26,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 3030ms
2014-07-01 16:07:32,814 INFO  [RpcServer.handler=26,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server sceplus-vm48.almaden.ibm.com,60020,1404255227840
2014-07-01 16:07:32,815 WARN  [RpcServer.handler=27,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 3030ms
2014-07-01 16:07:32,815 INFO  [RpcServer.handler=27,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server sceplus-vm48.almaden.ibm.com,60020,1404255227840
2014-07-01 16:07:32,815 WARN  [RpcServer.handler=13,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 3032ms
2014-07-01 16:07:32,815 INFO  [RpcServer.handler=13,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server sceplus-vm48.almaden.ibm.com,60020,1404255227840
2014-07-01 16:07:32,815 WARN  [RpcServer.handler=30,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 3032ms
2014-07-01 16:07:32,815 INFO  [RpcServer.handler=30,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server sceplus-vm48.almaden.ibm.com,60020,1404255227840
2014-07-01 16:07:32,815 WARN  [RpcServer.handler=16,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 3035ms
2014-07-01 16:07:32,815 INFO  [RpcServer.handler=16,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server sceplus-vm48.almaden.ibm.com,60020,1404255227840
2014-07-01 16:07:32,823 WARN  [RpcServer.handler=34,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 3055ms
2014-07-01 16:07:32,823 INFO  [RpcServer.handler=34,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server sceplus-vm48.almaden.ibm.com,60020,1404255227840
2014-07-01 16:07:32,823 WARN  [RpcServer.handler=9,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 3056ms
2014-07-01 16:07:32,823 INFO  [RpcServer.handler=9,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server sceplus-vm48.almaden.ibm.com,60020,1404255227840
2014-07-01 16:07:32,823 WARN  [RpcServer.handler=28,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 3063ms
2014-07-01 16:07:32,824 INFO  [RpcServer.handler=28,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server sceplus-vm48.almaden.ibm.com,60020,1404255227840
2014-07-01 16:07:32,824 WARN  [RpcServer.handler=0,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 3082ms
2014-07-01 16:07:32,824 INFO  [RpcServer.handler=0,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server sceplus-vm48.almaden.ibm.com,60020,1404255227840
2014-07-01 16:07:32,824 WARN  [RpcServer.handler=19,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 3084ms
2014-07-01 16:07:32,824 INFO  [RpcServer.handler=19,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server sceplus-vm48.almaden.ibm.com,60020,1404255227840
2014-07-01 16:07:32,824 WARN  [RpcServer.handler=39,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 3112ms
2014-07-01 16:07:32,824 INFO  [RpcServer.handler=39,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server sceplus-vm48.almaden.ibm.com,60020,1404255227840
2014-07-01 16:07:32,826 WARN  [RpcServer.handler=11,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 3115ms
2014-07-01 16:07:32,826 INFO  [RpcServer.handler=11,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server sceplus-vm48.almaden.ibm.com,60020,1404255227840
2014-07-01 16:07:32,826 WARN  [RpcServer.handler=6,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 3121ms
2014-07-01 16:07:32,827 INFO  [RpcServer.handler=6,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server sceplus-vm48.almaden.ibm.com,60020,1404255227840
2014-07-01 16:07:32,837 WARN  [RpcServer.handler=46,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 3132ms
2014-07-01 16:07:32,837 INFO  [RpcServer.handler=46,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server sceplus-vm48.almaden.ibm.com,60020,1404255227840
2014-07-01 16:07:33,371 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-01 16:07:33,465 DEBUG [MemStoreFlusher.1] util.FSUtils: DFS Client does not support most favored nodes create; using default create
2014-07-01 16:07:33,691 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 13113 synced till here 13075
2014-07-01 16:07:33,844 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1404255227840/sceplus-vm48.almaden.ibm.com%2C60020%2C1404255227840.1404256048316 with entries=121, filesize=93.2m; new WAL /hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1404255227840/sceplus-vm48.almaden.ibm.com%2C60020%2C1404255227840.1404256053372
2014-07-01 16:07:34,984 DEBUG [RpcServer.handler=17,port=60020] regionserver.HRegion: Flush requested on usertable,user5,1404255616194.3e76136f0a8be94ccf3637676c064402.
2014-07-01 16:07:35,242 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-01 16:07:35,291 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 13202 synced till here 13184
2014-07-01 16:07:35,704 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1404255227840/sceplus-vm48.almaden.ibm.com%2C60020%2C1404255227840.1404256053372 with entries=89, filesize=77.0m; new WAL /hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1404255227840/sceplus-vm48.almaden.ibm.com%2C60020%2C1404255227840.1404256055243
2014-07-01 16:07:36,928 INFO  [MemStoreFlusher.0] regionserver.DefaultStoreFlusher: Flushed, sequenceid=2083, memsize=507.5m, hasBloomFilter=true, into tmp file hdfs://master:54310/hbase/data/default/usertable/431aad56a67499b48936eca121e3178e/.tmp/bdca6fcd8c874aa8969b3b0bf198ef58
2014-07-01 16:07:36,950 DEBUG [MemStoreFlusher.0] regionserver.HRegionFileSystem: Committing store file hdfs://master:54310/hbase/data/default/usertable/431aad56a67499b48936eca121e3178e/.tmp/bdca6fcd8c874aa8969b3b0bf198ef58 as hdfs://master:54310/hbase/data/default/usertable/431aad56a67499b48936eca121e3178e/family/bdca6fcd8c874aa8969b3b0bf198ef58
2014-07-01 16:07:36,975 INFO  [MemStoreFlusher.0] regionserver.HStore: Added hdfs://master:54310/hbase/data/default/usertable/431aad56a67499b48936eca121e3178e/family/bdca6fcd8c874aa8969b3b0bf198ef58, entries=1847950, sequenceid=2083, filesize=131.6m
2014-07-01 16:07:36,975 INFO  [MemStoreFlusher.0] regionserver.HRegion: Finished memstore flush of ~919.6m/964301440, currentsize=452.0m/473942400 for region usertable,user7,1404255616194.431aad56a67499b48936eca121e3178e. in 29997ms, sequenceid=2083, compaction requested=true
2014-07-01 16:07:36,976 DEBUG [MemStoreFlusher.0] regionserver.CompactSplitThread: Small Compaction requested: system; Because: MemStoreFlusher.0; compaction_queue=(0:12), split_queue=0, merge_queue=0
2014-07-01 16:07:36,976 DEBUG [MemStoreFlusher.0] regionserver.HRegion: Started memstore flush for usertable,,1404255616194.510634b895ee706ff306728824babf96., current region memstore size 666.7m
2014-07-01 16:07:36,985 DEBUG [RpcServer.handler=19,port=60020] regionserver.HRegion: Flush requested on usertable,user7,1404255616194.431aad56a67499b48936eca121e3178e.
2014-07-01 16:07:37,086 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-01 16:07:37,154 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1404255227840/sceplus-vm48.almaden.ibm.com%2C60020%2C1404255227840.1404256055243 with entries=68, filesize=62.7m; new WAL /hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1404255227840/sceplus-vm48.almaden.ibm.com%2C60020%2C1404255227840.1404256057086
2014-07-01 16:07:37,154 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1404255227840/sceplus-vm48.almaden.ibm.com%2C60020%2C1404255227840.1404255946458
2014-07-01 16:07:37,154 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1404255227840/sceplus-vm48.almaden.ibm.com%2C60020%2C1404255227840.1404255952992
2014-07-01 16:07:37,155 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1404255227840/sceplus-vm48.almaden.ibm.com%2C60020%2C1404255227840.1404255954526
2014-07-01 16:07:37,155 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1404255227840/sceplus-vm48.almaden.ibm.com%2C60020%2C1404255227840.1404255956316
2014-07-01 16:07:37,155 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1404255227840/sceplus-vm48.almaden.ibm.com%2C60020%2C1404255227840.1404255958068
2014-07-01 16:07:37,155 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1404255227840/sceplus-vm48.almaden.ibm.com%2C60020%2C1404255227840.1404255960029
2014-07-01 16:07:37,155 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1404255227840/sceplus-vm48.almaden.ibm.com%2C60020%2C1404255227840.1404255961492
2014-07-01 16:07:37,155 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1404255227840/sceplus-vm48.almaden.ibm.com%2C60020%2C1404255227840.1404255963093
2014-07-01 16:07:37,156 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1404255227840/sceplus-vm48.almaden.ibm.com%2C60020%2C1404255227840.1404255965071
2014-07-01 16:07:37,156 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1404255227840/sceplus-vm48.almaden.ibm.com%2C60020%2C1404255227840.1404255967086
2014-07-01 16:07:38,168 DEBUG [MemStoreFlusher.0] util.FSUtils: DFS Client does not support most favored nodes create; using default create
2014-07-01 16:07:38,519 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-01 16:07:38,577 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 13357 synced till here 13343
2014-07-01 16:07:38,687 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1404255227840/sceplus-vm48.almaden.ibm.com%2C60020%2C1404255227840.1404256057086 with entries=87, filesize=71.6m; new WAL /hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1404255227840/sceplus-vm48.almaden.ibm.com%2C60020%2C1404255227840.1404256058519
2014-07-01 16:07:40,255 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-01 16:07:40,297 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 13445 synced till here 13435
2014-07-01 16:07:40,416 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1404255227840/sceplus-vm48.almaden.ibm.com%2C60020%2C1404255227840.1404256058519 with entries=88, filesize=72.4m; new WAL /hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1404255227840/sceplus-vm48.almaden.ibm.com%2C60020%2C1404255227840.1404256060259
2014-07-01 16:07:42,563 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-01 16:07:42,720 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 13544 synced till here 13525
2014-07-01 16:07:42,840 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1404255227840/sceplus-vm48.almaden.ibm.com%2C60020%2C1404255227840.1404256060259 with entries=99, filesize=78.9m; new WAL /hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1404255227840/sceplus-vm48.almaden.ibm.com%2C60020%2C1404255227840.1404256062564
2014-07-01 16:07:44,241 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-01 16:07:44,291 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 13625 synced till here 13616
2014-07-01 16:07:44,364 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1404255227840/sceplus-vm48.almaden.ibm.com%2C60020%2C1404255227840.1404256062564 with entries=81, filesize=66.5m; new WAL /hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1404255227840/sceplus-vm48.almaden.ibm.com%2C60020%2C1404255227840.1404256064241
2014-07-01 16:07:45,798 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-01 16:07:45,829 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 13716 synced till here 13708
2014-07-01 16:07:45,918 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1404255227840/sceplus-vm48.almaden.ibm.com%2C60020%2C1404255227840.1404256064241 with entries=91, filesize=74.0m; new WAL /hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1404255227840/sceplus-vm48.almaden.ibm.com%2C60020%2C1404255227840.1404256065799
2014-07-01 16:07:47,761 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-01 16:07:48,360 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 13810 synced till here 13805
2014-07-01 16:07:48,422 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1404255227840/sceplus-vm48.almaden.ibm.com%2C60020%2C1404255227840.1404256065799 with entries=94, filesize=79.9m; new WAL /hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1404255227840/sceplus-vm48.almaden.ibm.com%2C60020%2C1404255227840.1404256067761
2014-07-01 16:07:49,557 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-01 16:07:50,201 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 13894 synced till here 13880
2014-07-01 16:07:50,310 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1404255227840/sceplus-vm48.almaden.ibm.com%2C60020%2C1404255227840.1404256067761 with entries=84, filesize=69.2m; new WAL /hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1404255227840/sceplus-vm48.almaden.ibm.com%2C60020%2C1404255227840.1404256069557
2014-07-01 16:07:51,245 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-01 16:07:51,843 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 13986 synced till here 13978
2014-07-01 16:07:51,910 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1404255227840/sceplus-vm48.almaden.ibm.com%2C60020%2C1404255227840.1404256069557 with entries=92, filesize=68.4m; new WAL /hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1404255227840/sceplus-vm48.almaden.ibm.com%2C60020%2C1404255227840.1404256071246
2014-07-01 16:07:53,004 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-01 16:07:53,041 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 14072 synced till here 14064
2014-07-01 16:07:53,839 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1404255227840/sceplus-vm48.almaden.ibm.com%2C60020%2C1404255227840.1404256071246 with entries=86, filesize=67.8m; new WAL /hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1404255227840/sceplus-vm48.almaden.ibm.com%2C60020%2C1404255227840.1404256073004
2014-07-01 16:07:53,969 INFO  [RpcServer.handler=49,port=60020] regionserver.MemStoreFlusher: Blocking updates on sceplus-vm48.almaden.ibm.com,60020,1404255227840: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-01 16:07:54,182 INFO  [RpcServer.handler=20,port=60020] regionserver.MemStoreFlusher: Blocking updates on sceplus-vm48.almaden.ibm.com,60020,1404255227840: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-01 16:07:54,183 INFO  [RpcServer.handler=14,port=60020] regionserver.MemStoreFlusher: Blocking updates on sceplus-vm48.almaden.ibm.com,60020,1404255227840: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-01 16:07:54,183 INFO  [RpcServer.handler=7,port=60020] regionserver.MemStoreFlusher: Blocking updates on sceplus-vm48.almaden.ibm.com,60020,1404255227840: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-01 16:07:54,183 INFO  [RpcServer.handler=36,port=60020] regionserver.MemStoreFlusher: Blocking updates on sceplus-vm48.almaden.ibm.com,60020,1404255227840: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-01 16:07:54,184 INFO  [RpcServer.handler=24,port=60020] regionserver.MemStoreFlusher: Blocking updates on sceplus-vm48.almaden.ibm.com,60020,1404255227840: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-01 16:07:54,184 INFO  [RpcServer.handler=4,port=60020] regionserver.MemStoreFlusher: Blocking updates on sceplus-vm48.almaden.ibm.com,60020,1404255227840: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-01 16:07:54,184 INFO  [RpcServer.handler=30,port=60020] regionserver.MemStoreFlusher: Blocking updates on sceplus-vm48.almaden.ibm.com,60020,1404255227840: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-01 16:07:54,184 INFO  [RpcServer.handler=3,port=60020] regionserver.MemStoreFlusher: Blocking updates on sceplus-vm48.almaden.ibm.com,60020,1404255227840: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-01 16:07:54,185 INFO  [RpcServer.handler=13,port=60020] regionserver.MemStoreFlusher: Blocking updates on sceplus-vm48.almaden.ibm.com,60020,1404255227840: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-01 16:07:54,185 INFO  [RpcServer.handler=33,port=60020] regionserver.MemStoreFlusher: Blocking updates on sceplus-vm48.almaden.ibm.com,60020,1404255227840: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-01 16:07:54,185 INFO  [RpcServer.handler=12,port=60020] regionserver.MemStoreFlusher: Blocking updates on sceplus-vm48.almaden.ibm.com,60020,1404255227840: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-01 16:07:54,185 INFO  [RpcServer.handler=32,port=60020] regionserver.MemStoreFlusher: Blocking updates on sceplus-vm48.almaden.ibm.com,60020,1404255227840: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-01 16:07:54,185 INFO  [RpcServer.handler=26,port=60020] regionserver.MemStoreFlusher: Blocking updates on sceplus-vm48.almaden.ibm.com,60020,1404255227840: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-01 16:07:54,185 INFO  [RpcServer.handler=25,port=60020] regionserver.MemStoreFlusher: Blocking updates on sceplus-vm48.almaden.ibm.com,60020,1404255227840: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-01 16:07:54,186 INFO  [RpcServer.handler=42,port=60020] regionserver.MemStoreFlusher: Blocking updates on sceplus-vm48.almaden.ibm.com,60020,1404255227840: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-01 16:07:54,186 INFO  [RpcServer.handler=22,port=60020] regionserver.MemStoreFlusher: Blocking updates on sceplus-vm48.almaden.ibm.com,60020,1404255227840: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-01 16:07:54,186 INFO  [RpcServer.handler=9,port=60020] regionserver.MemStoreFlusher: Blocking updates on sceplus-vm48.almaden.ibm.com,60020,1404255227840: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-01 16:07:54,186 INFO  [RpcServer.handler=38,port=60020] regionserver.MemStoreFlusher: Blocking updates on sceplus-vm48.almaden.ibm.com,60020,1404255227840: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-01 16:07:54,649 INFO  [RpcServer.handler=23,port=60020] regionserver.MemStoreFlusher: Blocking updates on sceplus-vm48.almaden.ibm.com,60020,1404255227840: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-01 16:07:54,857 INFO  [RpcServer.handler=5,port=60020] regionserver.MemStoreFlusher: Blocking updates on sceplus-vm48.almaden.ibm.com,60020,1404255227840: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-01 16:07:56,197 INFO  [RpcServer.handler=46,port=60020] regionserver.MemStoreFlusher: Blocking updates on sceplus-vm48.almaden.ibm.com,60020,1404255227840: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-01 16:07:56,339 INFO  [RpcServer.handler=11,port=60020] regionserver.MemStoreFlusher: Blocking updates on sceplus-vm48.almaden.ibm.com,60020,1404255227840: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-01 16:07:56,360 INFO  [RpcServer.handler=0,port=60020] regionserver.MemStoreFlusher: Blocking updates on sceplus-vm48.almaden.ibm.com,60020,1404255227840: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-01 16:07:56,379 INFO  [RpcServer.handler=21,port=60020] regionserver.MemStoreFlusher: Blocking updates on sceplus-vm48.almaden.ibm.com,60020,1404255227840: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-01 16:07:56,397 INFO  [RpcServer.handler=17,port=60020] regionserver.MemStoreFlusher: Blocking updates on sceplus-vm48.almaden.ibm.com,60020,1404255227840: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-01 16:07:56,417 INFO  [RpcServer.handler=44,port=60020] regionserver.MemStoreFlusher: Blocking updates on sceplus-vm48.almaden.ibm.com,60020,1404255227840: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-01 16:07:56,436 INFO  [RpcServer.handler=41,port=60020] regionserver.MemStoreFlusher: Blocking updates on sceplus-vm48.almaden.ibm.com,60020,1404255227840: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-01 16:07:56,463 INFO  [RpcServer.handler=28,port=60020] regionserver.MemStoreFlusher: Blocking updates on sceplus-vm48.almaden.ibm.com,60020,1404255227840: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-01 16:07:56,485 INFO  [RpcServer.handler=19,port=60020] regionserver.MemStoreFlusher: Blocking updates on sceplus-vm48.almaden.ibm.com,60020,1404255227840: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-01 16:07:56,506 INFO  [RpcServer.handler=27,port=60020] regionserver.MemStoreFlusher: Blocking updates on sceplus-vm48.almaden.ibm.com,60020,1404255227840: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-01 16:07:56,510 INFO  [RpcServer.handler=8,port=60020] regionserver.MemStoreFlusher: Blocking updates on sceplus-vm48.almaden.ibm.com,60020,1404255227840: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-01 16:07:56,534 INFO  [RpcServer.handler=31,port=60020] regionserver.MemStoreFlusher: Blocking updates on sceplus-vm48.almaden.ibm.com,60020,1404255227840: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-01 16:07:56,554 INFO  [RpcServer.handler=16,port=60020] regionserver.MemStoreFlusher: Blocking updates on sceplus-vm48.almaden.ibm.com,60020,1404255227840: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-01 16:07:56,556 INFO  [RpcServer.handler=29,port=60020] regionserver.MemStoreFlusher: Blocking updates on sceplus-vm48.almaden.ibm.com,60020,1404255227840: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-01 16:07:56,576 INFO  [RpcServer.handler=47,port=60020] regionserver.MemStoreFlusher: Blocking updates on sceplus-vm48.almaden.ibm.com,60020,1404255227840: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-01 16:07:56,577 INFO  [RpcServer.handler=39,port=60020] regionserver.MemStoreFlusher: Blocking updates on sceplus-vm48.almaden.ibm.com,60020,1404255227840: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-01 16:07:56,577 INFO  [RpcServer.handler=35,port=60020] regionserver.MemStoreFlusher: Blocking updates on sceplus-vm48.almaden.ibm.com,60020,1404255227840: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-01 16:07:56,588 INFO  [RpcServer.handler=40,port=60020] regionserver.MemStoreFlusher: Blocking updates on sceplus-vm48.almaden.ibm.com,60020,1404255227840: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-01 16:07:56,824 INFO  [MemStoreFlusher.0] regionserver.DefaultStoreFlusher: Flushed, sequenceid=2360, memsize=373.4m, hasBloomFilter=true, into tmp file hdfs://master:54310/hbase/data/default/usertable/510634b895ee706ff306728824babf96/.tmp/3cba320e6a1c49f5926fbe0b01a9a19a
2014-07-01 16:07:56,834 DEBUG [MemStoreFlusher.0] regionserver.HRegionFileSystem: Committing store file hdfs://master:54310/hbase/data/default/usertable/510634b895ee706ff306728824babf96/.tmp/3cba320e6a1c49f5926fbe0b01a9a19a as hdfs://master:54310/hbase/data/default/usertable/510634b895ee706ff306728824babf96/family/3cba320e6a1c49f5926fbe0b01a9a19a
2014-07-01 16:07:56,842 INFO  [MemStoreFlusher.0] regionserver.HStore: Added hdfs://master:54310/hbase/data/default/usertable/510634b895ee706ff306728824babf96/family/3cba320e6a1c49f5926fbe0b01a9a19a, entries=1359600, sequenceid=2360, filesize=96.8m
2014-07-01 16:07:56,842 INFO  [MemStoreFlusher.0] regionserver.HRegion: Finished memstore flush of ~671.8m/704428720, currentsize=255.7m/268163280 for region usertable,,1404255616194.510634b895ee706ff306728824babf96. in 19866ms, sequenceid=2360, compaction requested=true
2014-07-01 16:07:56,843 DEBUG [MemStoreFlusher.0] regionserver.CompactSplitThread: Small Compaction requested: system; Because: MemStoreFlusher.0; compaction_queue=(0:13), split_queue=0, merge_queue=0
2014-07-01 16:07:56,843 WARN  [RpcServer.handler=40,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 255ms
2014-07-01 16:07:56,843 INFO  [RpcServer.handler=40,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server sceplus-vm48.almaden.ibm.com,60020,1404255227840
2014-07-01 16:07:56,843 DEBUG [MemStoreFlusher.0] regionserver.HRegion: Started memstore flush for usertable,user3,1404255616194.09108ac86cf3c374e79d1bb78f787b49., current region memstore size 1010.3m
2014-07-01 16:07:56,843 WARN  [RpcServer.handler=35,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 266ms
2014-07-01 16:07:56,843 INFO  [RpcServer.handler=35,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server sceplus-vm48.almaden.ibm.com,60020,1404255227840
2014-07-01 16:07:56,843 WARN  [RpcServer.handler=39,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 266ms
2014-07-01 16:07:56,843 INFO  [RpcServer.handler=39,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server sceplus-vm48.almaden.ibm.com,60020,1404255227840
2014-07-01 16:07:56,843 WARN  [RpcServer.handler=47,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 267ms
2014-07-01 16:07:56,843 INFO  [RpcServer.handler=47,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server sceplus-vm48.almaden.ibm.com,60020,1404255227840
2014-07-01 16:07:56,845 WARN  [RpcServer.handler=29,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 289ms
2014-07-01 16:07:56,845 INFO  [RpcServer.handler=29,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server sceplus-vm48.almaden.ibm.com,60020,1404255227840
2014-07-01 16:07:56,846 WARN  [RpcServer.handler=16,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 291ms
2014-07-01 16:07:56,846 INFO  [RpcServer.handler=16,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server sceplus-vm48.almaden.ibm.com,60020,1404255227840
2014-07-01 16:07:56,846 WARN  [RpcServer.handler=31,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 312ms
2014-07-01 16:07:56,846 INFO  [RpcServer.handler=31,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server sceplus-vm48.almaden.ibm.com,60020,1404255227840
2014-07-01 16:07:56,846 WARN  [RpcServer.handler=8,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 336ms
2014-07-01 16:07:56,846 INFO  [RpcServer.handler=8,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server sceplus-vm48.almaden.ibm.com,60020,1404255227840
2014-07-01 16:07:56,846 WARN  [RpcServer.handler=27,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 340ms
2014-07-01 16:07:56,846 INFO  [RpcServer.handler=27,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server sceplus-vm48.almaden.ibm.com,60020,1404255227840
2014-07-01 16:07:56,846 WARN  [RpcServer.handler=19,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 361ms
2014-07-01 16:07:56,846 INFO  [RpcServer.handler=19,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server sceplus-vm48.almaden.ibm.com,60020,1404255227840
2014-07-01 16:07:56,846 WARN  [RpcServer.handler=28,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 383ms
2014-07-01 16:07:56,846 INFO  [RpcServer.handler=28,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server sceplus-vm48.almaden.ibm.com,60020,1404255227840
2014-07-01 16:07:56,850 WARN  [RpcServer.handler=41,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 414ms
2014-07-01 16:07:56,850 INFO  [RpcServer.handler=41,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server sceplus-vm48.almaden.ibm.com,60020,1404255227840
2014-07-01 16:07:56,850 WARN  [RpcServer.handler=44,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 433ms
2014-07-01 16:07:56,851 INFO  [RpcServer.handler=44,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server sceplus-vm48.almaden.ibm.com,60020,1404255227840
2014-07-01 16:07:56,851 WARN  [RpcServer.handler=17,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 454ms
2014-07-01 16:07:56,851 INFO  [RpcServer.handler=17,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server sceplus-vm48.almaden.ibm.com,60020,1404255227840
2014-07-01 16:07:56,851 WARN  [RpcServer.handler=21,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 472ms
2014-07-01 16:07:56,851 INFO  [RpcServer.handler=21,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server sceplus-vm48.almaden.ibm.com,60020,1404255227840
2014-07-01 16:07:56,855 WARN  [RpcServer.handler=0,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 496ms
2014-07-01 16:07:56,855 INFO  [RpcServer.handler=0,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server sceplus-vm48.almaden.ibm.com,60020,1404255227840
2014-07-01 16:07:56,857 WARN  [RpcServer.handler=11,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 518ms
2014-07-01 16:07:56,857 INFO  [RpcServer.handler=11,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server sceplus-vm48.almaden.ibm.com,60020,1404255227840
2014-07-01 16:07:56,862 WARN  [RpcServer.handler=46,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 665ms
2014-07-01 16:07:56,862 INFO  [RpcServer.handler=46,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server sceplus-vm48.almaden.ibm.com,60020,1404255227840
2014-07-01 16:07:56,862 WARN  [RpcServer.handler=5,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 2005ms
2014-07-01 16:07:56,862 INFO  [RpcServer.handler=5,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server sceplus-vm48.almaden.ibm.com,60020,1404255227840
2014-07-01 16:07:56,866 WARN  [RpcServer.handler=23,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 2217ms
2014-07-01 16:07:56,866 INFO  [RpcServer.handler=23,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server sceplus-vm48.almaden.ibm.com,60020,1404255227840
2014-07-01 16:07:56,870 WARN  [RpcServer.handler=38,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 2901ms
2014-07-01 16:07:56,870 INFO  [RpcServer.handler=38,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server sceplus-vm48.almaden.ibm.com,60020,1404255227840
2014-07-01 16:07:56,874 WARN  [RpcServer.handler=9,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 2813ms
2014-07-01 16:07:56,874 INFO  [RpcServer.handler=9,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server sceplus-vm48.almaden.ibm.com,60020,1404255227840
2014-07-01 16:07:56,874 WARN  [RpcServer.handler=22,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 2811ms
2014-07-01 16:07:56,874 INFO  [RpcServer.handler=22,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server sceplus-vm48.almaden.ibm.com,60020,1404255227840
2014-07-01 16:07:56,875 WARN  [RpcServer.handler=42,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 2811ms
2014-07-01 16:07:56,875 INFO  [RpcServer.handler=42,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server sceplus-vm48.almaden.ibm.com,60020,1404255227840
2014-07-01 16:07:56,877 WARN  [RpcServer.handler=25,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 2813ms
2014-07-01 16:07:56,877 INFO  [RpcServer.handler=25,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server sceplus-vm48.almaden.ibm.com,60020,1404255227840
2014-07-01 16:07:56,877 WARN  [RpcServer.handler=26,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 2811ms
2014-07-01 16:07:56,877 INFO  [RpcServer.handler=26,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server sceplus-vm48.almaden.ibm.com,60020,1404255227840
2014-07-01 16:07:56,879 WARN  [RpcServer.handler=32,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 2810ms
2014-07-01 16:07:57,395 INFO  [RpcServer.handler=32,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server sceplus-vm48.almaden.ibm.com,60020,1404255227840
2014-07-01 16:07:57,398 WARN  [RpcServer.handler=12,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 3329ms
2014-07-01 16:07:57,398 INFO  [RpcServer.handler=12,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server sceplus-vm48.almaden.ibm.com,60020,1404255227840
2014-07-01 16:07:57,398 WARN  [RpcServer.handler=33,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 3328ms
2014-07-01 16:07:57,398 INFO  [RpcServer.handler=33,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server sceplus-vm48.almaden.ibm.com,60020,1404255227840
2014-07-01 16:07:57,400 WARN  [RpcServer.handler=13,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 3330ms
2014-07-01 16:07:57,400 INFO  [RpcServer.handler=13,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server sceplus-vm48.almaden.ibm.com,60020,1404255227840
2014-07-01 16:07:57,400 WARN  [RpcServer.handler=3,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 3330ms
2014-07-01 16:07:57,400 INFO  [RpcServer.handler=3,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server sceplus-vm48.almaden.ibm.com,60020,1404255227840
2014-07-01 16:07:57,400 WARN  [RpcServer.handler=30,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 3328ms
2014-07-01 16:07:57,400 INFO  [RpcServer.handler=30,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server sceplus-vm48.almaden.ibm.com,60020,1404255227840
2014-07-01 16:07:57,400 WARN  [RpcServer.handler=4,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 3325ms
2014-07-01 16:07:57,401 INFO  [RpcServer.handler=4,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server sceplus-vm48.almaden.ibm.com,60020,1404255227840
2014-07-01 16:07:57,405 WARN  [RpcServer.handler=24,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 3329ms
2014-07-01 16:07:57,405 INFO  [RpcServer.handler=24,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server sceplus-vm48.almaden.ibm.com,60020,1404255227840
2014-07-01 16:07:57,406 WARN  [RpcServer.handler=36,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 3327ms
2014-07-01 16:07:57,406 INFO  [RpcServer.handler=36,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server sceplus-vm48.almaden.ibm.com,60020,1404255227840
2014-07-01 16:07:57,414 WARN  [RpcServer.handler=7,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 3335ms
2014-07-01 16:07:57,414 INFO  [RpcServer.handler=7,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server sceplus-vm48.almaden.ibm.com,60020,1404255227840
2014-07-01 16:07:57,415 WARN  [RpcServer.handler=14,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 3331ms
2014-07-01 16:07:57,416 INFO  [RpcServer.handler=14,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server sceplus-vm48.almaden.ibm.com,60020,1404255227840
2014-07-01 16:07:57,416 WARN  [RpcServer.handler=20,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 3292ms
2014-07-01 16:07:57,416 INFO  [RpcServer.handler=20,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server sceplus-vm48.almaden.ibm.com,60020,1404255227840
2014-07-01 16:07:57,416 WARN  [RpcServer.handler=49,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 3447ms
2014-07-01 16:07:57,416 INFO  [RpcServer.handler=49,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server sceplus-vm48.almaden.ibm.com,60020,1404255227840
2014-07-01 16:07:57,870 DEBUG [RpcServer.handler=40,port=60020] regionserver.HRegion: Flush requested on usertable,,1404255616194.510634b895ee706ff306728824babf96.
2014-07-01 16:07:58,030 INFO  [MemStoreFlusher.1] regionserver.DefaultStoreFlusher: Flushed, sequenceid=3865, memsize=497.3m, hasBloomFilter=true, into tmp file hdfs://master:54310/hbase/data/default/usertable/9e9a6a0fd8aef92a09081d29900a50f3/.tmp/77f7a0ace3be4249ab059083a3f795fd
2014-07-01 16:07:58,040 DEBUG [MemStoreFlusher.1] regionserver.HRegionFileSystem: Committing store file hdfs://master:54310/hbase/data/default/usertable/9e9a6a0fd8aef92a09081d29900a50f3/.tmp/77f7a0ace3be4249ab059083a3f795fd as hdfs://master:54310/hbase/data/default/usertable/9e9a6a0fd8aef92a09081d29900a50f3/family/77f7a0ace3be4249ab059083a3f795fd
2014-07-01 16:07:58,087 INFO  [MemStoreFlusher.1] regionserver.HStore: Added hdfs://master:54310/hbase/data/default/usertable/9e9a6a0fd8aef92a09081d29900a50f3/family/77f7a0ace3be4249ab059083a3f795fd, entries=1810810, sequenceid=3865, filesize=129.0m
2014-07-01 16:07:58,091 INFO  [MemStoreFlusher.1] regionserver.HRegion: Finished memstore flush of ~657.2m/689107760, currentsize=261.1m/273833840 for region usertable,user8,1404255616195.9e9a6a0fd8aef92a09081d29900a50f3. in 26023ms, sequenceid=3865, compaction requested=true
2014-07-01 16:07:58,091 DEBUG [MemStoreFlusher.1] regionserver.CompactSplitThread: Small Compaction requested: system; Because: MemStoreFlusher.1; compaction_queue=(0:14), split_queue=0, merge_queue=0
2014-07-01 16:07:58,091 DEBUG [MemStoreFlusher.1] regionserver.HRegion: Started memstore flush for usertable,user5,1404255616194.3e76136f0a8be94ccf3637676c064402., current region memstore size 552.5m
2014-07-01 16:07:58,129 DEBUG [RpcServer.handler=38,port=60020] regionserver.HRegion: Flush requested on usertable,user8,1404255616195.9e9a6a0fd8aef92a09081d29900a50f3.
2014-07-01 16:07:58,214 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-01 16:07:58,313 DEBUG [MemStoreFlusher.0] util.FSUtils: DFS Client does not support most favored nodes create; using default create
2014-07-01 16:07:58,315 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 14180 synced till here 14163
2014-07-01 16:07:58,468 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1404255227840/sceplus-vm48.almaden.ibm.com%2C60020%2C1404255227840.1404256073004 with entries=108, filesize=79.9m; new WAL /hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1404255227840/sceplus-vm48.almaden.ibm.com%2C60020%2C1404255227840.1404256078215
2014-07-01 16:07:58,469 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1404255227840/sceplus-vm48.almaden.ibm.com%2C60020%2C1404255227840.1404255968888
2014-07-01 16:07:58,469 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1404255227840/sceplus-vm48.almaden.ibm.com%2C60020%2C1404255227840.1404255971400
2014-07-01 16:07:58,469 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1404255227840/sceplus-vm48.almaden.ibm.com%2C60020%2C1404255227840.1404255973441
2014-07-01 16:07:58,469 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1404255227840/sceplus-vm48.almaden.ibm.com%2C60020%2C1404255227840.1404255975282
2014-07-01 16:07:58,469 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1404255227840/sceplus-vm48.almaden.ibm.com%2C60020%2C1404255227840.1404255976948
2014-07-01 16:07:58,469 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1404255227840/sceplus-vm48.almaden.ibm.com%2C60020%2C1404255227840.1404255980241
2014-07-01 16:07:58,470 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1404255227840/sceplus-vm48.almaden.ibm.com%2C60020%2C1404255227840.1404255982670
2014-07-01 16:07:58,470 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1404255227840/sceplus-vm48.almaden.ibm.com%2C60020%2C1404255227840.1404255984624
2014-07-01 16:07:58,470 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1404255227840/sceplus-vm48.almaden.ibm.com%2C60020%2C1404255227840.1404255986436
2014-07-01 16:07:58,470 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1404255227840/sceplus-vm48.almaden.ibm.com%2C60020%2C1404255227840.1404255988907
2014-07-01 16:07:58,470 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1404255227840/sceplus-vm48.almaden.ibm.com%2C60020%2C1404255227840.1404255990711
2014-07-01 16:07:58,573 DEBUG [MemStoreFlusher.1] util.FSUtils: DFS Client does not support most favored nodes create; using default create
2014-07-01 16:07:59,780 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-01 16:07:59,807 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 14262 synced till here 14252
2014-07-01 16:07:59,881 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1404255227840/sceplus-vm48.almaden.ibm.com%2C60020%2C1404255227840.1404256078215 with entries=82, filesize=70.9m; new WAL /hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1404255227840/sceplus-vm48.almaden.ibm.com%2C60020%2C1404255227840.1404256079781
2014-07-01 16:08:01,381 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-01 16:08:01,435 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 14348 synced till here 14332
2014-07-01 16:08:01,643 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1404255227840/sceplus-vm48.almaden.ibm.com%2C60020%2C1404255227840.1404256079781 with entries=86, filesize=75.6m; new WAL /hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1404255227840/sceplus-vm48.almaden.ibm.com%2C60020%2C1404255227840.1404256081381
2014-07-01 16:08:03,753 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-01 16:08:03,816 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 14429 synced till here 14425
2014-07-01 16:08:03,876 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1404255227840/sceplus-vm48.almaden.ibm.com%2C60020%2C1404255227840.1404256081381 with entries=81, filesize=66.1m; new WAL /hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1404255227840/sceplus-vm48.almaden.ibm.com%2C60020%2C1404255227840.1404256083754
2014-07-01 16:08:05,571 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-01 16:08:05,588 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 14519 synced till here 14506
2014-07-01 16:08:05,822 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1404255227840/sceplus-vm48.almaden.ibm.com%2C60020%2C1404255227840.1404256083754 with entries=90, filesize=81.2m; new WAL /hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1404255227840/sceplus-vm48.almaden.ibm.com%2C60020%2C1404255227840.1404256085572
2014-07-01 16:08:07,650 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-01 16:08:07,695 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 14612 synced till here 14601
2014-07-01 16:08:07,771 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1404255227840/sceplus-vm48.almaden.ibm.com%2C60020%2C1404255227840.1404256085572 with entries=93, filesize=78.9m; new WAL /hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1404255227840/sceplus-vm48.almaden.ibm.com%2C60020%2C1404255227840.1404256087650
2014-07-01 16:08:09,763 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-01 16:08:09,787 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 14692 synced till here 14684
2014-07-01 16:08:09,918 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1404255227840/sceplus-vm48.almaden.ibm.com%2C60020%2C1404255227840.1404256087650 with entries=80, filesize=68.6m; new WAL /hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1404255227840/sceplus-vm48.almaden.ibm.com%2C60020%2C1404255227840.1404256089764
2014-07-01 16:08:11,673 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-01 16:08:11,705 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 14759 synced till here 14758
2014-07-01 16:08:11,722 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1404255227840/sceplus-vm48.almaden.ibm.com%2C60020%2C1404255227840.1404256089764 with entries=67, filesize=62.0m; new WAL /hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1404255227840/sceplus-vm48.almaden.ibm.com%2C60020%2C1404255227840.1404256091673
2014-07-01 16:08:13,586 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-01 16:08:13,767 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 14844 synced till here 14832
2014-07-01 16:08:13,848 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1404255227840/sceplus-vm48.almaden.ibm.com%2C60020%2C1404255227840.1404256091673 with entries=85, filesize=77.1m; new WAL /hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1404255227840/sceplus-vm48.almaden.ibm.com%2C60020%2C1404255227840.1404256093586
2014-07-01 16:08:15,648 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-01 16:08:15,723 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 14942 synced till here 14923
2014-07-01 16:08:15,738 INFO  [RpcServer.handler=21,port=60020] regionserver.MemStoreFlusher: Blocking updates on sceplus-vm48.almaden.ibm.com,60020,1404255227840: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-01 16:08:15,739 INFO  [RpcServer.handler=45,port=60020] regionserver.MemStoreFlusher: Blocking updates on sceplus-vm48.almaden.ibm.com,60020,1404255227840: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-01 16:08:15,739 INFO  [RpcServer.handler=2,port=60020] regionserver.MemStoreFlusher: Blocking updates on sceplus-vm48.almaden.ibm.com,60020,1404255227840: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-01 16:08:15,740 INFO  [RpcServer.handler=11,port=60020] regionserver.MemStoreFlusher: Blocking updates on sceplus-vm48.almaden.ibm.com,60020,1404255227840: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-01 16:08:15,741 INFO  [RpcServer.handler=18,port=60020] regionserver.MemStoreFlusher: Blocking updates on sceplus-vm48.almaden.ibm.com,60020,1404255227840: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-01 16:08:15,742 INFO  [RpcServer.handler=43,port=60020] regionserver.MemStoreFlusher: Blocking updates on sceplus-vm48.almaden.ibm.com,60020,1404255227840: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-01 16:08:15,742 INFO  [RpcServer.handler=9,port=60020] regionserver.MemStoreFlusher: Blocking updates on sceplus-vm48.almaden.ibm.com,60020,1404255227840: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-01 16:08:15,756 INFO  [RpcServer.handler=12,port=60020] regionserver.MemStoreFlusher: Blocking updates on sceplus-vm48.almaden.ibm.com,60020,1404255227840: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-01 16:08:15,766 INFO  [RpcServer.handler=16,port=60020] regionserver.MemStoreFlusher: Blocking updates on sceplus-vm48.almaden.ibm.com,60020,1404255227840: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-01 16:08:15,766 INFO  [RpcServer.handler=48,port=60020] regionserver.MemStoreFlusher: Blocking updates on sceplus-vm48.almaden.ibm.com,60020,1404255227840: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-01 16:08:15,768 INFO  [RpcServer.handler=3,port=60020] regionserver.MemStoreFlusher: Blocking updates on sceplus-vm48.almaden.ibm.com,60020,1404255227840: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-01 16:08:15,768 INFO  [RpcServer.handler=44,port=60020] regionserver.MemStoreFlusher: Blocking updates on sceplus-vm48.almaden.ibm.com,60020,1404255227840: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-01 16:08:15,769 INFO  [RpcServer.handler=47,port=60020] regionserver.MemStoreFlusher: Blocking updates on sceplus-vm48.almaden.ibm.com,60020,1404255227840: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-01 16:08:15,770 INFO  [RpcServer.handler=14,port=60020] regionserver.MemStoreFlusher: Blocking updates on sceplus-vm48.almaden.ibm.com,60020,1404255227840: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-01 16:08:15,810 INFO  [RpcServer.handler=20,port=60020] regionserver.MemStoreFlusher: Blocking updates on sceplus-vm48.almaden.ibm.com,60020,1404255227840: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-01 16:08:15,810 INFO  [RpcServer.handler=0,port=60020] regionserver.MemStoreFlusher: Blocking updates on sceplus-vm48.almaden.ibm.com,60020,1404255227840: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-01 16:08:15,810 INFO  [RpcServer.handler=6,port=60020] regionserver.MemStoreFlusher: Blocking updates on sceplus-vm48.almaden.ibm.com,60020,1404255227840: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-01 16:08:15,811 INFO  [RpcServer.handler=19,port=60020] regionserver.MemStoreFlusher: Blocking updates on sceplus-vm48.almaden.ibm.com,60020,1404255227840: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-01 16:08:15,821 INFO  [RpcServer.handler=27,port=60020] regionserver.MemStoreFlusher: Blocking updates on sceplus-vm48.almaden.ibm.com,60020,1404255227840: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-01 16:08:15,822 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1404255227840/sceplus-vm48.almaden.ibm.com%2C60020%2C1404255227840.1404256093586 with entries=98, filesize=82.1m; new WAL /hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1404255227840/sceplus-vm48.almaden.ibm.com%2C60020%2C1404255227840.1404256095649
2014-07-01 16:08:15,839 INFO  [RpcServer.handler=22,port=60020] regionserver.MemStoreFlusher: Blocking updates on sceplus-vm48.almaden.ibm.com,60020,1404255227840: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-01 16:08:15,892 INFO  [RpcServer.handler=26,port=60020] regionserver.MemStoreFlusher: Blocking updates on sceplus-vm48.almaden.ibm.com,60020,1404255227840: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-01 16:08:15,893 INFO  [RpcServer.handler=1,port=60020] regionserver.MemStoreFlusher: Blocking updates on sceplus-vm48.almaden.ibm.com,60020,1404255227840: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-01 16:08:16,781 INFO  [RpcServer.handler=15,port=60020] regionserver.MemStoreFlusher: Blocking updates on sceplus-vm48.almaden.ibm.com,60020,1404255227840: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-01 16:08:16,792 INFO  [RpcServer.handler=13,port=60020] regionserver.MemStoreFlusher: Blocking updates on sceplus-vm48.almaden.ibm.com,60020,1404255227840: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-01 16:08:16,803 INFO  [RpcServer.handler=7,port=60020] regionserver.MemStoreFlusher: Blocking updates on sceplus-vm48.almaden.ibm.com,60020,1404255227840: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-01 16:08:16,805 INFO  [RpcServer.handler=38,port=60020] regionserver.MemStoreFlusher: Blocking updates on sceplus-vm48.almaden.ibm.com,60020,1404255227840: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-01 16:08:16,816 INFO  [RpcServer.handler=24,port=60020] regionserver.MemStoreFlusher: Blocking updates on sceplus-vm48.almaden.ibm.com,60020,1404255227840: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-01 16:08:18,056 INFO  [RpcServer.handler=39,port=60020] regionserver.MemStoreFlusher: Blocking updates on sceplus-vm48.almaden.ibm.com,60020,1404255227840: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-01 16:08:18,056 INFO  [RpcServer.handler=29,port=60020] regionserver.MemStoreFlusher: Blocking updates on sceplus-vm48.almaden.ibm.com,60020,1404255227840: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-01 16:08:18,074 INFO  [RpcServer.handler=8,port=60020] regionserver.MemStoreFlusher: Blocking updates on sceplus-vm48.almaden.ibm.com,60020,1404255227840: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-01 16:08:18,112 INFO  [RpcServer.handler=31,port=60020] regionserver.MemStoreFlusher: Blocking updates on sceplus-vm48.almaden.ibm.com,60020,1404255227840: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-01 16:08:18,135 INFO  [RpcServer.handler=36,port=60020] regionserver.MemStoreFlusher: Blocking updates on sceplus-vm48.almaden.ibm.com,60020,1404255227840: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-01 16:08:18,153 INFO  [RpcServer.handler=5,port=60020] regionserver.MemStoreFlusher: Blocking updates on sceplus-vm48.almaden.ibm.com,60020,1404255227840: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-01 16:08:18,155 INFO  [RpcServer.handler=40,port=60020] regionserver.MemStoreFlusher: Blocking updates on sceplus-vm48.almaden.ibm.com,60020,1404255227840: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-01 16:08:18,175 INFO  [RpcServer.handler=17,port=60020] regionserver.MemStoreFlusher: Blocking updates on sceplus-vm48.almaden.ibm.com,60020,1404255227840: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-01 16:08:18,181 INFO  [RpcServer.handler=28,port=60020] regionserver.MemStoreFlusher: Blocking updates on sceplus-vm48.almaden.ibm.com,60020,1404255227840: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-01 16:08:18,196 INFO  [RpcServer.handler=33,port=60020] regionserver.MemStoreFlusher: Blocking updates on sceplus-vm48.almaden.ibm.com,60020,1404255227840: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-01 16:08:18,212 INFO  [RpcServer.handler=10,port=60020] regionserver.MemStoreFlusher: Blocking updates on sceplus-vm48.almaden.ibm.com,60020,1404255227840: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-01 16:08:18,237 INFO  [RpcServer.handler=42,port=60020] regionserver.MemStoreFlusher: Blocking updates on sceplus-vm48.almaden.ibm.com,60020,1404255227840: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-01 16:08:18,255 INFO  [RpcServer.handler=35,port=60020] regionserver.MemStoreFlusher: Blocking updates on sceplus-vm48.almaden.ibm.com,60020,1404255227840: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-01 16:08:18,277 INFO  [RpcServer.handler=37,port=60020] regionserver.MemStoreFlusher: Blocking updates on sceplus-vm48.almaden.ibm.com,60020,1404255227840: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-01 16:08:18,302 INFO  [RpcServer.handler=23,port=60020] regionserver.MemStoreFlusher: Blocking updates on sceplus-vm48.almaden.ibm.com,60020,1404255227840: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-01 16:08:18,329 INFO  [RpcServer.handler=32,port=60020] regionserver.MemStoreFlusher: Blocking updates on sceplus-vm48.almaden.ibm.com,60020,1404255227840: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-01 16:08:18,346 INFO  [RpcServer.handler=49,port=60020] regionserver.MemStoreFlusher: Blocking updates on sceplus-vm48.almaden.ibm.com,60020,1404255227840: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-01 16:08:18,361 INFO  [RpcServer.handler=34,port=60020] regionserver.MemStoreFlusher: Blocking updates on sceplus-vm48.almaden.ibm.com,60020,1404255227840: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-01 16:08:18,364 INFO  [RpcServer.handler=4,port=60020] regionserver.MemStoreFlusher: Blocking updates on sceplus-vm48.almaden.ibm.com,60020,1404255227840: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-01 16:08:18,367 INFO  [RpcServer.handler=46,port=60020] regionserver.MemStoreFlusher: Blocking updates on sceplus-vm48.almaden.ibm.com,60020,1404255227840: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-01 16:08:20,014 INFO  [RpcServer.handler=41,port=60020] regionserver.MemStoreFlusher: Blocking updates on sceplus-vm48.almaden.ibm.com,60020,1404255227840: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-01 16:08:20,040 INFO  [RpcServer.handler=25,port=60020] regionserver.MemStoreFlusher: Blocking updates on sceplus-vm48.almaden.ibm.com,60020,1404255227840: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-01 16:08:20,052 INFO  [RpcServer.handler=30,port=60020] regionserver.MemStoreFlusher: Blocking updates on sceplus-vm48.almaden.ibm.com,60020,1404255227840: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-01 16:08:20,254 INFO  [MemStoreFlusher.1] regionserver.DefaultStoreFlusher: Flushed, sequenceid=2504, memsize=389.3m, hasBloomFilter=true, into tmp file hdfs://master:54310/hbase/data/default/usertable/3e76136f0a8be94ccf3637676c064402/.tmp/79f18001f71240f782a850903a59dfaf
2014-07-01 16:08:20,265 DEBUG [MemStoreFlusher.1] regionserver.HRegionFileSystem: Committing store file hdfs://master:54310/hbase/data/default/usertable/3e76136f0a8be94ccf3637676c064402/.tmp/79f18001f71240f782a850903a59dfaf as hdfs://master:54310/hbase/data/default/usertable/3e76136f0a8be94ccf3637676c064402/family/79f18001f71240f782a850903a59dfaf
2014-07-01 16:08:20,276 INFO  [MemStoreFlusher.1] regionserver.HStore: Added hdfs://master:54310/hbase/data/default/usertable/3e76136f0a8be94ccf3637676c064402/family/79f18001f71240f782a850903a59dfaf, entries=1417490, sequenceid=2504, filesize=100.9m
2014-07-01 16:08:20,276 INFO  [MemStoreFlusher.1] regionserver.HRegion: Finished memstore flush of ~552.5m/579377040, currentsize=282.3m/296040720 for region usertable,user5,1404255616194.3e76136f0a8be94ccf3637676c064402. in 22185ms, sequenceid=2504, compaction requested=false
2014-07-01 16:08:20,277 WARN  [RpcServer.handler=30,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 226ms
2014-07-01 16:08:20,277 INFO  [RpcServer.handler=30,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server sceplus-vm48.almaden.ibm.com,60020,1404255227840
2014-07-01 16:08:20,277 WARN  [RpcServer.handler=25,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 238ms
2014-07-01 16:08:20,277 DEBUG [MemStoreFlusher.1] regionserver.HRegion: Started memstore flush for usertable,user7,1404255616194.431aad56a67499b48936eca121e3178e., current region memstore size 992.6m
2014-07-01 16:08:20,277 INFO  [RpcServer.handler=25,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server sceplus-vm48.almaden.ibm.com,60020,1404255227840
2014-07-01 16:08:20,278 WARN  [RpcServer.handler=41,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 263ms
2014-07-01 16:08:20,278 INFO  [RpcServer.handler=41,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server sceplus-vm48.almaden.ibm.com,60020,1404255227840
2014-07-01 16:08:20,287 WARN  [RpcServer.handler=46,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 1920ms
2014-07-01 16:08:20,287 INFO  [RpcServer.handler=46,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server sceplus-vm48.almaden.ibm.com,60020,1404255227840
2014-07-01 16:08:20,287 WARN  [RpcServer.handler=4,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 1924ms
2014-07-01 16:08:20,287 INFO  [RpcServer.handler=4,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server sceplus-vm48.almaden.ibm.com,60020,1404255227840
2014-07-01 16:08:20,287 WARN  [RpcServer.handler=34,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 1926ms
2014-07-01 16:08:20,287 INFO  [RpcServer.handler=34,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server sceplus-vm48.almaden.ibm.com,60020,1404255227840
2014-07-01 16:08:20,288 WARN  [RpcServer.handler=49,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 1942ms
2014-07-01 16:08:20,288 INFO  [RpcServer.handler=49,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server sceplus-vm48.almaden.ibm.com,60020,1404255227840
2014-07-01 16:08:20,289 WARN  [RpcServer.handler=32,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 1959ms
2014-07-01 16:08:20,289 INFO  [RpcServer.handler=32,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server sceplus-vm48.almaden.ibm.com,60020,1404255227840
2014-07-01 16:08:20,290 WARN  [RpcServer.handler=23,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 1987ms
2014-07-01 16:08:20,290 INFO  [RpcServer.handler=23,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server sceplus-vm48.almaden.ibm.com,60020,1404255227840
2014-07-01 16:08:20,294 WARN  [RpcServer.handler=37,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 2017ms
2014-07-01 16:08:20,294 INFO  [RpcServer.handler=37,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server sceplus-vm48.almaden.ibm.com,60020,1404255227840
2014-07-01 16:08:20,295 WARN  [RpcServer.handler=35,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 2041ms
2014-07-01 16:08:20,295 INFO  [RpcServer.handler=35,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server sceplus-vm48.almaden.ibm.com,60020,1404255227840
2014-07-01 16:08:20,295 WARN  [RpcServer.handler=42,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 2059ms
2014-07-01 16:08:20,295 INFO  [RpcServer.handler=42,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server sceplus-vm48.almaden.ibm.com,60020,1404255227840
2014-07-01 16:08:20,296 WARN  [RpcServer.handler=10,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 2084ms
2014-07-01 16:08:20,296 INFO  [RpcServer.handler=10,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server sceplus-vm48.almaden.ibm.com,60020,1404255227840
2014-07-01 16:08:20,298 WARN  [RpcServer.handler=33,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 2101ms
2014-07-01 16:08:20,298 INFO  [RpcServer.handler=33,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server sceplus-vm48.almaden.ibm.com,60020,1404255227840
2014-07-01 16:08:20,303 WARN  [RpcServer.handler=28,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 2122ms
2014-07-01 16:08:20,303 INFO  [RpcServer.handler=28,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server sceplus-vm48.almaden.ibm.com,60020,1404255227840
2014-07-01 16:08:20,303 WARN  [RpcServer.handler=17,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 2128ms
2014-07-01 16:08:20,303 INFO  [RpcServer.handler=17,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server sceplus-vm48.almaden.ibm.com,60020,1404255227840
2014-07-01 16:08:20,304 WARN  [RpcServer.handler=40,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 2149ms
2014-07-01 16:08:20,304 INFO  [RpcServer.handler=40,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server sceplus-vm48.almaden.ibm.com,60020,1404255227840
2014-07-01 16:08:20,305 WARN  [RpcServer.handler=5,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 2151ms
2014-07-01 16:08:20,305 INFO  [RpcServer.handler=5,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server sceplus-vm48.almaden.ibm.com,60020,1404255227840
2014-07-01 16:08:20,306 WARN  [RpcServer.handler=36,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 2170ms
2014-07-01 16:08:20,307 INFO  [RpcServer.handler=36,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server sceplus-vm48.almaden.ibm.com,60020,1404255227840
2014-07-01 16:08:20,311 WARN  [RpcServer.handler=31,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 2198ms
2014-07-01 16:08:20,311 INFO  [RpcServer.handler=31,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server sceplus-vm48.almaden.ibm.com,60020,1404255227840
2014-07-01 16:08:20,311 WARN  [RpcServer.handler=8,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 2237ms
2014-07-01 16:08:20,311 INFO  [RpcServer.handler=8,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server sceplus-vm48.almaden.ibm.com,60020,1404255227840
2014-07-01 16:08:20,313 WARN  [RpcServer.handler=29,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 2257ms
2014-07-01 16:08:20,313 INFO  [RpcServer.handler=29,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server sceplus-vm48.almaden.ibm.com,60020,1404255227840
2014-07-01 16:08:20,314 WARN  [RpcServer.handler=39,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 2259ms
2014-07-01 16:08:20,315 INFO  [RpcServer.handler=39,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server sceplus-vm48.almaden.ibm.com,60020,1404255227840
2014-07-01 16:08:20,316 WARN  [RpcServer.handler=24,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 3499ms
2014-07-01 16:08:20,316 INFO  [RpcServer.handler=24,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server sceplus-vm48.almaden.ibm.com,60020,1404255227840
2014-07-01 16:08:20,316 WARN  [RpcServer.handler=38,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 3511ms
2014-07-01 16:08:20,316 INFO  [RpcServer.handler=38,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server sceplus-vm48.almaden.ibm.com,60020,1404255227840
2014-07-01 16:08:20,318 WARN  [RpcServer.handler=7,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 3516ms
2014-07-01 16:08:20,318 INFO  [RpcServer.handler=7,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server sceplus-vm48.almaden.ibm.com,60020,1404255227840
2014-07-01 16:08:20,318 WARN  [RpcServer.handler=13,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 3526ms
2014-07-01 16:08:20,319 INFO  [RpcServer.handler=13,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server sceplus-vm48.almaden.ibm.com,60020,1404255227840
2014-07-01 16:08:20,320 WARN  [RpcServer.handler=15,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 3539ms
2014-07-01 16:08:20,320 INFO  [RpcServer.handler=15,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server sceplus-vm48.almaden.ibm.com,60020,1404255227840
2014-07-01 16:08:20,321 WARN  [RpcServer.handler=1,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 4428ms
2014-07-01 16:08:20,321 INFO  [RpcServer.handler=1,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server sceplus-vm48.almaden.ibm.com,60020,1404255227840
2014-07-01 16:08:20,321 WARN  [RpcServer.handler=26,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 4429ms
2014-07-01 16:08:20,321 INFO  [RpcServer.handler=26,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server sceplus-vm48.almaden.ibm.com,60020,1404255227840
2014-07-01 16:08:20,323 WARN  [RpcServer.handler=22,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 4483ms
2014-07-01 16:08:20,323 INFO  [RpcServer.handler=22,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server sceplus-vm48.almaden.ibm.com,60020,1404255227840
2014-07-01 16:08:20,323 WARN  [RpcServer.handler=27,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 4502ms
2014-07-01 16:08:20,323 INFO  [RpcServer.handler=27,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server sceplus-vm48.almaden.ibm.com,60020,1404255227840
2014-07-01 16:08:20,324 WARN  [RpcServer.handler=19,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 4512ms
2014-07-01 16:08:20,324 INFO  [RpcServer.handler=19,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server sceplus-vm48.almaden.ibm.com,60020,1404255227840
2014-07-01 16:08:20,330 WARN  [RpcServer.handler=6,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 4520ms
2014-07-01 16:08:20,330 INFO  [RpcServer.handler=6,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server sceplus-vm48.almaden.ibm.com,60020,1404255227840
2014-07-01 16:08:20,330 WARN  [RpcServer.handler=0,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 4520ms
2014-07-01 16:08:20,330 INFO  [RpcServer.handler=0,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server sceplus-vm48.almaden.ibm.com,60020,1404255227840
2014-07-01 16:08:20,331 WARN  [RpcServer.handler=20,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 4522ms
2014-07-01 16:08:20,331 INFO  [RpcServer.handler=20,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server sceplus-vm48.almaden.ibm.com,60020,1404255227840
2014-07-01 16:08:20,332 WARN  [RpcServer.handler=14,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 4562ms
2014-07-01 16:08:20,332 INFO  [RpcServer.handler=14,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server sceplus-vm48.almaden.ibm.com,60020,1404255227840
2014-07-01 16:08:20,333 WARN  [RpcServer.handler=47,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 4564ms
2014-07-01 16:08:20,334 INFO  [RpcServer.handler=47,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server sceplus-vm48.almaden.ibm.com,60020,1404255227840
2014-07-01 16:08:20,334 WARN  [RpcServer.handler=44,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 4566ms
2014-07-01 16:08:20,334 INFO  [RpcServer.handler=44,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server sceplus-vm48.almaden.ibm.com,60020,1404255227840
2014-07-01 16:08:20,334 WARN  [RpcServer.handler=3,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 4566ms
2014-07-01 16:08:20,334 INFO  [RpcServer.handler=3,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server sceplus-vm48.almaden.ibm.com,60020,1404255227840
2014-07-01 16:08:20,335 WARN  [RpcServer.handler=48,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 4569ms
2014-07-01 16:08:20,335 INFO  [RpcServer.handler=48,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server sceplus-vm48.almaden.ibm.com,60020,1404255227840
2014-07-01 16:08:20,381 WARN  [RpcServer.handler=16,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 4570ms
2014-07-01 16:08:20,381 INFO  [RpcServer.handler=16,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server sceplus-vm48.almaden.ibm.com,60020,1404255227840
2014-07-01 16:08:20,382 WARN  [RpcServer.handler=12,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 4626ms
2014-07-01 16:08:20,383 INFO  [RpcServer.handler=12,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server sceplus-vm48.almaden.ibm.com,60020,1404255227840
2014-07-01 16:08:20,385 WARN  [RpcServer.handler=9,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 4643ms
2014-07-01 16:08:20,386 INFO  [RpcServer.handler=9,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server sceplus-vm48.almaden.ibm.com,60020,1404255227840
2014-07-01 16:08:20,386 WARN  [RpcServer.handler=43,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 4644ms
2014-07-01 16:08:20,386 INFO  [RpcServer.handler=43,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server sceplus-vm48.almaden.ibm.com,60020,1404255227840
2014-07-01 16:08:20,390 WARN  [RpcServer.handler=18,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 4649ms
2014-07-01 16:08:20,390 INFO  [RpcServer.handler=18,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server sceplus-vm48.almaden.ibm.com,60020,1404255227840
2014-07-01 16:08:20,391 WARN  [RpcServer.handler=11,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 4650ms
2014-07-01 16:08:20,391 INFO  [RpcServer.handler=11,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server sceplus-vm48.almaden.ibm.com,60020,1404255227840
2014-07-01 16:08:21,010 WARN  [RpcServer.handler=2,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 4659ms
2014-07-01 16:08:21,011 INFO  [RpcServer.handler=2,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server sceplus-vm48.almaden.ibm.com,60020,1404255227840
2014-07-01 16:08:21,012 WARN  [RpcServer.handler=45,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5274ms
2014-07-01 16:08:21,012 INFO  [RpcServer.handler=45,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server sceplus-vm48.almaden.ibm.com,60020,1404255227840
2014-07-01 16:08:21,012 WARN  [RpcServer.handler=21,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 5274ms
2014-07-01 16:08:21,012 INFO  [RpcServer.handler=21,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server sceplus-vm48.almaden.ibm.com,60020,1404255227840
2014-07-01 16:08:21,193 DEBUG [RpcServer.handler=26,port=60020] regionserver.HRegion: Flush requested on usertable,user5,1404255616194.3e76136f0a8be94ccf3637676c064402.
2014-07-01 16:08:21,760 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-01 16:08:21,804 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 15064 synced till here 15028
2014-07-01 16:08:21,945 DEBUG [MemStoreFlusher.1] util.FSUtils: DFS Client does not support most favored nodes create; using default create
2014-07-01 16:08:22,883 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1404255227840/sceplus-vm48.almaden.ibm.com%2C60020%2C1404255227840.1404256095649 with entries=122, filesize=96.9m; new WAL /hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1404255227840/sceplus-vm48.almaden.ibm.com%2C60020%2C1404255227840.1404256101761
2014-07-01 16:08:24,712 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-01 16:08:24,771 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 15183 synced till here 15158
2014-07-01 16:08:24,936 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1404255227840/sceplus-vm48.almaden.ibm.com%2C60020%2C1404255227840.1404256101761 with entries=119, filesize=103.3m; new WAL /hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1404255227840/sceplus-vm48.almaden.ibm.com%2C60020%2C1404255227840.1404256104712
2014-07-01 16:08:26,138 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-01 16:08:26,174 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1404255227840/sceplus-vm48.almaden.ibm.com%2C60020%2C1404255227840.1404256104712 with entries=71, filesize=66.4m; new WAL /hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1404255227840/sceplus-vm48.almaden.ibm.com%2C60020%2C1404255227840.1404256106138
2014-07-01 16:08:26,599 INFO  [RpcServer.handler=39,port=60020] regionserver.MemStoreFlusher: Blocking updates on sceplus-vm48.almaden.ibm.com,60020,1404255227840: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-01 16:08:26,615 INFO  [RpcServer.handler=17,port=60020] regionserver.MemStoreFlusher: Blocking updates on sceplus-vm48.almaden.ibm.com,60020,1404255227840: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-01 16:08:26,632 INFO  [RpcServer.handler=41,port=60020] regionserver.MemStoreFlusher: Blocking updates on sceplus-vm48.almaden.ibm.com,60020,1404255227840: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-01 16:08:26,643 INFO  [RpcServer.handler=47,port=60020] regionserver.MemStoreFlusher: Blocking updates on sceplus-vm48.almaden.ibm.com,60020,1404255227840: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-01 16:08:26,676 INFO  [RpcServer.handler=44,port=60020] regionserver.MemStoreFlusher: Blocking updates on sceplus-vm48.almaden.ibm.com,60020,1404255227840: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-01 16:08:26,677 INFO  [RpcServer.handler=12,port=60020] regionserver.MemStoreFlusher: Blocking updates on sceplus-vm48.almaden.ibm.com,60020,1404255227840: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-01 16:08:26,679 INFO  [RpcServer.handler=6,port=60020] regionserver.MemStoreFlusher: Blocking updates on sceplus-vm48.almaden.ibm.com,60020,1404255227840: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-01 16:08:26,693 INFO  [RpcServer.handler=25,port=60020] regionserver.MemStoreFlusher: Blocking updates on sceplus-vm48.almaden.ibm.com,60020,1404255227840: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-01 16:08:26,706 INFO  [RpcServer.handler=29,port=60020] regionserver.MemStoreFlusher: Blocking updates on sceplus-vm48.almaden.ibm.com,60020,1404255227840: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-01 16:08:26,716 INFO  [RpcServer.handler=19,port=60020] regionserver.MemStoreFlusher: Blocking updates on sceplus-vm48.almaden.ibm.com,60020,1404255227840: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-01 16:08:26,720 INFO  [RpcServer.handler=7,port=60020] regionserver.MemStoreFlusher: Blocking updates on sceplus-vm48.almaden.ibm.com,60020,1404255227840: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-01 16:08:26,725 INFO  [RpcServer.handler=31,port=60020] regionserver.MemStoreFlusher: Blocking updates on sceplus-vm48.almaden.ibm.com,60020,1404255227840: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-01 16:08:26,732 INFO  [RpcServer.handler=30,port=60020] regionserver.MemStoreFlusher: Blocking updates on sceplus-vm48.almaden.ibm.com,60020,1404255227840: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-01 16:08:26,733 INFO  [RpcServer.handler=14,port=60020] regionserver.MemStoreFlusher: Blocking updates on sceplus-vm48.almaden.ibm.com,60020,1404255227840: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-01 16:08:26,736 INFO  [RpcServer.handler=16,port=60020] regionserver.MemStoreFlusher: Blocking updates on sceplus-vm48.almaden.ibm.com,60020,1404255227840: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-01 16:08:27,015 INFO  [RpcServer.handler=1,port=60020] regionserver.MemStoreFlusher: Blocking updates on sceplus-vm48.almaden.ibm.com,60020,1404255227840: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-01 16:08:27,190 INFO  [RpcServer.handler=11,port=60020] regionserver.MemStoreFlusher: Blocking updates on sceplus-vm48.almaden.ibm.com,60020,1404255227840: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-01 16:08:27,248 INFO  [RpcServer.handler=21,port=60020] regionserver.MemStoreFlusher: Blocking updates on sceplus-vm48.almaden.ibm.com,60020,1404255227840: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-01 16:08:27,454 INFO  [RpcServer.handler=5,port=60020] regionserver.MemStoreFlusher: Blocking updates on sceplus-vm48.almaden.ibm.com,60020,1404255227840: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-01 16:08:28,793 INFO  [RpcServer.handler=3,port=60020] regionserver.MemStoreFlusher: Blocking updates on sceplus-vm48.almaden.ibm.com,60020,1404255227840: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-01 16:08:28,829 INFO  [RpcServer.handler=27,port=60020] regionserver.MemStoreFlusher: Blocking updates on sceplus-vm48.almaden.ibm.com,60020,1404255227840: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-01 16:08:28,856 INFO  [RpcServer.handler=22,port=60020] regionserver.MemStoreFlusher: Blocking updates on sceplus-vm48.almaden.ibm.com,60020,1404255227840: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-01 16:08:28,881 INFO  [RpcServer.handler=0,port=60020] regionserver.MemStoreFlusher: Blocking updates on sceplus-vm48.almaden.ibm.com,60020,1404255227840: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-01 16:08:29,721 INFO  [RpcServer.handler=8,port=60020] regionserver.MemStoreFlusher: Blocking updates on sceplus-vm48.almaden.ibm.com,60020,1404255227840: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-01 16:08:29,739 INFO  [RpcServer.handler=26,port=60020] regionserver.MemStoreFlusher: Blocking updates on sceplus-vm48.almaden.ibm.com,60020,1404255227840: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-01 16:08:29,761 INFO  [RpcServer.handler=35,port=60020] regionserver.MemStoreFlusher: Blocking updates on sceplus-vm48.almaden.ibm.com,60020,1404255227840: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-01 16:08:29,776 INFO  [RpcServer.handler=23,port=60020] regionserver.MemStoreFlusher: Blocking updates on sceplus-vm48.almaden.ibm.com,60020,1404255227840: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-01 16:08:29,803 INFO  [RpcServer.handler=9,port=60020] regionserver.MemStoreFlusher: Blocking updates on sceplus-vm48.almaden.ibm.com,60020,1404255227840: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-01 16:08:29,826 INFO  [RpcServer.handler=2,port=60020] regionserver.MemStoreFlusher: Blocking updates on sceplus-vm48.almaden.ibm.com,60020,1404255227840: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-01 16:08:29,840 INFO  [RpcServer.handler=42,port=60020] regionserver.MemStoreFlusher: Blocking updates on sceplus-vm48.almaden.ibm.com,60020,1404255227840: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-01 16:08:30,081 INFO  [MemStoreFlusher.0] regionserver.DefaultStoreFlusher: Flushed, sequenceid=2497, memsize=558.6m, hasBloomFilter=true, into tmp file hdfs://master:54310/hbase/data/default/usertable/09108ac86cf3c374e79d1bb78f787b49/.tmp/25671ec018384cd28950bb9aa2d615d1
2014-07-01 16:08:30,092 DEBUG [MemStoreFlusher.0] regionserver.HRegionFileSystem: Committing store file hdfs://master:54310/hbase/data/default/usertable/09108ac86cf3c374e79d1bb78f787b49/.tmp/25671ec018384cd28950bb9aa2d615d1 as hdfs://master:54310/hbase/data/default/usertable/09108ac86cf3c374e79d1bb78f787b49/family/25671ec018384cd28950bb9aa2d615d1
2014-07-01 16:08:30,100 INFO  [MemStoreFlusher.0] regionserver.HStore: Added hdfs://master:54310/hbase/data/default/usertable/09108ac86cf3c374e79d1bb78f787b49/family/25671ec018384cd28950bb9aa2d615d1, entries=2033800, sequenceid=2497, filesize=144.8m
2014-07-01 16:08:30,100 INFO  [MemStoreFlusher.0] regionserver.HRegion: Finished memstore flush of ~1010.3m/1059408160, currentsize=428.1m/448916240 for region usertable,user3,1404255616194.09108ac86cf3c374e79d1bb78f787b49. in 33257ms, sequenceid=2497, compaction requested=true
2014-07-01 16:08:30,100 DEBUG [MemStoreFlusher.0] regionserver.CompactSplitThread: Small Compaction requested: system; Because: MemStoreFlusher.0; compaction_queue=(0:15), split_queue=0, merge_queue=0
2014-07-01 16:08:30,101 WARN  [RpcServer.handler=42,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 261ms
2014-07-01 16:08:30,101 INFO  [RpcServer.handler=42,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server sceplus-vm48.almaden.ibm.com,60020,1404255227840
2014-07-01 16:08:30,101 DEBUG [MemStoreFlusher.0] regionserver.HRegion: Started memstore flush for usertable,,1404255616194.510634b895ee706ff306728824babf96., current region memstore size 676.2m
2014-07-01 16:08:30,101 WARN  [RpcServer.handler=2,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 275ms
2014-07-01 16:08:30,101 INFO  [RpcServer.handler=2,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server sceplus-vm48.almaden.ibm.com,60020,1404255227840
2014-07-01 16:08:30,101 WARN  [RpcServer.handler=9,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 298ms
2014-07-01 16:08:30,101 INFO  [RpcServer.handler=9,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server sceplus-vm48.almaden.ibm.com,60020,1404255227840
2014-07-01 16:08:30,101 WARN  [RpcServer.handler=23,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 325ms
2014-07-01 16:08:30,101 INFO  [RpcServer.handler=23,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server sceplus-vm48.almaden.ibm.com,60020,1404255227840
2014-07-01 16:08:30,102 WARN  [RpcServer.handler=35,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 341ms
2014-07-01 16:08:30,102 INFO  [RpcServer.handler=35,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server sceplus-vm48.almaden.ibm.com,60020,1404255227840
2014-07-01 16:08:30,102 WARN  [RpcServer.handler=26,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 363ms
2014-07-01 16:08:30,102 INFO  [RpcServer.handler=26,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server sceplus-vm48.almaden.ibm.com,60020,1404255227840
2014-07-01 16:08:30,102 WARN  [RpcServer.handler=8,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 381ms
2014-07-01 16:08:30,102 INFO  [RpcServer.handler=8,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server sceplus-vm48.almaden.ibm.com,60020,1404255227840
2014-07-01 16:08:30,102 WARN  [RpcServer.handler=0,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 1221ms
2014-07-01 16:08:30,102 INFO  [RpcServer.handler=0,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server sceplus-vm48.almaden.ibm.com,60020,1404255227840
2014-07-01 16:08:30,103 WARN  [RpcServer.handler=22,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 1247ms
2014-07-01 16:08:30,103 INFO  [RpcServer.handler=22,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server sceplus-vm48.almaden.ibm.com,60020,1404255227840
2014-07-01 16:08:30,104 WARN  [RpcServer.handler=27,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 1275ms
2014-07-01 16:08:30,104 INFO  [RpcServer.handler=27,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server sceplus-vm48.almaden.ibm.com,60020,1404255227840
2014-07-01 16:08:30,105 WARN  [RpcServer.handler=3,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 1311ms
2014-07-01 16:08:30,105 INFO  [RpcServer.handler=3,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server sceplus-vm48.almaden.ibm.com,60020,1404255227840
2014-07-01 16:08:30,107 WARN  [RpcServer.handler=5,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 2653ms
2014-07-01 16:08:30,110 INFO  [RpcServer.handler=5,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server sceplus-vm48.almaden.ibm.com,60020,1404255227840
2014-07-01 16:08:30,110 WARN  [RpcServer.handler=21,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 2862ms
2014-07-01 16:08:30,110 INFO  [RpcServer.handler=21,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server sceplus-vm48.almaden.ibm.com,60020,1404255227840
2014-07-01 16:08:30,114 WARN  [RpcServer.handler=11,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 2924ms
2014-07-01 16:08:30,114 INFO  [RpcServer.handler=11,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server sceplus-vm48.almaden.ibm.com,60020,1404255227840
2014-07-01 16:08:30,114 WARN  [RpcServer.handler=1,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 3099ms
2014-07-01 16:08:30,115 INFO  [RpcServer.handler=1,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server sceplus-vm48.almaden.ibm.com,60020,1404255227840
2014-07-01 16:08:30,115 WARN  [RpcServer.handler=16,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 3379ms
2014-07-01 16:08:30,116 INFO  [RpcServer.handler=16,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server sceplus-vm48.almaden.ibm.com,60020,1404255227840
2014-07-01 16:08:30,116 WARN  [RpcServer.handler=14,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 3383ms
2014-07-01 16:08:30,116 INFO  [RpcServer.handler=14,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server sceplus-vm48.almaden.ibm.com,60020,1404255227840
2014-07-01 16:08:30,116 WARN  [RpcServer.handler=30,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 3384ms
2014-07-01 16:08:30,116 INFO  [RpcServer.handler=30,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server sceplus-vm48.almaden.ibm.com,60020,1404255227840
2014-07-01 16:08:30,116 WARN  [RpcServer.handler=31,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 3391ms
2014-07-01 16:08:30,117 INFO  [RpcServer.handler=31,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server sceplus-vm48.almaden.ibm.com,60020,1404255227840
2014-07-01 16:08:30,118 WARN  [RpcServer.handler=7,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 3397ms
2014-07-01 16:08:30,118 INFO  [RpcServer.handler=7,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server sceplus-vm48.almaden.ibm.com,60020,1404255227840
2014-07-01 16:08:30,118 WARN  [RpcServer.handler=19,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 3403ms
2014-07-01 16:08:30,118 INFO  [RpcServer.handler=19,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server sceplus-vm48.almaden.ibm.com,60020,1404255227840
2014-07-01 16:08:30,118 WARN  [RpcServer.handler=29,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 3412ms
2014-07-01 16:08:30,118 INFO  [RpcServer.handler=29,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server sceplus-vm48.almaden.ibm.com,60020,1404255227840
2014-07-01 16:08:30,118 WARN  [RpcServer.handler=25,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 3426ms
2014-07-01 16:08:30,119 INFO  [RpcServer.handler=25,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server sceplus-vm48.almaden.ibm.com,60020,1404255227840
2014-07-01 16:08:30,119 WARN  [RpcServer.handler=6,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 3440ms
2014-07-01 16:08:30,119 INFO  [RpcServer.handler=6,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server sceplus-vm48.almaden.ibm.com,60020,1404255227840
2014-07-01 16:08:30,120 WARN  [RpcServer.handler=12,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 3443ms
2014-07-01 16:08:30,120 INFO  [RpcServer.handler=12,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server sceplus-vm48.almaden.ibm.com,60020,1404255227840
2014-07-01 16:08:30,121 WARN  [RpcServer.handler=44,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 3445ms
2014-07-01 16:08:30,121 INFO  [RpcServer.handler=44,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server sceplus-vm48.almaden.ibm.com,60020,1404255227840
2014-07-01 16:08:30,121 WARN  [RpcServer.handler=47,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 3479ms
2014-07-01 16:08:30,121 INFO  [RpcServer.handler=47,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server sceplus-vm48.almaden.ibm.com,60020,1404255227840
2014-07-01 16:08:30,121 WARN  [RpcServer.handler=41,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 3489ms
2014-07-01 16:08:30,121 INFO  [RpcServer.handler=41,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server sceplus-vm48.almaden.ibm.com,60020,1404255227840
2014-07-01 16:08:30,126 WARN  [RpcServer.handler=17,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 3511ms
2014-07-01 16:08:30,126 INFO  [RpcServer.handler=17,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server sceplus-vm48.almaden.ibm.com,60020,1404255227840
2014-07-01 16:08:30,126 WARN  [RpcServer.handler=39,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 3527ms
2014-07-01 16:08:30,126 INFO  [RpcServer.handler=39,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server sceplus-vm48.almaden.ibm.com,60020,1404255227840
2014-07-01 16:08:30,449 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-01 16:08:30,524 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 15344 synced till here 15336
2014-07-01 16:08:30,567 DEBUG [RpcServer.handler=35,port=60020] regionserver.HRegion: Flush requested on usertable,user3,1404255616194.09108ac86cf3c374e79d1bb78f787b49.
2014-07-01 16:08:30,735 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1404255227840/sceplus-vm48.almaden.ibm.com%2C60020%2C1404255227840.1404256106138 with entries=90, filesize=76.6m; new WAL /hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1404255227840/sceplus-vm48.almaden.ibm.com%2C60020%2C1404255227840.1404256110449
2014-07-01 16:08:30,735 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1404255227840/sceplus-vm48.almaden.ibm.com%2C60020%2C1404255227840.1404255992832
2014-07-01 16:08:30,735 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1404255227840/sceplus-vm48.almaden.ibm.com%2C60020%2C1404255227840.1404256003624
2014-07-01 16:08:30,735 DEBUG [MemStoreFlusher.0] util.FSUtils: DFS Client does not support most favored nodes create; using default create
2014-07-01 16:08:30,735 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1404255227840/sceplus-vm48.almaden.ibm.com%2C60020%2C1404255227840.1404256005572
2014-07-01 16:08:30,736 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1404255227840/sceplus-vm48.almaden.ibm.com%2C60020%2C1404255227840.1404256007108
2014-07-01 16:08:30,736 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1404255227840/sceplus-vm48.almaden.ibm.com%2C60020%2C1404255227840.1404256008971
2014-07-01 16:08:30,736 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1404255227840/sceplus-vm48.almaden.ibm.com%2C60020%2C1404255227840.1404256010471
2014-07-01 16:08:30,736 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1404255227840/sceplus-vm48.almaden.ibm.com%2C60020%2C1404255227840.1404256012104
2014-07-01 16:08:30,736 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1404255227840/sceplus-vm48.almaden.ibm.com%2C60020%2C1404255227840.1404256013948
2014-07-01 16:08:30,736 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1404255227840/sceplus-vm48.almaden.ibm.com%2C60020%2C1404255227840.1404256015511
2014-07-01 16:08:30,736 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1404255227840/sceplus-vm48.almaden.ibm.com%2C60020%2C1404255227840.1404256017199
2014-07-01 16:08:31,944 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-01 16:08:31,964 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 15420 synced till here 15409
2014-07-01 16:08:32,064 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1404255227840/sceplus-vm48.almaden.ibm.com%2C60020%2C1404255227840.1404256110449 with entries=76, filesize=71.8m; new WAL /hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1404255227840/sceplus-vm48.almaden.ibm.com%2C60020%2C1404255227840.1404256111944
2014-07-01 16:08:33,306 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-01 16:08:33,319 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 15507 synced till here 15504
2014-07-01 16:08:33,333 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1404255227840/sceplus-vm48.almaden.ibm.com%2C60020%2C1404255227840.1404256111944 with entries=87, filesize=63.3m; new WAL /hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1404255227840/sceplus-vm48.almaden.ibm.com%2C60020%2C1404255227840.1404256113306
2014-07-01 16:08:34,750 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-01 16:08:34,796 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 15597 synced till here 15590
2014-07-01 16:08:34,842 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1404255227840/sceplus-vm48.almaden.ibm.com%2C60020%2C1404255227840.1404256113306 with entries=90, filesize=67.2m; new WAL /hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1404255227840/sceplus-vm48.almaden.ibm.com%2C60020%2C1404255227840.1404256114750
2014-07-01 16:08:36,169 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-01 16:08:36,202 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 15689 synced till here 15676
2014-07-01 16:08:36,326 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1404255227840/sceplus-vm48.almaden.ibm.com%2C60020%2C1404255227840.1404256114750 with entries=92, filesize=70.4m; new WAL /hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1404255227840/sceplus-vm48.almaden.ibm.com%2C60020%2C1404255227840.1404256116170
2014-07-01 16:08:37,587 DEBUG [regionserver60020-smallCompactions-1404255751957] regionserver.HRegionFileSystem: Committing store file hdfs://master:54310/hbase/data/default/usertable/3e76136f0a8be94ccf3637676c064402/.tmp/e9a284d290304b4a901a91aa250ff643 as hdfs://master:54310/hbase/data/default/usertable/3e76136f0a8be94ccf3637676c064402/family/e9a284d290304b4a901a91aa250ff643
2014-07-01 16:08:37,790 DEBUG [regionserver60020-smallCompactions-1404255751957] regionserver.HStore: Removing store files after compaction...
2014-07-01 16:08:38,002 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-01 16:08:38,071 DEBUG [regionserver60020-smallCompactions-1404255751957] backup.HFileArchiver: Finished archiving from class org.apache.hadoop.hbase.backup.HFileArchiver$FileableStoreFile, file:hdfs://master:54310/hbase/data/default/usertable/3e76136f0a8be94ccf3637676c064402/family/e49b6c77370249f388139db102764e09, to hdfs://master:54310/hbase/archive/data/default/usertable/3e76136f0a8be94ccf3637676c064402/family/e49b6c77370249f388139db102764e09
2014-07-01 16:08:38,231 DEBUG [regionserver60020-smallCompactions-1404255751957] backup.HFileArchiver: Finished archiving from class org.apache.hadoop.hbase.backup.HFileArchiver$FileableStoreFile, file:hdfs://master:54310/hbase/data/default/usertable/3e76136f0a8be94ccf3637676c064402/family/b3616e237843458abd24c08525f4e0b5, to hdfs://master:54310/hbase/archive/data/default/usertable/3e76136f0a8be94ccf3637676c064402/family/b3616e237843458abd24c08525f4e0b5
2014-07-01 16:08:38,255 DEBUG [regionserver60020-smallCompactions-1404255751957] backup.HFileArchiver: Finished archiving from class org.apache.hadoop.hbase.backup.HFileArchiver$FileableStoreFile, file:hdfs://master:54310/hbase/data/default/usertable/3e76136f0a8be94ccf3637676c064402/family/5b970a751db84b4ea94c5a41d59e7f96, to hdfs://master:54310/hbase/archive/data/default/usertable/3e76136f0a8be94ccf3637676c064402/family/5b970a751db84b4ea94c5a41d59e7f96
2014-07-01 16:08:38,260 DEBUG [regionserver60020-smallCompactions-1404255751957] backup.HFileArchiver: Finished archiving from class org.apache.hadoop.hbase.backup.HFileArchiver$FileableStoreFile, file:hdfs://master:54310/hbase/data/default/usertable/3e76136f0a8be94ccf3637676c064402/family/c6582863f548409586f782a8d6933006, to hdfs://master:54310/hbase/archive/data/default/usertable/3e76136f0a8be94ccf3637676c064402/family/c6582863f548409586f782a8d6933006
2014-07-01 16:08:38,262 DEBUG [regionserver60020-smallCompactions-1404255751957] backup.HFileArchiver: Finished archiving from class org.apache.hadoop.hbase.backup.HFileArchiver$FileableStoreFile, file:hdfs://master:54310/hbase/data/default/usertable/3e76136f0a8be94ccf3637676c064402/family/70db44eba76b4511a00fbd62c8850daf, to hdfs://master:54310/hbase/archive/data/default/usertable/3e76136f0a8be94ccf3637676c064402/family/70db44eba76b4511a00fbd62c8850daf
2014-07-01 16:08:38,263 INFO  [regionserver60020-smallCompactions-1404255751957] regionserver.HStore: Completed major compaction of 5 file(s) in family of usertable,user5,1404255616194.3e76136f0a8be94ccf3637676c064402. into e9a284d290304b4a901a91aa250ff643(size=444.1m), total size for store is 623.5m. This selection was in queue for 0sec, and took 1mins, 37sec to execute.
2014-07-01 16:08:38,263 INFO  [regionserver60020-smallCompactions-1404255751957] regionserver.CompactSplitThread: Completed compaction: Request = regionName=usertable,user5,1404255616194.3e76136f0a8be94ccf3637676c064402., storeName=family, fileCount=5, fileSize=468.0m, priority=15, time=76162914041049; duration=1mins, 37sec
2014-07-01 16:08:38,263 DEBUG [regionserver60020-smallCompactions-1404255751957] regionserver.CompactSplitThread: CompactSplitThread Status: compaction_queue=(0:15), split_queue=0, merge_queue=0
2014-07-01 16:08:38,263 DEBUG [regionserver60020-smallCompactions-1404255751957] compactions.RatioBasedCompactionPolicy: Selecting compaction from 6 store files, 0 compacting, 6 eligible, 20 blocking
2014-07-01 16:08:38,264 DEBUG [regionserver60020-smallCompactions-1404255751957] compactions.ExploringCompactionPolicy: Exploring compaction algorithm has selected 6 files of size 672272900 starting at candidate #0 after considering 10 permutations with 10 in ratio
2014-07-01 16:08:38,264 DEBUG [regionserver60020-smallCompactions-1404255751957] regionserver.HStore: 09108ac86cf3c374e79d1bb78f787b49 - family: Initiating major compaction
2014-07-01 16:08:38,264 INFO  [regionserver60020-smallCompactions-1404255751957] regionserver.HRegion: Starting compaction on family in region usertable,user3,1404255616194.09108ac86cf3c374e79d1bb78f787b49.
2014-07-01 16:08:38,264 INFO  [regionserver60020-smallCompactions-1404255751957] regionserver.HStore: Starting compaction of 6 file(s) in family of usertable,user3,1404255616194.09108ac86cf3c374e79d1bb78f787b49. into tmpdir=hdfs://master:54310/hbase/data/default/usertable/09108ac86cf3c374e79d1bb78f787b49/.tmp, totalSize=641.1m
2014-07-01 16:08:38,264 DEBUG [regionserver60020-smallCompactions-1404255751957] compactions.Compactor: Compacting hdfs://master:54310/hbase/data/default/usertable/09108ac86cf3c374e79d1bb78f787b49/family/b77ae91bf1934355ae9e1b0c228c9263, keycount=96578, bloomtype=ROW, size=68.8m, encoding=NONE, seqNum=494, earliestPutTs=1404255694579
2014-07-01 16:08:38,264 DEBUG [regionserver60020-smallCompactions-1404255751957] compactions.Compactor: Compacting hdfs://master:54310/hbase/data/default/usertable/09108ac86cf3c374e79d1bb78f787b49/family/d5ccec31c1f64850baaee00b94bd5a8a, keycount=118226, bloomtype=ROW, size=84.2m, encoding=NONE, seqNum=703, earliestPutTs=1404255745006
2014-07-01 16:08:38,265 DEBUG [regionserver60020-smallCompactions-1404255751957] compactions.Compactor: Compacting hdfs://master:54310/hbase/data/default/usertable/09108ac86cf3c374e79d1bb78f787b49/family/3e086712d9304c5f873e794c8e5e93aa, keycount=141755, bloomtype=ROW, size=100.9m, encoding=NONE, seqNum=923, earliestPutTs=1404255781264
2014-07-01 16:08:38,265 DEBUG [regionserver60020-smallCompactions-1404255751957] compactions.Compactor: Compacting hdfs://master:54310/hbase/data/default/usertable/09108ac86cf3c374e79d1bb78f787b49/family/1aaaed3484934af4972b2511e5e1a501, keycount=144093, bloomtype=ROW, size=102.6m, encoding=NONE, seqNum=1333, earliestPutTs=1404255822993
2014-07-01 16:08:38,265 DEBUG [regionserver60020-smallCompactions-1404255751957] compactions.Compactor: Compacting hdfs://master:54310/hbase/data/default/usertable/09108ac86cf3c374e79d1bb78f787b49/family/b1ace32190624cbf83bdb450c826b010, keycount=196367, bloomtype=ROW, size=139.8m, encoding=NONE, seqNum=1911, earliestPutTs=1404255899727
2014-07-01 16:08:38,265 DEBUG [regionserver60020-smallCompactions-1404255751957] compactions.Compactor: Compacting hdfs://master:54310/hbase/data/default/usertable/09108ac86cf3c374e79d1bb78f787b49/family/25671ec018384cd28950bb9aa2d615d1, keycount=203380, bloomtype=ROW, size=144.8m, encoding=NONE, seqNum=2497, earliestPutTs=1404256009251
2014-07-01 16:08:38,294 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1404255227840/sceplus-vm48.almaden.ibm.com%2C60020%2C1404255227840.1404256116170 with entries=108, filesize=86.5m; new WAL /hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1404255227840/sceplus-vm48.almaden.ibm.com%2C60020%2C1404255227840.1404256118003
2014-07-01 16:08:38,603 DEBUG [regionserver60020-smallCompactions-1404255751957] util.FSUtils: DFS Client does not support most favored nodes create; using default create
2014-07-01 16:08:40,413 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-01 16:08:40,559 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 15901 synced till here 15887
2014-07-01 16:08:40,669 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1404255227840/sceplus-vm48.almaden.ibm.com%2C60020%2C1404255227840.1404256118003 with entries=104, filesize=81.7m; new WAL /hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1404255227840/sceplus-vm48.almaden.ibm.com%2C60020%2C1404255227840.1404256120413
2014-07-01 16:08:42,777 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-01 16:08:42,822 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 15998 synced till here 15988
2014-07-01 16:08:42,944 INFO  [RpcServer.handler=26,port=60020] regionserver.MemStoreFlusher: Blocking updates on sceplus-vm48.almaden.ibm.com,60020,1404255227840: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-01 16:08:42,948 INFO  [RpcServer.handler=36,port=60020] regionserver.MemStoreFlusher: Blocking updates on sceplus-vm48.almaden.ibm.com,60020,1404255227840: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-01 16:08:43,201 INFO  [RpcServer.handler=29,port=60020] regionserver.MemStoreFlusher: Blocking updates on sceplus-vm48.almaden.ibm.com,60020,1404255227840: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-01 16:08:43,205 INFO  [RpcServer.handler=25,port=60020] regionserver.MemStoreFlusher: Blocking updates on sceplus-vm48.almaden.ibm.com,60020,1404255227840: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-01 16:08:43,207 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1404255227840/sceplus-vm48.almaden.ibm.com%2C60020%2C1404255227840.1404256120413 with entries=97, filesize=72.2m; new WAL /hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1404255227840/sceplus-vm48.almaden.ibm.com%2C60020%2C1404255227840.1404256122778
2014-07-01 16:08:43,208 INFO  [RpcServer.handler=27,port=60020] regionserver.MemStoreFlusher: Blocking updates on sceplus-vm48.almaden.ibm.com,60020,1404255227840: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-01 16:08:43,214 INFO  [RpcServer.handler=42,port=60020] regionserver.MemStoreFlusher: Blocking updates on sceplus-vm48.almaden.ibm.com,60020,1404255227840: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-01 16:08:43,215 INFO  [RpcServer.handler=0,port=60020] regionserver.MemStoreFlusher: Blocking updates on sceplus-vm48.almaden.ibm.com,60020,1404255227840: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-01 16:08:43,215 INFO  [RpcServer.handler=8,port=60020] regionserver.MemStoreFlusher: Blocking updates on sceplus-vm48.almaden.ibm.com,60020,1404255227840: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-01 16:08:43,215 INFO  [RpcServer.handler=33,port=60020] regionserver.MemStoreFlusher: Blocking updates on sceplus-vm48.almaden.ibm.com,60020,1404255227840: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-01 16:08:43,264 INFO  [RpcServer.handler=30,port=60020] regionserver.MemStoreFlusher: Blocking updates on sceplus-vm48.almaden.ibm.com,60020,1404255227840: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-01 16:08:43,264 INFO  [RpcServer.handler=1,port=60020] regionserver.MemStoreFlusher: Blocking updates on sceplus-vm48.almaden.ibm.com,60020,1404255227840: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-01 16:08:43,268 INFO  [RpcServer.handler=17,port=60020] regionserver.MemStoreFlusher: Blocking updates on sceplus-vm48.almaden.ibm.com,60020,1404255227840: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-01 16:08:43,271 INFO  [RpcServer.handler=12,port=60020] regionserver.MemStoreFlusher: Blocking updates on sceplus-vm48.almaden.ibm.com,60020,1404255227840: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-01 16:08:43,301 INFO  [RpcServer.handler=38,port=60020] regionserver.MemStoreFlusher: Blocking updates on sceplus-vm48.almaden.ibm.com,60020,1404255227840: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-01 16:08:43,348 INFO  [RpcServer.handler=3,port=60020] regionserver.MemStoreFlusher: Blocking updates on sceplus-vm48.almaden.ibm.com,60020,1404255227840: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-01 16:08:43,348 INFO  [RpcServer.handler=48,port=60020] regionserver.MemStoreFlusher: Blocking updates on sceplus-vm48.almaden.ibm.com,60020,1404255227840: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-01 16:08:43,349 INFO  [RpcServer.handler=2,port=60020] regionserver.MemStoreFlusher: Blocking updates on sceplus-vm48.almaden.ibm.com,60020,1404255227840: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-01 16:08:43,371 INFO  [RpcServer.handler=14,port=60020] regionserver.MemStoreFlusher: Blocking updates on sceplus-vm48.almaden.ibm.com,60020,1404255227840: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-01 16:08:43,372 INFO  [RpcServer.handler=24,port=60020] regionserver.MemStoreFlusher: Blocking updates on sceplus-vm48.almaden.ibm.com,60020,1404255227840: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-01 16:08:43,372 INFO  [RpcServer.handler=10,port=60020] regionserver.MemStoreFlusher: Blocking updates on sceplus-vm48.almaden.ibm.com,60020,1404255227840: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-01 16:08:43,497 INFO  [RpcServer.handler=40,port=60020] regionserver.MemStoreFlusher: Blocking updates on sceplus-vm48.almaden.ibm.com,60020,1404255227840: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-01 16:08:43,523 INFO  [RpcServer.handler=49,port=60020] regionserver.MemStoreFlusher: Blocking updates on sceplus-vm48.almaden.ibm.com,60020,1404255227840: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-01 16:08:44,138 INFO  [RpcServer.handler=9,port=60020] regionserver.MemStoreFlusher: Blocking updates on sceplus-vm48.almaden.ibm.com,60020,1404255227840: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-01 16:08:44,153 INFO  [RpcServer.handler=11,port=60020] regionserver.MemStoreFlusher: Blocking updates on sceplus-vm48.almaden.ibm.com,60020,1404255227840: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-01 16:08:45,076 INFO  [RpcServer.handler=4,port=60020] regionserver.MemStoreFlusher: Blocking updates on sceplus-vm48.almaden.ibm.com,60020,1404255227840: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-01 16:08:45,113 INFO  [RpcServer.handler=6,port=60020] regionserver.MemStoreFlusher: Blocking updates on sceplus-vm48.almaden.ibm.com,60020,1404255227840: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-01 16:08:45,149 INFO  [RpcServer.handler=18,port=60020] regionserver.MemStoreFlusher: Blocking updates on sceplus-vm48.almaden.ibm.com,60020,1404255227840: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-01 16:08:45,174 INFO  [RpcServer.handler=46,port=60020] regionserver.MemStoreFlusher: Blocking updates on sceplus-vm48.almaden.ibm.com,60020,1404255227840: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-01 16:08:45,195 INFO  [RpcServer.handler=31,port=60020] regionserver.MemStoreFlusher: Blocking updates on sceplus-vm48.almaden.ibm.com,60020,1404255227840: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-01 16:08:45,220 INFO  [RpcServer.handler=32,port=60020] regionserver.MemStoreFlusher: Blocking updates on sceplus-vm48.almaden.ibm.com,60020,1404255227840: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-01 16:08:45,247 INFO  [RpcServer.handler=39,port=60020] regionserver.MemStoreFlusher: Blocking updates on sceplus-vm48.almaden.ibm.com,60020,1404255227840: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-01 16:08:45,267 INFO  [RpcServer.handler=16,port=60020] regionserver.MemStoreFlusher: Blocking updates on sceplus-vm48.almaden.ibm.com,60020,1404255227840: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-01 16:08:45,292 INFO  [RpcServer.handler=22,port=60020] regionserver.MemStoreFlusher: Blocking updates on sceplus-vm48.almaden.ibm.com,60020,1404255227840: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-01 16:08:45,313 INFO  [RpcServer.handler=15,port=60020] regionserver.MemStoreFlusher: Blocking updates on sceplus-vm48.almaden.ibm.com,60020,1404255227840: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-01 16:08:47,065 INFO  [RpcServer.handler=41,port=60020] regionserver.MemStoreFlusher: Blocking updates on sceplus-vm48.almaden.ibm.com,60020,1404255227840: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-01 16:08:47,075 INFO  [RpcServer.handler=35,port=60020] regionserver.MemStoreFlusher: Blocking updates on sceplus-vm48.almaden.ibm.com,60020,1404255227840: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-01 16:08:47,076 INFO  [RpcServer.handler=21,port=60020] regionserver.MemStoreFlusher: Blocking updates on sceplus-vm48.almaden.ibm.com,60020,1404255227840: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-01 16:08:47,078 INFO  [RpcServer.handler=13,port=60020] regionserver.MemStoreFlusher: Blocking updates on sceplus-vm48.almaden.ibm.com,60020,1404255227840: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-01 16:08:47,078 INFO  [RpcServer.handler=20,port=60020] regionserver.MemStoreFlusher: Blocking updates on sceplus-vm48.almaden.ibm.com,60020,1404255227840: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-01 16:08:47,079 INFO  [RpcServer.handler=28,port=60020] regionserver.MemStoreFlusher: Blocking updates on sceplus-vm48.almaden.ibm.com,60020,1404255227840: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-01 16:08:47,080 INFO  [RpcServer.handler=19,port=60020] regionserver.MemStoreFlusher: Blocking updates on sceplus-vm48.almaden.ibm.com,60020,1404255227840: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-01 16:08:47,082 INFO  [RpcServer.handler=47,port=60020] regionserver.MemStoreFlusher: Blocking updates on sceplus-vm48.almaden.ibm.com,60020,1404255227840: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-01 16:08:47,132 INFO  [RpcServer.handler=5,port=60020] regionserver.MemStoreFlusher: Blocking updates on sceplus-vm48.almaden.ibm.com,60020,1404255227840: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-01 16:08:47,162 INFO  [RpcServer.handler=23,port=60020] regionserver.MemStoreFlusher: Blocking updates on sceplus-vm48.almaden.ibm.com,60020,1404255227840: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-01 16:08:47,192 INFO  [RpcServer.handler=34,port=60020] regionserver.MemStoreFlusher: Blocking updates on sceplus-vm48.almaden.ibm.com,60020,1404255227840: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-01 16:08:47,222 INFO  [RpcServer.handler=43,port=60020] regionserver.MemStoreFlusher: Blocking updates on sceplus-vm48.almaden.ibm.com,60020,1404255227840: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-01 16:08:47,237 INFO  [RpcServer.handler=44,port=60020] regionserver.MemStoreFlusher: Blocking updates on sceplus-vm48.almaden.ibm.com,60020,1404255227840: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-01 16:08:47,254 INFO  [RpcServer.handler=37,port=60020] regionserver.MemStoreFlusher: Blocking updates on sceplus-vm48.almaden.ibm.com,60020,1404255227840: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-01 16:08:47,545 INFO  [MemStoreFlusher.0] regionserver.DefaultStoreFlusher: Flushed, sequenceid=2757, memsize=331.0m, hasBloomFilter=true, into tmp file hdfs://master:54310/hbase/data/default/usertable/510634b895ee706ff306728824babf96/.tmp/264e326d8c1b48629cc72749b12b782b
2014-07-01 16:08:47,555 DEBUG [MemStoreFlusher.0] regionserver.HRegionFileSystem: Committing store file hdfs://master:54310/hbase/data/default/usertable/510634b895ee706ff306728824babf96/.tmp/264e326d8c1b48629cc72749b12b782b as hdfs://master:54310/hbase/data/default/usertable/510634b895ee706ff306728824babf96/family/264e326d8c1b48629cc72749b12b782b
2014-07-01 16:08:47,563 INFO  [MemStoreFlusher.0] regionserver.HStore: Added hdfs://master:54310/hbase/data/default/usertable/510634b895ee706ff306728824babf96/family/264e326d8c1b48629cc72749b12b782b, entries=1205270, sequenceid=2757, filesize=85.8m
2014-07-01 16:08:47,563 INFO  [MemStoreFlusher.0] regionserver.HRegion: Finished memstore flush of ~676.2m/709035280, currentsize=208.2m/218281200 for region usertable,,1404255616194.510634b895ee706ff306728824babf96. in 17462ms, sequenceid=2757, compaction requested=true
2014-07-01 16:08:47,564 DEBUG [MemStoreFlusher.0] regionserver.CompactSplitThread: Small Compaction requested: system; Because: MemStoreFlusher.0; compaction_queue=(0:15), split_queue=0, merge_queue=0
2014-07-01 16:08:47,564 WARN  [RpcServer.handler=37,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 310ms
2014-07-01 16:08:47,564 INFO  [RpcServer.handler=37,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server sceplus-vm48.almaden.ibm.com,60020,1404255227840
2014-07-01 16:08:47,564 WARN  [RpcServer.handler=44,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 328ms
2014-07-01 16:08:47,564 DEBUG [MemStoreFlusher.0] regionserver.HRegion: Started memstore flush for usertable,user8,1404255616195.9e9a6a0fd8aef92a09081d29900a50f3., current region memstore size 589.9m
2014-07-01 16:08:47,564 INFO  [RpcServer.handler=44,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server sceplus-vm48.almaden.ibm.com,60020,1404255227840
2014-07-01 16:08:47,565 WARN  [RpcServer.handler=43,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 342ms
2014-07-01 16:08:47,565 INFO  [RpcServer.handler=43,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server sceplus-vm48.almaden.ibm.com,60020,1404255227840
2014-07-01 16:08:47,565 WARN  [RpcServer.handler=34,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 373ms
2014-07-01 16:08:47,565 INFO  [RpcServer.handler=34,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server sceplus-vm48.almaden.ibm.com,60020,1404255227840
2014-07-01 16:08:47,566 WARN  [RpcServer.handler=23,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 404ms
2014-07-01 16:08:47,566 INFO  [RpcServer.handler=23,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server sceplus-vm48.almaden.ibm.com,60020,1404255227840
2014-07-01 16:08:47,566 WARN  [RpcServer.handler=5,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 434ms
2014-07-01 16:08:47,566 INFO  [RpcServer.handler=5,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server sceplus-vm48.almaden.ibm.com,60020,1404255227840
2014-07-01 16:08:47,566 WARN  [RpcServer.handler=47,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 487ms
2014-07-01 16:08:47,566 INFO  [RpcServer.handler=47,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server sceplus-vm48.almaden.ibm.com,60020,1404255227840
2014-07-01 16:08:47,566 WARN  [RpcServer.handler=19,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 486ms
2014-07-01 16:08:47,566 INFO  [RpcServer.handler=19,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server sceplus-vm48.almaden.ibm.com,60020,1404255227840
2014-07-01 16:08:47,566 WARN  [RpcServer.handler=28,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 488ms
2014-07-01 16:08:47,567 INFO  [RpcServer.handler=28,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server sceplus-vm48.almaden.ibm.com,60020,1404255227840
2014-07-01 16:08:47,567 WARN  [RpcServer.handler=20,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 489ms
2014-07-01 16:08:47,567 INFO  [RpcServer.handler=20,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server sceplus-vm48.almaden.ibm.com,60020,1404255227840
2014-07-01 16:08:47,567 WARN  [RpcServer.handler=13,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 489ms
2014-07-01 16:08:47,567 INFO  [RpcServer.handler=13,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server sceplus-vm48.almaden.ibm.com,60020,1404255227840
2014-07-01 16:08:47,567 WARN  [RpcServer.handler=21,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 491ms
2014-07-01 16:08:47,567 INFO  [RpcServer.handler=21,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server sceplus-vm48.almaden.ibm.com,60020,1404255227840
2014-07-01 16:08:47,569 WARN  [RpcServer.handler=35,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 494ms
2014-07-01 16:08:47,570 INFO  [RpcServer.handler=35,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server sceplus-vm48.almaden.ibm.com,60020,1404255227840
2014-07-01 16:08:47,570 WARN  [RpcServer.handler=41,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 505ms
2014-07-01 16:08:47,570 INFO  [RpcServer.handler=41,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server sceplus-vm48.almaden.ibm.com,60020,1404255227840
2014-07-01 16:08:47,570 WARN  [RpcServer.handler=15,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 2257ms
2014-07-01 16:08:47,571 INFO  [RpcServer.handler=15,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server sceplus-vm48.almaden.ibm.com,60020,1404255227840
2014-07-01 16:08:47,572 WARN  [RpcServer.handler=22,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 2279ms
2014-07-01 16:08:47,573 INFO  [RpcServer.handler=22,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server sceplus-vm48.almaden.ibm.com,60020,1404255227840
2014-07-01 16:08:47,574 WARN  [RpcServer.handler=16,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 2307ms
2014-07-01 16:08:47,574 INFO  [RpcServer.handler=16,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server sceplus-vm48.almaden.ibm.com,60020,1404255227840
2014-07-01 16:08:47,575 WARN  [RpcServer.handler=39,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 2328ms
2014-07-01 16:08:47,584 INFO  [RpcServer.handler=39,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server sceplus-vm48.almaden.ibm.com,60020,1404255227840
2014-07-01 16:08:47,588 WARN  [RpcServer.handler=32,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 2368ms
2014-07-01 16:08:47,588 INFO  [RpcServer.handler=32,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server sceplus-vm48.almaden.ibm.com,60020,1404255227840
2014-07-01 16:08:47,588 WARN  [RpcServer.handler=31,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 2393ms
2014-07-01 16:08:47,588 INFO  [RpcServer.handler=31,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server sceplus-vm48.almaden.ibm.com,60020,1404255227840
2014-07-01 16:08:47,594 WARN  [RpcServer.handler=46,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 2420ms
2014-07-01 16:08:47,594 INFO  [RpcServer.handler=46,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server sceplus-vm48.almaden.ibm.com,60020,1404255227840
2014-07-01 16:08:47,596 WARN  [RpcServer.handler=18,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 2447ms
2014-07-01 16:08:47,596 INFO  [RpcServer.handler=18,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server sceplus-vm48.almaden.ibm.com,60020,1404255227840
2014-07-01 16:08:47,597 WARN  [RpcServer.handler=6,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 2483ms
2014-07-01 16:08:47,597 INFO  [RpcServer.handler=6,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server sceplus-vm48.almaden.ibm.com,60020,1404255227840
2014-07-01 16:08:47,598 WARN  [RpcServer.handler=4,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 2522ms
2014-07-01 16:08:47,598 INFO  [RpcServer.handler=4,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server sceplus-vm48.almaden.ibm.com,60020,1404255227840
2014-07-01 16:08:47,598 WARN  [RpcServer.handler=11,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 3445ms
2014-07-01 16:08:47,598 INFO  [RpcServer.handler=11,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server sceplus-vm48.almaden.ibm.com,60020,1404255227840
2014-07-01 16:08:47,598 WARN  [RpcServer.handler=9,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 3460ms
2014-07-01 16:08:47,598 INFO  [RpcServer.handler=9,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server sceplus-vm48.almaden.ibm.com,60020,1404255227840
2014-07-01 16:08:47,598 WARN  [RpcServer.handler=49,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 4075ms
2014-07-01 16:08:47,599 INFO  [RpcServer.handler=49,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server sceplus-vm48.almaden.ibm.com,60020,1404255227840
2014-07-01 16:08:47,599 WARN  [RpcServer.handler=40,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 4103ms
2014-07-01 16:08:47,599 INFO  [RpcServer.handler=40,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server sceplus-vm48.almaden.ibm.com,60020,1404255227840
2014-07-01 16:08:47,600 WARN  [RpcServer.handler=10,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 4228ms
2014-07-01 16:08:47,600 INFO  [RpcServer.handler=10,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server sceplus-vm48.almaden.ibm.com,60020,1404255227840
2014-07-01 16:08:47,600 WARN  [RpcServer.handler=24,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 4228ms
2014-07-01 16:08:47,600 INFO  [RpcServer.handler=24,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server sceplus-vm48.almaden.ibm.com,60020,1404255227840
2014-07-01 16:08:47,602 WARN  [RpcServer.handler=14,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 4231ms
2014-07-01 16:08:47,602 INFO  [RpcServer.handler=14,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server sceplus-vm48.almaden.ibm.com,60020,1404255227840
2014-07-01 16:08:47,610 WARN  [RpcServer.handler=2,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 4262ms
2014-07-01 16:08:47,610 INFO  [RpcServer.handler=2,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server sceplus-vm48.almaden.ibm.com,60020,1404255227840
2014-07-01 16:08:47,614 WARN  [RpcServer.handler=48,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 4266ms
2014-07-01 16:08:47,615 INFO  [RpcServer.handler=48,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server sceplus-vm48.almaden.ibm.com,60020,1404255227840
2014-07-01 16:08:47,615 WARN  [RpcServer.handler=3,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 4267ms
2014-07-01 16:08:47,615 INFO  [RpcServer.handler=3,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server sceplus-vm48.almaden.ibm.com,60020,1404255227840
2014-07-01 16:08:47,615 WARN  [RpcServer.handler=38,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 4314ms
2014-07-01 16:08:47,615 INFO  [RpcServer.handler=38,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server sceplus-vm48.almaden.ibm.com,60020,1404255227840
2014-07-01 16:08:47,619 WARN  [RpcServer.handler=12,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 4347ms
2014-07-01 16:08:47,619 INFO  [RpcServer.handler=12,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server sceplus-vm48.almaden.ibm.com,60020,1404255227840
2014-07-01 16:08:47,619 WARN  [RpcServer.handler=17,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 4351ms
2014-07-01 16:08:47,620 INFO  [RpcServer.handler=17,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server sceplus-vm48.almaden.ibm.com,60020,1404255227840
2014-07-01 16:08:47,620 WARN  [RpcServer.handler=1,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 4356ms
2014-07-01 16:08:47,620 INFO  [RpcServer.handler=1,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server sceplus-vm48.almaden.ibm.com,60020,1404255227840
2014-07-01 16:08:47,620 WARN  [RpcServer.handler=30,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 4356ms
2014-07-01 16:08:47,620 INFO  [RpcServer.handler=30,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server sceplus-vm48.almaden.ibm.com,60020,1404255227840
2014-07-01 16:08:47,621 WARN  [RpcServer.handler=33,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 4406ms
2014-07-01 16:08:47,622 INFO  [RpcServer.handler=33,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server sceplus-vm48.almaden.ibm.com,60020,1404255227840
2014-07-01 16:08:47,622 WARN  [RpcServer.handler=8,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 4410ms
2014-07-01 16:08:47,622 INFO  [RpcServer.handler=8,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server sceplus-vm48.almaden.ibm.com,60020,1404255227840
2014-07-01 16:08:47,622 WARN  [RpcServer.handler=0,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 4409ms
2014-07-01 16:08:47,622 INFO  [RpcServer.handler=0,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server sceplus-vm48.almaden.ibm.com,60020,1404255227840
2014-07-01 16:08:47,623 WARN  [RpcServer.handler=42,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 4413ms
2014-07-01 16:08:47,623 INFO  [RpcServer.handler=42,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server sceplus-vm48.almaden.ibm.com,60020,1404255227840
2014-07-01 16:08:47,624 WARN  [RpcServer.handler=27,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 4415ms
2014-07-01 16:08:47,624 INFO  [RpcServer.handler=27,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server sceplus-vm48.almaden.ibm.com,60020,1404255227840
2014-07-01 16:08:47,625 WARN  [RpcServer.handler=25,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 4420ms
2014-07-01 16:08:47,625 INFO  [RpcServer.handler=25,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server sceplus-vm48.almaden.ibm.com,60020,1404255227840
2014-07-01 16:08:47,625 WARN  [RpcServer.handler=29,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 4424ms
2014-07-01 16:08:47,625 INFO  [RpcServer.handler=29,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server sceplus-vm48.almaden.ibm.com,60020,1404255227840
2014-07-01 16:08:47,630 WARN  [RpcServer.handler=36,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 4682ms
2014-07-01 16:08:47,630 INFO  [RpcServer.handler=36,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server sceplus-vm48.almaden.ibm.com,60020,1404255227840
2014-07-01 16:08:47,631 WARN  [RpcServer.handler=26,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 4686ms
2014-07-01 16:08:47,631 INFO  [RpcServer.handler=26,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server sceplus-vm48.almaden.ibm.com,60020,1404255227840
2014-07-01 16:08:48,283 DEBUG [LruStats #0] hfile.LruBlockCache: Total=4.78 MB, free=3.95 GB, max=3.96 GB, blocks=5, accesses=67537, hits=17199, hitRatio=25.46%, , cachingAccesses=17210, cachingHits=17193, cachingHitsRatio=99.90%, evictions=0, evicted=12, evictedPerRun=Infinity
2014-07-01 16:08:48,636 DEBUG [MemStoreFlusher.0] util.FSUtils: DFS Client does not support most favored nodes create; using default create
2014-07-01 16:08:48,720 INFO  [MemStoreFlusher.1] regionserver.DefaultStoreFlusher: Flushed, sequenceid=2662, memsize=512.6m, hasBloomFilter=true, into tmp file hdfs://master:54310/hbase/data/default/usertable/431aad56a67499b48936eca121e3178e/.tmp/d828beccd789449ab138ae26f3bf1f9e
2014-07-01 16:08:48,744 DEBUG [MemStoreFlusher.1] regionserver.HRegionFileSystem: Committing store file hdfs://master:54310/hbase/data/default/usertable/431aad56a67499b48936eca121e3178e/.tmp/d828beccd789449ab138ae26f3bf1f9e as hdfs://master:54310/hbase/data/default/usertable/431aad56a67499b48936eca121e3178e/family/d828beccd789449ab138ae26f3bf1f9e
2014-07-01 16:08:48,753 INFO  [MemStoreFlusher.1] regionserver.HStore: Added hdfs://master:54310/hbase/data/default/usertable/431aad56a67499b48936eca121e3178e/family/d828beccd789449ab138ae26f3bf1f9e, entries=1866470, sequenceid=2662, filesize=132.9m
2014-07-01 16:08:48,754 INFO  [MemStoreFlusher.1] regionserver.HRegion: Finished memstore flush of ~992.6m/1040827600, currentsize=348.2m/365068240 for region usertable,user7,1404255616194.431aad56a67499b48936eca121e3178e. in 28477ms, sequenceid=2662, compaction requested=true
2014-07-01 16:08:48,754 DEBUG [MemStoreFlusher.1] regionserver.CompactSplitThread: Small Compaction requested: system; Because: MemStoreFlusher.1; compaction_queue=(0:16), split_queue=0, merge_queue=0
2014-07-01 16:08:48,754 DEBUG [MemStoreFlusher.1] regionserver.HRegion: Started memstore flush for usertable,user5,1404255616194.3e76136f0a8be94ccf3637676c064402., current region memstore size 629.2m
2014-07-01 16:08:48,851 DEBUG [RpcServer.handler=2,port=60020] regionserver.HRegion: Flush requested on usertable,user7,1404255616194.431aad56a67499b48936eca121e3178e.
2014-07-01 16:08:48,853 DEBUG [RpcServer.handler=14,port=60020] regionserver.HRegion: Flush requested on usertable,,1404255616194.510634b895ee706ff306728824babf96.
2014-07-01 16:08:49,029 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-01 16:08:49,058 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 16122 synced till here 16104
2014-07-01 16:08:49,154 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1404255227840/sceplus-vm48.almaden.ibm.com%2C60020%2C1404255227840.1404256122778 with entries=124, filesize=89.4m; new WAL /hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1404255227840/sceplus-vm48.almaden.ibm.com%2C60020%2C1404255227840.1404256129029
2014-07-01 16:08:49,154 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1404255227840/sceplus-vm48.almaden.ibm.com%2C60020%2C1404255227840.1404256019038
2014-07-01 16:08:49,154 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1404255227840/sceplus-vm48.almaden.ibm.com%2C60020%2C1404255227840.1404256027974
2014-07-01 16:08:49,154 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1404255227840/sceplus-vm48.almaden.ibm.com%2C60020%2C1404255227840.1404256030355
2014-07-01 16:08:49,154 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1404255227840/sceplus-vm48.almaden.ibm.com%2C60020%2C1404255227840.1404256031995
2014-07-01 16:08:49,154 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1404255227840/sceplus-vm48.almaden.ibm.com%2C60020%2C1404255227840.1404256033455
2014-07-01 16:08:49,154 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1404255227840/sceplus-vm48.almaden.ibm.com%2C60020%2C1404255227840.1404256035057
2014-07-01 16:08:49,155 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1404255227840/sceplus-vm48.almaden.ibm.com%2C60020%2C1404255227840.1404256036738
2014-07-01 16:08:49,155 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1404255227840/sceplus-vm48.almaden.ibm.com%2C60020%2C1404255227840.1404256038248
2014-07-01 16:08:49,155 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1404255227840/sceplus-vm48.almaden.ibm.com%2C60020%2C1404255227840.1404256040260
2014-07-01 16:08:49,155 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1404255227840/sceplus-vm48.almaden.ibm.com%2C60020%2C1404255227840.1404256042072
2014-07-01 16:08:49,155 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1404255227840/sceplus-vm48.almaden.ibm.com%2C60020%2C1404255227840.1404256044422
2014-07-01 16:08:49,155 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1404255227840/sceplus-vm48.almaden.ibm.com%2C60020%2C1404255227840.1404256046039
2014-07-01 16:08:49,966 DEBUG [MemStoreFlusher.1] util.FSUtils: DFS Client does not support most favored nodes create; using default create
2014-07-01 16:08:50,419 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-01 16:08:50,489 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 16211 synced till here 16195
2014-07-01 16:08:50,598 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1404255227840/sceplus-vm48.almaden.ibm.com%2C60020%2C1404255227840.1404256129029 with entries=89, filesize=81.0m; new WAL /hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1404255227840/sceplus-vm48.almaden.ibm.com%2C60020%2C1404255227840.1404256130419
2014-07-01 16:08:52,465 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-01 16:08:52,503 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 16282 synced till here 16279
2014-07-01 16:08:52,576 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1404255227840/sceplus-vm48.almaden.ibm.com%2C60020%2C1404255227840.1404256130419 with entries=71, filesize=65.2m; new WAL /hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1404255227840/sceplus-vm48.almaden.ibm.com%2C60020%2C1404255227840.1404256132466
2014-07-01 16:08:53,955 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-01 16:08:54,020 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 16362 synced till here 16348
2014-07-01 16:08:54,171 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1404255227840/sceplus-vm48.almaden.ibm.com%2C60020%2C1404255227840.1404256132466 with entries=80, filesize=70.4m; new WAL /hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1404255227840/sceplus-vm48.almaden.ibm.com%2C60020%2C1404255227840.1404256133955
2014-07-01 16:08:55,895 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-01 16:08:56,698 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 16479 synced till here 16468
2014-07-01 16:08:56,833 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1404255227840/sceplus-vm48.almaden.ibm.com%2C60020%2C1404255227840.1404256133955 with entries=117, filesize=96.6m; new WAL /hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1404255227840/sceplus-vm48.almaden.ibm.com%2C60020%2C1404255227840.1404256135896
2014-07-01 16:08:58,307 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-01 16:08:58,323 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 16563 synced till here 16554
2014-07-01 16:08:58,389 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1404255227840/sceplus-vm48.almaden.ibm.com%2C60020%2C1404255227840.1404256135896 with entries=84, filesize=73.5m; new WAL /hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1404255227840/sceplus-vm48.almaden.ibm.com%2C60020%2C1404255227840.1404256138308
2014-07-01 16:08:59,756 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-01 16:08:59,800 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1404255227840/sceplus-vm48.almaden.ibm.com%2C60020%2C1404255227840.1404256138308 with entries=79, filesize=62.9m; new WAL /hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1404255227840/sceplus-vm48.almaden.ibm.com%2C60020%2C1404255227840.1404256139756
2014-07-01 16:09:02,466 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-01 16:09:02,725 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 16766 synced till here 16756
2014-07-01 16:09:02,812 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1404255227840/sceplus-vm48.almaden.ibm.com%2C60020%2C1404255227840.1404256139756 with entries=124, filesize=87.3m; new WAL /hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1404255227840/sceplus-vm48.almaden.ibm.com%2C60020%2C1404255227840.1404256142467
2014-07-01 16:09:04,322 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-01 16:09:04,449 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1404255227840/sceplus-vm48.almaden.ibm.com%2C60020%2C1404255227840.1404256142467 with entries=92, filesize=66.7m; new WAL /hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1404255227840/sceplus-vm48.almaden.ibm.com%2C60020%2C1404255227840.1404256144323
2014-07-01 16:09:05,880 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-01 16:09:05,907 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 16950 synced till here 16943
2014-07-01 16:09:06,007 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1404255227840/sceplus-vm48.almaden.ibm.com%2C60020%2C1404255227840.1404256144323 with entries=92, filesize=66.4m; new WAL /hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1404255227840/sceplus-vm48.almaden.ibm.com%2C60020%2C1404255227840.1404256145880
2014-07-01 16:09:06,843 INFO  [MemStoreFlusher.1] regionserver.DefaultStoreFlusher: Flushed, sequenceid=2866, memsize=302.1m, hasBloomFilter=true, into tmp file hdfs://master:54310/hbase/data/default/usertable/3e76136f0a8be94ccf3637676c064402/.tmp/46ebd0509b294d349f686f77448af8fe
2014-07-01 16:09:06,856 DEBUG [MemStoreFlusher.1] regionserver.HRegionFileSystem: Committing store file hdfs://master:54310/hbase/data/default/usertable/3e76136f0a8be94ccf3637676c064402/.tmp/46ebd0509b294d349f686f77448af8fe as hdfs://master:54310/hbase/data/default/usertable/3e76136f0a8be94ccf3637676c064402/family/46ebd0509b294d349f686f77448af8fe
2014-07-01 16:09:07,290 INFO  [MemStoreFlusher.1] regionserver.HStore: Added hdfs://master:54310/hbase/data/default/usertable/3e76136f0a8be94ccf3637676c064402/family/46ebd0509b294d349f686f77448af8fe, entries=1100030, sequenceid=2866, filesize=78.3m
2014-07-01 16:09:07,291 INFO  [MemStoreFlusher.1] regionserver.HRegion: Finished memstore flush of ~631.1m/661708320, currentsize=301.5m/316123440 for region usertable,user5,1404255616194.3e76136f0a8be94ccf3637676c064402. in 18536ms, sequenceid=2866, compaction requested=true
2014-07-01 16:09:07,291 DEBUG [MemStoreFlusher.1] regionserver.CompactSplitThread: Small Compaction requested: system; Because: MemStoreFlusher.1; compaction_queue=(0:17), split_queue=0, merge_queue=0
2014-07-01 16:09:07,291 DEBUG [MemStoreFlusher.1] regionserver.HRegion: Started memstore flush for usertable,user3,1404255616194.09108ac86cf3c374e79d1bb78f787b49., current region memstore size 943.3m
2014-07-01 16:09:07,395 DEBUG [RpcServer.handler=9,port=60020] regionserver.HRegion: Flush requested on usertable,user5,1404255616194.3e76136f0a8be94ccf3637676c064402.
2014-07-01 16:09:07,708 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-01 16:09:07,959 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 17082 synced till here 17081
2014-07-01 16:09:07,987 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1404255227840/sceplus-vm48.almaden.ibm.com%2C60020%2C1404255227840.1404256145880 with entries=132, filesize=90.1m; new WAL /hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1404255227840/sceplus-vm48.almaden.ibm.com%2C60020%2C1404255227840.1404256147708
2014-07-01 16:09:09,123 DEBUG [MemStoreFlusher.1] util.FSUtils: DFS Client does not support most favored nodes create; using default create
2014-07-01 16:09:09,547 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-01 16:09:09,678 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 17180 synced till here 17168
2014-07-01 16:09:09,823 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1404255227840/sceplus-vm48.almaden.ibm.com%2C60020%2C1404255227840.1404256147708 with entries=98, filesize=77.4m; new WAL /hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1404255227840/sceplus-vm48.almaden.ibm.com%2C60020%2C1404255227840.1404256149547
2014-07-01 16:09:11,824 INFO  [MemStoreFlusher.0] regionserver.DefaultStoreFlusher: Flushed, sequenceid=4596, memsize=467.3m, hasBloomFilter=true, into tmp file hdfs://master:54310/hbase/data/default/usertable/9e9a6a0fd8aef92a09081d29900a50f3/.tmp/1896d5d755e348db8ff88123491292cb
2014-07-01 16:09:11,837 DEBUG [MemStoreFlusher.0] regionserver.HRegionFileSystem: Committing store file hdfs://master:54310/hbase/data/default/usertable/9e9a6a0fd8aef92a09081d29900a50f3/.tmp/1896d5d755e348db8ff88123491292cb as hdfs://master:54310/hbase/data/default/usertable/9e9a6a0fd8aef92a09081d29900a50f3/family/1896d5d755e348db8ff88123491292cb
2014-07-01 16:09:11,851 INFO  [MemStoreFlusher.0] regionserver.HStore: Added hdfs://master:54310/hbase/data/default/usertable/9e9a6a0fd8aef92a09081d29900a50f3/family/1896d5d755e348db8ff88123491292cb, entries=1701490, sequenceid=4596, filesize=121.1m
2014-07-01 16:09:11,852 INFO  [MemStoreFlusher.0] regionserver.HRegion: Finished memstore flush of ~589.9m/618589360, currentsize=266.9m/279813360 for region usertable,user8,1404255616195.9e9a6a0fd8aef92a09081d29900a50f3. in 24288ms, sequenceid=4596, compaction requested=true
2014-07-01 16:09:11,852 DEBUG [MemStoreFlusher.0] regionserver.CompactSplitThread: Small Compaction requested: system; Because: MemStoreFlusher.0; compaction_queue=(0:18), split_queue=0, merge_queue=0
2014-07-01 16:09:11,852 DEBUG [MemStoreFlusher.0] regionserver.HRegion: Started memstore flush for usertable,user7,1404255616194.431aad56a67499b48936eca121e3178e., current region memstore size 710.5m
2014-07-01 16:09:11,878 DEBUG [RpcServer.handler=14,port=60020] regionserver.HRegion: Flush requested on usertable,user8,1404255616195.9e9a6a0fd8aef92a09081d29900a50f3.
2014-07-01 16:09:12,402 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-01 16:09:12,445 DEBUG [MemStoreFlusher.0] util.FSUtils: DFS Client does not support most favored nodes create; using default create
2014-07-01 16:09:12,671 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 17297 synced till here 17278
2014-07-01 16:09:13,355 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1404255227840/sceplus-vm48.almaden.ibm.com%2C60020%2C1404255227840.1404256149547 with entries=117, filesize=84.3m; new WAL /hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1404255227840/sceplus-vm48.almaden.ibm.com%2C60020%2C1404255227840.1404256152402
2014-07-01 16:09:13,355 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1404255227840/sceplus-vm48.almaden.ibm.com%2C60020%2C1404255227840.1404256048316
2014-07-01 16:09:13,355 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1404255227840/sceplus-vm48.almaden.ibm.com%2C60020%2C1404255227840.1404256053372
2014-07-01 16:09:13,355 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1404255227840/sceplus-vm48.almaden.ibm.com%2C60020%2C1404255227840.1404256055243
2014-07-01 16:09:13,355 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1404255227840/sceplus-vm48.almaden.ibm.com%2C60020%2C1404255227840.1404256057086
2014-07-01 16:09:13,356 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1404255227840/sceplus-vm48.almaden.ibm.com%2C60020%2C1404255227840.1404256058519
2014-07-01 16:09:13,356 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1404255227840/sceplus-vm48.almaden.ibm.com%2C60020%2C1404255227840.1404256060259
2014-07-01 16:09:13,356 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1404255227840/sceplus-vm48.almaden.ibm.com%2C60020%2C1404255227840.1404256062564
2014-07-01 16:09:13,356 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1404255227840/sceplus-vm48.almaden.ibm.com%2C60020%2C1404255227840.1404256064241
2014-07-01 16:09:13,356 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1404255227840/sceplus-vm48.almaden.ibm.com%2C60020%2C1404255227840.1404256065799
2014-07-01 16:09:13,356 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1404255227840/sceplus-vm48.almaden.ibm.com%2C60020%2C1404255227840.1404256067761
2014-07-01 16:09:13,356 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1404255227840/sceplus-vm48.almaden.ibm.com%2C60020%2C1404255227840.1404256069557
2014-07-01 16:09:13,356 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1404255227840/sceplus-vm48.almaden.ibm.com%2C60020%2C1404255227840.1404256071246
2014-07-01 16:09:14,591 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-01 16:09:15,622 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 17406 synced till here 17392
2014-07-01 16:09:15,747 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1404255227840/sceplus-vm48.almaden.ibm.com%2C60020%2C1404255227840.1404256152402 with entries=109, filesize=89.9m; new WAL /hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1404255227840/sceplus-vm48.almaden.ibm.com%2C60020%2C1404255227840.1404256154591
2014-07-01 16:09:17,176 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-01 16:09:17,236 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 17511 synced till here 17498
2014-07-01 16:09:17,342 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1404255227840/sceplus-vm48.almaden.ibm.com%2C60020%2C1404255227840.1404256154591 with entries=105, filesize=71.0m; new WAL /hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1404255227840/sceplus-vm48.almaden.ibm.com%2C60020%2C1404255227840.1404256157177
2014-07-01 16:09:19,233 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-01 16:09:19,284 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 17615 synced till here 17601
2014-07-01 16:09:19,378 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1404255227840/sceplus-vm48.almaden.ibm.com%2C60020%2C1404255227840.1404256157177 with entries=104, filesize=76.7m; new WAL /hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1404255227840/sceplus-vm48.almaden.ibm.com%2C60020%2C1404255227840.1404256159234
2014-07-01 16:09:21,419 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-01 16:09:21,608 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 17724 synced till here 17711
2014-07-01 16:09:21,706 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1404255227840/sceplus-vm48.almaden.ibm.com%2C60020%2C1404255227840.1404256159234 with entries=109, filesize=80.5m; new WAL /hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1404255227840/sceplus-vm48.almaden.ibm.com%2C60020%2C1404255227840.1404256161419
2014-07-01 16:09:23,192 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-01 16:09:23,358 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 17817 synced till here 17802
2014-07-01 16:09:23,528 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1404255227840/sceplus-vm48.almaden.ibm.com%2C60020%2C1404255227840.1404256161419 with entries=93, filesize=74.4m; new WAL /hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1404255227840/sceplus-vm48.almaden.ibm.com%2C60020%2C1404255227840.1404256163193
2014-07-01 16:09:26,154 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-01 16:09:26,235 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 17908 synced till here 17900
2014-07-01 16:09:27,141 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1404255227840/sceplus-vm48.almaden.ibm.com%2C60020%2C1404255227840.1404256163193 with entries=91, filesize=75.3m; new WAL /hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1404255227840/sceplus-vm48.almaden.ibm.com%2C60020%2C1404255227840.1404256166155
2014-07-01 16:09:27,816 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-01 16:09:27,866 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 18005 synced till here 17985
2014-07-01 16:09:27,965 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1404255227840/sceplus-vm48.almaden.ibm.com%2C60020%2C1404255227840.1404256166155 with entries=97, filesize=74.4m; new WAL /hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1404255227840/sceplus-vm48.almaden.ibm.com%2C60020%2C1404255227840.1404256167817
2014-07-01 16:09:29,133 INFO  [RpcServer.handler=4,port=60020] regionserver.MemStoreFlusher: Blocking updates on sceplus-vm48.almaden.ibm.com,60020,1404255227840: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-01 16:09:29,135 INFO  [RpcServer.handler=9,port=60020] regionserver.MemStoreFlusher: Blocking updates on sceplus-vm48.almaden.ibm.com,60020,1404255227840: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-01 16:09:29,136 INFO  [RpcServer.handler=47,port=60020] regionserver.MemStoreFlusher: Blocking updates on sceplus-vm48.almaden.ibm.com,60020,1404255227840: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-01 16:09:29,146 INFO  [RpcServer.handler=7,port=60020] regionserver.MemStoreFlusher: Blocking updates on sceplus-vm48.almaden.ibm.com,60020,1404255227840: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-01 16:09:29,154 INFO  [RpcServer.handler=26,port=60020] regionserver.MemStoreFlusher: Blocking updates on sceplus-vm48.almaden.ibm.com,60020,1404255227840: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-01 16:09:29,155 INFO  [RpcServer.handler=19,port=60020] regionserver.MemStoreFlusher: Blocking updates on sceplus-vm48.almaden.ibm.com,60020,1404255227840: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-01 16:09:29,175 INFO  [RpcServer.handler=32,port=60020] regionserver.MemStoreFlusher: Blocking updates on sceplus-vm48.almaden.ibm.com,60020,1404255227840: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-01 16:09:29,176 INFO  [RpcServer.handler=41,port=60020] regionserver.MemStoreFlusher: Blocking updates on sceplus-vm48.almaden.ibm.com,60020,1404255227840: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-01 16:09:29,192 INFO  [RpcServer.handler=17,port=60020] regionserver.MemStoreFlusher: Blocking updates on sceplus-vm48.almaden.ibm.com,60020,1404255227840: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-01 16:09:29,214 INFO  [RpcServer.handler=48,port=60020] regionserver.MemStoreFlusher: Blocking updates on sceplus-vm48.almaden.ibm.com,60020,1404255227840: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-01 16:09:29,238 INFO  [RpcServer.handler=30,port=60020] regionserver.MemStoreFlusher: Blocking updates on sceplus-vm48.almaden.ibm.com,60020,1404255227840: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-01 16:09:29,238 INFO  [RpcServer.handler=16,port=60020] regionserver.MemStoreFlusher: Blocking updates on sceplus-vm48.almaden.ibm.com,60020,1404255227840: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-01 16:09:29,378 INFO  [RpcServer.handler=31,port=60020] regionserver.MemStoreFlusher: Blocking updates on sceplus-vm48.almaden.ibm.com,60020,1404255227840: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-01 16:09:29,462 INFO  [RpcServer.handler=38,port=60020] regionserver.MemStoreFlusher: Blocking updates on sceplus-vm48.almaden.ibm.com,60020,1404255227840: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-01 16:09:29,670 INFO  [RpcServer.handler=6,port=60020] regionserver.MemStoreFlusher: Blocking updates on sceplus-vm48.almaden.ibm.com,60020,1404255227840: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-01 16:09:31,677 INFO  [RpcServer.handler=8,port=60020] regionserver.MemStoreFlusher: Blocking updates on sceplus-vm48.almaden.ibm.com,60020,1404255227840: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-01 16:09:31,681 INFO  [RpcServer.handler=10,port=60020] regionserver.MemStoreFlusher: Blocking updates on sceplus-vm48.almaden.ibm.com,60020,1404255227840: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-01 16:09:31,693 INFO  [RpcServer.handler=25,port=60020] regionserver.MemStoreFlusher: Blocking updates on sceplus-vm48.almaden.ibm.com,60020,1404255227840: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-01 16:09:31,704 INFO  [RpcServer.handler=20,port=60020] regionserver.MemStoreFlusher: Blocking updates on sceplus-vm48.almaden.ibm.com,60020,1404255227840: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-01 16:09:31,720 INFO  [RpcServer.handler=23,port=60020] regionserver.MemStoreFlusher: Blocking updates on sceplus-vm48.almaden.ibm.com,60020,1404255227840: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-01 16:09:31,743 INFO  [RpcServer.handler=2,port=60020] regionserver.MemStoreFlusher: Blocking updates on sceplus-vm48.almaden.ibm.com,60020,1404255227840: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-01 16:09:31,772 INFO  [RpcServer.handler=39,port=60020] regionserver.MemStoreFlusher: Blocking updates on sceplus-vm48.almaden.ibm.com,60020,1404255227840: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-01 16:09:31,794 INFO  [RpcServer.handler=36,port=60020] regionserver.MemStoreFlusher: Blocking updates on sceplus-vm48.almaden.ibm.com,60020,1404255227840: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-01 16:09:31,822 INFO  [RpcServer.handler=49,port=60020] regionserver.MemStoreFlusher: Blocking updates on sceplus-vm48.almaden.ibm.com,60020,1404255227840: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-01 16:09:31,843 INFO  [RpcServer.handler=21,port=60020] regionserver.MemStoreFlusher: Blocking updates on sceplus-vm48.almaden.ibm.com,60020,1404255227840: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-01 16:09:31,865 INFO  [RpcServer.handler=18,port=60020] regionserver.MemStoreFlusher: Blocking updates on sceplus-vm48.almaden.ibm.com,60020,1404255227840: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-01 16:09:31,889 INFO  [RpcServer.handler=28,port=60020] regionserver.MemStoreFlusher: Blocking updates on sceplus-vm48.almaden.ibm.com,60020,1404255227840: the global memstore size 4.0g is >= than blocking 4.0g size
2014-07-01 16:09:33,009 INFO  [MemStoreFlusher.0] regionserver.DefaultStoreFlusher: Flushed, sequenceid=3070, memsize=408.6m, hasBloomFilter=true, into tmp file hdfs://master:54310/hbase/data/default/usertable/431aad56a67499b48936eca121e3178e/.tmp/37775b9024734bfa8830c482fae9c296
2014-07-01 16:09:33,020 DEBUG [MemStoreFlusher.0] regionserver.HRegionFileSystem: Committing store file hdfs://master:54310/hbase/data/default/usertable/431aad56a67499b48936eca121e3178e/.tmp/37775b9024734bfa8830c482fae9c296 as hdfs://master:54310/hbase/data/default/usertable/431aad56a67499b48936eca121e3178e/family/37775b9024734bfa8830c482fae9c296
2014-07-01 16:09:33,028 INFO  [MemStoreFlusher.0] regionserver.HStore: Added hdfs://master:54310/hbase/data/default/usertable/431aad56a67499b48936eca121e3178e/family/37775b9024734bfa8830c482fae9c296, entries=1487610, sequenceid=3070, filesize=105.9m
2014-07-01 16:09:33,028 INFO  [MemStoreFlusher.0] regionserver.HRegion: Finished memstore flush of ~710.5m/744994400, currentsize=248.2m/260257200 for region usertable,user7,1404255616194.431aad56a67499b48936eca121e3178e. in 21176ms, sequenceid=3070, compaction requested=true
2014-07-01 16:09:33,028 DEBUG [MemStoreFlusher.0] regionserver.CompactSplitThread: Small Compaction requested: system; Because: MemStoreFlusher.0; compaction_queue=(0:19), split_queue=0, merge_queue=0
2014-07-01 16:09:33,029 WARN  [RpcServer.handler=28,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 1140ms
2014-07-01 16:09:33,029 INFO  [RpcServer.handler=28,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server sceplus-vm48.almaden.ibm.com,60020,1404255227840
2014-07-01 16:09:33,029 WARN  [RpcServer.handler=18,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 1164ms
2014-07-01 16:09:33,029 DEBUG [MemStoreFlusher.0] regionserver.HRegion: Started memstore flush for usertable,,1404255616194.510634b895ee706ff306728824babf96., current region memstore size 821.6m
2014-07-01 16:09:33,029 INFO  [RpcServer.handler=18,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server sceplus-vm48.almaden.ibm.com,60020,1404255227840
2014-07-01 16:09:33,029 WARN  [RpcServer.handler=21,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 1187ms
2014-07-01 16:09:33,029 INFO  [RpcServer.handler=21,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server sceplus-vm48.almaden.ibm.com,60020,1404255227840
2014-07-01 16:09:33,029 WARN  [RpcServer.handler=49,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 1208ms
2014-07-01 16:09:33,029 INFO  [RpcServer.handler=49,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server sceplus-vm48.almaden.ibm.com,60020,1404255227840
2014-07-01 16:09:33,029 WARN  [RpcServer.handler=36,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 1235ms
2014-07-01 16:09:33,029 INFO  [RpcServer.handler=36,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server sceplus-vm48.almaden.ibm.com,60020,1404255227840
2014-07-01 16:09:33,032 WARN  [RpcServer.handler=39,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 1260ms
2014-07-01 16:09:33,032 INFO  [RpcServer.handler=39,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server sceplus-vm48.almaden.ibm.com,60020,1404255227840
2014-07-01 16:09:33,034 WARN  [RpcServer.handler=2,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 1291ms
2014-07-01 16:09:33,034 INFO  [RpcServer.handler=2,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server sceplus-vm48.almaden.ibm.com,60020,1404255227840
2014-07-01 16:09:33,035 WARN  [RpcServer.handler=23,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 1315ms
2014-07-01 16:09:33,035 INFO  [RpcServer.handler=23,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server sceplus-vm48.almaden.ibm.com,60020,1404255227840
2014-07-01 16:09:33,035 WARN  [RpcServer.handler=20,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 1331ms
2014-07-01 16:09:33,035 INFO  [RpcServer.handler=20,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server sceplus-vm48.almaden.ibm.com,60020,1404255227840
2014-07-01 16:09:33,035 WARN  [RpcServer.handler=25,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 1342ms
2014-07-01 16:09:33,035 INFO  [RpcServer.handler=25,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server sceplus-vm48.almaden.ibm.com,60020,1404255227840
2014-07-01 16:09:33,037 WARN  [RpcServer.handler=10,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 1356ms
2014-07-01 16:09:33,038 INFO  [RpcServer.handler=10,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server sceplus-vm48.almaden.ibm.com,60020,1404255227840
2014-07-01 16:09:33,038 WARN  [RpcServer.handler=8,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 1361ms
2014-07-01 16:09:33,038 INFO  [RpcServer.handler=8,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server sceplus-vm48.almaden.ibm.com,60020,1404255227840
2014-07-01 16:09:33,038 WARN  [RpcServer.handler=6,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 3368ms
2014-07-01 16:09:33,038 INFO  [RpcServer.handler=6,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server sceplus-vm48.almaden.ibm.com,60020,1404255227840
2014-07-01 16:09:33,038 WARN  [RpcServer.handler=38,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 3576ms
2014-07-01 16:09:33,038 INFO  [RpcServer.handler=38,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server sceplus-vm48.almaden.ibm.com,60020,1404255227840
2014-07-01 16:09:33,045 WARN  [RpcServer.handler=31,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 3666ms
2014-07-01 16:09:33,045 INFO  [RpcServer.handler=31,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server sceplus-vm48.almaden.ibm.com,60020,1404255227840
2014-07-01 16:09:33,045 WARN  [RpcServer.handler=16,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 3807ms
2014-07-01 16:09:33,045 INFO  [RpcServer.handler=16,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server sceplus-vm48.almaden.ibm.com,60020,1404255227840
2014-07-01 16:09:33,045 WARN  [RpcServer.handler=30,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 3807ms
2014-07-01 16:09:33,045 INFO  [RpcServer.handler=30,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server sceplus-vm48.almaden.ibm.com,60020,1404255227840
2014-07-01 16:09:33,045 WARN  [RpcServer.handler=48,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 3831ms
2014-07-01 16:09:33,045 INFO  [RpcServer.handler=48,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server sceplus-vm48.almaden.ibm.com,60020,1404255227840
2014-07-01 16:09:33,045 WARN  [RpcServer.handler=17,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 3854ms
2014-07-01 16:09:33,045 INFO  [RpcServer.handler=17,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server sceplus-vm48.almaden.ibm.com,60020,1404255227840
2014-07-01 16:09:33,046 WARN  [RpcServer.handler=41,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 3870ms
2014-07-01 16:09:33,046 INFO  [RpcServer.handler=41,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server sceplus-vm48.almaden.ibm.com,60020,1404255227840
2014-07-01 16:09:33,046 WARN  [RpcServer.handler=32,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 3871ms
2014-07-01 16:09:33,046 INFO  [RpcServer.handler=32,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server sceplus-vm48.almaden.ibm.com,60020,1404255227840
2014-07-01 16:09:33,046 WARN  [RpcServer.handler=19,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 3892ms
2014-07-01 16:09:33,046 INFO  [RpcServer.handler=19,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server sceplus-vm48.almaden.ibm.com,60020,1404255227840
2014-07-01 16:09:33,056 WARN  [RpcServer.handler=26,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 3902ms
2014-07-01 16:09:33,056 INFO  [RpcServer.handler=26,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server sceplus-vm48.almaden.ibm.com,60020,1404255227840
2014-07-01 16:09:33,056 WARN  [RpcServer.handler=7,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 3910ms
2014-07-01 16:09:33,056 INFO  [RpcServer.handler=7,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server sceplus-vm48.almaden.ibm.com,60020,1404255227840
2014-07-01 16:09:33,056 WARN  [RpcServer.handler=47,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 3921ms
2014-07-01 16:09:33,056 INFO  [RpcServer.handler=47,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server sceplus-vm48.almaden.ibm.com,60020,1404255227840
2014-07-01 16:09:33,056 WARN  [RpcServer.handler=9,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 3921ms
2014-07-01 16:09:33,056 INFO  [RpcServer.handler=9,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server sceplus-vm48.almaden.ibm.com,60020,1404255227840
2014-07-01 16:09:33,056 WARN  [RpcServer.handler=4,port=60020] regionserver.MemStoreFlusher: Memstore is above high water mark and block 3923ms
2014-07-01 16:09:33,056 INFO  [RpcServer.handler=4,port=60020] regionserver.MemStoreFlusher: Unblocking updates for server sceplus-vm48.almaden.ibm.com,60020,1404255227840
2014-07-01 16:09:33,356 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-01 16:09:33,636 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 18124 synced till here 18115
2014-07-01 16:09:33,717 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1404255227840/sceplus-vm48.almaden.ibm.com%2C60020%2C1404255227840.1404256167817 with entries=119, filesize=103.6m; new WAL /hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1404255227840/sceplus-vm48.almaden.ibm.com%2C60020%2C1404255227840.1404256173356
2014-07-01 16:09:34,318 DEBUG [RpcServer.handler=26,port=60020] regionserver.HRegion: Flush requested on usertable,user7,1404255616194.431aad56a67499b48936eca121e3178e.
2014-07-01 16:09:34,336 DEBUG [MemStoreFlusher.0] util.FSUtils: DFS Client does not support most favored nodes create; using default create
2014-07-01 16:09:34,420 INFO  [MemStoreFlusher.1] regionserver.DefaultStoreFlusher: Flushed, sequenceid=3046, memsize=469.1m, hasBloomFilter=true, into tmp file hdfs://master:54310/hbase/data/default/usertable/09108ac86cf3c374e79d1bb78f787b49/.tmp/48162c72cbdf466e92a01de74764907f
2014-07-01 16:09:34,429 DEBUG [MemStoreFlusher.1] regionserver.HRegionFileSystem: Committing store file hdfs://master:54310/hbase/data/default/usertable/09108ac86cf3c374e79d1bb78f787b49/.tmp/48162c72cbdf466e92a01de74764907f as hdfs://master:54310/hbase/data/default/usertable/09108ac86cf3c374e79d1bb78f787b49/family/48162c72cbdf466e92a01de74764907f
2014-07-01 16:09:34,437 INFO  [MemStoreFlusher.1] regionserver.HStore: Added hdfs://master:54310/hbase/data/default/usertable/09108ac86cf3c374e79d1bb78f787b49/family/48162c72cbdf466e92a01de74764907f, entries=1707860, sequenceid=3046, filesize=121.6m
2014-07-01 16:09:34,437 INFO  [MemStoreFlusher.1] regionserver.HRegion: Finished memstore flush of ~947.0m/992954160, currentsize=335.1m/351407520 for region usertable,user3,1404255616194.09108ac86cf3c374e79d1bb78f787b49. in 27146ms, sequenceid=3046, compaction requested=false
2014-07-01 16:09:34,437 DEBUG [MemStoreFlusher.1] regionserver.HRegion: Started memstore flush for usertable,user5,1404255616194.3e76136f0a8be94ccf3637676c064402., current region memstore size 617.2m
2014-07-01 16:09:34,721 DEBUG [RpcServer.handler=43,port=60020] regionserver.HRegion: Flush requested on usertable,user3,1404255616194.09108ac86cf3c374e79d1bb78f787b49.
2014-07-01 16:09:34,899 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-01 16:09:35,119 DEBUG [MemStoreFlusher.1] util.FSUtils: DFS Client does not support most favored nodes create; using default create
2014-07-01 16:09:35,121 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 18206 synced till here 18205
2014-07-01 16:09:35,138 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1404255227840/sceplus-vm48.almaden.ibm.com%2C60020%2C1404255227840.1404256173356 with entries=82, filesize=75.8m; new WAL /hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1404255227840/sceplus-vm48.almaden.ibm.com%2C60020%2C1404255227840.1404256174899
2014-07-01 16:09:35,138 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1404255227840/sceplus-vm48.almaden.ibm.com%2C60020%2C1404255227840.1404256073004
2014-07-01 16:09:35,138 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1404255227840/sceplus-vm48.almaden.ibm.com%2C60020%2C1404255227840.1404256078215
2014-07-01 16:09:35,138 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1404255227840/sceplus-vm48.almaden.ibm.com%2C60020%2C1404255227840.1404256079781
2014-07-01 16:09:35,139 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1404255227840/sceplus-vm48.almaden.ibm.com%2C60020%2C1404255227840.1404256081381
2014-07-01 16:09:35,139 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1404255227840/sceplus-vm48.almaden.ibm.com%2C60020%2C1404255227840.1404256083754
2014-07-01 16:09:35,139 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1404255227840/sceplus-vm48.almaden.ibm.com%2C60020%2C1404255227840.1404256085572
2014-07-01 16:09:35,139 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1404255227840/sceplus-vm48.almaden.ibm.com%2C60020%2C1404255227840.1404256087650
2014-07-01 16:09:35,139 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1404255227840/sceplus-vm48.almaden.ibm.com%2C60020%2C1404255227840.1404256089764
2014-07-01 16:09:35,139 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1404255227840/sceplus-vm48.almaden.ibm.com%2C60020%2C1404255227840.1404256091673
2014-07-01 16:09:35,139 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1404255227840/sceplus-vm48.almaden.ibm.com%2C60020%2C1404255227840.1404256093586
2014-07-01 16:09:35,139 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1404255227840/sceplus-vm48.almaden.ibm.com%2C60020%2C1404255227840.1404256095649
2014-07-01 16:09:35,139 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1404255227840/sceplus-vm48.almaden.ibm.com%2C60020%2C1404255227840.1404256101761
2014-07-01 16:09:35,139 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1404255227840/sceplus-vm48.almaden.ibm.com%2C60020%2C1404255227840.1404256104712
2014-07-01 16:09:36,278 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-01 16:09:36,304 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 18284 synced till here 18279
2014-07-01 16:09:36,377 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1404255227840/sceplus-vm48.almaden.ibm.com%2C60020%2C1404255227840.1404256174899 with entries=78, filesize=67.7m; new WAL /hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1404255227840/sceplus-vm48.almaden.ibm.com%2C60020%2C1404255227840.1404256176278
2014-07-01 16:09:37,612 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-01 16:09:37,738 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 18379 synced till here 18368
2014-07-01 16:09:38,180 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1404255227840/sceplus-vm48.almaden.ibm.com%2C60020%2C1404255227840.1404256176278 with entries=95, filesize=70.5m; new WAL /hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1404255227840/sceplus-vm48.almaden.ibm.com%2C60020%2C1404255227840.1404256177612
2014-07-01 16:09:39,422 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-01 16:09:39,556 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 18475 synced till here 18468
2014-07-01 16:09:39,803 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1404255227840/sceplus-vm48.almaden.ibm.com%2C60020%2C1404255227840.1404256177612 with entries=96, filesize=74.7m; new WAL /hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1404255227840/sceplus-vm48.almaden.ibm.com%2C60020%2C1404255227840.1404256179422
2014-07-01 16:09:41,189 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-01 16:09:41,204 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 18568 synced till here 18563
2014-07-01 16:09:41,295 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1404255227840/sceplus-vm48.almaden.ibm.com%2C60020%2C1404255227840.1404256179422 with entries=93, filesize=65.6m; new WAL /hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1404255227840/sceplus-vm48.almaden.ibm.com%2C60020%2C1404255227840.1404256181190
2014-07-01 16:09:42,533 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-01 16:09:42,679 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 18669 synced till here 18667
2014-07-01 16:09:42,765 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1404255227840/sceplus-vm48.almaden.ibm.com%2C60020%2C1404255227840.1404256181190 with entries=101, filesize=69.7m; new WAL /hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1404255227840/sceplus-vm48.almaden.ibm.com%2C60020%2C1404255227840.1404256182533
2014-07-01 16:09:44,203 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-01 16:09:44,265 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 18765 synced till here 18755
2014-07-01 16:09:44,334 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1404255227840/sceplus-vm48.almaden.ibm.com%2C60020%2C1404255227840.1404256182533 with entries=96, filesize=71.6m; new WAL /hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1404255227840/sceplus-vm48.almaden.ibm.com%2C60020%2C1404255227840.1404256184203
2014-07-01 16:09:45,800 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-01 16:09:45,832 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 18847 synced till here 18845
2014-07-01 16:09:45,859 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1404255227840/sceplus-vm48.almaden.ibm.com%2C60020%2C1404255227840.1404256184203 with entries=82, filesize=63.9m; new WAL /hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1404255227840/sceplus-vm48.almaden.ibm.com%2C60020%2C1404255227840.1404256185800
2014-07-01 16:09:47,282 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-01 16:09:47,300 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 18940 synced till here 18938
2014-07-01 16:09:47,340 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1404255227840/sceplus-vm48.almaden.ibm.com%2C60020%2C1404255227840.1404256185800 with entries=93, filesize=63.5m; new WAL /hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1404255227840/sceplus-vm48.almaden.ibm.com%2C60020%2C1404255227840.1404256187283
2014-07-01 16:09:48,999 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-01 16:09:49,032 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 19034 synced till here 19033
2014-07-01 16:09:49,065 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1404255227840/sceplus-vm48.almaden.ibm.com%2C60020%2C1404255227840.1404256187283 with entries=94, filesize=64.3m; new WAL /hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1404255227840/sceplus-vm48.almaden.ibm.com%2C60020%2C1404255227840.1404256189000
2014-07-01 16:09:50,720 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-01 16:09:50,769 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1404255227840/sceplus-vm48.almaden.ibm.com%2C60020%2C1404255227840.1404256189000 with entries=88, filesize=62.2m; new WAL /hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1404255227840/sceplus-vm48.almaden.ibm.com%2C60020%2C1404255227840.1404256190720
2014-07-01 16:09:52,442 INFO  [MemStoreFlusher.1] regionserver.DefaultStoreFlusher: Flushed, sequenceid=3243, memsize=409.0m, hasBloomFilter=true, into tmp file hdfs://master:54310/hbase/data/default/usertable/3e76136f0a8be94ccf3637676c064402/.tmp/2c92bb563dc1425a9dc9728f8a7d5c27
2014-07-01 16:09:52,513 DEBUG [MemStoreFlusher.1] regionserver.HRegionFileSystem: Committing store file hdfs://master:54310/hbase/data/default/usertable/3e76136f0a8be94ccf3637676c064402/.tmp/2c92bb563dc1425a9dc9728f8a7d5c27 as hdfs://master:54310/hbase/data/default/usertable/3e76136f0a8be94ccf3637676c064402/family/2c92bb563dc1425a9dc9728f8a7d5c27
2014-07-01 16:09:52,524 INFO  [MemStoreFlusher.1] regionserver.HStore: Added hdfs://master:54310/hbase/data/default/usertable/3e76136f0a8be94ccf3637676c064402/family/2c92bb563dc1425a9dc9728f8a7d5c27, entries=1489020, sequenceid=3243, filesize=106.0m
2014-07-01 16:09:52,524 INFO  [MemStoreFlusher.1] regionserver.HRegion: Finished memstore flush of ~642.3m/673463760, currentsize=274.1m/287413440 for region usertable,user5,1404255616194.3e76136f0a8be94ccf3637676c064402. in 18087ms, sequenceid=3243, compaction requested=true
2014-07-01 16:09:52,524 DEBUG [MemStoreFlusher.1] regionserver.CompactSplitThread: Small Compaction requested: system; Because: MemStoreFlusher.1; compaction_queue=(0:20), split_queue=0, merge_queue=0
2014-07-01 16:09:52,524 DEBUG [MemStoreFlusher.1] regionserver.HRegion: Started memstore flush for usertable,user8,1404255616195.9e9a6a0fd8aef92a09081d29900a50f3., current region memstore size 708.8m
2014-07-01 16:09:52,742 DEBUG [RpcServer.handler=45,port=60020] regionserver.HRegion: Flush requested on usertable,user5,1404255616194.3e76136f0a8be94ccf3637676c064402.
2014-07-01 16:09:53,217 DEBUG [MemStoreFlusher.1] util.FSUtils: DFS Client does not support most favored nodes create; using default create
2014-07-01 16:09:54,355 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-01 16:09:54,387 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1404255227840/sceplus-vm48.almaden.ibm.com%2C60020%2C1404255227840.1404256190720 with entries=87, filesize=61.1m; new WAL /hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1404255227840/sceplus-vm48.almaden.ibm.com%2C60020%2C1404255227840.1404256194355
2014-07-01 16:09:56,455 INFO  [MemStoreFlusher.0] regionserver.DefaultStoreFlusher: Flushed, sequenceid=3238, memsize=562.3m, hasBloomFilter=true, into tmp file hdfs://master:54310/hbase/data/default/usertable/510634b895ee706ff306728824babf96/.tmp/8363e5cc184d451b8e97e84e29e863ed
2014-07-01 16:09:56,468 DEBUG [MemStoreFlusher.0] regionserver.HRegionFileSystem: Committing store file hdfs://master:54310/hbase/data/default/usertable/510634b895ee706ff306728824babf96/.tmp/8363e5cc184d451b8e97e84e29e863ed as hdfs://master:54310/hbase/data/default/usertable/510634b895ee706ff306728824babf96/family/8363e5cc184d451b8e97e84e29e863ed
2014-07-01 16:09:56,477 INFO  [MemStoreFlusher.0] regionserver.HStore: Added hdfs://master:54310/hbase/data/default/usertable/510634b895ee706ff306728824babf96/family/8363e5cc184d451b8e97e84e29e863ed, entries=2047340, sequenceid=3238, filesize=145.7m
2014-07-01 16:09:56,477 INFO  [MemStoreFlusher.0] regionserver.HRegion: Finished memstore flush of ~821.6m/861503120, currentsize=327.5m/343408800 for region usertable,,1404255616194.510634b895ee706ff306728824babf96. in 23448ms, sequenceid=3238, compaction requested=true
2014-07-01 16:09:56,478 DEBUG [MemStoreFlusher.0] regionserver.CompactSplitThread: Small Compaction requested: system; Because: MemStoreFlusher.0; compaction_queue=(0:21), split_queue=0, merge_queue=0
2014-07-01 16:09:56,478 DEBUG [MemStoreFlusher.0] regionserver.HRegion: Started memstore flush for usertable,user7,1404255616194.431aad56a67499b48936eca121e3178e., current region memstore size 586.5m
2014-07-01 16:09:56,513 DEBUG [RpcServer.handler=32,port=60020] regionserver.HRegion: Flush requested on usertable,,1404255616194.510634b895ee706ff306728824babf96.
2014-07-01 16:09:56,944 DEBUG [MemStoreFlusher.0] util.FSUtils: DFS Client does not support most favored nodes create; using default create
2014-07-01 16:10:12,036 INFO  [MemStoreFlusher.0] regionserver.DefaultStoreFlusher: Flushed, sequenceid=3412, memsize=464.5m, hasBloomFilter=true, into tmp file hdfs://master:54310/hbase/data/default/usertable/431aad56a67499b48936eca121e3178e/.tmp/ab424a234f524d209e8e5d65108ed8fa
2014-07-01 16:10:12,047 DEBUG [MemStoreFlusher.0] regionserver.HRegionFileSystem: Committing store file hdfs://master:54310/hbase/data/default/usertable/431aad56a67499b48936eca121e3178e/.tmp/ab424a234f524d209e8e5d65108ed8fa as hdfs://master:54310/hbase/data/default/usertable/431aad56a67499b48936eca121e3178e/family/ab424a234f524d209e8e5d65108ed8fa
2014-07-01 16:10:12,056 INFO  [MemStoreFlusher.0] regionserver.HStore: Added hdfs://master:54310/hbase/data/default/usertable/431aad56a67499b48936eca121e3178e/family/ab424a234f524d209e8e5d65108ed8fa, entries=1691350, sequenceid=3412, filesize=120.5m
2014-07-01 16:10:12,056 INFO  [MemStoreFlusher.0] regionserver.HRegion: Finished memstore flush of ~586.5m/614971120, currentsize=7.1m/7432880 for region usertable,user7,1404255616194.431aad56a67499b48936eca121e3178e. in 15578ms, sequenceid=3412, compaction requested=true
2014-07-01 16:10:12,057 DEBUG [MemStoreFlusher.0] regionserver.CompactSplitThread: Small Compaction requested: system; Because: MemStoreFlusher.0; compaction_queue=(0:22), split_queue=0, merge_queue=0
2014-07-01 16:10:12,057 DEBUG [MemStoreFlusher.0] regionserver.HRegion: Started memstore flush for usertable,user3,1404255616194.09108ac86cf3c374e79d1bb78f787b49., current region memstore size 638.7m
2014-07-01 16:10:12,581 DEBUG [MemStoreFlusher.0] util.FSUtils: DFS Client does not support most favored nodes create; using default create
2014-07-01 16:10:13,749 INFO  [RpcServer.handler=15,port=60020] compress.CodecPool: Got brand-new decompressor
2014-07-01 16:10:13,898 INFO  [RpcServer.handler=3,port=60020] compress.CodecPool: Got brand-new decompressor
2014-07-01 16:10:13,898 INFO  [RpcServer.handler=46,port=60020] compress.CodecPool: Got brand-new decompressor
2014-07-01 16:10:13,903 INFO  [RpcServer.handler=22,port=60020] compress.CodecPool: Got brand-new decompressor
2014-07-01 16:10:14,391 INFO  [MemStoreFlusher.1] regionserver.DefaultStoreFlusher: Flushed, sequenceid=5608, memsize=656.1m, hasBloomFilter=true, into tmp file hdfs://master:54310/hbase/data/default/usertable/9e9a6a0fd8aef92a09081d29900a50f3/.tmp/52723ffacf474f7bb0def0ad4556aecf
2014-07-01 16:10:14,406 DEBUG [MemStoreFlusher.1] regionserver.HRegionFileSystem: Committing store file hdfs://master:54310/hbase/data/default/usertable/9e9a6a0fd8aef92a09081d29900a50f3/.tmp/52723ffacf474f7bb0def0ad4556aecf as hdfs://master:54310/hbase/data/default/usertable/9e9a6a0fd8aef92a09081d29900a50f3/family/52723ffacf474f7bb0def0ad4556aecf
2014-07-01 16:10:14,433 INFO  [MemStoreFlusher.1] regionserver.HStore: Added hdfs://master:54310/hbase/data/default/usertable/9e9a6a0fd8aef92a09081d29900a50f3/family/52723ffacf474f7bb0def0ad4556aecf, entries=2388910, sequenceid=5608, filesize=170.0m
2014-07-01 16:10:14,460 INFO  [MemStoreFlusher.1] regionserver.HRegion: Finished memstore flush of ~711.9m/746526320, currentsize=26.5m/27768400 for region usertable,user8,1404255616195.9e9a6a0fd8aef92a09081d29900a50f3. in 21936ms, sequenceid=5608, compaction requested=true
2014-07-01 16:10:14,462 DEBUG [MemStoreFlusher.1] regionserver.CompactSplitThread: Small Compaction requested: system; Because: MemStoreFlusher.1; compaction_queue=(0:23), split_queue=0, merge_queue=0
2014-07-01 16:10:14,462 DEBUG [MemStoreFlusher.1] regionserver.HRegion: Started memstore flush for usertable,user5,1404255616194.3e76136f0a8be94ccf3637676c064402., current region memstore size 303.6m
2014-07-01 16:10:14,656 INFO  [RpcServer.handler=14,port=60020] compress.CodecPool: Got brand-new decompressor
2014-07-01 16:10:14,656 INFO  [RpcServer.handler=1,port=60020] compress.CodecPool: Got brand-new decompressor
2014-07-01 16:10:14,656 INFO  [regionserver60020-smallCompactions-1404255751957] compress.CodecPool: Got brand-new decompressor
2014-07-01 16:10:14,656 INFO  [RpcServer.handler=15,port=60020] compress.CodecPool: Got brand-new decompressor
2014-07-01 16:10:14,656 INFO  [RpcServer.handler=39,port=60020] compress.CodecPool: Got brand-new decompressor
2014-07-01 16:10:14,766 DEBUG [MemStoreFlusher.1] util.FSUtils: DFS Client does not support most favored nodes create; using default create
2014-07-01 16:10:25,836 INFO  [MemStoreFlusher.1] regionserver.DefaultStoreFlusher: Flushed, sequenceid=3419, memsize=303.6m, hasBloomFilter=true, into tmp file hdfs://master:54310/hbase/data/default/usertable/3e76136f0a8be94ccf3637676c064402/.tmp/555e9164845f4732bcee95e73b5a46ca
2014-07-01 16:10:25,847 DEBUG [MemStoreFlusher.1] regionserver.HRegionFileSystem: Committing store file hdfs://master:54310/hbase/data/default/usertable/3e76136f0a8be94ccf3637676c064402/.tmp/555e9164845f4732bcee95e73b5a46ca as hdfs://master:54310/hbase/data/default/usertable/3e76136f0a8be94ccf3637676c064402/family/555e9164845f4732bcee95e73b5a46ca
2014-07-01 16:10:25,865 INFO  [MemStoreFlusher.1] regionserver.HStore: Added hdfs://master:54310/hbase/data/default/usertable/3e76136f0a8be94ccf3637676c064402/family/555e9164845f4732bcee95e73b5a46ca, entries=1105470, sequenceid=3419, filesize=78.7m
2014-07-01 16:10:25,866 INFO  [MemStoreFlusher.1] regionserver.HRegion: Finished memstore flush of ~303.6m/318366880, currentsize=0.0/0 for region usertable,user5,1404255616194.3e76136f0a8be94ccf3637676c064402. in 11404ms, sequenceid=3419, compaction requested=true
2014-07-01 16:10:25,867 DEBUG [MemStoreFlusher.1] regionserver.CompactSplitThread: Small Compaction requested: system; Because: MemStoreFlusher.1; compaction_queue=(0:24), split_queue=0, merge_queue=0
2014-07-01 16:10:25,871 DEBUG [MemStoreFlusher.1] regionserver.HRegion: Started memstore flush for usertable,,1404255616194.510634b895ee706ff306728824babf96., current region memstore size 334.6m
2014-07-01 16:10:26,152 DEBUG [MemStoreFlusher.1] util.FSUtils: DFS Client does not support most favored nodes create; using default create
2014-07-01 16:10:30,331 DEBUG [regionserver60020-smallCompactions-1404255751957] regionserver.HRegionFileSystem: Committing store file hdfs://master:54310/hbase/data/default/usertable/09108ac86cf3c374e79d1bb78f787b49/.tmp/f6a64121a53e4c0d89579e1fbac8b38c as hdfs://master:54310/hbase/data/default/usertable/09108ac86cf3c374e79d1bb78f787b49/family/f6a64121a53e4c0d89579e1fbac8b38c
2014-07-01 16:10:30,345 DEBUG [regionserver60020-smallCompactions-1404255751957] regionserver.HStore: Removing store files after compaction...
2014-07-01 16:10:30,372 DEBUG [regionserver60020-smallCompactions-1404255751957] backup.HFileArchiver: Finished archiving from class org.apache.hadoop.hbase.backup.HFileArchiver$FileableStoreFile, file:hdfs://master:54310/hbase/data/default/usertable/09108ac86cf3c374e79d1bb78f787b49/family/b77ae91bf1934355ae9e1b0c228c9263, to hdfs://master:54310/hbase/archive/data/default/usertable/09108ac86cf3c374e79d1bb78f787b49/family/b77ae91bf1934355ae9e1b0c228c9263
2014-07-01 16:10:30,376 DEBUG [regionserver60020-smallCompactions-1404255751957] backup.HFileArchiver: Finished archiving from class org.apache.hadoop.hbase.backup.HFileArchiver$FileableStoreFile, file:hdfs://master:54310/hbase/data/default/usertable/09108ac86cf3c374e79d1bb78f787b49/family/d5ccec31c1f64850baaee00b94bd5a8a, to hdfs://master:54310/hbase/archive/data/default/usertable/09108ac86cf3c374e79d1bb78f787b49/family/d5ccec31c1f64850baaee00b94bd5a8a
2014-07-01 16:10:30,378 DEBUG [regionserver60020-smallCompactions-1404255751957] backup.HFileArchiver: Finished archiving from class org.apache.hadoop.hbase.backup.HFileArchiver$FileableStoreFile, file:hdfs://master:54310/hbase/data/default/usertable/09108ac86cf3c374e79d1bb78f787b49/family/3e086712d9304c5f873e794c8e5e93aa, to hdfs://master:54310/hbase/archive/data/default/usertable/09108ac86cf3c374e79d1bb78f787b49/family/3e086712d9304c5f873e794c8e5e93aa
2014-07-01 16:10:30,381 DEBUG [regionserver60020-smallCompactions-1404255751957] backup.HFileArchiver: Finished archiving from class org.apache.hadoop.hbase.backup.HFileArchiver$FileableStoreFile, file:hdfs://master:54310/hbase/data/default/usertable/09108ac86cf3c374e79d1bb78f787b49/family/1aaaed3484934af4972b2511e5e1a501, to hdfs://master:54310/hbase/archive/data/default/usertable/09108ac86cf3c374e79d1bb78f787b49/family/1aaaed3484934af4972b2511e5e1a501
2014-07-01 16:10:30,383 DEBUG [regionserver60020-smallCompactions-1404255751957] backup.HFileArchiver: Finished archiving from class org.apache.hadoop.hbase.backup.HFileArchiver$FileableStoreFile, file:hdfs://master:54310/hbase/data/default/usertable/09108ac86cf3c374e79d1bb78f787b49/family/b1ace32190624cbf83bdb450c826b010, to hdfs://master:54310/hbase/archive/data/default/usertable/09108ac86cf3c374e79d1bb78f787b49/family/b1ace32190624cbf83bdb450c826b010
2014-07-01 16:10:30,385 DEBUG [regionserver60020-smallCompactions-1404255751957] backup.HFileArchiver: Finished archiving from class org.apache.hadoop.hbase.backup.HFileArchiver$FileableStoreFile, file:hdfs://master:54310/hbase/data/default/usertable/09108ac86cf3c374e79d1bb78f787b49/family/25671ec018384cd28950bb9aa2d615d1, to hdfs://master:54310/hbase/archive/data/default/usertable/09108ac86cf3c374e79d1bb78f787b49/family/25671ec018384cd28950bb9aa2d615d1
2014-07-01 16:10:30,385 INFO  [regionserver60020-smallCompactions-1404255751957] regionserver.HStore: Completed major compaction of 6 file(s) in family of usertable,user3,1404255616194.09108ac86cf3c374e79d1bb78f787b49. into f6a64121a53e4c0d89579e1fbac8b38c(size=608.2m), total size for store is 729.9m. This selection was in queue for 0sec, and took 1mins, 52sec to execute.
2014-07-01 16:10:30,386 INFO  [regionserver60020-smallCompactions-1404255751957] regionserver.CompactSplitThread: Completed compaction: Request = regionName=usertable,user3,1404255616194.09108ac86cf3c374e79d1bb78f787b49., storeName=family, fileCount=6, fileSize=641.1m, priority=14, time=76260621450496; duration=1mins, 52sec
2014-07-01 16:10:30,386 DEBUG [regionserver60020-smallCompactions-1404255751957] regionserver.CompactSplitThread: CompactSplitThread Status: compaction_queue=(0:24), split_queue=0, merge_queue=0
2014-07-01 16:10:30,387 DEBUG [regionserver60020-smallCompactions-1404255751957] compactions.RatioBasedCompactionPolicy: Selecting compaction from 7 store files, 0 compacting, 7 eligible, 20 blocking
2014-07-01 16:10:30,387 DEBUG [regionserver60020-smallCompactions-1404255751957] compactions.ExploringCompactionPolicy: Exploring compaction algorithm has selected 7 files of size 931274436 starting at candidate #0 after considering 15 permutations with 15 in ratio
2014-07-01 16:10:30,387 DEBUG [regionserver60020-smallCompactions-1404255751957] regionserver.HStore: 431aad56a67499b48936eca121e3178e - family: Initiating major compaction
2014-07-01 16:10:30,387 INFO  [regionserver60020-smallCompactions-1404255751957] regionserver.HRegion: Starting compaction on family in region usertable,user7,1404255616194.431aad56a67499b48936eca121e3178e.
2014-07-01 16:10:30,388 INFO  [regionserver60020-smallCompactions-1404255751957] regionserver.HStore: Starting compaction of 7 file(s) in family of usertable,user7,1404255616194.431aad56a67499b48936eca121e3178e. into tmpdir=hdfs://master:54310/hbase/data/default/usertable/431aad56a67499b48936eca121e3178e/.tmp, totalSize=888.1m
2014-07-01 16:10:30,388 DEBUG [regionserver60020-smallCompactions-1404255751957] compactions.Compactor: Compacting hdfs://master:54310/hbase/data/default/usertable/431aad56a67499b48936eca121e3178e/family/29207a3e75a446f28386936686268649, keycount=215515, bloomtype=ROW, size=153.4m, encoding=NONE, seqNum=721, earliestPutTs=1404255694911
2014-07-01 16:10:30,388 DEBUG [regionserver60020-smallCompactions-1404255751957] compactions.Compactor: Compacting hdfs://master:54310/hbase/data/default/usertable/431aad56a67499b48936eca121e3178e/family/e20b07cf980149b4af089656e5ab685a, keycount=165918, bloomtype=ROW, size=118.2m, encoding=NONE, seqNum=994, earliestPutTs=1404255785527
2014-07-01 16:10:30,388 DEBUG [regionserver60020-smallCompactions-1404255751957] compactions.Compactor: Compacting hdfs://master:54310/hbase/data/default/usertable/431aad56a67499b48936eca121e3178e/family/86e5e86be9ec4cb4b3651e20e4b1f05b, keycount=176488, bloomtype=ROW, size=125.6m, encoding=NONE, seqNum=1545, earliestPutTs=1404255846243
2014-07-01 16:10:30,388 DEBUG [regionserver60020-smallCompactions-1404255751957] compactions.Compactor: Compacting hdfs://master:54310/hbase/data/default/usertable/431aad56a67499b48936eca121e3178e/family/bdca6fcd8c874aa8969b3b0bf198ef58, keycount=184795, bloomtype=ROW, size=131.6m, encoding=NONE, seqNum=2083, earliestPutTs=1404255952822
2014-07-01 16:10:30,388 DEBUG [regionserver60020-smallCompactions-1404255751957] compactions.Compactor: Compacting hdfs://master:54310/hbase/data/default/usertable/431aad56a67499b48936eca121e3178e/family/d828beccd789449ab138ae26f3bf1f9e, keycount=186647, bloomtype=ROW, size=132.9m, encoding=NONE, seqNum=2662, earliestPutTs=1404256033601
2014-07-01 16:10:30,388 DEBUG [regionserver60020-smallCompactions-1404255751957] compactions.Compactor: Compacting hdfs://master:54310/hbase/data/default/usertable/431aad56a67499b48936eca121e3178e/family/37775b9024734bfa8830c482fae9c296, keycount=148761, bloomtype=ROW, size=105.9m, encoding=NONE, seqNum=3070, earliestPutTs=1404256105360
2014-07-01 16:10:30,389 DEBUG [regionserver60020-smallCompactions-1404255751957] compactions.Compactor: Compacting hdfs://master:54310/hbase/data/default/usertable/431aad56a67499b48936eca121e3178e/family/ab424a234f524d209e8e5d65108ed8fa, keycount=169135, bloomtype=ROW, size=120.5m, encoding=NONE, seqNum=3412, earliestPutTs=1404256153661
2014-07-01 16:10:30,546 DEBUG [regionserver60020-smallCompactions-1404255751957] util.FSUtils: DFS Client does not support most favored nodes create; using default create
2014-07-01 16:10:32,820 INFO  [MemStoreFlusher.0] regionserver.DefaultStoreFlusher: Flushed, sequenceid=3417, memsize=512.3m, hasBloomFilter=true, into tmp file hdfs://master:54310/hbase/data/default/usertable/09108ac86cf3c374e79d1bb78f787b49/.tmp/105beca76b4b44f0a57fdcba9f4ca399
2014-07-01 16:10:32,834 DEBUG [MemStoreFlusher.0] regionserver.HRegionFileSystem: Committing store file hdfs://master:54310/hbase/data/default/usertable/09108ac86cf3c374e79d1bb78f787b49/.tmp/105beca76b4b44f0a57fdcba9f4ca399 as hdfs://master:54310/hbase/data/default/usertable/09108ac86cf3c374e79d1bb78f787b49/family/105beca76b4b44f0a57fdcba9f4ca399
2014-07-01 16:10:32,842 INFO  [MemStoreFlusher.0] regionserver.HStore: Added hdfs://master:54310/hbase/data/default/usertable/09108ac86cf3c374e79d1bb78f787b49/family/105beca76b4b44f0a57fdcba9f4ca399, entries=1865350, sequenceid=3417, filesize=132.8m
2014-07-01 16:10:32,842 INFO  [MemStoreFlusher.0] regionserver.HRegion: Finished memstore flush of ~638.7m/669687680, currentsize=0.0/0 for region usertable,user3,1404255616194.09108ac86cf3c374e79d1bb78f787b49. in 20785ms, sequenceid=3417, compaction requested=true
2014-07-01 16:10:32,843 DEBUG [MemStoreFlusher.0] regionserver.CompactSplitThread: Small Compaction requested: system; Because: MemStoreFlusher.0; compaction_queue=(0:24), split_queue=0, merge_queue=0
2014-07-01 16:10:36,289 INFO  [MemStoreFlusher.1] regionserver.DefaultStoreFlusher: Flushed, sequenceid=3432, memsize=307.0m, hasBloomFilter=true, into tmp file hdfs://master:54310/hbase/data/default/usertable/510634b895ee706ff306728824babf96/.tmp/bb9e775140c341dfaca1c2e2a0ab20e2
2014-07-01 16:10:36,304 DEBUG [MemStoreFlusher.1] regionserver.HRegionFileSystem: Committing store file hdfs://master:54310/hbase/data/default/usertable/510634b895ee706ff306728824babf96/.tmp/bb9e775140c341dfaca1c2e2a0ab20e2 as hdfs://master:54310/hbase/data/default/usertable/510634b895ee706ff306728824babf96/family/bb9e775140c341dfaca1c2e2a0ab20e2
2014-07-01 16:10:36,313 INFO  [MemStoreFlusher.1] regionserver.HStore: Added hdfs://master:54310/hbase/data/default/usertable/510634b895ee706ff306728824babf96/family/bb9e775140c341dfaca1c2e2a0ab20e2, entries=1117730, sequenceid=3432, filesize=79.6m
2014-07-01 16:10:36,315 INFO  [MemStoreFlusher.1] regionserver.HRegion: Finished memstore flush of ~334.6m/350810160, currentsize=0.0/0 for region usertable,,1404255616194.510634b895ee706ff306728824babf96. in 10443ms, sequenceid=3432, compaction requested=true
2014-07-01 16:10:36,317 DEBUG [MemStoreFlusher.1] regionserver.CompactSplitThread: Small Compaction requested: system; Because: MemStoreFlusher.1; compaction_queue=(0:25), split_queue=0, merge_queue=0
2014-07-01 16:12:46,951 DEBUG [regionserver60020-smallCompactions-1404255751957] regionserver.HRegionFileSystem: Committing store file hdfs://master:54310/hbase/data/default/usertable/431aad56a67499b48936eca121e3178e/.tmp/d5ef68ab109d42e68a0eab8dddd0ce94 as hdfs://master:54310/hbase/data/default/usertable/431aad56a67499b48936eca121e3178e/family/d5ef68ab109d42e68a0eab8dddd0ce94
2014-07-01 16:12:46,995 DEBUG [regionserver60020-smallCompactions-1404255751957] regionserver.HStore: Removing store files after compaction...
2014-07-01 16:12:47,039 DEBUG [regionserver60020-smallCompactions-1404255751957] backup.HFileArchiver: Finished archiving from class org.apache.hadoop.hbase.backup.HFileArchiver$FileableStoreFile, file:hdfs://master:54310/hbase/data/default/usertable/431aad56a67499b48936eca121e3178e/family/29207a3e75a446f28386936686268649, to hdfs://master:54310/hbase/archive/data/default/usertable/431aad56a67499b48936eca121e3178e/family/29207a3e75a446f28386936686268649
2014-07-01 16:12:47,041 DEBUG [regionserver60020-smallCompactions-1404255751957] backup.HFileArchiver: Finished archiving from class org.apache.hadoop.hbase.backup.HFileArchiver$FileableStoreFile, file:hdfs://master:54310/hbase/data/default/usertable/431aad56a67499b48936eca121e3178e/family/e20b07cf980149b4af089656e5ab685a, to hdfs://master:54310/hbase/archive/data/default/usertable/431aad56a67499b48936eca121e3178e/family/e20b07cf980149b4af089656e5ab685a
2014-07-01 16:12:47,043 DEBUG [regionserver60020-smallCompactions-1404255751957] backup.HFileArchiver: Finished archiving from class org.apache.hadoop.hbase.backup.HFileArchiver$FileableStoreFile, file:hdfs://master:54310/hbase/data/default/usertable/431aad56a67499b48936eca121e3178e/family/86e5e86be9ec4cb4b3651e20e4b1f05b, to hdfs://master:54310/hbase/archive/data/default/usertable/431aad56a67499b48936eca121e3178e/family/86e5e86be9ec4cb4b3651e20e4b1f05b
2014-07-01 16:12:47,045 DEBUG [regionserver60020-smallCompactions-1404255751957] backup.HFileArchiver: Finished archiving from class org.apache.hadoop.hbase.backup.HFileArchiver$FileableStoreFile, file:hdfs://master:54310/hbase/data/default/usertable/431aad56a67499b48936eca121e3178e/family/bdca6fcd8c874aa8969b3b0bf198ef58, to hdfs://master:54310/hbase/archive/data/default/usertable/431aad56a67499b48936eca121e3178e/family/bdca6fcd8c874aa8969b3b0bf198ef58
2014-07-01 16:12:47,047 DEBUG [regionserver60020-smallCompactions-1404255751957] backup.HFileArchiver: Finished archiving from class org.apache.hadoop.hbase.backup.HFileArchiver$FileableStoreFile, file:hdfs://master:54310/hbase/data/default/usertable/431aad56a67499b48936eca121e3178e/family/d828beccd789449ab138ae26f3bf1f9e, to hdfs://master:54310/hbase/archive/data/default/usertable/431aad56a67499b48936eca121e3178e/family/d828beccd789449ab138ae26f3bf1f9e
2014-07-01 16:12:47,049 DEBUG [regionserver60020-smallCompactions-1404255751957] backup.HFileArchiver: Finished archiving from class org.apache.hadoop.hbase.backup.HFileArchiver$FileableStoreFile, file:hdfs://master:54310/hbase/data/default/usertable/431aad56a67499b48936eca121e3178e/family/37775b9024734bfa8830c482fae9c296, to hdfs://master:54310/hbase/archive/data/default/usertable/431aad56a67499b48936eca121e3178e/family/37775b9024734bfa8830c482fae9c296
2014-07-01 16:12:47,051 DEBUG [regionserver60020-smallCompactions-1404255751957] backup.HFileArchiver: Finished archiving from class org.apache.hadoop.hbase.backup.HFileArchiver$FileableStoreFile, file:hdfs://master:54310/hbase/data/default/usertable/431aad56a67499b48936eca121e3178e/family/ab424a234f524d209e8e5d65108ed8fa, to hdfs://master:54310/hbase/archive/data/default/usertable/431aad56a67499b48936eca121e3178e/family/ab424a234f524d209e8e5d65108ed8fa
2014-07-01 16:12:47,051 INFO  [regionserver60020-smallCompactions-1404255751957] regionserver.HStore: Completed major compaction of 7 file(s) in family of usertable,user7,1404255616194.431aad56a67499b48936eca121e3178e. into d5ef68ab109d42e68a0eab8dddd0ce94(size=854.8m), total size for store is 854.8m. This selection was in queue for 0sec, and took 2mins, 16sec to execute.
2014-07-01 16:12:47,052 INFO  [regionserver60020-smallCompactions-1404255751957] regionserver.CompactSplitThread: Completed compaction: Request = regionName=usertable,user7,1404255616194.431aad56a67499b48936eca121e3178e., storeName=family, fileCount=7, fileSize=888.1m, priority=13, time=76372744949937; duration=2mins, 16sec
2014-07-01 16:12:47,056 DEBUG [regionserver60020-smallCompactions-1404255751957] regionserver.CompactSplitThread: CompactSplitThread Status: compaction_queue=(0:25), split_queue=0, merge_queue=0
2014-07-01 16:12:47,056 DEBUG [regionserver60020-smallCompactions-1404255751957] compactions.RatioBasedCompactionPolicy: Selecting compaction from 7 store files, 0 compacting, 7 eligible, 20 blocking
2014-07-01 16:12:47,064 DEBUG [regionserver60020-smallCompactions-1404255751957] compactions.ExploringCompactionPolicy: Exploring compaction algorithm has selected 7 files of size 942746859 starting at candidate #0 after considering 15 permutations with 14 in ratio
2014-07-01 16:12:47,064 DEBUG [regionserver60020-smallCompactions-1404255751957] regionserver.HStore: 510634b895ee706ff306728824babf96 - family: Initiating major compaction
2014-07-01 16:12:47,064 INFO  [regionserver60020-smallCompactions-1404255751957] regionserver.HRegion: Starting compaction on family in region usertable,,1404255616194.510634b895ee706ff306728824babf96.
2014-07-01 16:12:47,064 INFO  [regionserver60020-smallCompactions-1404255751957] regionserver.HStore: Starting compaction of 7 file(s) in family of usertable,,1404255616194.510634b895ee706ff306728824babf96. into tmpdir=hdfs://master:54310/hbase/data/default/usertable/510634b895ee706ff306728824babf96/.tmp, totalSize=899.1m
2014-07-01 16:12:47,065 DEBUG [regionserver60020-smallCompactions-1404255751957] compactions.Compactor: Compacting hdfs://master:54310/hbase/data/default/usertable/510634b895ee706ff306728824babf96/family/91cc3ebeaa7946fa8d06d8dc5f4f4a7c, keycount=430528, bloomtype=ROW, size=306.2m, encoding=NONE, seqNum=1161, earliestPutTs=1404255694239
2014-07-01 16:12:47,065 DEBUG [regionserver60020-smallCompactions-1404255751957] compactions.Compactor: Compacting hdfs://master:54310/hbase/data/default/usertable/510634b895ee706ff306728824babf96/family/b4f220a200b543afba21ccef263dc906, keycount=120294, bloomtype=ROW, size=85.6m, encoding=NONE, seqNum=1548, earliestPutTs=1404255867597
2014-07-01 16:12:47,065 DEBUG [regionserver60020-smallCompactions-1404255751957] compactions.Compactor: Compacting hdfs://master:54310/hbase/data/default/usertable/510634b895ee706ff306728824babf96/family/f947715524054d1688a810aa255ef5d4, keycount=139409, bloomtype=ROW, size=99.3m, encoding=NONE, seqNum=1966, earliestPutTs=1404255952184
2014-07-01 16:12:47,065 DEBUG [regionserver60020-smallCompactions-1404255751957] compactions.Compactor: Compacting hdfs://master:54310/hbase/data/default/usertable/510634b895ee706ff306728824babf96/family/3cba320e6a1c49f5926fbe0b01a9a19a, keycount=135960, bloomtype=ROW, size=96.8m, encoding=NONE, seqNum=2360, earliestPutTs=1404256008940
2014-07-01 16:12:47,065 DEBUG [regionserver60020-smallCompactions-1404255751957] compactions.Compactor: Compacting hdfs://master:54310/hbase/data/default/usertable/510634b895ee706ff306728824babf96/family/264e326d8c1b48629cc72749b12b782b, keycount=120527, bloomtype=ROW, size=85.8m, encoding=NONE, seqNum=2757, earliestPutTs=1404256056992
2014-07-01 16:12:47,065 DEBUG [regionserver60020-smallCompactions-1404255751957] compactions.Compactor: Compacting hdfs://master:54310/hbase/data/default/usertable/510634b895ee706ff306728824babf96/family/8363e5cc184d451b8e97e84e29e863ed, keycount=204734, bloomtype=ROW, size=145.7m, encoding=NONE, seqNum=3238, earliestPutTs=1404256110104
2014-07-01 16:12:47,065 DEBUG [regionserver60020-smallCompactions-1404255751957] compactions.Compactor: Compacting hdfs://master:54310/hbase/data/default/usertable/510634b895ee706ff306728824babf96/family/bb9e775140c341dfaca1c2e2a0ab20e2, keycount=111773, bloomtype=ROW, size=79.6m, encoding=NONE, seqNum=3432, earliestPutTs=1404256173205
2014-07-01 16:12:47,141 DEBUG [regionserver60020-smallCompactions-1404255751957] util.FSUtils: DFS Client does not support most favored nodes create; using default create
2014-07-01 16:13:47,928 DEBUG [LruStats #0] hfile.LruBlockCache: Total=872.72 MB, free=3.11 GB, max=3.96 GB, blocks=13655, accesses=4446822, hits=4318974, hitRatio=97.12%, , cachingAccesses=4332857, cachingHits=4315414, cachingHitsRatio=99.59%, evictions=0, evicted=3430, evictedPerRun=Infinity
2014-07-01 16:14:54,993 DEBUG [regionserver60020-smallCompactions-1404255751957] regionserver.HRegionFileSystem: Committing store file hdfs://master:54310/hbase/data/default/usertable/510634b895ee706ff306728824babf96/.tmp/ff9c5e8f3a5d42b594b5aa2d2f972b95 as hdfs://master:54310/hbase/data/default/usertable/510634b895ee706ff306728824babf96/family/ff9c5e8f3a5d42b594b5aa2d2f972b95
2014-07-01 16:14:55,017 DEBUG [regionserver60020-smallCompactions-1404255751957] regionserver.HStore: Removing store files after compaction...
2014-07-01 16:14:55,054 DEBUG [regionserver60020-smallCompactions-1404255751957] backup.HFileArchiver: Finished archiving from class org.apache.hadoop.hbase.backup.HFileArchiver$FileableStoreFile, file:hdfs://master:54310/hbase/data/default/usertable/510634b895ee706ff306728824babf96/family/91cc3ebeaa7946fa8d06d8dc5f4f4a7c, to hdfs://master:54310/hbase/archive/data/default/usertable/510634b895ee706ff306728824babf96/family/91cc3ebeaa7946fa8d06d8dc5f4f4a7c
2014-07-01 16:14:55,062 DEBUG [regionserver60020-smallCompactions-1404255751957] backup.HFileArchiver: Finished archiving from class org.apache.hadoop.hbase.backup.HFileArchiver$FileableStoreFile, file:hdfs://master:54310/hbase/data/default/usertable/510634b895ee706ff306728824babf96/family/b4f220a200b543afba21ccef263dc906, to hdfs://master:54310/hbase/archive/data/default/usertable/510634b895ee706ff306728824babf96/family/b4f220a200b543afba21ccef263dc906
2014-07-01 16:14:55,070 DEBUG [regionserver60020-smallCompactions-1404255751957] backup.HFileArchiver: Finished archiving from class org.apache.hadoop.hbase.backup.HFileArchiver$FileableStoreFile, file:hdfs://master:54310/hbase/data/default/usertable/510634b895ee706ff306728824babf96/family/f947715524054d1688a810aa255ef5d4, to hdfs://master:54310/hbase/archive/data/default/usertable/510634b895ee706ff306728824babf96/family/f947715524054d1688a810aa255ef5d4
2014-07-01 16:14:55,079 DEBUG [regionserver60020-smallCompactions-1404255751957] backup.HFileArchiver: Finished archiving from class org.apache.hadoop.hbase.backup.HFileArchiver$FileableStoreFile, file:hdfs://master:54310/hbase/data/default/usertable/510634b895ee706ff306728824babf96/family/3cba320e6a1c49f5926fbe0b01a9a19a, to hdfs://master:54310/hbase/archive/data/default/usertable/510634b895ee706ff306728824babf96/family/3cba320e6a1c49f5926fbe0b01a9a19a
2014-07-01 16:14:55,094 DEBUG [regionserver60020-smallCompactions-1404255751957] backup.HFileArchiver: Finished archiving from class org.apache.hadoop.hbase.backup.HFileArchiver$FileableStoreFile, file:hdfs://master:54310/hbase/data/default/usertable/510634b895ee706ff306728824babf96/family/264e326d8c1b48629cc72749b12b782b, to hdfs://master:54310/hbase/archive/data/default/usertable/510634b895ee706ff306728824babf96/family/264e326d8c1b48629cc72749b12b782b
2014-07-01 16:14:55,096 DEBUG [regionserver60020-smallCompactions-1404255751957] backup.HFileArchiver: Finished archiving from class org.apache.hadoop.hbase.backup.HFileArchiver$FileableStoreFile, file:hdfs://master:54310/hbase/data/default/usertable/510634b895ee706ff306728824babf96/family/8363e5cc184d451b8e97e84e29e863ed, to hdfs://master:54310/hbase/archive/data/default/usertable/510634b895ee706ff306728824babf96/family/8363e5cc184d451b8e97e84e29e863ed
2014-07-01 16:14:55,112 DEBUG [regionserver60020-smallCompactions-1404255751957] backup.HFileArchiver: Finished archiving from class org.apache.hadoop.hbase.backup.HFileArchiver$FileableStoreFile, file:hdfs://master:54310/hbase/data/default/usertable/510634b895ee706ff306728824babf96/family/bb9e775140c341dfaca1c2e2a0ab20e2, to hdfs://master:54310/hbase/archive/data/default/usertable/510634b895ee706ff306728824babf96/family/bb9e775140c341dfaca1c2e2a0ab20e2
2014-07-01 16:14:55,112 INFO  [regionserver60020-smallCompactions-1404255751957] regionserver.HStore: Completed major compaction of 7 file(s) in family of usertable,,1404255616194.510634b895ee706ff306728824babf96. into ff9c5e8f3a5d42b594b5aa2d2f972b95(size=856.7m), total size for store is 856.7m. This selection was in queue for 0sec, and took 2mins, 8sec to execute.
2014-07-01 16:14:55,113 INFO  [regionserver60020-smallCompactions-1404255751957] regionserver.CompactSplitThread: Completed compaction: Request = regionName=usertable,,1404255616194.510634b895ee706ff306728824babf96., storeName=family, fileCount=7, fileSize=899.1m, priority=13, time=76509421635337; duration=2mins, 8sec
2014-07-01 16:14:55,113 DEBUG [regionserver60020-smallCompactions-1404255751957] regionserver.CompactSplitThread: CompactSplitThread Status: compaction_queue=(0:24), split_queue=0, merge_queue=0
2014-07-01 16:14:55,113 DEBUG [regionserver60020-smallCompactions-1404255751957] compactions.RatioBasedCompactionPolicy: Selecting compaction from 6 store files, 0 compacting, 6 eligible, 20 blocking
2014-07-01 16:14:55,113 DEBUG [regionserver60020-smallCompactions-1404255751957] compactions.ExploringCompactionPolicy: Exploring compaction algorithm has selected 6 files of size 902644373 starting at candidate #0 after considering 10 permutations with 10 in ratio
2014-07-01 16:14:55,114 DEBUG [regionserver60020-smallCompactions-1404255751957] regionserver.HStore: 9e9a6a0fd8aef92a09081d29900a50f3 - family: Initiating major compaction
2014-07-01 16:14:55,114 INFO  [regionserver60020-smallCompactions-1404255751957] regionserver.HRegion: Starting compaction on family in region usertable,user8,1404255616195.9e9a6a0fd8aef92a09081d29900a50f3.
2014-07-01 16:14:55,114 INFO  [regionserver60020-smallCompactions-1404255751957] regionserver.HStore: Starting compaction of 6 file(s) in family of usertable,user8,1404255616195.9e9a6a0fd8aef92a09081d29900a50f3. into tmpdir=hdfs://master:54310/hbase/data/default/usertable/9e9a6a0fd8aef92a09081d29900a50f3/.tmp, totalSize=860.8m
2014-07-01 16:14:55,114 DEBUG [regionserver60020-smallCompactions-1404255751957] compactions.Compactor: Compacting hdfs://master:54310/hbase/data/default/usertable/9e9a6a0fd8aef92a09081d29900a50f3/family/41114ac488ec418c96706a7d6dbfda7b, keycount=260073, bloomtype=ROW, size=185.1m, encoding=NONE, seqNum=1573, earliestPutTs=1404255633975
2014-07-01 16:14:55,114 DEBUG [regionserver60020-smallCompactions-1404255751957] compactions.Compactor: Compacting hdfs://master:54310/hbase/data/default/usertable/9e9a6a0fd8aef92a09081d29900a50f3/family/7aba3925a1bb4009a59ff08ef6dc8b77, keycount=160351, bloomtype=ROW, size=114.1m, encoding=NONE, seqNum=2285, earliestPutTs=1404255800219
2014-07-01 16:14:55,114 DEBUG [regionserver60020-smallCompactions-1404255751957] compactions.Compactor: Compacting hdfs://master:54310/hbase/data/default/usertable/9e9a6a0fd8aef92a09081d29900a50f3/family/26459e61df9c4e4c843ed5735437f446, keycount=198893, bloomtype=ROW, size=141.6m, encoding=NONE, seqNum=3052, earliestPutTs=1404255861182
2014-07-01 16:14:55,114 DEBUG [regionserver60020-smallCompactions-1404255751957] compactions.Compactor: Compacting hdfs://master:54310/hbase/data/default/usertable/9e9a6a0fd8aef92a09081d29900a50f3/family/77f7a0ace3be4249ab059083a3f795fd, keycount=181081, bloomtype=ROW, size=129.0m, encoding=NONE, seqNum=3865, earliestPutTs=1404255970438
2014-07-01 16:14:55,115 DEBUG [regionserver60020-smallCompactions-1404255751957] compactions.Compactor: Compacting hdfs://master:54310/hbase/data/default/usertable/9e9a6a0fd8aef92a09081d29900a50f3/family/1896d5d755e348db8ff88123491292cb, keycount=170149, bloomtype=ROW, size=121.1m, encoding=NONE, seqNum=4596, earliestPutTs=1404256052824
2014-07-01 16:14:55,115 DEBUG [regionserver60020-smallCompactions-1404255751957] compactions.Compactor: Compacting hdfs://master:54310/hbase/data/default/usertable/9e9a6a0fd8aef92a09081d29900a50f3/family/52723ffacf474f7bb0def0ad4556aecf, keycount=238891, bloomtype=ROW, size=170.0m, encoding=NONE, seqNum=5608, earliestPutTs=1404256127620
2014-07-01 16:14:55,167 DEBUG [regionserver60020-smallCompactions-1404255751957] util.FSUtils: DFS Client does not support most favored nodes create; using default create
2014-07-01 16:16:59,215 DEBUG [regionserver60020-smallCompactions-1404255751957] regionserver.HRegionFileSystem: Committing store file hdfs://master:54310/hbase/data/default/usertable/9e9a6a0fd8aef92a09081d29900a50f3/.tmp/80fa6bc9f6ab4197808aa3963426e088 as hdfs://master:54310/hbase/data/default/usertable/9e9a6a0fd8aef92a09081d29900a50f3/family/80fa6bc9f6ab4197808aa3963426e088
2014-07-01 16:16:59,239 DEBUG [regionserver60020-smallCompactions-1404255751957] regionserver.HStore: Removing store files after compaction...
2014-07-01 16:16:59,257 DEBUG [regionserver60020-smallCompactions-1404255751957] backup.HFileArchiver: Finished archiving from class org.apache.hadoop.hbase.backup.HFileArchiver$FileableStoreFile, file:hdfs://master:54310/hbase/data/default/usertable/9e9a6a0fd8aef92a09081d29900a50f3/family/41114ac488ec418c96706a7d6dbfda7b, to hdfs://master:54310/hbase/archive/data/default/usertable/9e9a6a0fd8aef92a09081d29900a50f3/family/41114ac488ec418c96706a7d6dbfda7b
2014-07-01 16:16:59,259 DEBUG [regionserver60020-smallCompactions-1404255751957] backup.HFileArchiver: Finished archiving from class org.apache.hadoop.hbase.backup.HFileArchiver$FileableStoreFile, file:hdfs://master:54310/hbase/data/default/usertable/9e9a6a0fd8aef92a09081d29900a50f3/family/7aba3925a1bb4009a59ff08ef6dc8b77, to hdfs://master:54310/hbase/archive/data/default/usertable/9e9a6a0fd8aef92a09081d29900a50f3/family/7aba3925a1bb4009a59ff08ef6dc8b77
2014-07-01 16:16:59,262 DEBUG [regionserver60020-smallCompactions-1404255751957] backup.HFileArchiver: Finished archiving from class org.apache.hadoop.hbase.backup.HFileArchiver$FileableStoreFile, file:hdfs://master:54310/hbase/data/default/usertable/9e9a6a0fd8aef92a09081d29900a50f3/family/26459e61df9c4e4c843ed5735437f446, to hdfs://master:54310/hbase/archive/data/default/usertable/9e9a6a0fd8aef92a09081d29900a50f3/family/26459e61df9c4e4c843ed5735437f446
2014-07-01 16:16:59,266 DEBUG [regionserver60020-smallCompactions-1404255751957] backup.HFileArchiver: Finished archiving from class org.apache.hadoop.hbase.backup.HFileArchiver$FileableStoreFile, file:hdfs://master:54310/hbase/data/default/usertable/9e9a6a0fd8aef92a09081d29900a50f3/family/77f7a0ace3be4249ab059083a3f795fd, to hdfs://master:54310/hbase/archive/data/default/usertable/9e9a6a0fd8aef92a09081d29900a50f3/family/77f7a0ace3be4249ab059083a3f795fd
2014-07-01 16:16:59,268 DEBUG [regionserver60020-smallCompactions-1404255751957] backup.HFileArchiver: Finished archiving from class org.apache.hadoop.hbase.backup.HFileArchiver$FileableStoreFile, file:hdfs://master:54310/hbase/data/default/usertable/9e9a6a0fd8aef92a09081d29900a50f3/family/1896d5d755e348db8ff88123491292cb, to hdfs://master:54310/hbase/archive/data/default/usertable/9e9a6a0fd8aef92a09081d29900a50f3/family/1896d5d755e348db8ff88123491292cb
2014-07-01 16:16:59,271 DEBUG [regionserver60020-smallCompactions-1404255751957] backup.HFileArchiver: Finished archiving from class org.apache.hadoop.hbase.backup.HFileArchiver$FileableStoreFile, file:hdfs://master:54310/hbase/data/default/usertable/9e9a6a0fd8aef92a09081d29900a50f3/family/52723ffacf474f7bb0def0ad4556aecf, to hdfs://master:54310/hbase/archive/data/default/usertable/9e9a6a0fd8aef92a09081d29900a50f3/family/52723ffacf474f7bb0def0ad4556aecf
2014-07-01 16:16:59,271 INFO  [regionserver60020-smallCompactions-1404255751957] regionserver.HStore: Completed major compaction of 6 file(s) in family of usertable,user8,1404255616195.9e9a6a0fd8aef92a09081d29900a50f3. into 80fa6bc9f6ab4197808aa3963426e088(size=849.7m), total size for store is 849.7m. This selection was in queue for 0sec, and took 2mins, 4sec to execute.
2014-07-01 16:16:59,271 INFO  [regionserver60020-smallCompactions-1404255751957] regionserver.CompactSplitThread: Completed compaction: Request = regionName=usertable,user8,1404255616195.9e9a6a0fd8aef92a09081d29900a50f3., storeName=family, fileCount=6, fileSize=860.8m, priority=14, time=76637471290510; duration=2mins, 4sec
2014-07-01 16:16:59,271 DEBUG [regionserver60020-smallCompactions-1404255751957] regionserver.CompactSplitThread: CompactSplitThread Status: compaction_queue=(0:23), split_queue=0, merge_queue=0
2014-07-01 16:16:59,272 DEBUG [regionserver60020-smallCompactions-1404255751957] compactions.RatioBasedCompactionPolicy: Selecting compaction from 6 store files, 0 compacting, 6 eligible, 20 blocking
2014-07-01 16:16:59,272 DEBUG [regionserver60020-smallCompactions-1404255751957] compactions.ExploringCompactionPolicy: Exploring compaction algorithm has selected 6 files of size 929642658 starting at candidate #0 after considering 10 permutations with 7 in ratio
2014-07-01 16:16:59,272 DEBUG [regionserver60020-smallCompactions-1404255751957] regionserver.HStore: 3e76136f0a8be94ccf3637676c064402 - family: Initiating major compaction
2014-07-01 16:16:59,272 INFO  [regionserver60020-smallCompactions-1404255751957] regionserver.HRegion: Starting compaction on family in region usertable,user5,1404255616194.3e76136f0a8be94ccf3637676c064402.
2014-07-01 16:16:59,272 INFO  [regionserver60020-smallCompactions-1404255751957] regionserver.HStore: Starting compaction of 6 file(s) in family of usertable,user5,1404255616194.3e76136f0a8be94ccf3637676c064402. into tmpdir=hdfs://master:54310/hbase/data/default/usertable/3e76136f0a8be94ccf3637676c064402/.tmp, totalSize=886.6m
2014-07-01 16:16:59,273 DEBUG [regionserver60020-smallCompactions-1404255751957] compactions.Compactor: Compacting hdfs://master:54310/hbase/data/default/usertable/3e76136f0a8be94ccf3637676c064402/family/e9a284d290304b4a901a91aa250ff643, keycount=624386, bloomtype=ROW, size=444.1m, encoding=NONE, seqNum=1789, earliestPutTs=1404255694787
2014-07-01 16:16:59,273 DEBUG [regionserver60020-smallCompactions-1404255751957] compactions.Compactor: Compacting hdfs://master:54310/hbase/data/default/usertable/3e76136f0a8be94ccf3637676c064402/family/792af824f9b14a9b94cf8593f0c25e14, keycount=110171, bloomtype=ROW, size=78.5m, encoding=NONE, seqNum=2181, earliestPutTs=1404255982570
2014-07-01 16:16:59,273 DEBUG [regionserver60020-smallCompactions-1404255751957] compactions.Compactor: Compacting hdfs://master:54310/hbase/data/default/usertable/3e76136f0a8be94ccf3637676c064402/family/79f18001f71240f782a850903a59dfaf, keycount=141749, bloomtype=ROW, size=100.9m, encoding=NONE, seqNum=2504, earliestPutTs=1404256035166
2014-07-01 16:16:59,273 DEBUG [regionserver60020-smallCompactions-1404255751957] compactions.Compactor: Compacting hdfs://master:54310/hbase/data/default/usertable/3e76136f0a8be94ccf3637676c064402/family/46ebd0509b294d349f686f77448af8fe, keycount=110003, bloomtype=ROW, size=78.3m, encoding=NONE, seqNum=2866, earliestPutTs=1404256079522
2014-07-01 16:16:59,273 DEBUG [regionserver60020-smallCompactions-1404255751957] compactions.Compactor: Compacting hdfs://master:54310/hbase/data/default/usertable/3e76136f0a8be94ccf3637676c064402/family/2c92bb563dc1425a9dc9728f8a7d5c27, keycount=148902, bloomtype=ROW, size=106.0m, encoding=NONE, seqNum=3243, earliestPutTs=1404256130308
2014-07-01 16:16:59,273 DEBUG [regionserver60020-smallCompactions-1404255751957] compactions.Compactor: Compacting hdfs://master:54310/hbase/data/default/usertable/3e76136f0a8be94ccf3637676c064402/family/555e9164845f4732bcee95e73b5a46ca, keycount=110547, bloomtype=ROW, size=78.7m, encoding=NONE, seqNum=3419, earliestPutTs=1404256174727
2014-07-01 16:16:59,370 DEBUG [regionserver60020-smallCompactions-1404255751957] util.FSUtils: DFS Client does not support most favored nodes create; using default create
2014-07-01 16:18:47,928 DEBUG [LruStats #0] hfile.LruBlockCache: Total=810.6 MB, free=3.17 GB, max=3.96 GB, blocks=12683, accesses=6737731, hits=6542130, hitRatio=97.09%, , cachingAccesses=6553897, cachingHits=6531337, cachingHitsRatio=99.65%, evictions=0, evicted=9423, evictedPerRun=Infinity
2014-07-01 16:19:05,385 DEBUG [regionserver60020-smallCompactions-1404255751957] regionserver.HRegionFileSystem: Committing store file hdfs://master:54310/hbase/data/default/usertable/3e76136f0a8be94ccf3637676c064402/.tmp/befc0cba68a0492db1ecb6ae0d684f0b as hdfs://master:54310/hbase/data/default/usertable/3e76136f0a8be94ccf3637676c064402/family/befc0cba68a0492db1ecb6ae0d684f0b
2014-07-01 16:19:05,427 DEBUG [regionserver60020-smallCompactions-1404255751957] regionserver.HStore: Removing store files after compaction...
2014-07-01 16:19:05,447 DEBUG [regionserver60020-smallCompactions-1404255751957] backup.HFileArchiver: Finished archiving from class org.apache.hadoop.hbase.backup.HFileArchiver$FileableStoreFile, file:hdfs://master:54310/hbase/data/default/usertable/3e76136f0a8be94ccf3637676c064402/family/e9a284d290304b4a901a91aa250ff643, to hdfs://master:54310/hbase/archive/data/default/usertable/3e76136f0a8be94ccf3637676c064402/family/e9a284d290304b4a901a91aa250ff643
2014-07-01 16:19:05,449 DEBUG [regionserver60020-smallCompactions-1404255751957] backup.HFileArchiver: Finished archiving from class org.apache.hadoop.hbase.backup.HFileArchiver$FileableStoreFile, file:hdfs://master:54310/hbase/data/default/usertable/3e76136f0a8be94ccf3637676c064402/family/792af824f9b14a9b94cf8593f0c25e14, to hdfs://master:54310/hbase/archive/data/default/usertable/3e76136f0a8be94ccf3637676c064402/family/792af824f9b14a9b94cf8593f0c25e14
2014-07-01 16:19:05,451 DEBUG [regionserver60020-smallCompactions-1404255751957] backup.HFileArchiver: Finished archiving from class org.apache.hadoop.hbase.backup.HFileArchiver$FileableStoreFile, file:hdfs://master:54310/hbase/data/default/usertable/3e76136f0a8be94ccf3637676c064402/family/79f18001f71240f782a850903a59dfaf, to hdfs://master:54310/hbase/archive/data/default/usertable/3e76136f0a8be94ccf3637676c064402/family/79f18001f71240f782a850903a59dfaf
2014-07-01 16:19:05,454 DEBUG [regionserver60020-smallCompactions-1404255751957] backup.HFileArchiver: Finished archiving from class org.apache.hadoop.hbase.backup.HFileArchiver$FileableStoreFile, file:hdfs://master:54310/hbase/data/default/usertable/3e76136f0a8be94ccf3637676c064402/family/46ebd0509b294d349f686f77448af8fe, to hdfs://master:54310/hbase/archive/data/default/usertable/3e76136f0a8be94ccf3637676c064402/family/46ebd0509b294d349f686f77448af8fe
2014-07-01 16:19:05,456 DEBUG [regionserver60020-smallCompactions-1404255751957] backup.HFileArchiver: Finished archiving from class org.apache.hadoop.hbase.backup.HFileArchiver$FileableStoreFile, file:hdfs://master:54310/hbase/data/default/usertable/3e76136f0a8be94ccf3637676c064402/family/2c92bb563dc1425a9dc9728f8a7d5c27, to hdfs://master:54310/hbase/archive/data/default/usertable/3e76136f0a8be94ccf3637676c064402/family/2c92bb563dc1425a9dc9728f8a7d5c27
2014-07-01 16:19:05,459 DEBUG [regionserver60020-smallCompactions-1404255751957] backup.HFileArchiver: Finished archiving from class org.apache.hadoop.hbase.backup.HFileArchiver$FileableStoreFile, file:hdfs://master:54310/hbase/data/default/usertable/3e76136f0a8be94ccf3637676c064402/family/555e9164845f4732bcee95e73b5a46ca, to hdfs://master:54310/hbase/archive/data/default/usertable/3e76136f0a8be94ccf3637676c064402/family/555e9164845f4732bcee95e73b5a46ca
2014-07-01 16:19:05,460 INFO  [regionserver60020-smallCompactions-1404255751957] regionserver.HStore: Completed major compaction of 6 file(s) in family of usertable,user5,1404255616194.3e76136f0a8be94ccf3637676c064402. into befc0cba68a0492db1ecb6ae0d684f0b(size=856.7m), total size for store is 856.7m. This selection was in queue for 0sec, and took 2mins, 6sec to execute.
2014-07-01 16:19:05,460 INFO  [regionserver60020-smallCompactions-1404255751957] regionserver.CompactSplitThread: Completed compaction: Request = regionName=usertable,user5,1404255616194.3e76136f0a8be94ccf3637676c064402., storeName=family, fileCount=6, fileSize=886.6m, priority=14, time=76761629772995; duration=2mins, 6sec
2014-07-01 16:19:05,460 DEBUG [regionserver60020-smallCompactions-1404255751957] regionserver.CompactSplitThread: CompactSplitThread Status: compaction_queue=(0:22), split_queue=0, merge_queue=0
2014-07-01 16:19:05,470 DEBUG [regionserver60020-smallCompactions-1404255751957] compactions.RatioBasedCompactionPolicy: Selecting compaction from 3 store files, 0 compacting, 3 eligible, 20 blocking
2014-07-01 16:19:05,470 DEBUG [regionserver60020-smallCompactions-1404255751957] compactions.ExploringCompactionPolicy: Exploring compaction algorithm has selected 0 files of size 0 starting at candidate #-1 after considering 1 permutations with 0 in ratio
2014-07-01 16:19:05,470 DEBUG [regionserver60020-smallCompactions-1404255751957] compactions.RatioBasedCompactionPolicy: Not compacting files because we only have 0 files ready for compaction. Need 3 to initiate.
2014-07-01 16:19:05,471 DEBUG [regionserver60020-smallCompactions-1404255751957] regionserver.CompactSplitThread: Not compacting usertable,user3,1404255616194.09108ac86cf3c374e79d1bb78f787b49. because compaction request was cancelled
2014-07-01 16:19:05,471 DEBUG [regionserver60020-smallCompactions-1404255751957] compactions.RatioBasedCompactionPolicy: Selecting compaction from 3 store files, 0 compacting, 3 eligible, 20 blocking
2014-07-01 16:19:05,471 DEBUG [regionserver60020-smallCompactions-1404255751957] compactions.ExploringCompactionPolicy: Exploring compaction algorithm has selected 0 files of size 0 starting at candidate #-1 after considering 1 permutations with 0 in ratio
2014-07-01 16:19:05,471 DEBUG [regionserver60020-smallCompactions-1404255751957] compactions.RatioBasedCompactionPolicy: Not compacting files because we only have 0 files ready for compaction. Need 3 to initiate.
2014-07-01 16:19:05,471 DEBUG [regionserver60020-smallCompactions-1404255751957] regionserver.CompactSplitThread: Not compacting usertable,user3,1404255616194.09108ac86cf3c374e79d1bb78f787b49. because compaction request was cancelled
2014-07-01 16:19:05,471 DEBUG [regionserver60020-smallCompactions-1404255751957] compactions.RatioBasedCompactionPolicy: Selecting compaction from 3 store files, 0 compacting, 3 eligible, 20 blocking
2014-07-01 16:19:05,471 DEBUG [regionserver60020-smallCompactions-1404255751957] compactions.ExploringCompactionPolicy: Exploring compaction algorithm has selected 0 files of size 0 starting at candidate #-1 after considering 1 permutations with 0 in ratio
2014-07-01 16:19:05,471 DEBUG [regionserver60020-smallCompactions-1404255751957] compactions.RatioBasedCompactionPolicy: Not compacting files because we only have 0 files ready for compaction. Need 3 to initiate.
2014-07-01 16:19:05,471 DEBUG [regionserver60020-smallCompactions-1404255751957] regionserver.CompactSplitThread: Not compacting usertable,user3,1404255616194.09108ac86cf3c374e79d1bb78f787b49. because compaction request was cancelled
2014-07-01 16:19:05,471 DEBUG [regionserver60020-smallCompactions-1404255751957] compactions.RatioBasedCompactionPolicy: Selecting compaction from 3 store files, 0 compacting, 3 eligible, 20 blocking
2014-07-01 16:19:05,471 DEBUG [regionserver60020-smallCompactions-1404255751957] compactions.ExploringCompactionPolicy: Exploring compaction algorithm has selected 0 files of size 0 starting at candidate #-1 after considering 1 permutations with 0 in ratio
2014-07-01 16:19:05,471 DEBUG [regionserver60020-smallCompactions-1404255751957] compactions.RatioBasedCompactionPolicy: Not compacting files because we only have 0 files ready for compaction. Need 3 to initiate.
2014-07-01 16:19:05,471 DEBUG [regionserver60020-smallCompactions-1404255751957] regionserver.CompactSplitThread: Not compacting usertable,user3,1404255616194.09108ac86cf3c374e79d1bb78f787b49. because compaction request was cancelled
2014-07-01 16:19:05,472 DEBUG [regionserver60020-smallCompactions-1404255751957] compactions.RatioBasedCompactionPolicy: Selecting compaction from 1 store files, 0 compacting, 1 eligible, 20 blocking
2014-07-01 16:19:05,472 DEBUG [regionserver60020-smallCompactions-1404255751957] compactions.ExploringCompactionPolicy: Exploring compaction algorithm has selected 0 files of size 0 starting at candidate #-1 after considering 0 permutations with 0 in ratio
2014-07-01 16:19:05,472 DEBUG [regionserver60020-smallCompactions-1404255751957] compactions.RatioBasedCompactionPolicy: Not compacting files because we only have 0 files ready for compaction. Need 3 to initiate.
2014-07-01 16:19:05,472 DEBUG [regionserver60020-smallCompactions-1404255751957] regionserver.CompactSplitThread: Not compacting usertable,user8,1404255616195.9e9a6a0fd8aef92a09081d29900a50f3. because compaction request was cancelled
2014-07-01 16:19:05,472 DEBUG [regionserver60020-smallCompactions-1404255751957] compactions.RatioBasedCompactionPolicy: Selecting compaction from 1 store files, 0 compacting, 1 eligible, 20 blocking
2014-07-01 16:19:05,472 DEBUG [regionserver60020-smallCompactions-1404255751957] compactions.ExploringCompactionPolicy: Exploring compaction algorithm has selected 0 files of size 0 starting at candidate #-1 after considering 0 permutations with 0 in ratio
2014-07-01 16:19:05,472 DEBUG [regionserver60020-smallCompactions-1404255751957] compactions.RatioBasedCompactionPolicy: Not compacting files because we only have 0 files ready for compaction. Need 3 to initiate.
2014-07-01 16:19:05,472 DEBUG [regionserver60020-smallCompactions-1404255751957] regionserver.CompactSplitThread: Not compacting usertable,user8,1404255616195.9e9a6a0fd8aef92a09081d29900a50f3. because compaction request was cancelled
2014-07-01 16:19:05,472 DEBUG [regionserver60020-smallCompactions-1404255751957] compactions.RatioBasedCompactionPolicy: Selecting compaction from 1 store files, 0 compacting, 1 eligible, 20 blocking
2014-07-01 16:19:05,472 DEBUG [regionserver60020-smallCompactions-1404255751957] compactions.ExploringCompactionPolicy: Exploring compaction algorithm has selected 0 files of size 0 starting at candidate #-1 after considering 0 permutations with 0 in ratio
2014-07-01 16:19:05,472 DEBUG [regionserver60020-smallCompactions-1404255751957] compactions.RatioBasedCompactionPolicy: Not compacting files because we only have 0 files ready for compaction. Need 3 to initiate.
2014-07-01 16:19:05,472 DEBUG [regionserver60020-smallCompactions-1404255751957] regionserver.CompactSplitThread: Not compacting usertable,user5,1404255616194.3e76136f0a8be94ccf3637676c064402. because compaction request was cancelled
2014-07-01 16:19:05,472 DEBUG [regionserver60020-smallCompactions-1404255751957] compactions.RatioBasedCompactionPolicy: Selecting compaction from 1 store files, 0 compacting, 1 eligible, 20 blocking
2014-07-01 16:19:05,472 DEBUG [regionserver60020-smallCompactions-1404255751957] compactions.ExploringCompactionPolicy: Exploring compaction algorithm has selected 0 files of size 0 starting at candidate #-1 after considering 0 permutations with 0 in ratio
2014-07-01 16:19:05,472 DEBUG [regionserver60020-smallCompactions-1404255751957] compactions.RatioBasedCompactionPolicy: Not compacting files because we only have 0 files ready for compaction. Need 3 to initiate.
2014-07-01 16:19:05,472 DEBUG [regionserver60020-smallCompactions-1404255751957] regionserver.CompactSplitThread: Not compacting usertable,user7,1404255616194.431aad56a67499b48936eca121e3178e. because compaction request was cancelled
2014-07-01 16:19:05,472 DEBUG [regionserver60020-smallCompactions-1404255751957] compactions.RatioBasedCompactionPolicy: Selecting compaction from 1 store files, 0 compacting, 1 eligible, 20 blocking
2014-07-01 16:19:05,472 DEBUG [regionserver60020-smallCompactions-1404255751957] compactions.ExploringCompactionPolicy: Exploring compaction algorithm has selected 0 files of size 0 starting at candidate #-1 after considering 0 permutations with 0 in ratio
2014-07-01 16:19:05,473 DEBUG [regionserver60020-smallCompactions-1404255751957] compactions.RatioBasedCompactionPolicy: Not compacting files because we only have 0 files ready for compaction. Need 3 to initiate.
2014-07-01 16:19:05,473 DEBUG [regionserver60020-smallCompactions-1404255751957] regionserver.CompactSplitThread: Not compacting usertable,user5,1404255616194.3e76136f0a8be94ccf3637676c064402. because compaction request was cancelled
2014-07-01 16:19:05,473 DEBUG [regionserver60020-smallCompactions-1404255751957] compactions.RatioBasedCompactionPolicy: Selecting compaction from 1 store files, 0 compacting, 1 eligible, 20 blocking
2014-07-01 16:19:05,473 DEBUG [regionserver60020-smallCompactions-1404255751957] compactions.ExploringCompactionPolicy: Exploring compaction algorithm has selected 0 files of size 0 starting at candidate #-1 after considering 0 permutations with 0 in ratio
2014-07-01 16:19:05,473 DEBUG [regionserver60020-smallCompactions-1404255751957] compactions.RatioBasedCompactionPolicy: Not compacting files because we only have 0 files ready for compaction. Need 3 to initiate.
2014-07-01 16:19:05,473 DEBUG [regionserver60020-smallCompactions-1404255751957] regionserver.CompactSplitThread: Not compacting usertable,user7,1404255616194.431aad56a67499b48936eca121e3178e. because compaction request was cancelled
2014-07-01 16:19:05,473 DEBUG [regionserver60020-smallCompactions-1404255751957] compactions.RatioBasedCompactionPolicy: Selecting compaction from 1 store files, 0 compacting, 1 eligible, 20 blocking
2014-07-01 16:19:05,473 DEBUG [regionserver60020-smallCompactions-1404255751957] compactions.ExploringCompactionPolicy: Exploring compaction algorithm has selected 0 files of size 0 starting at candidate #-1 after considering 0 permutations with 0 in ratio
2014-07-01 16:19:05,473 DEBUG [regionserver60020-smallCompactions-1404255751957] compactions.RatioBasedCompactionPolicy: Not compacting files because we only have 0 files ready for compaction. Need 3 to initiate.
2014-07-01 16:19:05,473 DEBUG [regionserver60020-smallCompactions-1404255751957] regionserver.CompactSplitThread: Not compacting usertable,user5,1404255616194.3e76136f0a8be94ccf3637676c064402. because compaction request was cancelled
2014-07-01 16:19:05,473 DEBUG [regionserver60020-smallCompactions-1404255751957] compactions.RatioBasedCompactionPolicy: Selecting compaction from 1 store files, 0 compacting, 1 eligible, 20 blocking
2014-07-01 16:19:05,474 DEBUG [regionserver60020-smallCompactions-1404255751957] compactions.ExploringCompactionPolicy: Exploring compaction algorithm has selected 0 files of size 0 starting at candidate #-1 after considering 0 permutations with 0 in ratio
2014-07-01 16:19:05,474 DEBUG [regionserver60020-smallCompactions-1404255751957] compactions.RatioBasedCompactionPolicy: Not compacting files because we only have 0 files ready for compaction. Need 3 to initiate.
2014-07-01 16:19:05,474 DEBUG [regionserver60020-smallCompactions-1404255751957] regionserver.CompactSplitThread: Not compacting usertable,,1404255616194.510634b895ee706ff306728824babf96. because compaction request was cancelled
2014-07-01 16:19:05,474 DEBUG [regionserver60020-smallCompactions-1404255751957] compactions.RatioBasedCompactionPolicy: Selecting compaction from 1 store files, 0 compacting, 1 eligible, 20 blocking
2014-07-01 16:19:05,474 DEBUG [regionserver60020-smallCompactions-1404255751957] compactions.ExploringCompactionPolicy: Exploring compaction algorithm has selected 0 files of size 0 starting at candidate #-1 after considering 0 permutations with 0 in ratio
2014-07-01 16:19:05,474 DEBUG [regionserver60020-smallCompactions-1404255751957] compactions.RatioBasedCompactionPolicy: Not compacting files because we only have 0 files ready for compaction. Need 3 to initiate.
2014-07-01 16:19:05,474 DEBUG [regionserver60020-smallCompactions-1404255751957] regionserver.CompactSplitThread: Not compacting usertable,user8,1404255616195.9e9a6a0fd8aef92a09081d29900a50f3. because compaction request was cancelled
2014-07-01 16:19:05,474 DEBUG [regionserver60020-smallCompactions-1404255751957] compactions.RatioBasedCompactionPolicy: Selecting compaction from 1 store files, 0 compacting, 1 eligible, 20 blocking
2014-07-01 16:19:05,474 DEBUG [regionserver60020-smallCompactions-1404255751957] compactions.ExploringCompactionPolicy: Exploring compaction algorithm has selected 0 files of size 0 starting at candidate #-1 after considering 0 permutations with 0 in ratio
2014-07-01 16:19:05,474 DEBUG [regionserver60020-smallCompactions-1404255751957] compactions.RatioBasedCompactionPolicy: Not compacting files because we only have 0 files ready for compaction. Need 3 to initiate.
2014-07-01 16:19:05,474 DEBUG [regionserver60020-smallCompactions-1404255751957] regionserver.CompactSplitThread: Not compacting usertable,,1404255616194.510634b895ee706ff306728824babf96. because compaction request was cancelled
2014-07-01 16:19:05,474 DEBUG [regionserver60020-smallCompactions-1404255751957] compactions.RatioBasedCompactionPolicy: Selecting compaction from 1 store files, 0 compacting, 1 eligible, 20 blocking
2014-07-01 16:19:05,474 DEBUG [regionserver60020-smallCompactions-1404255751957] compactions.ExploringCompactionPolicy: Exploring compaction algorithm has selected 0 files of size 0 starting at candidate #-1 after considering 0 permutations with 0 in ratio
2014-07-01 16:19:05,474 DEBUG [regionserver60020-smallCompactions-1404255751957] compactions.RatioBasedCompactionPolicy: Not compacting files because we only have 0 files ready for compaction. Need 3 to initiate.
2014-07-01 16:19:05,474 DEBUG [regionserver60020-smallCompactions-1404255751957] regionserver.CompactSplitThread: Not compacting usertable,user7,1404255616194.431aad56a67499b48936eca121e3178e. because compaction request was cancelled
2014-07-01 16:19:05,474 DEBUG [regionserver60020-smallCompactions-1404255751957] compactions.RatioBasedCompactionPolicy: Selecting compaction from 1 store files, 0 compacting, 1 eligible, 20 blocking
2014-07-01 16:19:05,474 DEBUG [regionserver60020-smallCompactions-1404255751957] compactions.ExploringCompactionPolicy: Exploring compaction algorithm has selected 0 files of size 0 starting at candidate #-1 after considering 0 permutations with 0 in ratio
2014-07-01 16:19:05,474 DEBUG [regionserver60020-smallCompactions-1404255751957] compactions.RatioBasedCompactionPolicy: Not compacting files because we only have 0 files ready for compaction. Need 3 to initiate.
2014-07-01 16:19:05,474 DEBUG [regionserver60020-smallCompactions-1404255751957] regionserver.CompactSplitThread: Not compacting usertable,user8,1404255616195.9e9a6a0fd8aef92a09081d29900a50f3. because compaction request was cancelled
2014-07-01 16:19:05,475 DEBUG [regionserver60020-smallCompactions-1404255751957] compactions.RatioBasedCompactionPolicy: Selecting compaction from 1 store files, 0 compacting, 1 eligible, 20 blocking
2014-07-01 16:19:05,475 DEBUG [regionserver60020-smallCompactions-1404255751957] compactions.ExploringCompactionPolicy: Exploring compaction algorithm has selected 0 files of size 0 starting at candidate #-1 after considering 0 permutations with 0 in ratio
2014-07-01 16:19:05,475 DEBUG [regionserver60020-smallCompactions-1404255751957] compactions.RatioBasedCompactionPolicy: Not compacting files because we only have 0 files ready for compaction. Need 3 to initiate.
2014-07-01 16:19:05,475 DEBUG [regionserver60020-smallCompactions-1404255751957] regionserver.CompactSplitThread: Not compacting usertable,user5,1404255616194.3e76136f0a8be94ccf3637676c064402. because compaction request was cancelled
2014-07-01 16:19:05,475 DEBUG [regionserver60020-smallCompactions-1404255751957] compactions.RatioBasedCompactionPolicy: Selecting compaction from 1 store files, 0 compacting, 1 eligible, 20 blocking
2014-07-01 16:19:05,475 DEBUG [regionserver60020-smallCompactions-1404255751957] compactions.ExploringCompactionPolicy: Exploring compaction algorithm has selected 0 files of size 0 starting at candidate #-1 after considering 0 permutations with 0 in ratio
2014-07-01 16:19:05,475 DEBUG [regionserver60020-smallCompactions-1404255751957] compactions.RatioBasedCompactionPolicy: Not compacting files because we only have 0 files ready for compaction. Need 3 to initiate.
2014-07-01 16:19:05,475 DEBUG [regionserver60020-smallCompactions-1404255751957] regionserver.CompactSplitThread: Not compacting usertable,,1404255616194.510634b895ee706ff306728824babf96. because compaction request was cancelled
2014-07-01 16:19:05,475 DEBUG [regionserver60020-smallCompactions-1404255751957] compactions.RatioBasedCompactionPolicy: Selecting compaction from 1 store files, 0 compacting, 1 eligible, 20 blocking
2014-07-01 16:19:05,475 DEBUG [regionserver60020-smallCompactions-1404255751957] compactions.ExploringCompactionPolicy: Exploring compaction algorithm has selected 0 files of size 0 starting at candidate #-1 after considering 0 permutations with 0 in ratio
2014-07-01 16:19:05,475 DEBUG [regionserver60020-smallCompactions-1404255751957] compactions.RatioBasedCompactionPolicy: Not compacting files because we only have 0 files ready for compaction. Need 3 to initiate.
2014-07-01 16:19:05,475 DEBUG [regionserver60020-smallCompactions-1404255751957] regionserver.CompactSplitThread: Not compacting usertable,,1404255616194.510634b895ee706ff306728824babf96. because compaction request was cancelled
2014-07-01 16:19:05,475 DEBUG [regionserver60020-smallCompactions-1404255751957] compactions.RatioBasedCompactionPolicy: Selecting compaction from 1 store files, 0 compacting, 1 eligible, 20 blocking
2014-07-01 16:19:05,475 DEBUG [regionserver60020-smallCompactions-1404255751957] compactions.ExploringCompactionPolicy: Exploring compaction algorithm has selected 0 files of size 0 starting at candidate #-1 after considering 0 permutations with 0 in ratio
2014-07-01 16:19:05,475 DEBUG [regionserver60020-smallCompactions-1404255751957] compactions.RatioBasedCompactionPolicy: Not compacting files because we only have 0 files ready for compaction. Need 3 to initiate.
2014-07-01 16:19:05,475 DEBUG [regionserver60020-smallCompactions-1404255751957] regionserver.CompactSplitThread: Not compacting usertable,user7,1404255616194.431aad56a67499b48936eca121e3178e. because compaction request was cancelled
2014-07-01 16:19:05,475 DEBUG [regionserver60020-smallCompactions-1404255751957] compactions.RatioBasedCompactionPolicy: Selecting compaction from 1 store files, 0 compacting, 1 eligible, 20 blocking
2014-07-01 16:19:05,475 DEBUG [regionserver60020-smallCompactions-1404255751957] compactions.ExploringCompactionPolicy: Exploring compaction algorithm has selected 0 files of size 0 starting at candidate #-1 after considering 0 permutations with 0 in ratio
2014-07-01 16:19:05,475 DEBUG [regionserver60020-smallCompactions-1404255751957] compactions.RatioBasedCompactionPolicy: Not compacting files because we only have 0 files ready for compaction. Need 3 to initiate.
2014-07-01 16:19:05,476 DEBUG [regionserver60020-smallCompactions-1404255751957] regionserver.CompactSplitThread: Not compacting usertable,user7,1404255616194.431aad56a67499b48936eca121e3178e. because compaction request was cancelled
2014-07-01 16:19:05,476 DEBUG [regionserver60020-smallCompactions-1404255751957] compactions.RatioBasedCompactionPolicy: Selecting compaction from 1 store files, 0 compacting, 1 eligible, 20 blocking
2014-07-01 16:19:05,476 DEBUG [regionserver60020-smallCompactions-1404255751957] compactions.ExploringCompactionPolicy: Exploring compaction algorithm has selected 0 files of size 0 starting at candidate #-1 after considering 0 permutations with 0 in ratio
2014-07-01 16:19:05,476 DEBUG [regionserver60020-smallCompactions-1404255751957] compactions.RatioBasedCompactionPolicy: Not compacting files because we only have 0 files ready for compaction. Need 3 to initiate.
2014-07-01 16:19:05,476 DEBUG [regionserver60020-smallCompactions-1404255751957] regionserver.CompactSplitThread: Not compacting usertable,,1404255616194.510634b895ee706ff306728824babf96. because compaction request was cancelled
2014-07-01 16:23:47,928 DEBUG [LruStats #0] hfile.LruBlockCache: Total=781.29 MB, free=3.19 GB, max=3.96 GB, blocks=12227, accesses=8354143, hits=8152312, hitRatio=97.58%, , cachingAccesses=8166204, cachingHits=8141208, cachingHitsRatio=99.69%, evictions=0, evicted=12275, evictedPerRun=Infinity
2014-07-01 16:26:22,212 INFO  [JvmPauseMonitor] util.JvmPauseMonitor: Detected pause in JVM or host machine (eg GC): pause of approximately 1429ms
GC pool 'ParNew' had collection(s): count=1 time=1804ms
2014-07-01 16:26:57,206 WARN  [regionserver60020.compactionChecker] util.Sleeper: We slept 31321ms instead of 10000ms, this is likely due to a long garbage collecting pause and it's usually bad, see http://hbase.apache.org/book.html#trouble.rs.runtime.zkexpired
2014-07-01 16:26:57,206 WARN  [RpcServer.handler=45,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":30197,"call":"Get(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$GetRequest)","client":"9.1.143.58:54574","starttimems":1404257187008,"queuetimems":0,"class":"HRegionServer","responsesize":1543,"method":"Get"}
2014-07-01 16:26:57,206 WARN  [regionserver60020.periodicFlusher] util.Sleeper: We slept 31361ms instead of 10000ms, this is likely due to a long garbage collecting pause and it's usually bad, see http://hbase.apache.org/book.html#trouble.rs.runtime.zkexpired
2014-07-01 16:26:57,207 WARN  [regionserver60020] util.Sleeper: We slept 31991ms instead of 3000ms, this is likely due to a long garbage collecting pause and it's usually bad, see http://hbase.apache.org/book.html#trouble.rs.runtime.zkexpired
2014-07-01 16:26:57,207 WARN  [JvmPauseMonitor] util.JvmPauseMonitor: Detected pause in JVM or host machine (eg GC): pause of approximately 29992ms
GC pool 'ParNew' had collection(s): count=1 time=1722ms
GC pool 'ConcurrentMarkSweep' had collection(s): count=1 time=28471ms
2014-07-01 16:26:57,208 WARN  [RpcServer.handler=2,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":30200,"call":"Get(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$GetRequest)","client":"9.1.143.58:54574","starttimems":1404257187008,"queuetimems":0,"class":"HRegionServer","responsesize":1553,"method":"Get"}
2014-07-01 16:26:57,208 WARN  [RpcServer.handler=4,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":30200,"call":"Get(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$GetRequest)","client":"9.1.143.58:54574","starttimems":1404257187008,"queuetimems":0,"class":"HRegionServer","responsesize":1553,"method":"Get"}
2014-07-01 16:26:57,212 WARN  [RpcServer.handler=47,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":30207,"call":"Get(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$GetRequest)","client":"9.1.143.58:54574","starttimems":1404257187005,"queuetimems":0,"class":"HRegionServer","responsesize":1553,"method":"Get"}
2014-07-01 16:26:57,214 WARN  [RpcServer.handler=17,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":30207,"call":"Get(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$GetRequest)","client":"9.1.143.58:54574","starttimems":1404257187005,"queuetimems":0,"class":"HRegionServer","responsesize":1553,"method":"Get"}
2014-07-01 16:26:57,214 WARN  [RpcServer.handler=43,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":30207,"call":"Get(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$GetRequest)","client":"9.1.143.58:54574","starttimems":1404257187007,"queuetimems":0,"class":"HRegionServer","responsesize":1553,"method":"Get"}
2014-07-01 16:26:57,217 WARN  [RpcServer.handler=8,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":30209,"call":"Get(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$GetRequest)","client":"9.1.143.58:54574","starttimems":1404257187008,"queuetimems":0,"class":"HRegionServer","responsesize":1553,"method":"Get"}
2014-07-01 16:27:19,463 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-01 16:27:19,498 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1404255227840/sceplus-vm48.almaden.ibm.com%2C60020%2C1404255227840.1404256194355 with entries=109, filesize=62.3m; new WAL /hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1404255227840/sceplus-vm48.almaden.ibm.com%2C60020%2C1404255227840.1404257239463
2014-07-01 16:27:19,499 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1404255227840/sceplus-vm48.almaden.ibm.com%2C60020%2C1404255227840.1404256106138
2014-07-01 16:27:19,499 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1404255227840/sceplus-vm48.almaden.ibm.com%2C60020%2C1404255227840.1404256110449
2014-07-01 16:27:19,499 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1404255227840/sceplus-vm48.almaden.ibm.com%2C60020%2C1404255227840.1404256111944
2014-07-01 16:27:19,499 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1404255227840/sceplus-vm48.almaden.ibm.com%2C60020%2C1404255227840.1404256113306
2014-07-01 16:27:19,499 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1404255227840/sceplus-vm48.almaden.ibm.com%2C60020%2C1404255227840.1404256114750
2014-07-01 16:27:19,499 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1404255227840/sceplus-vm48.almaden.ibm.com%2C60020%2C1404255227840.1404256116170
2014-07-01 16:27:19,499 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1404255227840/sceplus-vm48.almaden.ibm.com%2C60020%2C1404255227840.1404256118003
2014-07-01 16:27:19,499 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1404255227840/sceplus-vm48.almaden.ibm.com%2C60020%2C1404255227840.1404256120413
2014-07-01 16:27:19,499 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1404255227840/sceplus-vm48.almaden.ibm.com%2C60020%2C1404255227840.1404256122778
2014-07-01 16:27:19,499 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1404255227840/sceplus-vm48.almaden.ibm.com%2C60020%2C1404255227840.1404256129029
2014-07-01 16:27:19,499 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1404255227840/sceplus-vm48.almaden.ibm.com%2C60020%2C1404255227840.1404256130419
2014-07-01 16:27:19,499 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1404255227840/sceplus-vm48.almaden.ibm.com%2C60020%2C1404255227840.1404256132466
2014-07-01 16:27:19,499 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1404255227840/sceplus-vm48.almaden.ibm.com%2C60020%2C1404255227840.1404256133955
2014-07-01 16:27:19,499 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1404255227840/sceplus-vm48.almaden.ibm.com%2C60020%2C1404255227840.1404256135896
2014-07-01 16:27:19,499 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1404255227840/sceplus-vm48.almaden.ibm.com%2C60020%2C1404255227840.1404256138308
2014-07-01 16:27:19,499 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1404255227840/sceplus-vm48.almaden.ibm.com%2C60020%2C1404255227840.1404256139756
2014-07-01 16:27:19,499 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1404255227840/sceplus-vm48.almaden.ibm.com%2C60020%2C1404255227840.1404256142467
2014-07-01 16:27:19,499 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1404255227840/sceplus-vm48.almaden.ibm.com%2C60020%2C1404255227840.1404256144323
2014-07-01 16:27:19,499 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1404255227840/sceplus-vm48.almaden.ibm.com%2C60020%2C1404255227840.1404256145880
2014-07-01 16:27:19,499 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1404255227840/sceplus-vm48.almaden.ibm.com%2C60020%2C1404255227840.1404256147708
2014-07-01 16:27:19,500 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1404255227840/sceplus-vm48.almaden.ibm.com%2C60020%2C1404255227840.1404256149547
2014-07-01 16:27:19,500 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1404255227840/sceplus-vm48.almaden.ibm.com%2C60020%2C1404255227840.1404256152402
2014-07-01 16:27:19,500 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1404255227840/sceplus-vm48.almaden.ibm.com%2C60020%2C1404255227840.1404256154591
2014-07-01 16:27:19,500 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1404255227840/sceplus-vm48.almaden.ibm.com%2C60020%2C1404255227840.1404256157177
2014-07-01 16:27:19,500 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1404255227840/sceplus-vm48.almaden.ibm.com%2C60020%2C1404255227840.1404256159234
2014-07-01 16:27:19,500 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1404255227840/sceplus-vm48.almaden.ibm.com%2C60020%2C1404255227840.1404256161419
2014-07-01 16:27:19,500 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1404255227840/sceplus-vm48.almaden.ibm.com%2C60020%2C1404255227840.1404256163193
2014-07-01 16:27:19,500 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1404255227840/sceplus-vm48.almaden.ibm.com%2C60020%2C1404255227840.1404256166155
2014-07-01 16:27:19,500 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1404255227840/sceplus-vm48.almaden.ibm.com%2C60020%2C1404255227840.1404256167817
2014-07-01 16:27:19,500 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1404255227840/sceplus-vm48.almaden.ibm.com%2C60020%2C1404255227840.1404256173356
2014-07-01 16:27:19,500 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1404255227840/sceplus-vm48.almaden.ibm.com%2C60020%2C1404255227840.1404256174899
2014-07-01 16:27:19,500 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1404255227840/sceplus-vm48.almaden.ibm.com%2C60020%2C1404255227840.1404256176278
2014-07-01 16:27:19,500 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1404255227840/sceplus-vm48.almaden.ibm.com%2C60020%2C1404255227840.1404256177612
2014-07-01 16:27:19,500 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1404255227840/sceplus-vm48.almaden.ibm.com%2C60020%2C1404255227840.1404256179422
2014-07-01 16:27:19,500 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1404255227840/sceplus-vm48.almaden.ibm.com%2C60020%2C1404255227840.1404256181190
2014-07-01 16:27:19,500 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1404255227840/sceplus-vm48.almaden.ibm.com%2C60020%2C1404255227840.1404256182533
2014-07-01 16:27:19,500 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1404255227840/sceplus-vm48.almaden.ibm.com%2C60020%2C1404255227840.1404256184203
2014-07-01 16:27:19,500 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1404255227840/sceplus-vm48.almaden.ibm.com%2C60020%2C1404255227840.1404256185800
2014-07-01 16:27:19,500 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1404255227840/sceplus-vm48.almaden.ibm.com%2C60020%2C1404255227840.1404256187283
2014-07-01 16:27:19,500 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1404255227840/sceplus-vm48.almaden.ibm.com%2C60020%2C1404255227840.1404256189000
2014-07-01 16:27:43,529 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-01 16:27:43,556 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 19427 synced till here 19425
2014-07-01 16:27:43,576 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1404255227840/sceplus-vm48.almaden.ibm.com%2C60020%2C1404255227840.1404257239463 with entries=109, filesize=63.7m; new WAL /hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1404255227840/sceplus-vm48.almaden.ibm.com%2C60020%2C1404255227840.1404257263530
2014-07-01 16:27:58,631 INFO  [JvmPauseMonitor] util.JvmPauseMonitor: Detected pause in JVM or host machine (eg GC): pause of approximately 1354ms
GC pool 'ParNew' had collection(s): count=1 time=1572ms
2014-07-01 16:28:00,160 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-01 16:28:00,198 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1404255227840/sceplus-vm48.almaden.ibm.com%2C60020%2C1404255227840.1404257263530 with entries=71, filesize=60.9m; new WAL /hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1404255227840/sceplus-vm48.almaden.ibm.com%2C60020%2C1404255227840.1404257280161
2014-07-01 16:28:03,091 INFO  [JvmPauseMonitor] util.JvmPauseMonitor: Detected pause in JVM or host machine (eg GC): pause of approximately 1459ms
GC pool 'ParNew' had collection(s): count=1 time=1749ms
2014-07-01 16:28:03,781 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-01 16:28:03,793 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 19570 synced till here 19569
2014-07-01 16:28:03,818 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1404255227840/sceplus-vm48.almaden.ibm.com%2C60020%2C1404255227840.1404257280161 with entries=72, filesize=61.8m; new WAL /hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1404255227840/sceplus-vm48.almaden.ibm.com%2C60020%2C1404255227840.1404257283781
2014-07-01 16:28:16,169 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-01 16:28:16,194 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 19669 synced till here 19668
2014-07-01 16:28:16,202 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1404255227840/sceplus-vm48.almaden.ibm.com%2C60020%2C1404255227840.1404257283781 with entries=99, filesize=62.7m; new WAL /hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1404255227840/sceplus-vm48.almaden.ibm.com%2C60020%2C1404255227840.1404257296169
2014-07-01 16:28:34,996 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-01 16:28:35,103 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 19764 synced till here 19748
2014-07-01 16:28:35,277 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1404255227840/sceplus-vm48.almaden.ibm.com%2C60020%2C1404255227840.1404257296169 with entries=95, filesize=74.4m; new WAL /hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1404255227840/sceplus-vm48.almaden.ibm.com%2C60020%2C1404255227840.1404257314997
2014-07-01 16:28:37,875 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-01 16:28:38,095 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 19852 synced till here 19849
2014-07-01 16:28:38,115 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1404255227840/sceplus-vm48.almaden.ibm.com%2C60020%2C1404255227840.1404257314997 with entries=88, filesize=75.8m; new WAL /hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1404255227840/sceplus-vm48.almaden.ibm.com%2C60020%2C1404255227840.1404257317876
2014-07-01 16:28:41,310 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-01 16:28:41,334 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 19925 synced till here 19924
2014-07-01 16:28:41,349 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1404255227840/sceplus-vm48.almaden.ibm.com%2C60020%2C1404255227840.1404257317876 with entries=73, filesize=62.6m; new WAL /hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1404255227840/sceplus-vm48.almaden.ibm.com%2C60020%2C1404255227840.1404257321310
2014-07-01 16:28:47,928 DEBUG [LruStats #0] hfile.LruBlockCache: Total=3.76 GB, free=202.68 MB, max=3.96 GB, blocks=60768, accesses=10419297, hits=10166195, hitRatio=97.57%, , cachingAccesses=10231358, cachingHits=10155091, cachingHitsRatio=99.25%, evictions=1, evicted=14842, evictedPerRun=14842.0
2014-07-01 16:29:08,553 WARN  [RpcServer.handler=42,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":26449,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.58:60942","starttimems":1404257322103,"queuetimems":0,"class":"HRegionServer","responsesize":20146,"method":"Multi"}
2014-07-01 16:29:08,739 WARN  [RpcServer.handler=26,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":26541,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.58:60942","starttimems":1404257322197,"queuetimems":0,"class":"HRegionServer","responsesize":19592,"method":"Multi"}
2014-07-01 16:29:08,957 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-01 16:29:09,010 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 20003 synced till here 19996
2014-07-01 16:29:09,240 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1404255227840/sceplus-vm48.almaden.ibm.com%2C60020%2C1404255227840.1404257321310 with entries=78, filesize=66.8m; new WAL /hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1404255227840/sceplus-vm48.almaden.ibm.com%2C60020%2C1404255227840.1404257348957
2014-07-01 16:29:09,436 WARN  [RpcServer.handler=40,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":25911,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.58:60942","starttimems":1404257323524,"queuetimems":0,"class":"HRegionServer","responsesize":19996,"method":"Multi"}
2014-07-01 16:29:09,436 WARN  [RpcServer.handler=48,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":25950,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.58:60942","starttimems":1404257323485,"queuetimems":1,"class":"HRegionServer","responsesize":20046,"method":"Multi"}
2014-07-01 16:29:16,199 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-01 16:29:16,263 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 20077 synced till here 20075
2014-07-01 16:29:16,362 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1404255227840/sceplus-vm48.almaden.ibm.com%2C60020%2C1404255227840.1404257348957 with entries=74, filesize=63.6m; new WAL /hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1404255227840/sceplus-vm48.almaden.ibm.com%2C60020%2C1404255227840.1404257356199
2014-07-01 16:29:23,534 DEBUG [RpcServer.handler=45,port=60020] regionserver.HRegion: Flush requested on usertable,user8,1404255616195.9e9a6a0fd8aef92a09081d29900a50f3.
2014-07-01 16:29:23,535 DEBUG [MemStoreFlusher.0] regionserver.HRegion: Started memstore flush for usertable,user8,1404255616195.9e9a6a0fd8aef92a09081d29900a50f3., current region memstore size 256.2m
2014-07-01 16:29:23,730 DEBUG [MemStoreFlusher.0] util.FSUtils: DFS Client does not support most favored nodes create; using default create
2014-07-01 16:29:25,840 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-01 16:29:25,867 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 20165 synced till here 20163
2014-07-01 16:29:25,920 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1404255227840/sceplus-vm48.almaden.ibm.com%2C60020%2C1404255227840.1404257356199 with entries=88, filesize=62.6m; new WAL /hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1404255227840/sceplus-vm48.almaden.ibm.com%2C60020%2C1404255227840.1404257365840
2014-07-01 16:29:28,579 INFO  [MemStoreFlusher.0] regionserver.DefaultStoreFlusher: Flushed, sequenceid=5830, memsize=158.2m, hasBloomFilter=true, into tmp file hdfs://master:54310/hbase/data/default/usertable/9e9a6a0fd8aef92a09081d29900a50f3/.tmp/96871cf8f6904e1ebc0f6578bf92435a
2014-07-01 16:29:28,593 DEBUG [MemStoreFlusher.0] regionserver.HRegionFileSystem: Committing store file hdfs://master:54310/hbase/data/default/usertable/9e9a6a0fd8aef92a09081d29900a50f3/.tmp/96871cf8f6904e1ebc0f6578bf92435a as hdfs://master:54310/hbase/data/default/usertable/9e9a6a0fd8aef92a09081d29900a50f3/family/96871cf8f6904e1ebc0f6578bf92435a
2014-07-01 16:29:28,615 INFO  [MemStoreFlusher.0] regionserver.HStore: Added hdfs://master:54310/hbase/data/default/usertable/9e9a6a0fd8aef92a09081d29900a50f3/family/96871cf8f6904e1ebc0f6578bf92435a, entries=575910, sequenceid=5830, filesize=41.1m
2014-07-01 16:29:28,615 INFO  [MemStoreFlusher.0] regionserver.HRegion: Finished memstore flush of ~257.5m/269989760, currentsize=17.2m/18085840 for region usertable,user8,1404255616195.9e9a6a0fd8aef92a09081d29900a50f3. in 5080ms, sequenceid=5830, compaction requested=false
2014-07-01 16:29:49,100 INFO  [Priority.RpcServer.handler=9,port=60020] regionserver.HRegionServer: Close 510634b895ee706ff306728824babf96, via zk=yes, znode version=0, on null
2014-07-01 16:29:49,101 INFO  [Priority.RpcServer.handler=9,port=60020] regionserver.HRegionServer: Close 431aad56a67499b48936eca121e3178e, via zk=yes, znode version=0, on null
2014-07-01 16:29:49,101 INFO  [Priority.RpcServer.handler=9,port=60020] regionserver.HRegionServer: Close 09108ac86cf3c374e79d1bb78f787b49, via zk=yes, znode version=0, on null
2014-07-01 16:29:49,102 INFO  [Priority.RpcServer.handler=9,port=60020] regionserver.HRegionServer: Close 9e9a6a0fd8aef92a09081d29900a50f3, via zk=yes, znode version=0, on null
2014-07-01 16:29:49,102 INFO  [Priority.RpcServer.handler=9,port=60020] regionserver.HRegionServer: Close 3e76136f0a8be94ccf3637676c064402, via zk=yes, znode version=0, on null
2014-07-01 16:29:49,133 DEBUG [RS_CLOSE_REGION-sceplus-vm48:60020-2] handler.CloseRegionHandler: Processing close of usertable,,1404255616194.510634b895ee706ff306728824babf96.
2014-07-01 16:29:49,136 DEBUG [RS_CLOSE_REGION-sceplus-vm48:60020-2] regionserver.HRegion: Closing usertable,,1404255616194.510634b895ee706ff306728824babf96.: disabling compactions & flushes
2014-07-01 16:29:49,136 INFO  [RS_CLOSE_REGION-sceplus-vm48:60020-2] regionserver.HRegion: Running close preflush of usertable,,1404255616194.510634b895ee706ff306728824babf96.
2014-07-01 16:29:49,137 DEBUG [RS_CLOSE_REGION-sceplus-vm48:60020-2] regionserver.HRegion: Started memstore flush for usertable,,1404255616194.510634b895ee706ff306728824babf96., current region memstore size 248.4m
2014-07-01 16:29:49,150 DEBUG [RS_CLOSE_REGION-sceplus-vm48:60020-0] handler.CloseRegionHandler: Processing close of usertable,user7,1404255616194.431aad56a67499b48936eca121e3178e.
2014-07-01 16:29:49,184 DEBUG [RS_CLOSE_REGION-sceplus-vm48:60020-1] handler.CloseRegionHandler: Processing close of usertable,user3,1404255616194.09108ac86cf3c374e79d1bb78f787b49.
2014-07-01 16:29:49,185 DEBUG [RS_CLOSE_REGION-sceplus-vm48:60020-0] regionserver.HRegion: Closing usertable,user7,1404255616194.431aad56a67499b48936eca121e3178e.: disabling compactions & flushes
2014-07-01 16:29:49,185 INFO  [RS_CLOSE_REGION-sceplus-vm48:60020-0] regionserver.HRegion: Running close preflush of usertable,user7,1404255616194.431aad56a67499b48936eca121e3178e.
2014-07-01 16:29:49,185 DEBUG [RS_CLOSE_REGION-sceplus-vm48:60020-0] regionserver.HRegion: Started memstore flush for usertable,user7,1404255616194.431aad56a67499b48936eca121e3178e., current region memstore size 255.3m
2014-07-01 16:29:49,186 DEBUG [RS_CLOSE_REGION-sceplus-vm48:60020-1] regionserver.HRegion: Closing usertable,user3,1404255616194.09108ac86cf3c374e79d1bb78f787b49.: disabling compactions & flushes
2014-07-01 16:29:49,186 INFO  [RS_CLOSE_REGION-sceplus-vm48:60020-1] regionserver.HRegion: Running close preflush of usertable,user3,1404255616194.09108ac86cf3c374e79d1bb78f787b49.
2014-07-01 16:29:49,186 DEBUG [RS_CLOSE_REGION-sceplus-vm48:60020-1] regionserver.HRegion: Started memstore flush for usertable,user3,1404255616194.09108ac86cf3c374e79d1bb78f787b49., current region memstore size 248.3m
2014-07-01 16:29:49,333 DEBUG [RS_CLOSE_REGION-sceplus-vm48:60020-2] util.FSUtils: DFS Client does not support most favored nodes create; using default create
2014-07-01 16:29:49,349 DEBUG [RS_CLOSE_REGION-sceplus-vm48:60020-1] util.FSUtils: DFS Client does not support most favored nodes create; using default create
2014-07-01 16:29:49,362 DEBUG [RS_CLOSE_REGION-sceplus-vm48:60020-0] util.FSUtils: DFS Client does not support most favored nodes create; using default create
2014-07-01 16:29:54,449 INFO  [RS_CLOSE_REGION-sceplus-vm48:60020-2] regionserver.DefaultStoreFlusher: Flushed, sequenceid=3614, memsize=148.9m, hasBloomFilter=true, into tmp file hdfs://master:54310/hbase/data/default/usertable/510634b895ee706ff306728824babf96/.tmp/46a11a13c36040a2a3a9d85c48e6f075
2014-07-01 16:29:54,459 DEBUG [RS_CLOSE_REGION-sceplus-vm48:60020-2] regionserver.HRegionFileSystem: Committing store file hdfs://master:54310/hbase/data/default/usertable/510634b895ee706ff306728824babf96/.tmp/46a11a13c36040a2a3a9d85c48e6f075 as hdfs://master:54310/hbase/data/default/usertable/510634b895ee706ff306728824babf96/family/46a11a13c36040a2a3a9d85c48e6f075
2014-07-01 16:29:54,467 INFO  [RS_CLOSE_REGION-sceplus-vm48:60020-2] regionserver.HStore: Added hdfs://master:54310/hbase/data/default/usertable/510634b895ee706ff306728824babf96/family/46a11a13c36040a2a3a9d85c48e6f075, entries=541980, sequenceid=3614, filesize=38.6m
2014-07-01 16:29:54,467 INFO  [RS_CLOSE_REGION-sceplus-vm48:60020-2] regionserver.HRegion: Finished memstore flush of ~248.4m/260475040, currentsize=0.0/0 for region usertable,,1404255616194.510634b895ee706ff306728824babf96. in 5330ms, sequenceid=3614, compaction requested=false
2014-07-01 16:29:54,467 DEBUG [RS_CLOSE_REGION-sceplus-vm48:60020-2] regionserver.HRegion: Updates disabled for region usertable,,1404255616194.510634b895ee706ff306728824babf96.
2014-07-01 16:29:54,499 INFO  [RS_CLOSE_REGION-sceplus-vm48:60020-1] regionserver.DefaultStoreFlusher: Flushed, sequenceid=3599, memsize=148.9m, hasBloomFilter=true, into tmp file hdfs://master:54310/hbase/data/default/usertable/09108ac86cf3c374e79d1bb78f787b49/.tmp/e913a339c38f46ee831574f4009e6286
2014-07-01 16:29:54,509 DEBUG [RS_CLOSE_REGION-sceplus-vm48:60020-1] regionserver.HRegionFileSystem: Committing store file hdfs://master:54310/hbase/data/default/usertable/09108ac86cf3c374e79d1bb78f787b49/.tmp/e913a339c38f46ee831574f4009e6286 as hdfs://master:54310/hbase/data/default/usertable/09108ac86cf3c374e79d1bb78f787b49/family/e913a339c38f46ee831574f4009e6286
2014-07-01 16:29:54,511 INFO  [StoreCloserThread-usertable,,1404255616194.510634b895ee706ff306728824babf96.-1] regionserver.HStore: Closed family
2014-07-01 16:29:54,512 INFO  [RS_CLOSE_REGION-sceplus-vm48:60020-2] regionserver.HRegion: Closed usertable,,1404255616194.510634b895ee706ff306728824babf96.
2014-07-01 16:29:54,512 DEBUG [RS_CLOSE_REGION-sceplus-vm48:60020-2] zookeeper.ZKAssign: regionserver:60020-0x46f4224a300001-0x46f4224a300001-0x46f4224a300001, quorum=sceplus-vm49.almaden.ibm.com:2181,sceplus-vm48.almaden.ibm.com:2181, baseZNode=/hbase Transitioning 510634b895ee706ff306728824babf96 from M_ZK_REGION_CLOSING to RS_ZK_REGION_CLOSED
2014-07-01 16:29:54,518 INFO  [RS_CLOSE_REGION-sceplus-vm48:60020-1] regionserver.HStore: Added hdfs://master:54310/hbase/data/default/usertable/09108ac86cf3c374e79d1bb78f787b49/family/e913a339c38f46ee831574f4009e6286, entries=542200, sequenceid=3599, filesize=38.6m
2014-07-01 16:29:54,518 INFO  [RS_CLOSE_REGION-sceplus-vm48:60020-1] regionserver.HRegion: Finished memstore flush of ~248.3m/260329600, currentsize=0.0/0 for region usertable,user3,1404255616194.09108ac86cf3c374e79d1bb78f787b49. in 5332ms, sequenceid=3599, compaction requested=true
2014-07-01 16:29:54,518 DEBUG [RS_CLOSE_REGION-sceplus-vm48:60020-1] regionserver.HRegion: Updates disabled for region usertable,user3,1404255616194.09108ac86cf3c374e79d1bb78f787b49.
2014-07-01 16:29:54,519 DEBUG [RS_CLOSE_REGION-sceplus-vm48:60020-2] zookeeper.ZKAssign: regionserver:60020-0x46f4224a300001-0x46f4224a300001-0x46f4224a300001, quorum=sceplus-vm49.almaden.ibm.com:2181,sceplus-vm48.almaden.ibm.com:2181, baseZNode=/hbase Transitioned node 510634b895ee706ff306728824babf96 from M_ZK_REGION_CLOSING to RS_ZK_REGION_CLOSED
2014-07-01 16:29:54,519 DEBUG [RS_CLOSE_REGION-sceplus-vm48:60020-2] handler.CloseRegionHandler: Set closed state in zk for usertable,,1404255616194.510634b895ee706ff306728824babf96. on sceplus-vm48.almaden.ibm.com,60020,1404255227840
2014-07-01 16:29:54,519 DEBUG [RS_CLOSE_REGION-sceplus-vm48:60020-2] handler.CloseRegionHandler: Closed usertable,,1404255616194.510634b895ee706ff306728824babf96.
2014-07-01 16:29:54,519 DEBUG [RS_CLOSE_REGION-sceplus-vm48:60020-2] handler.CloseRegionHandler: Processing close of usertable,user8,1404255616195.9e9a6a0fd8aef92a09081d29900a50f3.
2014-07-01 16:29:54,521 DEBUG [RS_CLOSE_REGION-sceplus-vm48:60020-2] regionserver.HRegion: Closing usertable,user8,1404255616195.9e9a6a0fd8aef92a09081d29900a50f3.: disabling compactions & flushes
2014-07-01 16:29:54,521 INFO  [RS_CLOSE_REGION-sceplus-vm48:60020-2] regionserver.HRegion: Running close preflush of usertable,user8,1404255616195.9e9a6a0fd8aef92a09081d29900a50f3.
2014-07-01 16:29:54,521 DEBUG [RS_CLOSE_REGION-sceplus-vm48:60020-2] regionserver.HRegion: Started memstore flush for usertable,user8,1404255616195.9e9a6a0fd8aef92a09081d29900a50f3., current region memstore size 17.2m
2014-07-01 16:29:54,529 DEBUG [RS_CLOSE_REGION-sceplus-vm48:60020-2] util.FSUtils: DFS Client does not support most favored nodes create; using default create
2014-07-01 16:29:54,536 INFO  [StoreCloserThread-usertable,user3,1404255616194.09108ac86cf3c374e79d1bb78f787b49.-1] regionserver.HStore: Closed family
2014-07-01 16:29:54,537 INFO  [RS_CLOSE_REGION-sceplus-vm48:60020-1] regionserver.HRegion: Closed usertable,user3,1404255616194.09108ac86cf3c374e79d1bb78f787b49.
2014-07-01 16:29:54,537 DEBUG [RS_CLOSE_REGION-sceplus-vm48:60020-1] zookeeper.ZKAssign: regionserver:60020-0x46f4224a300001-0x46f4224a300001-0x46f4224a300001, quorum=sceplus-vm49.almaden.ibm.com:2181,sceplus-vm48.almaden.ibm.com:2181, baseZNode=/hbase Transitioning 09108ac86cf3c374e79d1bb78f787b49 from M_ZK_REGION_CLOSING to RS_ZK_REGION_CLOSED
2014-07-01 16:29:54,541 DEBUG [RS_CLOSE_REGION-sceplus-vm48:60020-1] zookeeper.ZKAssign: regionserver:60020-0x46f4224a300001-0x46f4224a300001-0x46f4224a300001, quorum=sceplus-vm49.almaden.ibm.com:2181,sceplus-vm48.almaden.ibm.com:2181, baseZNode=/hbase Transitioned node 09108ac86cf3c374e79d1bb78f787b49 from M_ZK_REGION_CLOSING to RS_ZK_REGION_CLOSED
2014-07-01 16:29:54,541 DEBUG [RS_CLOSE_REGION-sceplus-vm48:60020-1] handler.CloseRegionHandler: Set closed state in zk for usertable,user3,1404255616194.09108ac86cf3c374e79d1bb78f787b49. on sceplus-vm48.almaden.ibm.com,60020,1404255227840
2014-07-01 16:29:54,541 DEBUG [RS_CLOSE_REGION-sceplus-vm48:60020-1] handler.CloseRegionHandler: Closed usertable,user3,1404255616194.09108ac86cf3c374e79d1bb78f787b49.
2014-07-01 16:29:54,541 DEBUG [RS_CLOSE_REGION-sceplus-vm48:60020-1] handler.CloseRegionHandler: Processing close of usertable,user5,1404255616194.3e76136f0a8be94ccf3637676c064402.
2014-07-01 16:29:54,542 DEBUG [RS_CLOSE_REGION-sceplus-vm48:60020-1] regionserver.HRegion: Closing usertable,user5,1404255616194.3e76136f0a8be94ccf3637676c064402.: disabling compactions & flushes
2014-07-01 16:29:54,542 INFO  [RS_CLOSE_REGION-sceplus-vm48:60020-1] regionserver.HRegion: Running close preflush of usertable,user5,1404255616194.3e76136f0a8be94ccf3637676c064402.
2014-07-01 16:29:54,542 DEBUG [RS_CLOSE_REGION-sceplus-vm48:60020-1] regionserver.HRegion: Started memstore flush for usertable,user5,1404255616194.3e76136f0a8be94ccf3637676c064402., current region memstore size 248.6m
2014-07-01 16:29:54,695 DEBUG [RS_CLOSE_REGION-sceplus-vm48:60020-1] util.FSUtils: DFS Client does not support most favored nodes create; using default create
2014-07-01 16:29:54,723 INFO  [RS_CLOSE_REGION-sceplus-vm48:60020-0] regionserver.DefaultStoreFlusher: Flushed, sequenceid=3598, memsize=155.9m, hasBloomFilter=true, into tmp file hdfs://master:54310/hbase/data/default/usertable/431aad56a67499b48936eca121e3178e/.tmp/0dda4e8840724c6fa24d32bbc2342c86
2014-07-01 16:29:54,731 DEBUG [RS_CLOSE_REGION-sceplus-vm48:60020-0] regionserver.HRegionFileSystem: Committing store file hdfs://master:54310/hbase/data/default/usertable/431aad56a67499b48936eca121e3178e/.tmp/0dda4e8840724c6fa24d32bbc2342c86 as hdfs://master:54310/hbase/data/default/usertable/431aad56a67499b48936eca121e3178e/family/0dda4e8840724c6fa24d32bbc2342c86
2014-07-01 16:29:54,738 INFO  [RS_CLOSE_REGION-sceplus-vm48:60020-0] regionserver.HStore: Added hdfs://master:54310/hbase/data/default/usertable/431aad56a67499b48936eca121e3178e/family/0dda4e8840724c6fa24d32bbc2342c86, entries=567740, sequenceid=3598, filesize=40.5m
2014-07-01 16:29:54,738 INFO  [RS_CLOSE_REGION-sceplus-vm48:60020-0] regionserver.HRegion: Finished memstore flush of ~255.3m/267667760, currentsize=0.0/0 for region usertable,user7,1404255616194.431aad56a67499b48936eca121e3178e. in 5553ms, sequenceid=3598, compaction requested=false
2014-07-01 16:29:54,738 DEBUG [RS_CLOSE_REGION-sceplus-vm48:60020-0] regionserver.HRegion: Updates disabled for region usertable,user7,1404255616194.431aad56a67499b48936eca121e3178e.
2014-07-01 16:29:54,754 INFO  [StoreCloserThread-usertable,user7,1404255616194.431aad56a67499b48936eca121e3178e.-1] regionserver.HStore: Closed family
2014-07-01 16:29:54,754 INFO  [RS_CLOSE_REGION-sceplus-vm48:60020-0] regionserver.HRegion: Closed usertable,user7,1404255616194.431aad56a67499b48936eca121e3178e.
2014-07-01 16:29:54,754 DEBUG [RS_CLOSE_REGION-sceplus-vm48:60020-0] zookeeper.ZKAssign: regionserver:60020-0x46f4224a300001-0x46f4224a300001-0x46f4224a300001, quorum=sceplus-vm49.almaden.ibm.com:2181,sceplus-vm48.almaden.ibm.com:2181, baseZNode=/hbase Transitioning 431aad56a67499b48936eca121e3178e from M_ZK_REGION_CLOSING to RS_ZK_REGION_CLOSED
2014-07-01 16:29:54,763 DEBUG [RS_CLOSE_REGION-sceplus-vm48:60020-0] zookeeper.ZKAssign: regionserver:60020-0x46f4224a300001-0x46f4224a300001-0x46f4224a300001, quorum=sceplus-vm49.almaden.ibm.com:2181,sceplus-vm48.almaden.ibm.com:2181, baseZNode=/hbase Transitioned node 431aad56a67499b48936eca121e3178e from M_ZK_REGION_CLOSING to RS_ZK_REGION_CLOSED
2014-07-01 16:29:54,763 DEBUG [RS_CLOSE_REGION-sceplus-vm48:60020-0] handler.CloseRegionHandler: Set closed state in zk for usertable,user7,1404255616194.431aad56a67499b48936eca121e3178e. on sceplus-vm48.almaden.ibm.com,60020,1404255227840
2014-07-01 16:29:54,763 DEBUG [RS_CLOSE_REGION-sceplus-vm48:60020-0] handler.CloseRegionHandler: Closed usertable,user7,1404255616194.431aad56a67499b48936eca121e3178e.
2014-07-01 16:29:55,005 INFO  [RS_CLOSE_REGION-sceplus-vm48:60020-2] regionserver.DefaultStoreFlusher: Flushed, sequenceid=5845, memsize=17.2m, hasBloomFilter=true, into tmp file hdfs://master:54310/hbase/data/default/usertable/9e9a6a0fd8aef92a09081d29900a50f3/.tmp/65b56e20541544619d7089fcf75442a6
2014-07-01 16:29:55,013 DEBUG [RS_CLOSE_REGION-sceplus-vm48:60020-2] regionserver.HRegionFileSystem: Committing store file hdfs://master:54310/hbase/data/default/usertable/9e9a6a0fd8aef92a09081d29900a50f3/.tmp/65b56e20541544619d7089fcf75442a6 as hdfs://master:54310/hbase/data/default/usertable/9e9a6a0fd8aef92a09081d29900a50f3/family/65b56e20541544619d7089fcf75442a6
2014-07-01 16:29:55,020 INFO  [RS_CLOSE_REGION-sceplus-vm48:60020-2] regionserver.HStore: Added hdfs://master:54310/hbase/data/default/usertable/9e9a6a0fd8aef92a09081d29900a50f3/family/65b56e20541544619d7089fcf75442a6, entries=62800, sequenceid=5845, filesize=4.5m
2014-07-01 16:29:55,021 INFO  [RS_CLOSE_REGION-sceplus-vm48:60020-2] regionserver.HRegion: Finished memstore flush of ~17.2m/18085840, currentsize=0.0/0 for region usertable,user8,1404255616195.9e9a6a0fd8aef92a09081d29900a50f3. in 500ms, sequenceid=5845, compaction requested=true
2014-07-01 16:29:55,021 DEBUG [RS_CLOSE_REGION-sceplus-vm48:60020-2] regionserver.HRegion: Updates disabled for region usertable,user8,1404255616195.9e9a6a0fd8aef92a09081d29900a50f3.
2014-07-01 16:29:55,045 INFO  [StoreCloserThread-usertable,user8,1404255616195.9e9a6a0fd8aef92a09081d29900a50f3.-1] regionserver.HStore: Closed family
2014-07-01 16:29:55,045 INFO  [RS_CLOSE_REGION-sceplus-vm48:60020-2] regionserver.HRegion: Closed usertable,user8,1404255616195.9e9a6a0fd8aef92a09081d29900a50f3.
2014-07-01 16:29:55,045 DEBUG [RS_CLOSE_REGION-sceplus-vm48:60020-2] zookeeper.ZKAssign: regionserver:60020-0x46f4224a300001-0x46f4224a300001-0x46f4224a300001, quorum=sceplus-vm49.almaden.ibm.com:2181,sceplus-vm48.almaden.ibm.com:2181, baseZNode=/hbase Transitioning 9e9a6a0fd8aef92a09081d29900a50f3 from M_ZK_REGION_CLOSING to RS_ZK_REGION_CLOSED
2014-07-01 16:29:55,055 DEBUG [RS_CLOSE_REGION-sceplus-vm48:60020-2] zookeeper.ZKAssign: regionserver:60020-0x46f4224a300001-0x46f4224a300001-0x46f4224a300001, quorum=sceplus-vm49.almaden.ibm.com:2181,sceplus-vm48.almaden.ibm.com:2181, baseZNode=/hbase Transitioned node 9e9a6a0fd8aef92a09081d29900a50f3 from M_ZK_REGION_CLOSING to RS_ZK_REGION_CLOSED
2014-07-01 16:29:55,055 DEBUG [RS_CLOSE_REGION-sceplus-vm48:60020-2] handler.CloseRegionHandler: Set closed state in zk for usertable,user8,1404255616195.9e9a6a0fd8aef92a09081d29900a50f3. on sceplus-vm48.almaden.ibm.com,60020,1404255227840
2014-07-01 16:29:55,056 DEBUG [RS_CLOSE_REGION-sceplus-vm48:60020-2] handler.CloseRegionHandler: Closed usertable,user8,1404255616195.9e9a6a0fd8aef92a09081d29900a50f3.
2014-07-01 16:29:59,647 INFO  [RS_CLOSE_REGION-sceplus-vm48:60020-1] regionserver.DefaultStoreFlusher: Flushed, sequenceid=3601, memsize=149.0m, hasBloomFilter=true, into tmp file hdfs://master:54310/hbase/data/default/usertable/3e76136f0a8be94ccf3637676c064402/.tmp/1f6be551b7ad432bafeeffd8faa8f379
2014-07-01 16:29:59,662 DEBUG [RS_CLOSE_REGION-sceplus-vm48:60020-1] regionserver.HRegionFileSystem: Committing store file hdfs://master:54310/hbase/data/default/usertable/3e76136f0a8be94ccf3637676c064402/.tmp/1f6be551b7ad432bafeeffd8faa8f379 as hdfs://master:54310/hbase/data/default/usertable/3e76136f0a8be94ccf3637676c064402/family/1f6be551b7ad432bafeeffd8faa8f379
2014-07-01 16:29:59,672 INFO  [RS_CLOSE_REGION-sceplus-vm48:60020-1] regionserver.HStore: Added hdfs://master:54310/hbase/data/default/usertable/3e76136f0a8be94ccf3637676c064402/family/1f6be551b7ad432bafeeffd8faa8f379, entries=542690, sequenceid=3601, filesize=38.6m
2014-07-01 16:29:59,673 INFO  [RS_CLOSE_REGION-sceplus-vm48:60020-1] regionserver.HRegion: Finished memstore flush of ~248.6m/260684640, currentsize=0.0/0 for region usertable,user5,1404255616194.3e76136f0a8be94ccf3637676c064402. in 5131ms, sequenceid=3601, compaction requested=false
2014-07-01 16:29:59,673 DEBUG [RS_CLOSE_REGION-sceplus-vm48:60020-1] regionserver.HRegion: Updates disabled for region usertable,user5,1404255616194.3e76136f0a8be94ccf3637676c064402.
2014-07-01 16:29:59,692 INFO  [StoreCloserThread-usertable,user5,1404255616194.3e76136f0a8be94ccf3637676c064402.-1] regionserver.HStore: Closed family
2014-07-01 16:29:59,692 INFO  [RS_CLOSE_REGION-sceplus-vm48:60020-1] regionserver.HRegion: Closed usertable,user5,1404255616194.3e76136f0a8be94ccf3637676c064402.
2014-07-01 16:29:59,692 DEBUG [RS_CLOSE_REGION-sceplus-vm48:60020-1] zookeeper.ZKAssign: regionserver:60020-0x46f4224a300001-0x46f4224a300001-0x46f4224a300001, quorum=sceplus-vm49.almaden.ibm.com:2181,sceplus-vm48.almaden.ibm.com:2181, baseZNode=/hbase Transitioning 3e76136f0a8be94ccf3637676c064402 from M_ZK_REGION_CLOSING to RS_ZK_REGION_CLOSED
2014-07-01 16:29:59,696 DEBUG [RS_CLOSE_REGION-sceplus-vm48:60020-1] zookeeper.ZKAssign: regionserver:60020-0x46f4224a300001-0x46f4224a300001-0x46f4224a300001, quorum=sceplus-vm49.almaden.ibm.com:2181,sceplus-vm48.almaden.ibm.com:2181, baseZNode=/hbase Transitioned node 3e76136f0a8be94ccf3637676c064402 from M_ZK_REGION_CLOSING to RS_ZK_REGION_CLOSED
2014-07-01 16:29:59,696 DEBUG [RS_CLOSE_REGION-sceplus-vm48:60020-1] handler.CloseRegionHandler: Set closed state in zk for usertable,user5,1404255616194.3e76136f0a8be94ccf3637676c064402. on sceplus-vm48.almaden.ibm.com,60020,1404255227840
2014-07-01 16:29:59,696 DEBUG [RS_CLOSE_REGION-sceplus-vm48:60020-1] handler.CloseRegionHandler: Closed usertable,user5,1404255616194.3e76136f0a8be94ccf3637676c064402.
2014-07-01 16:33:47,928 DEBUG [LruStats #0] hfile.LruBlockCache: Total=4.16 MB, free=3.95 GB, max=3.96 GB, blocks=0, accesses=10456985, hits=10201085, hitRatio=97.55%, , cachingAccesses=10269046, cachingHits=10189981, cachingHitsRatio=99.23%, evictions=2, evicted=78407, evictedPerRun=39203.5
2014-07-01 16:35:15,248 INFO  [Priority.RpcServer.handler=4,port=60020] regionserver.HRegionServer: Open usertable,user3,1404257714774.a59b89c303508bc329bc1095d0beb9e5.
2014-07-01 16:35:15,257 INFO  [Priority.RpcServer.handler=4,port=60020] regionserver.HRegionServer: Open usertable,user2,1404257714774.2ed053f05f5eec791a57de24e16f0043.
2014-07-01 16:35:15,257 INFO  [Priority.RpcServer.handler=4,port=60020] regionserver.HRegionServer: Open usertable,user8,1404257714774.b6ae40892bbb2a1c545234ed91d62d1a.
2014-07-01 16:35:15,257 INFO  [Priority.RpcServer.handler=4,port=60020] regionserver.HRegionServer: Open usertable,user9,1404257714774.84635e5ca79006f09dceafa095992aab.
2014-07-01 16:35:15,257 INFO  [Priority.RpcServer.handler=4,port=60020] regionserver.HRegionServer: Open usertable,user4,1404257714774.c831a0ac8e8f14e82fdaef2ba3a5e1f6.
2014-07-01 16:35:15,266 DEBUG [RS_OPEN_REGION-sceplus-vm48:60020-0] zookeeper.ZKAssign: regionserver:60020-0x46f4224a300001-0x46f4224a300001-0x46f4224a300001, quorum=sceplus-vm49.almaden.ibm.com:2181,sceplus-vm48.almaden.ibm.com:2181, baseZNode=/hbase Transitioning a59b89c303508bc329bc1095d0beb9e5 from M_ZK_REGION_OFFLINE to RS_ZK_REGION_OPENING
2014-07-01 16:35:15,268 DEBUG [RS_OPEN_REGION-sceplus-vm48:60020-1] zookeeper.ZKAssign: regionserver:60020-0x46f4224a300001-0x46f4224a300001-0x46f4224a300001, quorum=sceplus-vm49.almaden.ibm.com:2181,sceplus-vm48.almaden.ibm.com:2181, baseZNode=/hbase Transitioning 2ed053f05f5eec791a57de24e16f0043 from M_ZK_REGION_OFFLINE to RS_ZK_REGION_OPENING
2014-07-01 16:35:15,275 DEBUG [RS_OPEN_REGION-sceplus-vm48:60020-2] zookeeper.ZKAssign: regionserver:60020-0x46f4224a300001-0x46f4224a300001-0x46f4224a300001, quorum=sceplus-vm49.almaden.ibm.com:2181,sceplus-vm48.almaden.ibm.com:2181, baseZNode=/hbase Transitioning b6ae40892bbb2a1c545234ed91d62d1a from M_ZK_REGION_OFFLINE to RS_ZK_REGION_OPENING
2014-07-01 16:35:15,276 DEBUG [RS_OPEN_REGION-sceplus-vm48:60020-0] zookeeper.ZKAssign: regionserver:60020-0x46f4224a300001-0x46f4224a300001-0x46f4224a300001, quorum=sceplus-vm49.almaden.ibm.com:2181,sceplus-vm48.almaden.ibm.com:2181, baseZNode=/hbase Transitioned node a59b89c303508bc329bc1095d0beb9e5 from M_ZK_REGION_OFFLINE to RS_ZK_REGION_OPENING
2014-07-01 16:35:15,277 DEBUG [RS_OPEN_REGION-sceplus-vm48:60020-1] zookeeper.ZKAssign: regionserver:60020-0x46f4224a300001-0x46f4224a300001-0x46f4224a300001, quorum=sceplus-vm49.almaden.ibm.com:2181,sceplus-vm48.almaden.ibm.com:2181, baseZNode=/hbase Transitioned node 2ed053f05f5eec791a57de24e16f0043 from M_ZK_REGION_OFFLINE to RS_ZK_REGION_OPENING
2014-07-01 16:35:15,277 DEBUG [RS_OPEN_REGION-sceplus-vm48:60020-0] regionserver.HRegion: Opening region: {ENCODED => a59b89c303508bc329bc1095d0beb9e5, NAME => 'usertable,user3,1404257714774.a59b89c303508bc329bc1095d0beb9e5.', STARTKEY => 'user3', ENDKEY => 'user4'}
2014-07-01 16:35:15,277 DEBUG [RS_OPEN_REGION-sceplus-vm48:60020-1] regionserver.HRegion: Opening region: {ENCODED => 2ed053f05f5eec791a57de24e16f0043, NAME => 'usertable,user2,1404257714774.2ed053f05f5eec791a57de24e16f0043.', STARTKEY => 'user2', ENDKEY => 'user3'}
2014-07-01 16:35:15,278 DEBUG [RS_OPEN_REGION-sceplus-vm48:60020-1] regionserver.MetricsRegionSourceImpl: Creating new MetricsRegionSourceImpl for table usertable 2ed053f05f5eec791a57de24e16f0043
2014-07-01 16:35:15,278 DEBUG [RS_OPEN_REGION-sceplus-vm48:60020-0] regionserver.MetricsRegionSourceImpl: Creating new MetricsRegionSourceImpl for table usertable a59b89c303508bc329bc1095d0beb9e5
2014-07-01 16:35:15,279 DEBUG [RS_OPEN_REGION-sceplus-vm48:60020-1] regionserver.HRegion: Instantiated usertable,user2,1404257714774.2ed053f05f5eec791a57de24e16f0043.
2014-07-01 16:35:15,279 DEBUG [RS_OPEN_REGION-sceplus-vm48:60020-0] regionserver.HRegion: Instantiated usertable,user3,1404257714774.a59b89c303508bc329bc1095d0beb9e5.
2014-07-01 16:35:15,283 DEBUG [RS_OPEN_REGION-sceplus-vm48:60020-2] zookeeper.ZKAssign: regionserver:60020-0x46f4224a300001-0x46f4224a300001-0x46f4224a300001, quorum=sceplus-vm49.almaden.ibm.com:2181,sceplus-vm48.almaden.ibm.com:2181, baseZNode=/hbase Transitioned node b6ae40892bbb2a1c545234ed91d62d1a from M_ZK_REGION_OFFLINE to RS_ZK_REGION_OPENING
2014-07-01 16:35:15,284 DEBUG [RS_OPEN_REGION-sceplus-vm48:60020-2] regionserver.HRegion: Opening region: {ENCODED => b6ae40892bbb2a1c545234ed91d62d1a, NAME => 'usertable,user8,1404257714774.b6ae40892bbb2a1c545234ed91d62d1a.', STARTKEY => 'user8', ENDKEY => 'user9'}
2014-07-01 16:35:15,284 DEBUG [RS_OPEN_REGION-sceplus-vm48:60020-2] regionserver.MetricsRegionSourceImpl: Creating new MetricsRegionSourceImpl for table usertable b6ae40892bbb2a1c545234ed91d62d1a
2014-07-01 16:35:15,284 DEBUG [RS_OPEN_REGION-sceplus-vm48:60020-2] regionserver.HRegion: Instantiated usertable,user8,1404257714774.b6ae40892bbb2a1c545234ed91d62d1a.
2014-07-01 16:35:15,287 INFO  [StoreOpener-2ed053f05f5eec791a57de24e16f0043-1] compactions.CompactionConfiguration: size [268435456, 128); files [3, 10); ratio 1.200000; off-peak ratio 5.000000; throttle point 5368709120; delete expired; major period 0, major jitter 0.500000
2014-07-01 16:35:15,289 INFO  [StoreOpener-a59b89c303508bc329bc1095d0beb9e5-1] compactions.CompactionConfiguration: size [268435456, 128); files [3, 10); ratio 1.200000; off-peak ratio 5.000000; throttle point 5368709120; delete expired; major period 0, major jitter 0.500000
2014-07-01 16:35:15,291 DEBUG [RS_OPEN_REGION-sceplus-vm48:60020-1] regionserver.HRegion: Found 0 recovered edits file(s) under hdfs://master:54310/hbase/data/default/usertable/2ed053f05f5eec791a57de24e16f0043
2014-07-01 16:35:15,293 DEBUG [RS_OPEN_REGION-sceplus-vm48:60020-0] regionserver.HRegion: Found 0 recovered edits file(s) under hdfs://master:54310/hbase/data/default/usertable/a59b89c303508bc329bc1095d0beb9e5
2014-07-01 16:35:15,293 INFO  [StoreOpener-b6ae40892bbb2a1c545234ed91d62d1a-1] compactions.CompactionConfiguration: size [268435456, 128); files [3, 10); ratio 1.200000; off-peak ratio 5.000000; throttle point 5368709120; delete expired; major period 0, major jitter 0.500000
2014-07-01 16:35:15,293 INFO  [RS_OPEN_REGION-sceplus-vm48:60020-1] regionserver.HRegion: Onlined 2ed053f05f5eec791a57de24e16f0043; next sequenceid=1
2014-07-01 16:35:15,294 DEBUG [RS_OPEN_REGION-sceplus-vm48:60020-1] zookeeper.ZKAssign: regionserver:60020-0x46f4224a300001-0x46f4224a300001-0x46f4224a300001, quorum=sceplus-vm49.almaden.ibm.com:2181,sceplus-vm48.almaden.ibm.com:2181, baseZNode=/hbase Attempting to retransition opening state of node 2ed053f05f5eec791a57de24e16f0043
2014-07-01 16:35:15,296 INFO  [RS_OPEN_REGION-sceplus-vm48:60020-0] regionserver.HRegion: Onlined a59b89c303508bc329bc1095d0beb9e5; next sequenceid=1
2014-07-01 16:35:15,296 DEBUG [RS_OPEN_REGION-sceplus-vm48:60020-0] zookeeper.ZKAssign: regionserver:60020-0x46f4224a300001-0x46f4224a300001-0x46f4224a300001, quorum=sceplus-vm49.almaden.ibm.com:2181,sceplus-vm48.almaden.ibm.com:2181, baseZNode=/hbase Attempting to retransition opening state of node a59b89c303508bc329bc1095d0beb9e5
2014-07-01 16:35:15,297 INFO  [PostOpenDeployTasks:2ed053f05f5eec791a57de24e16f0043] regionserver.HRegionServer: Post open deploy tasks for region=usertable,user2,1404257714774.2ed053f05f5eec791a57de24e16f0043.
2014-07-01 16:35:15,298 INFO  [PostOpenDeployTasks:a59b89c303508bc329bc1095d0beb9e5] regionserver.HRegionServer: Post open deploy tasks for region=usertable,user3,1404257714774.a59b89c303508bc329bc1095d0beb9e5.
2014-07-01 16:35:15,308 INFO  [PostOpenDeployTasks:2ed053f05f5eec791a57de24e16f0043] catalog.MetaEditor: Updated row usertable,user2,1404257714774.2ed053f05f5eec791a57de24e16f0043. with server=sceplus-vm48.almaden.ibm.com,60020,1404255227840
2014-07-01 16:35:15,308 INFO  [PostOpenDeployTasks:2ed053f05f5eec791a57de24e16f0043] regionserver.HRegionServer: Finished post open deploy task for usertable,user2,1404257714774.2ed053f05f5eec791a57de24e16f0043.
2014-07-01 16:35:15,310 DEBUG [RS_OPEN_REGION-sceplus-vm48:60020-1] zookeeper.ZKAssign: regionserver:60020-0x46f4224a300001-0x46f4224a300001-0x46f4224a300001, quorum=sceplus-vm49.almaden.ibm.com:2181,sceplus-vm48.almaden.ibm.com:2181, baseZNode=/hbase Transitioning 2ed053f05f5eec791a57de24e16f0043 from RS_ZK_REGION_OPENING to RS_ZK_REGION_OPENED
2014-07-01 16:35:15,314 DEBUG [RS_OPEN_REGION-sceplus-vm48:60020-1] zookeeper.ZKAssign: regionserver:60020-0x46f4224a300001-0x46f4224a300001-0x46f4224a300001, quorum=sceplus-vm49.almaden.ibm.com:2181,sceplus-vm48.almaden.ibm.com:2181, baseZNode=/hbase Transitioned node 2ed053f05f5eec791a57de24e16f0043 from RS_ZK_REGION_OPENING to RS_ZK_REGION_OPENED
2014-07-01 16:35:15,314 DEBUG [RS_OPEN_REGION-sceplus-vm48:60020-1] handler.OpenRegionHandler: Transitioned 2ed053f05f5eec791a57de24e16f0043 to OPENED in zk on sceplus-vm48.almaden.ibm.com,60020,1404255227840
2014-07-01 16:35:15,314 DEBUG [RS_OPEN_REGION-sceplus-vm48:60020-1] handler.OpenRegionHandler: Opened usertable,user2,1404257714774.2ed053f05f5eec791a57de24e16f0043. on sceplus-vm48.almaden.ibm.com,60020,1404255227840
2014-07-01 16:35:15,314 DEBUG [RS_OPEN_REGION-sceplus-vm48:60020-1] zookeeper.ZKAssign: regionserver:60020-0x46f4224a300001-0x46f4224a300001-0x46f4224a300001, quorum=sceplus-vm49.almaden.ibm.com:2181,sceplus-vm48.almaden.ibm.com:2181, baseZNode=/hbase Transitioning 84635e5ca79006f09dceafa095992aab from M_ZK_REGION_OFFLINE to RS_ZK_REGION_OPENING
2014-07-01 16:35:15,319 INFO  [PostOpenDeployTasks:a59b89c303508bc329bc1095d0beb9e5] catalog.MetaEditor: Updated row usertable,user3,1404257714774.a59b89c303508bc329bc1095d0beb9e5. with server=sceplus-vm48.almaden.ibm.com,60020,1404255227840
2014-07-01 16:35:15,319 INFO  [PostOpenDeployTasks:a59b89c303508bc329bc1095d0beb9e5] regionserver.HRegionServer: Finished post open deploy task for usertable,user3,1404257714774.a59b89c303508bc329bc1095d0beb9e5.
2014-07-01 16:35:15,319 DEBUG [RS_OPEN_REGION-sceplus-vm48:60020-0] zookeeper.ZKAssign: regionserver:60020-0x46f4224a300001-0x46f4224a300001-0x46f4224a300001, quorum=sceplus-vm49.almaden.ibm.com:2181,sceplus-vm48.almaden.ibm.com:2181, baseZNode=/hbase Transitioning a59b89c303508bc329bc1095d0beb9e5 from RS_ZK_REGION_OPENING to RS_ZK_REGION_OPENED
2014-07-01 16:35:15,325 DEBUG [RS_OPEN_REGION-sceplus-vm48:60020-1] zookeeper.ZKAssign: regionserver:60020-0x46f4224a300001-0x46f4224a300001-0x46f4224a300001, quorum=sceplus-vm49.almaden.ibm.com:2181,sceplus-vm48.almaden.ibm.com:2181, baseZNode=/hbase Transitioned node 84635e5ca79006f09dceafa095992aab from M_ZK_REGION_OFFLINE to RS_ZK_REGION_OPENING
2014-07-01 16:35:15,326 DEBUG [RS_OPEN_REGION-sceplus-vm48:60020-1] regionserver.HRegion: Opening region: {ENCODED => 84635e5ca79006f09dceafa095992aab, NAME => 'usertable,user9,1404257714774.84635e5ca79006f09dceafa095992aab.', STARTKEY => 'user9', ENDKEY => ''}
2014-07-01 16:35:15,326 DEBUG [RS_OPEN_REGION-sceplus-vm48:60020-1] regionserver.MetricsRegionSourceImpl: Creating new MetricsRegionSourceImpl for table usertable 84635e5ca79006f09dceafa095992aab
2014-07-01 16:35:15,326 DEBUG [RS_OPEN_REGION-sceplus-vm48:60020-1] regionserver.HRegion: Instantiated usertable,user9,1404257714774.84635e5ca79006f09dceafa095992aab.
2014-07-01 16:35:15,327 DEBUG [RS_OPEN_REGION-sceplus-vm48:60020-2] regionserver.HRegion: Found 0 recovered edits file(s) under hdfs://master:54310/hbase/data/default/usertable/b6ae40892bbb2a1c545234ed91d62d1a
2014-07-01 16:35:15,329 INFO  [RS_OPEN_REGION-sceplus-vm48:60020-2] regionserver.HRegion: Onlined b6ae40892bbb2a1c545234ed91d62d1a; next sequenceid=1
2014-07-01 16:35:15,329 DEBUG [RS_OPEN_REGION-sceplus-vm48:60020-0] zookeeper.ZKAssign: regionserver:60020-0x46f4224a300001-0x46f4224a300001-0x46f4224a300001, quorum=sceplus-vm49.almaden.ibm.com:2181,sceplus-vm48.almaden.ibm.com:2181, baseZNode=/hbase Transitioned node a59b89c303508bc329bc1095d0beb9e5 from RS_ZK_REGION_OPENING to RS_ZK_REGION_OPENED
2014-07-01 16:35:15,329 DEBUG [RS_OPEN_REGION-sceplus-vm48:60020-2] zookeeper.ZKAssign: regionserver:60020-0x46f4224a300001-0x46f4224a300001-0x46f4224a300001, quorum=sceplus-vm49.almaden.ibm.com:2181,sceplus-vm48.almaden.ibm.com:2181, baseZNode=/hbase Attempting to retransition opening state of node b6ae40892bbb2a1c545234ed91d62d1a
2014-07-01 16:35:15,329 DEBUG [RS_OPEN_REGION-sceplus-vm48:60020-0] handler.OpenRegionHandler: Transitioned a59b89c303508bc329bc1095d0beb9e5 to OPENED in zk on sceplus-vm48.almaden.ibm.com,60020,1404255227840
2014-07-01 16:35:15,329 DEBUG [RS_OPEN_REGION-sceplus-vm48:60020-0] handler.OpenRegionHandler: Opened usertable,user3,1404257714774.a59b89c303508bc329bc1095d0beb9e5. on sceplus-vm48.almaden.ibm.com,60020,1404255227840
2014-07-01 16:35:15,330 DEBUG [RS_OPEN_REGION-sceplus-vm48:60020-0] zookeeper.ZKAssign: regionserver:60020-0x46f4224a300001-0x46f4224a300001-0x46f4224a300001, quorum=sceplus-vm49.almaden.ibm.com:2181,sceplus-vm48.almaden.ibm.com:2181, baseZNode=/hbase Transitioning c831a0ac8e8f14e82fdaef2ba3a5e1f6 from M_ZK_REGION_OFFLINE to RS_ZK_REGION_OPENING
2014-07-01 16:35:15,331 INFO  [PostOpenDeployTasks:b6ae40892bbb2a1c545234ed91d62d1a] regionserver.HRegionServer: Post open deploy tasks for region=usertable,user8,1404257714774.b6ae40892bbb2a1c545234ed91d62d1a.
2014-07-01 16:35:15,343 DEBUG [RS_OPEN_REGION-sceplus-vm48:60020-0] zookeeper.ZKAssign: regionserver:60020-0x46f4224a300001-0x46f4224a300001-0x46f4224a300001, quorum=sceplus-vm49.almaden.ibm.com:2181,sceplus-vm48.almaden.ibm.com:2181, baseZNode=/hbase Transitioned node c831a0ac8e8f14e82fdaef2ba3a5e1f6 from M_ZK_REGION_OFFLINE to RS_ZK_REGION_OPENING
2014-07-01 16:35:15,344 DEBUG [RS_OPEN_REGION-sceplus-vm48:60020-0] regionserver.HRegion: Opening region: {ENCODED => c831a0ac8e8f14e82fdaef2ba3a5e1f6, NAME => 'usertable,user4,1404257714774.c831a0ac8e8f14e82fdaef2ba3a5e1f6.', STARTKEY => 'user4', ENDKEY => 'user5'}
2014-07-01 16:35:15,344 DEBUG [RS_OPEN_REGION-sceplus-vm48:60020-0] regionserver.MetricsRegionSourceImpl: Creating new MetricsRegionSourceImpl for table usertable c831a0ac8e8f14e82fdaef2ba3a5e1f6
2014-07-01 16:35:15,344 DEBUG [RS_OPEN_REGION-sceplus-vm48:60020-0] regionserver.HRegion: Instantiated usertable,user4,1404257714774.c831a0ac8e8f14e82fdaef2ba3a5e1f6.
2014-07-01 16:35:15,348 INFO  [PostOpenDeployTasks:b6ae40892bbb2a1c545234ed91d62d1a] catalog.MetaEditor: Updated row usertable,user8,1404257714774.b6ae40892bbb2a1c545234ed91d62d1a. with server=sceplus-vm48.almaden.ibm.com,60020,1404255227840
2014-07-01 16:35:15,348 INFO  [StoreOpener-84635e5ca79006f09dceafa095992aab-1] compactions.CompactionConfiguration: size [268435456, 128); files [3, 10); ratio 1.200000; off-peak ratio 5.000000; throttle point 5368709120; delete expired; major period 0, major jitter 0.500000
2014-07-01 16:35:15,348 INFO  [PostOpenDeployTasks:b6ae40892bbb2a1c545234ed91d62d1a] regionserver.HRegionServer: Finished post open deploy task for usertable,user8,1404257714774.b6ae40892bbb2a1c545234ed91d62d1a.
2014-07-01 16:35:15,350 DEBUG [RS_OPEN_REGION-sceplus-vm48:60020-2] zookeeper.ZKAssign: regionserver:60020-0x46f4224a300001-0x46f4224a300001-0x46f4224a300001, quorum=sceplus-vm49.almaden.ibm.com:2181,sceplus-vm48.almaden.ibm.com:2181, baseZNode=/hbase Transitioning b6ae40892bbb2a1c545234ed91d62d1a from RS_ZK_REGION_OPENING to RS_ZK_REGION_OPENED
2014-07-01 16:35:15,353 INFO  [StoreOpener-c831a0ac8e8f14e82fdaef2ba3a5e1f6-1] compactions.CompactionConfiguration: size [268435456, 128); files [3, 10); ratio 1.200000; off-peak ratio 5.000000; throttle point 5368709120; delete expired; major period 0, major jitter 0.500000
2014-07-01 16:35:15,354 DEBUG [RS_OPEN_REGION-sceplus-vm48:60020-1] regionserver.HRegion: Found 0 recovered edits file(s) under hdfs://master:54310/hbase/data/default/usertable/84635e5ca79006f09dceafa095992aab
2014-07-01 16:35:15,356 DEBUG [RS_OPEN_REGION-sceplus-vm48:60020-0] regionserver.HRegion: Found 0 recovered edits file(s) under hdfs://master:54310/hbase/data/default/usertable/c831a0ac8e8f14e82fdaef2ba3a5e1f6
2014-07-01 16:35:15,357 INFO  [RS_OPEN_REGION-sceplus-vm48:60020-1] regionserver.HRegion: Onlined 84635e5ca79006f09dceafa095992aab; next sequenceid=1
2014-07-01 16:35:15,357 DEBUG [RS_OPEN_REGION-sceplus-vm48:60020-1] zookeeper.ZKAssign: regionserver:60020-0x46f4224a300001-0x46f4224a300001-0x46f4224a300001, quorum=sceplus-vm49.almaden.ibm.com:2181,sceplus-vm48.almaden.ibm.com:2181, baseZNode=/hbase Attempting to retransition opening state of node 84635e5ca79006f09dceafa095992aab
2014-07-01 16:35:15,359 DEBUG [RS_OPEN_REGION-sceplus-vm48:60020-2] zookeeper.ZKAssign: regionserver:60020-0x46f4224a300001-0x46f4224a300001-0x46f4224a300001, quorum=sceplus-vm49.almaden.ibm.com:2181,sceplus-vm48.almaden.ibm.com:2181, baseZNode=/hbase Transitioned node b6ae40892bbb2a1c545234ed91d62d1a from RS_ZK_REGION_OPENING to RS_ZK_REGION_OPENED
2014-07-01 16:35:15,359 DEBUG [RS_OPEN_REGION-sceplus-vm48:60020-2] handler.OpenRegionHandler: Transitioned b6ae40892bbb2a1c545234ed91d62d1a to OPENED in zk on sceplus-vm48.almaden.ibm.com,60020,1404255227840
2014-07-01 16:35:15,359 DEBUG [RS_OPEN_REGION-sceplus-vm48:60020-2] handler.OpenRegionHandler: Opened usertable,user8,1404257714774.b6ae40892bbb2a1c545234ed91d62d1a. on sceplus-vm48.almaden.ibm.com,60020,1404255227840
2014-07-01 16:35:15,360 INFO  [PostOpenDeployTasks:84635e5ca79006f09dceafa095992aab] regionserver.HRegionServer: Post open deploy tasks for region=usertable,user9,1404257714774.84635e5ca79006f09dceafa095992aab.
2014-07-01 16:35:15,366 INFO  [PostOpenDeployTasks:84635e5ca79006f09dceafa095992aab] catalog.MetaEditor: Updated row usertable,user9,1404257714774.84635e5ca79006f09dceafa095992aab. with server=sceplus-vm48.almaden.ibm.com,60020,1404255227840
2014-07-01 16:35:15,366 INFO  [PostOpenDeployTasks:84635e5ca79006f09dceafa095992aab] regionserver.HRegionServer: Finished post open deploy task for usertable,user9,1404257714774.84635e5ca79006f09dceafa095992aab.
2014-07-01 16:35:15,366 DEBUG [RS_OPEN_REGION-sceplus-vm48:60020-1] zookeeper.ZKAssign: regionserver:60020-0x46f4224a300001-0x46f4224a300001-0x46f4224a300001, quorum=sceplus-vm49.almaden.ibm.com:2181,sceplus-vm48.almaden.ibm.com:2181, baseZNode=/hbase Transitioning 84635e5ca79006f09dceafa095992aab from RS_ZK_REGION_OPENING to RS_ZK_REGION_OPENED
2014-07-01 16:35:15,371 DEBUG [RS_OPEN_REGION-sceplus-vm48:60020-1] zookeeper.ZKAssign: regionserver:60020-0x46f4224a300001-0x46f4224a300001-0x46f4224a300001, quorum=sceplus-vm49.almaden.ibm.com:2181,sceplus-vm48.almaden.ibm.com:2181, baseZNode=/hbase Transitioned node 84635e5ca79006f09dceafa095992aab from RS_ZK_REGION_OPENING to RS_ZK_REGION_OPENED
2014-07-01 16:35:15,371 DEBUG [RS_OPEN_REGION-sceplus-vm48:60020-1] handler.OpenRegionHandler: Transitioned 84635e5ca79006f09dceafa095992aab to OPENED in zk on sceplus-vm48.almaden.ibm.com,60020,1404255227840
2014-07-01 16:35:15,371 DEBUG [RS_OPEN_REGION-sceplus-vm48:60020-1] handler.OpenRegionHandler: Opened usertable,user9,1404257714774.84635e5ca79006f09dceafa095992aab. on sceplus-vm48.almaden.ibm.com,60020,1404255227840
2014-07-01 16:35:15,395 INFO  [RS_OPEN_REGION-sceplus-vm48:60020-0] regionserver.HRegion: Onlined c831a0ac8e8f14e82fdaef2ba3a5e1f6; next sequenceid=1
2014-07-01 16:35:15,395 DEBUG [RS_OPEN_REGION-sceplus-vm48:60020-0] zookeeper.ZKAssign: regionserver:60020-0x46f4224a300001-0x46f4224a300001-0x46f4224a300001, quorum=sceplus-vm49.almaden.ibm.com:2181,sceplus-vm48.almaden.ibm.com:2181, baseZNode=/hbase Attempting to retransition opening state of node c831a0ac8e8f14e82fdaef2ba3a5e1f6
2014-07-01 16:35:15,397 INFO  [PostOpenDeployTasks:c831a0ac8e8f14e82fdaef2ba3a5e1f6] regionserver.HRegionServer: Post open deploy tasks for region=usertable,user4,1404257714774.c831a0ac8e8f14e82fdaef2ba3a5e1f6.
2014-07-01 16:35:15,403 INFO  [PostOpenDeployTasks:c831a0ac8e8f14e82fdaef2ba3a5e1f6] catalog.MetaEditor: Updated row usertable,user4,1404257714774.c831a0ac8e8f14e82fdaef2ba3a5e1f6. with server=sceplus-vm48.almaden.ibm.com,60020,1404255227840
2014-07-01 16:35:15,403 INFO  [PostOpenDeployTasks:c831a0ac8e8f14e82fdaef2ba3a5e1f6] regionserver.HRegionServer: Finished post open deploy task for usertable,user4,1404257714774.c831a0ac8e8f14e82fdaef2ba3a5e1f6.
2014-07-01 16:35:15,404 DEBUG [RS_OPEN_REGION-sceplus-vm48:60020-0] zookeeper.ZKAssign: regionserver:60020-0x46f4224a300001-0x46f4224a300001-0x46f4224a300001, quorum=sceplus-vm49.almaden.ibm.com:2181,sceplus-vm48.almaden.ibm.com:2181, baseZNode=/hbase Transitioning c831a0ac8e8f14e82fdaef2ba3a5e1f6 from RS_ZK_REGION_OPENING to RS_ZK_REGION_OPENED
2014-07-01 16:35:15,410 DEBUG [RS_OPEN_REGION-sceplus-vm48:60020-0] zookeeper.ZKAssign: regionserver:60020-0x46f4224a300001-0x46f4224a300001-0x46f4224a300001, quorum=sceplus-vm49.almaden.ibm.com:2181,sceplus-vm48.almaden.ibm.com:2181, baseZNode=/hbase Transitioned node c831a0ac8e8f14e82fdaef2ba3a5e1f6 from RS_ZK_REGION_OPENING to RS_ZK_REGION_OPENED
2014-07-01 16:35:15,410 DEBUG [RS_OPEN_REGION-sceplus-vm48:60020-0] handler.OpenRegionHandler: Transitioned c831a0ac8e8f14e82fdaef2ba3a5e1f6 to OPENED in zk on sceplus-vm48.almaden.ibm.com,60020,1404255227840
2014-07-01 16:35:15,410 DEBUG [RS_OPEN_REGION-sceplus-vm48:60020-0] handler.OpenRegionHandler: Opened usertable,user4,1404257714774.c831a0ac8e8f14e82fdaef2ba3a5e1f6. on sceplus-vm48.almaden.ibm.com,60020,1404255227840
2014-07-01 16:35:33,474 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-01 16:35:33,519 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 20262 synced till here 20252
2014-07-01 16:35:33,712 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1404255227840/sceplus-vm48.almaden.ibm.com%2C60020%2C1404255227840.1404257365840 with entries=97, filesize=74.8m; new WAL /hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1404255227840/sceplus-vm48.almaden.ibm.com%2C60020%2C1404255227840.1404257733475
2014-07-01 16:35:33,712 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1404255227840/sceplus-vm48.almaden.ibm.com%2C60020%2C1404255227840.1404256190720
2014-07-01 16:35:33,712 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1404255227840/sceplus-vm48.almaden.ibm.com%2C60020%2C1404255227840.1404256194355
2014-07-01 16:35:33,712 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1404255227840/sceplus-vm48.almaden.ibm.com%2C60020%2C1404255227840.1404257239463
2014-07-01 16:35:33,713 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1404255227840/sceplus-vm48.almaden.ibm.com%2C60020%2C1404255227840.1404257263530
2014-07-01 16:35:33,713 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1404255227840/sceplus-vm48.almaden.ibm.com%2C60020%2C1404255227840.1404257280161
2014-07-01 16:35:33,713 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1404255227840/sceplus-vm48.almaden.ibm.com%2C60020%2C1404255227840.1404257283781
2014-07-01 16:35:33,713 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1404255227840/sceplus-vm48.almaden.ibm.com%2C60020%2C1404255227840.1404257296169
2014-07-01 16:35:33,713 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1404255227840/sceplus-vm48.almaden.ibm.com%2C60020%2C1404255227840.1404257314997
2014-07-01 16:35:33,713 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1404255227840/sceplus-vm48.almaden.ibm.com%2C60020%2C1404255227840.1404257317876
2014-07-01 16:35:33,713 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1404255227840/sceplus-vm48.almaden.ibm.com%2C60020%2C1404255227840.1404257321310
2014-07-01 16:35:33,713 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1404255227840/sceplus-vm48.almaden.ibm.com%2C60020%2C1404255227840.1404257348957
2014-07-01 16:35:33,713 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1404255227840/sceplus-vm48.almaden.ibm.com%2C60020%2C1404255227840.1404257356199
2014-07-01 16:35:38,102 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-01 16:35:38,129 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 20357 synced till here 20344
2014-07-01 16:35:38,438 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1404255227840/sceplus-vm48.almaden.ibm.com%2C60020%2C1404255227840.1404257733475 with entries=95, filesize=70.7m; new WAL /hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1404255227840/sceplus-vm48.almaden.ibm.com%2C60020%2C1404255227840.1404257738103
2014-07-01 16:35:41,603 INFO  [JvmPauseMonitor] util.JvmPauseMonitor: Detected pause in JVM or host machine (eg GC): pause of approximately 2286ms
GC pool 'ParNew' had collection(s): count=1 time=2599ms
2014-07-01 16:35:41,811 WARN  [RpcServer.handler=19,port=60020] ipc.RpcServer: RpcServer.respondercallId: 63 service: ClientService methodName: Multi size: 3.0m connection: 9.1.143.58:41338: output error
2014-07-01 16:35:41,835 WARN  [RpcServer.handler=23,port=60020] ipc.RpcServer: RpcServer.respondercallId: 49 service: ClientService methodName: Multi size: 3.0m connection: 9.1.143.58:41338: output error
2014-07-01 16:35:41,835 WARN  [RpcServer.handler=19,port=60020] ipc.RpcServer: RpcServer.handler=19,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-07-01 16:35:41,835 WARN  [RpcServer.handler=23,port=60020] ipc.RpcServer: RpcServer.handler=23,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-07-01 16:35:41,835 WARN  [RpcServer.handler=25,port=60020] ipc.RpcServer: RpcServer.respondercallId: 62 service: ClientService methodName: Multi size: 3.0m connection: 9.1.143.58:41338: output error
2014-07-01 16:35:41,835 WARN  [RpcServer.handler=25,port=60020] ipc.RpcServer: RpcServer.handler=25,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-07-01 16:35:41,835 WARN  [RpcServer.handler=39,port=60020] ipc.RpcServer: RpcServer.respondercallId: 60 service: ClientService methodName: Multi size: 2.9m connection: 9.1.143.58:41338: output error
2014-07-01 16:35:41,836 WARN  [RpcServer.handler=39,port=60020] ipc.RpcServer: RpcServer.handler=39,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-07-01 16:35:41,836 WARN  [RpcServer.handler=24,port=60020] ipc.RpcServer: RpcServer.respondercallId: 65 service: ClientService methodName: Multi size: 3.0m connection: 9.1.143.58:41338: output error
2014-07-01 16:35:41,836 WARN  [RpcServer.handler=24,port=60020] ipc.RpcServer: RpcServer.handler=24,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-07-01 16:35:41,836 WARN  [RpcServer.handler=10,port=60020] ipc.RpcServer: RpcServer.respondercallId: 59 service: ClientService methodName: Multi size: 3.0m connection: 9.1.143.58:41338: output error
2014-07-01 16:35:41,836 WARN  [RpcServer.handler=10,port=60020] ipc.RpcServer: RpcServer.handler=10,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-07-01 16:35:41,847 WARN  [RpcServer.handler=41,port=60020] ipc.RpcServer: RpcServer.respondercallId: 57 service: ClientService methodName: Multi size: 3.0m connection: 9.1.143.58:41338: output error
2014-07-01 16:35:41,847 WARN  [RpcServer.handler=41,port=60020] ipc.RpcServer: RpcServer.handler=41,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-07-01 16:35:41,847 WARN  [RpcServer.handler=40,port=60020] ipc.RpcServer: RpcServer.respondercallId: 58 service: ClientService methodName: Multi size: 3.0m connection: 9.1.143.58:41338: output error
2014-07-01 16:35:41,847 WARN  [RpcServer.handler=40,port=60020] ipc.RpcServer: RpcServer.handler=40,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-07-01 16:35:41,847 WARN  [RpcServer.handler=30,port=60020] ipc.RpcServer: RpcServer.respondercallId: 64 service: ClientService methodName: Multi size: 2.9m connection: 9.1.143.58:41338: output error
2014-07-01 16:35:41,847 WARN  [RpcServer.handler=30,port=60020] ipc.RpcServer: RpcServer.handler=30,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-07-01 16:35:42,525 WARN  [RpcServer.handler=5,port=60020] ipc.RpcServer: RpcServer.respondercallId: 71 service: ClientService methodName: Multi size: 3.0m connection: 9.1.143.58:41338: output error
2014-07-01 16:35:42,526 WARN  [RpcServer.handler=5,port=60020] ipc.RpcServer: RpcServer.handler=5,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-07-01 16:35:42,999 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-01 16:35:43,049 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 20474 synced till here 20440
2014-07-01 16:35:49,081 INFO  [JvmPauseMonitor] util.JvmPauseMonitor: Detected pause in JVM or host machine (eg GC): pause of approximately 4476ms
GC pool 'ParNew' had collection(s): count=1 time=4586ms
2014-07-01 16:35:49,340 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1404255227840/sceplus-vm48.almaden.ibm.com%2C60020%2C1404255227840.1404257738103 with entries=117, filesize=89.8m; new WAL /hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1404255227840/sceplus-vm48.almaden.ibm.com%2C60020%2C1404255227840.1404257743000
2014-07-01 16:35:50,528 WARN  [RpcServer.handler=0,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":11779,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.58:41338","starttimems":1404257738748,"queuetimems":0,"class":"HRegionServer","responsesize":17228,"method":"Multi"}
2014-07-01 16:35:50,528 WARN  [RpcServer.handler=3,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":11822,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.58:41338","starttimems":1404257738705,"queuetimems":0,"class":"HRegionServer","responsesize":17277,"method":"Multi"}
2014-07-01 16:35:50,529 WARN  [RpcServer.handler=0,port=60020] ipc.RpcServer: RpcServer.respondercallId: 91 service: ClientService methodName: Multi size: 3.0m connection: 9.1.143.58:41338: output error
2014-07-01 16:35:50,529 WARN  [RpcServer.handler=0,port=60020] ipc.RpcServer: RpcServer.handler=0,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-07-01 16:35:50,529 WARN  [RpcServer.handler=3,port=60020] ipc.RpcServer: RpcServer.respondercallId: 75 service: ClientService methodName: Multi size: 3.0m connection: 9.1.143.58:41338: output error
2014-07-01 16:35:50,529 WARN  [RpcServer.handler=3,port=60020] ipc.RpcServer: RpcServer.handler=3,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-07-01 16:35:50,535 WARN  [RpcServer.handler=8,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":11656,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.58:41338","starttimems":1404257738879,"queuetimems":0,"class":"HRegionServer","responsesize":16755,"method":"Multi"}
2014-07-01 16:35:50,535 WARN  [RpcServer.handler=45,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":11710,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.58:41338","starttimems":1404257738825,"queuetimems":0,"class":"HRegionServer","responsesize":16996,"method":"Multi"}
2014-07-01 16:35:50,535 WARN  [RpcServer.handler=8,port=60020] ipc.RpcServer: RpcServer.respondercallId: 86 service: ClientService methodName: Multi size: 2.9m connection: 9.1.143.58:41338: output error
2014-07-01 16:35:50,535 WARN  [RpcServer.handler=8,port=60020] ipc.RpcServer: RpcServer.handler=8,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-07-01 16:35:50,535 WARN  [RpcServer.handler=45,port=60020] ipc.RpcServer: RpcServer.respondercallId: 88 service: ClientService methodName: Multi size: 3.0m connection: 9.1.143.58:41338: output error
2014-07-01 16:35:50,536 WARN  [RpcServer.handler=45,port=60020] ipc.RpcServer: RpcServer.handler=45,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-07-01 16:35:50,541 WARN  [RpcServer.handler=9,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":11636,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.58:41338","starttimems":1404257738904,"queuetimems":0,"class":"HRegionServer","responsesize":16888,"method":"Multi"}
2014-07-01 16:35:50,541 WARN  [RpcServer.handler=38,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":11767,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.58:41338","starttimems":1404257738773,"queuetimems":0,"class":"HRegionServer","responsesize":16745,"method":"Multi"}
2014-07-01 16:35:50,541 WARN  [RpcServer.handler=9,port=60020] ipc.RpcServer: RpcServer.respondercallId: 85 service: ClientService methodName: Multi size: 3.0m connection: 9.1.143.58:41338: output error
2014-07-01 16:35:50,541 WARN  [RpcServer.handler=9,port=60020] ipc.RpcServer: RpcServer.handler=9,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-07-01 16:35:50,541 WARN  [RpcServer.handler=38,port=60020] ipc.RpcServer: RpcServer.respondercallId: 90 service: ClientService methodName: Multi size: 2.9m connection: 9.1.143.58:41338: output error
2014-07-01 16:35:50,541 WARN  [RpcServer.handler=38,port=60020] ipc.RpcServer: RpcServer.handler=38,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-07-01 16:35:50,547 WARN  [RpcServer.handler=26,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":11747,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.58:41338","starttimems":1404257738799,"queuetimems":0,"class":"HRegionServer","responsesize":16882,"method":"Multi"}
2014-07-01 16:35:50,547 WARN  [RpcServer.handler=26,port=60020] ipc.RpcServer: RpcServer.respondercallId: 89 service: ClientService methodName: Multi size: 3.0m connection: 9.1.143.58:41338: output error
2014-07-01 16:35:50,547 WARN  [RpcServer.handler=26,port=60020] ipc.RpcServer: RpcServer.handler=26,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-07-01 16:35:50,555 WARN  [RpcServer.handler=35,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":11702,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.58:41338","starttimems":1404257738852,"queuetimems":0,"class":"HRegionServer","responsesize":16418,"method":"Multi"}
2014-07-01 16:35:50,555 WARN  [RpcServer.handler=35,port=60020] ipc.RpcServer: RpcServer.respondercallId: 87 service: ClientService methodName: Multi size: 2.9m connection: 9.1.143.58:41338: output error
2014-07-01 16:35:50,555 WARN  [RpcServer.handler=35,port=60020] ipc.RpcServer: RpcServer.handler=35,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-07-01 16:35:50,555 WARN  [RpcServer.handler=32,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":11827,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.58:41338","starttimems":1404257738727,"queuetimems":0,"class":"HRegionServer","responsesize":17073,"method":"Multi"}
2014-07-01 16:35:50,556 WARN  [RpcServer.handler=32,port=60020] ipc.RpcServer: RpcServer.respondercallId: 92 service: ClientService methodName: Multi size: 3.0m connection: 9.1.143.58:41338: output error
2014-07-01 16:35:50,556 WARN  [RpcServer.handler=32,port=60020] ipc.RpcServer: RpcServer.handler=32,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-07-01 16:35:52,737 INFO  [JvmPauseMonitor] util.JvmPauseMonitor: Detected pause in JVM or host machine (eg GC): pause of approximately 1654ms
GC pool 'ParNew' had collection(s): count=1 time=1969ms
2014-07-01 16:35:52,813 WARN  [RpcServer.handler=4,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":11198,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.58:41338","starttimems":1404257741614,"queuetimems":0,"class":"HRegionServer","responsesize":17003,"method":"Multi"}
2014-07-01 16:35:52,813 WARN  [RpcServer.handler=48,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":13885,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.58:41338","starttimems":1404257738927,"queuetimems":1,"class":"HRegionServer","responsesize":16694,"method":"Multi"}
2014-07-01 16:35:52,814 WARN  [RpcServer.handler=4,port=60020] ipc.RpcServer: RpcServer.respondercallId: 81 service: ClientService methodName: Multi size: 3.0m connection: 9.1.143.58:41338: output error
2014-07-01 16:35:52,814 WARN  [RpcServer.handler=4,port=60020] ipc.RpcServer: RpcServer.handler=4,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-07-01 16:35:52,814 WARN  [RpcServer.handler=7,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":11166,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.58:41338","starttimems":1404257741647,"queuetimems":1,"class":"HRegionServer","responsesize":16968,"method":"Multi"}
2014-07-01 16:35:52,814 WARN  [RpcServer.handler=48,port=60020] ipc.RpcServer: RpcServer.respondercallId: 84 service: ClientService methodName: Multi size: 2.9m connection: 9.1.143.58:41338: output error
2014-07-01 16:35:52,814 WARN  [RpcServer.handler=48,port=60020] ipc.RpcServer: RpcServer.handler=48,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-07-01 16:35:52,814 WARN  [RpcServer.handler=31,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":11178,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.58:41338","starttimems":1404257741636,"queuetimems":1,"class":"HRegionServer","responsesize":17383,"method":"Multi"}
2014-07-01 16:35:52,814 WARN  [RpcServer.handler=7,port=60020] ipc.RpcServer: RpcServer.respondercallId: 78 service: ClientService methodName: Multi size: 3.0m connection: 9.1.143.58:41338: output error
2014-07-01 16:35:52,814 WARN  [RpcServer.handler=37,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":11189,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.58:41338","starttimems":1404257741625,"queuetimems":0,"class":"HRegionServer","responsesize":16571,"method":"Multi"}
2014-07-01 16:35:52,814 WARN  [RpcServer.handler=44,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":13859,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.58:41338","starttimems":1404257738954,"queuetimems":0,"class":"HRegionServer","responsesize":17176,"method":"Multi"}
2014-07-01 16:35:52,814 WARN  [RpcServer.handler=1,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":13833,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.58:41338","starttimems":1404257738981,"queuetimems":1,"class":"HRegionServer","responsesize":17148,"method":"Multi"}
2014-07-01 16:35:52,814 WARN  [RpcServer.handler=7,port=60020] ipc.RpcServer: RpcServer.handler=7,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-07-01 16:35:52,814 WARN  [RpcServer.handler=31,port=60020] ipc.RpcServer: RpcServer.respondercallId: 79 service: ClientService methodName: Multi size: 3.0m connection: 9.1.143.58:41338: output error
2014-07-01 16:35:52,815 WARN  [RpcServer.handler=31,port=60020] ipc.RpcServer: RpcServer.handler=31,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-07-01 16:35:52,815 WARN  [RpcServer.handler=1,port=60020] ipc.RpcServer: RpcServer.respondercallId: 82 service: ClientService methodName: Multi size: 3.0m connection: 9.1.143.58:41338: output error
2014-07-01 16:35:52,815 WARN  [RpcServer.handler=1,port=60020] ipc.RpcServer: RpcServer.handler=1,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-07-01 16:35:52,815 WARN  [RpcServer.handler=44,port=60020] ipc.RpcServer: RpcServer.respondercallId: 83 service: ClientService methodName: Multi size: 3.0m connection: 9.1.143.58:41338: output error
2014-07-01 16:35:52,815 WARN  [RpcServer.handler=44,port=60020] ipc.RpcServer: RpcServer.handler=44,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-07-01 16:35:52,815 WARN  [RpcServer.handler=37,port=60020] ipc.RpcServer: RpcServer.respondercallId: 80 service: ClientService methodName: Multi size: 2.9m connection: 9.1.143.58:41338: output error
2014-07-01 16:35:52,815 WARN  [RpcServer.handler=37,port=60020] ipc.RpcServer: RpcServer.handler=37,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-07-01 16:35:53,901 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-01 16:35:54,006 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 20585 synced till here 20574
2014-07-01 16:35:54,348 WARN  [RpcServer.handler=2,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":11894,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.58:41348","starttimems":1404257742453,"queuetimems":0,"class":"HRegionServer","responsesize":17277,"method":"Multi"}
2014-07-01 16:35:54,349 WARN  [RpcServer.handler=2,port=60020] ipc.RpcServer: RpcServer.respondercallId: 109 service: ClientService methodName: Multi size: 3.0m connection: 9.1.143.58:41348: output error
2014-07-01 16:35:54,353 WARN  [RpcServer.handler=2,port=60020] ipc.RpcServer: RpcServer.handler=2,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-07-01 16:35:54,412 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1404255227840/sceplus-vm48.almaden.ibm.com%2C60020%2C1404255227840.1404257743000 with entries=111, filesize=84.5m; new WAL /hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1404255227840/sceplus-vm48.almaden.ibm.com%2C60020%2C1404255227840.1404257753902
2014-07-01 16:35:55,425 WARN  [RpcServer.handler=6,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":12572,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.58:41348","starttimems":1404257742852,"queuetimems":1,"class":"HRegionServer","responsesize":16745,"method":"Multi"}
2014-07-01 16:35:55,426 WARN  [RpcServer.handler=12,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":12931,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.58:41348","starttimems":1404257742495,"queuetimems":1,"class":"HRegionServer","responsesize":17073,"method":"Multi"}
2014-07-01 16:35:55,428 WARN  [RpcServer.handler=33,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":12481,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.58:41348","starttimems":1404257742946,"queuetimems":0,"class":"HRegionServer","responsesize":16996,"method":"Multi"}
2014-07-01 16:35:57,594 WARN  [RpcServer.handler=6,port=60020] ipc.RpcServer: RpcServer.respondercallId: 120 service: ClientService methodName: Multi size: 2.9m connection: 9.1.143.58:41348: output error
2014-07-01 16:35:57,595 WARN  [RpcServer.handler=6,port=60020] ipc.RpcServer: RpcServer.handler=6,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-07-01 16:35:57,602 WARN  [RpcServer.handler=33,port=60020] ipc.RpcServer: RpcServer.respondercallId: 112 service: ClientService methodName: Multi size: 3.0m connection: 9.1.143.58:41348: output error
2014-07-01 16:35:57,603 WARN  [RpcServer.handler=33,port=60020] ipc.RpcServer: RpcServer.handler=33,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-07-01 16:35:57,603 WARN  [RpcServer.handler=12,port=60020] ipc.RpcServer: RpcServer.respondercallId: 138 service: ClientService methodName: Multi size: 3.0m connection: 9.1.143.58:41348: output error
2014-07-01 16:35:57,603 WARN  [RpcServer.handler=12,port=60020] ipc.RpcServer: RpcServer.handler=12,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-07-01 16:35:57,594 INFO  [JvmPauseMonitor] util.JvmPauseMonitor: Detected pause in JVM or host machine (eg GC): pause of approximately 1853ms
GC pool 'ParNew' had collection(s): count=1 time=2157ms
2014-07-01 16:35:57,645 WARN  [RpcServer.handler=43,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":14924,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.58:41348","starttimems":1404257742721,"queuetimems":0,"class":"HRegionServer","responsesize":17201,"method":"Multi"}
2014-07-01 16:35:57,645 WARN  [RpcServer.handler=47,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":14675,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.58:41348","starttimems":1404257742970,"queuetimems":1,"class":"HRegionServer","responsesize":17205,"method":"Multi"}
2014-07-01 16:35:57,645 WARN  [RpcServer.handler=34,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":15009,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.58:41348","starttimems":1404257742636,"queuetimems":0,"class":"HRegionServer","responsesize":17003,"method":"Multi"}
2014-07-01 16:35:57,646 WARN  [RpcServer.handler=43,port=60020] ipc.RpcServer: RpcServer.respondercallId: 127 service: ClientService methodName: Multi size: 3.0m connection: 9.1.143.58:41348: output error
2014-07-01 16:35:57,646 WARN  [RpcServer.handler=28,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":14992,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.58:41348","starttimems":1404257742653,"queuetimems":1,"class":"HRegionServer","responsesize":16694,"method":"Multi"}
2014-07-01 16:35:57,646 WARN  [RpcServer.handler=43,port=60020] ipc.RpcServer: RpcServer.handler=43,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-07-01 16:35:57,646 WARN  [RpcServer.handler=34,port=60020] ipc.RpcServer: RpcServer.respondercallId: 131 service: ClientService methodName: Multi size: 3.0m connection: 9.1.143.58:41348: output error
2014-07-01 16:35:57,646 WARN  [RpcServer.handler=34,port=60020] ipc.RpcServer: RpcServer.handler=34,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-07-01 16:35:57,646 WARN  [RpcServer.handler=47,port=60020] ipc.RpcServer: RpcServer.respondercallId: 156 service: ClientService methodName: Multi size: 3.0m connection: 9.1.143.58:41348: output error
2014-07-01 16:35:57,646 WARN  [RpcServer.handler=47,port=60020] ipc.RpcServer: RpcServer.handler=47,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-07-01 16:35:57,666 WARN  [RpcServer.handler=28,port=60020] ipc.RpcServer: RpcServer.respondercallId: 130 service: ClientService methodName: Multi size: 2.9m connection: 9.1.143.58:41348: output error
2014-07-01 16:35:57,667 WARN  [RpcServer.handler=28,port=60020] ipc.RpcServer: RpcServer.handler=28,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-07-01 16:35:57,818 WARN  [RpcServer.handler=16,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":15123,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.58:41348","starttimems":1404257742695,"queuetimems":0,"class":"HRegionServer","responsesize":16418,"method":"Multi"}
2014-07-01 16:35:57,818 WARN  [RpcServer.handler=23,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":14727,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.58:41348","starttimems":1404257743091,"queuetimems":1,"class":"HRegionServer","responsesize":16929,"method":"Multi"}
2014-07-01 16:35:57,819 WARN  [RpcServer.handler=27,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":15033,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.58:41348","starttimems":1404257742786,"queuetimems":0,"class":"HRegionServer","responsesize":16888,"method":"Multi"}
2014-07-01 16:35:57,819 WARN  [RpcServer.handler=36,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":14946,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.58:41348","starttimems":1404257742873,"queuetimems":0,"class":"HRegionServer","responsesize":17255,"method":"Multi"}
2014-07-01 16:35:57,819 WARN  [RpcServer.handler=46,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":15257,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.58:41348","starttimems":1404257742562,"queuetimems":1,"class":"HRegionServer","responsesize":16937,"method":"Multi"}
2014-07-01 16:35:57,818 WARN  [RpcServer.handler=14,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":15073,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.58:41348","starttimems":1404257742745,"queuetimems":1,"class":"HRegionServer","responsesize":17176,"method":"Multi"}
2014-07-01 16:35:57,819 WARN  [RpcServer.handler=22,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":15057,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.58:41348","starttimems":1404257742762,"queuetimems":0,"class":"HRegionServer","responsesize":16882,"method":"Multi"}
2014-07-01 16:35:57,818 WARN  [RpcServer.handler=13,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":14985,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.58:41348","starttimems":1404257742833,"queuetimems":0,"class":"HRegionServer","responsesize":17383,"method":"Multi"}
2014-07-01 16:35:57,818 WARN  [RpcServer.handler=17,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":15196,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.58:41348","starttimems":1404257742622,"queuetimems":0,"class":"HRegionServer","responsesize":16914,"method":"Multi"}
2014-07-01 16:35:57,818 WARN  [RpcServer.handler=21,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":14919,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.58:41348","starttimems":1404257742899,"queuetimems":0,"class":"HRegionServer","responsesize":17003,"method":"Multi"}
2014-07-01 16:35:57,820 WARN  [RpcServer.handler=15,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":14998,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.58:41348","starttimems":1404257742821,"queuetimems":1,"class":"HRegionServer","responsesize":16692,"method":"Multi"}
2014-07-01 16:35:57,819 WARN  [RpcServer.handler=49,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":15242,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.58:41348","starttimems":1404257742577,"queuetimems":0,"class":"HRegionServer","responsesize":17100,"method":"Multi"}
2014-07-01 16:35:57,819 WARN  [RpcServer.handler=11,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":14901,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.58:41348","starttimems":1404257742918,"queuetimems":1,"class":"HRegionServer","responsesize":17228,"method":"Multi"}
2014-07-01 16:35:57,819 WARN  [RpcServer.handler=20,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":14884,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.58:41348","starttimems":1404257742935,"queuetimems":0,"class":"HRegionServer","responsesize":16571,"method":"Multi"}
2014-07-01 16:35:57,819 WARN  [RpcServer.handler=18,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":15141,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.58:41348","starttimems":1404257742678,"queuetimems":0,"class":"HRegionServer","responsesize":16755,"method":"Multi"}
2014-07-01 16:35:57,819 WARN  [RpcServer.handler=16,port=60020] ipc.RpcServer: RpcServer.respondercallId: 128 service: ClientService methodName: Multi size: 2.9m connection: 9.1.143.58:41348: output error
2014-07-01 16:35:57,820 WARN  [RpcServer.handler=29,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":14835,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.58:41348","starttimems":1404257742984,"queuetimems":0,"class":"HRegionServer","responsesize":16462,"method":"Multi"}
2014-07-01 16:35:57,820 WARN  [RpcServer.handler=18,port=60020] ipc.RpcServer: RpcServer.respondercallId: 129 service: ClientService methodName: Multi size: 2.9m connection: 9.1.143.58:41348: output error
2014-07-01 16:35:57,820 WARN  [RpcServer.handler=16,port=60020] ipc.RpcServer: RpcServer.handler=16,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-07-01 16:35:57,820 WARN  [RpcServer.handler=20,port=60020] ipc.RpcServer: RpcServer.respondercallId: 115 service: ClientService methodName: Multi size: 2.9m connection: 9.1.143.58:41348: output error
2014-07-01 16:35:57,820 WARN  [RpcServer.handler=18,port=60020] ipc.RpcServer: RpcServer.handler=18,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-07-01 16:35:57,821 WARN  [RpcServer.handler=20,port=60020] ipc.RpcServer: RpcServer.handler=20,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-07-01 16:35:57,821 WARN  [RpcServer.handler=11,port=60020] ipc.RpcServer: RpcServer.respondercallId: 116 service: ClientService methodName: Multi size: 3.0m connection: 9.1.143.58:41348: output error
2014-07-01 16:35:57,821 WARN  [RpcServer.handler=11,port=60020] ipc.RpcServer: RpcServer.handler=11,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-07-01 16:35:57,821 WARN  [RpcServer.handler=49,port=60020] ipc.RpcServer: RpcServer.respondercallId: 136 service: ClientService methodName: Multi size: 3.0m connection: 9.1.143.58:41348: output error
2014-07-01 16:35:57,821 WARN  [RpcServer.handler=49,port=60020] ipc.RpcServer: RpcServer.handler=49,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-07-01 16:35:57,821 WARN  [RpcServer.handler=15,port=60020] ipc.RpcServer: RpcServer.respondercallId: 122 service: ClientService methodName: Multi size: 2.9m connection: 9.1.143.58:41348: output error
2014-07-01 16:35:57,821 WARN  [RpcServer.handler=15,port=60020] ipc.RpcServer: RpcServer.handler=15,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-07-01 16:35:57,821 WARN  [RpcServer.handler=21,port=60020] ipc.RpcServer: RpcServer.respondercallId: 118 service: ClientService methodName: Multi size: 3.0m connection: 9.1.143.58:41348: output error
2014-07-01 16:35:57,821 WARN  [RpcServer.handler=21,port=60020] ipc.RpcServer: RpcServer.handler=21,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-07-01 16:35:57,821 WARN  [RpcServer.handler=17,port=60020] ipc.RpcServer: RpcServer.respondercallId: 132 service: ClientService methodName: Multi size: 3.0m connection: 9.1.143.58:41348: output error
2014-07-01 16:35:57,821 WARN  [RpcServer.handler=17,port=60020] ipc.RpcServer: RpcServer.handler=17,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-07-01 16:35:57,822 WARN  [RpcServer.handler=13,port=60020] ipc.RpcServer: RpcServer.respondercallId: 121 service: ClientService methodName: Multi size: 3.0m connection: 9.1.143.58:41348: output error
2014-07-01 16:35:57,822 WARN  [RpcServer.handler=13,port=60020] ipc.RpcServer: RpcServer.handler=13,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-07-01 16:35:57,822 WARN  [RpcServer.handler=22,port=60020] ipc.RpcServer: RpcServer.respondercallId: 124 service: ClientService methodName: Multi size: 3.0m connection: 9.1.143.58:41348: output error
2014-07-01 16:35:57,822 WARN  [RpcServer.handler=22,port=60020] ipc.RpcServer: RpcServer.handler=22,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-07-01 16:35:57,824 WARN  [RpcServer.handler=14,port=60020] ipc.RpcServer: RpcServer.respondercallId: 125 service: ClientService methodName: Multi size: 3.0m connection: 9.1.143.58:41348: output error
2014-07-01 16:35:57,825 WARN  [RpcServer.handler=14,port=60020] ipc.RpcServer: RpcServer.handler=14,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-07-01 16:35:57,830 WARN  [RpcServer.handler=46,port=60020] ipc.RpcServer: RpcServer.respondercallId: 137 service: ClientService methodName: Multi size: 3.0m connection: 9.1.143.58:41348: output error
2014-07-01 16:35:57,830 WARN  [RpcServer.handler=46,port=60020] ipc.RpcServer: RpcServer.handler=46,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-07-01 16:35:57,830 WARN  [RpcServer.handler=36,port=60020] ipc.RpcServer: RpcServer.respondercallId: 119 service: ClientService methodName: Multi size: 3.0m connection: 9.1.143.58:41348: output error
2014-07-01 16:35:57,830 WARN  [RpcServer.handler=36,port=60020] ipc.RpcServer: RpcServer.handler=36,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-07-01 16:35:57,830 WARN  [RpcServer.handler=27,port=60020] ipc.RpcServer: RpcServer.respondercallId: 123 service: ClientService methodName: Multi size: 3.0m connection: 9.1.143.58:41348: output error
2014-07-01 16:35:57,830 WARN  [RpcServer.handler=27,port=60020] ipc.RpcServer: RpcServer.handler=27,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-07-01 16:35:57,833 WARN  [RpcServer.handler=23,port=60020] ipc.RpcServer: RpcServer.respondercallId: 146 service: ClientService methodName: Multi size: 3.0m connection: 9.1.143.58:41348: output error
2014-07-01 16:35:57,833 WARN  [RpcServer.handler=23,port=60020] ipc.RpcServer: RpcServer.handler=23,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-07-01 16:35:57,833 WARN  [RpcServer.handler=29,port=60020] ipc.RpcServer: RpcServer.respondercallId: 155 service: ClientService methodName: Multi size: 2.9m connection: 9.1.143.58:41348: output error
2014-07-01 16:35:57,833 WARN  [RpcServer.handler=29,port=60020] ipc.RpcServer: RpcServer.handler=29,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-07-01 16:35:58,670 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-01 16:35:58,730 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 20720 synced till here 20693
2014-07-01 16:35:59,027 WARN  [RpcServer.handler=19,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":15957,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.58:41348","starttimems":1404257743070,"queuetimems":1,"class":"HRegionServer","responsesize":17343,"method":"Multi"}
2014-07-01 16:35:59,027 WARN  [RpcServer.handler=24,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":15877,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.58:41348","starttimems":1404257743150,"queuetimems":0,"class":"HRegionServer","responsesize":16968,"method":"Multi"}
2014-07-01 16:35:59,028 WARN  [RpcServer.handler=42,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":15991,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.58:41348","starttimems":1404257743036,"queuetimems":1,"class":"HRegionServer","responsesize":16846,"method":"Multi"}
2014-07-01 16:35:59,027 WARN  [RpcServer.handler=25,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":15913,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.58:41348","starttimems":1404257743114,"queuetimems":0,"class":"HRegionServer","responsesize":17148,"method":"Multi"}
2014-07-01 16:35:59,027 WARN  [RpcServer.handler=41,port=60020] ipc.RpcServer: RpcServer.respondercallId: 172 service: ClientService methodName: Multi size: 2.9m connection: 9.1.143.58:41348: output error
2014-07-01 16:35:59,030 WARN  [RpcServer.handler=41,port=60020] ipc.RpcServer: RpcServer.handler=41,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-07-01 16:35:59,030 WARN  [RpcServer.handler=25,port=60020] ipc.RpcServer: RpcServer.respondercallId: 144 service: ClientService methodName: Multi size: 3.0m connection: 9.1.143.58:41348: output error
2014-07-01 16:35:59,030 WARN  [RpcServer.handler=25,port=60020] ipc.RpcServer: RpcServer.handler=25,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-07-01 16:35:59,028 WARN  [RpcServer.handler=39,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":15897,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.58:41348","starttimems":1404257743131,"queuetimems":0,"class":"HRegionServer","responsesize":16879,"method":"Multi"}
2014-07-01 16:35:59,032 WARN  [RpcServer.handler=39,port=60020] ipc.RpcServer: RpcServer.respondercallId: 142 service: ClientService methodName: Multi size: 3.0m connection: 9.1.143.58:41348: output error
2014-07-01 16:35:59,033 WARN  [RpcServer.handler=39,port=60020] ipc.RpcServer: RpcServer.handler=39,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-07-01 16:35:59,035 WARN  [RpcServer.handler=42,port=60020] ipc.RpcServer: RpcServer.respondercallId: 153 service: ClientService methodName: Multi size: 2.9m connection: 9.1.143.58:41348: output error
2014-07-01 16:35:59,035 WARN  [RpcServer.handler=42,port=60020] ipc.RpcServer: RpcServer.handler=42,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-07-01 16:35:59,036 WARN  [RpcServer.handler=10,port=60020] ipc.RpcServer: RpcServer.respondercallId: 160 service: ClientService methodName: Multi size: 3.0m connection: 9.1.143.58:41348: output error
2014-07-01 16:35:59,037 WARN  [RpcServer.handler=10,port=60020] ipc.RpcServer: RpcServer.handler=10,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-07-01 16:35:59,039 WARN  [RpcServer.handler=40,port=60020] ipc.RpcServer: RpcServer.respondercallId: 171 service: ClientService methodName: Multi size: 2.9m connection: 9.1.143.58:41348: output error
2014-07-01 16:35:59,039 WARN  [RpcServer.handler=40,port=60020] ipc.RpcServer: RpcServer.handler=40,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-07-01 16:35:59,047 WARN  [RpcServer.handler=24,port=60020] ipc.RpcServer: RpcServer.respondercallId: 140 service: ClientService methodName: Multi size: 3.0m connection: 9.1.143.58:41348: output error
2014-07-01 16:35:59,047 WARN  [RpcServer.handler=24,port=60020] ipc.RpcServer: RpcServer.handler=24,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-07-01 16:35:59,050 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1404255227840/sceplus-vm48.almaden.ibm.com%2C60020%2C1404255227840.1404257753902 with entries=135, filesize=94.9m; new WAL /hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1404255227840/sceplus-vm48.almaden.ibm.com%2C60020%2C1404255227840.1404257758671
2014-07-01 16:35:59,054 WARN  [RpcServer.handler=19,port=60020] ipc.RpcServer: RpcServer.respondercallId: 148 service: ClientService methodName: Multi size: 3.0m connection: 9.1.143.58:41348: output error
2014-07-01 16:35:59,054 WARN  [RpcServer.handler=19,port=60020] ipc.RpcServer: RpcServer.handler=19,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-07-01 16:36:01,997 INFO  [JvmPauseMonitor] util.JvmPauseMonitor: Detected pause in JVM or host machine (eg GC): pause of approximately 1631ms
GC pool 'ParNew' had collection(s): count=1 time=2089ms
2014-07-01 16:36:02,680 WARN  [RpcServer.handler=5,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":12877,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.58:41353","starttimems":1404257749802,"queuetimems":1,"class":"HRegionServer","responsesize":17343,"method":"Multi"}
2014-07-01 16:36:02,687 WARN  [RpcServer.handler=9,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":12145,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.58:41353","starttimems":1404257750541,"queuetimems":627,"class":"HRegionServer","responsesize":17003,"method":"Multi"}
2014-07-01 16:36:02,699 WARN  [RpcServer.handler=45,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":12162,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.58:41353","starttimems":1404257750536,"queuetimems":644,"class":"HRegionServer","responsesize":16914,"method":"Multi"}
2014-07-01 16:36:02,703 WARN  [RpcServer.handler=0,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":12174,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.58:41353","starttimems":1404257750529,"queuetimems":704,"class":"HRegionServer","responsesize":17074,"method":"Multi"}
2014-07-01 16:36:02,815 WARN  [RpcServer.handler=26,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":12267,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.58:41353","starttimems":1404257750547,"queuetimems":589,"class":"HRegionServer","responsesize":16888,"method":"Multi"}
2014-07-01 16:36:02,817 WARN  [RpcServer.handler=35,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":12262,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.58:41353","starttimems":1404257750555,"queuetimems":569,"class":"HRegionServer","responsesize":16968,"method":"Multi"}
2014-07-01 16:36:02,820 WARN  [RpcServer.handler=32,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":12263,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.58:41353","starttimems":1404257750556,"queuetimems":554,"class":"HRegionServer","responsesize":16745,"method":"Multi"}
2014-07-01 16:36:02,831 WARN  [RpcServer.handler=44,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":10015,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.58:41353","starttimems":1404257752815,"queuetimems":2704,"class":"HRegionServer","responsesize":17632,"method":"Multi"}
2014-07-01 16:36:02,839 WARN  [RpcServer.handler=38,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":12298,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.58:41353","starttimems":1404257750541,"queuetimems":605,"class":"HRegionServer","responsesize":17255,"method":"Multi"}
2014-07-01 16:36:02,847 WARN  [RpcServer.handler=1,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":10031,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.58:41353","starttimems":1404257752815,"queuetimems":2720,"class":"HRegionServer","responsesize":16418,"method":"Multi"}
2014-07-01 16:36:04,193 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-01 16:36:06,739 INFO  [JvmPauseMonitor] util.JvmPauseMonitor: Detected pause in JVM or host machine (eg GC): pause of approximately 2241ms
GC pool 'ParNew' had collection(s): count=1 time=2482ms
2014-07-01 16:36:06,812 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 20843 synced till here 20840
2014-07-01 16:36:06,927 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1404255227840/sceplus-vm48.almaden.ibm.com%2C60020%2C1404255227840.1404257758671 with entries=123, filesize=94.2m; new WAL /hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1404255227840/sceplus-vm48.almaden.ibm.com%2C60020%2C1404255227840.1404257764193
2014-07-01 16:36:08,816 WARN  [RpcServer.handler=47,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":11169,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.58:41353","starttimems":1404257757646,"queuetimems":7382,"class":"HRegionServer","responsesize":17205,"method":"Multi"}
2014-07-01 16:36:08,816 WARN  [RpcServer.handler=43,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":11169,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.58:41353","starttimems":1404257757646,"queuetimems":7413,"class":"HRegionServer","responsesize":16929,"method":"Multi"}
2014-07-01 16:36:09,115 WARN  [RpcServer.handler=11,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":11294,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.58:41353","starttimems":1404257757821,"queuetimems":7456,"class":"HRegionServer","responsesize":17148,"method":"Multi"}
2014-07-01 16:36:09,115 WARN  [RpcServer.handler=18,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":11294,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.58:41353","starttimems":1404257757821,"queuetimems":7502,"class":"HRegionServer","responsesize":17073,"method":"Multi"}
2014-07-01 16:36:09,115 WARN  [RpcServer.handler=20,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":11294,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.58:41353","starttimems":1404257757821,"queuetimems":7475,"class":"HRegionServer","responsesize":16937,"method":"Multi"}
2014-07-01 16:36:09,115 WARN  [RpcServer.handler=17,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":11293,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.58:41353","starttimems":1404257757822,"queuetimems":7362,"class":"HRegionServer","responsesize":17201,"method":"Multi"}
2014-07-01 16:36:09,116 WARN  [RpcServer.handler=23,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":11282,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.58:41353","starttimems":1404257757833,"queuetimems":5035,"class":"HRegionServer","responsesize":16692,"method":"Multi"}
2014-07-01 16:36:09,116 WARN  [RpcServer.handler=49,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":11295,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.58:41353","starttimems":1404257757821,"queuetimems":7434,"class":"HRegionServer","responsesize":16882,"method":"Multi"}
2014-07-01 16:36:09,115 WARN  [RpcServer.handler=15,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":11294,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.58:41353","starttimems":1404257757821,"queuetimems":7417,"class":"HRegionServer","responsesize":16571,"method":"Multi"}
2014-07-01 16:36:09,116 WARN  [RpcServer.handler=22,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":11294,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.58:41353","starttimems":1404257757822,"queuetimems":5075,"class":"HRegionServer","responsesize":16731,"method":"Multi"}
2014-07-01 16:36:09,116 WARN  [RpcServer.handler=12,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":11513,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.58:41353","starttimems":1404257757603,"queuetimems":7387,"class":"HRegionServer","responsesize":16846,"method":"Multi"}
2014-07-01 16:36:09,117 WARN  [RpcServer.handler=36,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":11286,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.58:41353","starttimems":1404257757830,"queuetimems":5054,"class":"HRegionServer","responsesize":17255,"method":"Multi"}
2014-07-01 16:36:09,117 WARN  [RpcServer.handler=14,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":11292,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.58:41353","starttimems":1404257757825,"queuetimems":5068,"class":"HRegionServer","responsesize":17632,"method":"Multi"}
2014-07-01 16:36:09,117 WARN  [RpcServer.handler=16,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":11296,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.58:41353","starttimems":1404257757821,"queuetimems":7523,"class":"HRegionServer","responsesize":16755,"method":"Multi"}
2014-07-01 16:36:09,117 WARN  [RpcServer.handler=6,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":11522,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.58:41353","starttimems":1404257757595,"queuetimems":7416,"class":"HRegionServer","responsesize":17277,"method":"Multi"}
2014-07-01 16:36:09,118 WARN  [RpcServer.handler=28,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":11450,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.58:41353","starttimems":1404257757667,"queuetimems":7381,"class":"HRegionServer","responsesize":17383,"method":"Multi"}
2014-07-01 16:36:09,116 WARN  [RpcServer.handler=27,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":11286,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.58:41353","starttimems":1404257757830,"queuetimems":5045,"class":"HRegionServer","responsesize":17103,"method":"Multi"}
2014-07-01 16:36:09,116 WARN  [RpcServer.handler=34,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":11469,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.58:41353","starttimems":1404257757646,"queuetimems":7399,"class":"HRegionServer","responsesize":16996,"method":"Multi"}
2014-07-01 16:36:09,117 WARN  [RpcServer.handler=21,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":11296,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.58:41353","starttimems":1404257757821,"queuetimems":7400,"class":"HRegionServer","responsesize":17003,"method":"Multi"}
2014-07-01 16:36:09,116 WARN  [RpcServer.handler=13,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":11294,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.58:41353","starttimems":1404257757822,"queuetimems":7337,"class":"HRegionServer","responsesize":17228,"method":"Multi"}
2014-07-01 16:36:10,462 WARN  [RpcServer.handler=29,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":12629,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.58:41353","starttimems":1404257757833,"queuetimems":5009,"class":"HRegionServer","responsesize":16937,"method":"Multi"}
2014-07-01 16:36:10,493 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-01 16:36:10,559 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 20974 synced till here 20941
2014-07-01 16:36:12,792 INFO  [JvmPauseMonitor] util.JvmPauseMonitor: Detected pause in JVM or host machine (eg GC): pause of approximately 1550ms
GC pool 'ParNew' had collection(s): count=1 time=1839ms
2014-07-01 16:36:13,523 DEBUG [RpcServer.handler=49,port=60020] regionserver.HRegion: Flush requested on usertable,user2,1404257714774.2ed053f05f5eec791a57de24e16f0043.
2014-07-01 16:36:13,524 DEBUG [MemStoreFlusher.1] regionserver.HRegion: Started memstore flush for usertable,user2,1404257714774.2ed053f05f5eec791a57de24e16f0043., current region memstore size 278.0m
2014-07-01 16:36:13,831 WARN  [RpcServer.handler=39,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":14797,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.58:41353","starttimems":1404257759033,"queuetimems":6061,"class":"HRegionServer","responsesize":16929,"method":"Multi"}
2014-07-01 16:36:13,831 WARN  [RpcServer.handler=30,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":14801,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.58:41353","starttimems":1404257759029,"queuetimems":6132,"class":"HRegionServer","responsesize":16846,"method":"Multi"}
2014-07-01 16:36:13,833 WARN  [RpcServer.handler=10,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":14795,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.58:41353","starttimems":1404257759037,"queuetimems":5251,"class":"HRegionServer","responsesize":16571,"method":"Multi"}
2014-07-01 16:36:13,835 WARN  [RpcServer.handler=46,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":16004,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.58:41353","starttimems":1404257757830,"queuetimems":5064,"class":"HRegionServer","responsesize":16743,"method":"Multi"}
2014-07-01 16:36:13,835 WARN  [RpcServer.handler=19,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":14780,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.58:41353","starttimems":1404257759054,"queuetimems":5182,"class":"HRegionServer","responsesize":17383,"method":"Multi"}
2014-07-01 16:36:13,837 WARN  [RpcServer.handler=3,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":14808,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.58:41353","starttimems":1404257759028,"queuetimems":6177,"class":"HRegionServer","responsesize":17100,"method":"Multi"}
2014-07-01 16:36:13,838 WARN  [RpcServer.handler=41,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":14807,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.58:41353","starttimems":1404257759030,"queuetimems":6108,"class":"HRegionServer","responsesize":17205,"method":"Multi"}
2014-07-01 16:36:13,839 WARN  [RpcServer.handler=42,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":14803,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.58:41353","starttimems":1404257759035,"queuetimems":6030,"class":"HRegionServer","responsesize":16947,"method":"Multi"}
2014-07-01 16:36:13,839 WARN  [RpcServer.handler=8,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":14810,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.58:41353","starttimems":1404257759028,"queuetimems":6151,"class":"HRegionServer","responsesize":16879,"method":"Multi"}
2014-07-01 16:36:13,837 WARN  [RpcServer.handler=40,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":14797,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.58:41353","starttimems":1404257759039,"queuetimems":5207,"class":"HRegionServer","responsesize":17148,"method":"Multi"}
2014-07-01 16:36:13,837 WARN  [RpcServer.handler=24,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":14790,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.58:41353","starttimems":1404257759047,"queuetimems":5195,"class":"HRegionServer","responsesize":17003,"method":"Multi"}
2014-07-01 16:36:13,837 WARN  [RpcServer.handler=25,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":14806,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.58:41353","starttimems":1404257759030,"queuetimems":6086,"class":"HRegionServer","responsesize":16914,"method":"Multi"}
2014-07-01 16:36:13,846 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1404255227840/sceplus-vm48.almaden.ibm.com%2C60020%2C1404255227840.1404257764193 with entries=131, filesize=93.1m; new WAL /hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1404255227840/sceplus-vm48.almaden.ibm.com%2C60020%2C1404255227840.1404257770494
2014-07-01 16:36:14,016 DEBUG [MemStoreFlusher.1] util.FSUtils: DFS Client does not support most favored nodes create; using default create
2014-07-01 16:36:15,782 DEBUG [RpcServer.handler=14,port=60020] regionserver.HRegion: Flush requested on usertable,user3,1404257714774.a59b89c303508bc329bc1095d0beb9e5.
2014-07-01 16:36:15,783 DEBUG [MemStoreFlusher.0] regionserver.HRegion: Started memstore flush for usertable,user3,1404257714774.a59b89c303508bc329bc1095d0beb9e5., current region memstore size 267.6m
2014-07-01 16:36:16,061 DEBUG [MemStoreFlusher.0] util.FSUtils: DFS Client does not support most favored nodes create; using default create
2014-07-01 16:36:16,063 WARN  [RpcServer.handler=7,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":13259,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.58:41353","starttimems":1404257762803,"queuetimems":8633,"class":"HRegionServer","responsesize":16882,"method":"Multi"}
2014-07-01 16:36:16,063 WARN  [RpcServer.handler=45,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":13362,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.58:41353","starttimems":1404257762700,"queuetimems":8661,"class":"HRegionServer","responsesize":16745,"method":"Multi"}
2014-07-01 16:36:16,063 WARN  [RpcServer.handler=48,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":13367,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.58:41353","starttimems":1404257762695,"queuetimems":8690,"class":"HRegionServer","responsesize":17176,"method":"Multi"}
2014-07-01 16:36:16,063 WARN  [RpcServer.handler=44,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":13231,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.58:41353","starttimems":1404257762831,"queuetimems":8576,"class":"HRegionServer","responsesize":16694,"method":"Multi"}
2014-07-01 16:36:17,922 INFO  [MemStoreFlusher.1] regionserver.DefaultStoreFlusher: Flushed, sequenceid=181, memsize=61.0m, hasBloomFilter=true, into tmp file hdfs://master:54310/hbase/data/default/usertable/2ed053f05f5eec791a57de24e16f0043/.tmp/8688ad18b421453dba583dbf1af639d7
2014-07-01 16:36:17,933 DEBUG [MemStoreFlusher.1] regionserver.HRegionFileSystem: Committing store file hdfs://master:54310/hbase/data/default/usertable/2ed053f05f5eec791a57de24e16f0043/.tmp/8688ad18b421453dba583dbf1af639d7 as hdfs://master:54310/hbase/data/default/usertable/2ed053f05f5eec791a57de24e16f0043/family/8688ad18b421453dba583dbf1af639d7
2014-07-01 16:36:17,940 INFO  [MemStoreFlusher.1] regionserver.HStore: Added hdfs://master:54310/hbase/data/default/usertable/2ed053f05f5eec791a57de24e16f0043/family/8688ad18b421453dba583dbf1af639d7, entries=222060, sequenceid=181, filesize=15.8m
2014-07-01 16:36:17,941 INFO  [MemStoreFlusher.1] regionserver.HRegion: Finished memstore flush of ~279.6m/293232560, currentsize=18.8m/19663920 for region usertable,user2,1404257714774.2ed053f05f5eec791a57de24e16f0043. in 4417ms, sequenceid=181, compaction requested=false
2014-07-01 16:36:18,255 WARN  [RpcServer.handler=32,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":15435,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.58:41353","starttimems":1404257762820,"queuetimems":8580,"class":"HRegionServer","responsesize":17228,"method":"Multi"}
2014-07-01 16:36:18,257 DEBUG [RpcServer.handler=16,port=60020] regionserver.HRegion: Flush requested on usertable,user4,1404257714774.c831a0ac8e8f14e82fdaef2ba3a5e1f6.
2014-07-01 16:36:18,258 WARN  [RpcServer.handler=9,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":15570,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.58:41353","starttimems":1404257762687,"queuetimems":8734,"class":"HRegionServer","responsesize":17201,"method":"Multi"}
2014-07-01 16:36:18,255 WARN  [RpcServer.handler=35,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":15438,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.58:41353","starttimems":1404257762817,"queuetimems":8593,"class":"HRegionServer","responsesize":16888,"method":"Multi"}
2014-07-01 16:36:18,262 WARN  [RpcServer.handler=33,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":11459,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.58:41353","starttimems":1404257766802,"queuetimems":12500,"class":"HRegionServer","responsesize":17074,"method":"Multi"}
2014-07-01 16:36:18,262 WARN  [RpcServer.handler=37,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":15453,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.58:41353","starttimems":1404257762808,"queuetimems":8616,"class":"HRegionServer","responsesize":17277,"method":"Multi"}
2014-07-01 16:36:18,271 WARN  [RpcServer.handler=38,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":15431,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.58:41353","starttimems":1404257762840,"queuetimems":8570,"class":"HRegionServer","responsesize":16462,"method":"Multi"}
2014-07-01 16:36:18,261 WARN  [RpcServer.handler=0,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":15557,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.58:41353","starttimems":1404257762704,"queuetimems":8619,"class":"HRegionServer","responsesize":16755,"method":"Multi"}
2014-07-01 16:36:18,275 WARN  [RpcServer.handler=2,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":15479,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.58:41353","starttimems":1404257762795,"queuetimems":8650,"class":"HRegionServer","responsesize":17003,"method":"Multi"}
2014-07-01 16:36:18,275 WARN  [RpcServer.handler=31,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":15568,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.58:41353","starttimems":1404257762706,"queuetimems":8605,"class":"HRegionServer","responsesize":16418,"method":"Multi"}
2014-07-01 16:36:18,261 WARN  [RpcServer.handler=5,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":15580,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.58:41353","starttimems":1404257762681,"queuetimems":8786,"class":"HRegionServer","responsesize":16968,"method":"Multi"}
2014-07-01 16:36:18,275 WARN  [RpcServer.handler=4,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":15486,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.58:41353","starttimems":1404257762788,"queuetimems":8669,"class":"HRegionServer","responsesize":16996,"method":"Multi"}
2014-07-01 16:36:18,276 WARN  [RpcServer.handler=26,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":15460,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.58:41353","starttimems":1404257762815,"queuetimems":8606,"class":"HRegionServer","responsesize":17073,"method":"Multi"}
2014-07-01 16:36:18,261 WARN  [RpcServer.handler=1,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":15414,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.58:41353","starttimems":1404257762847,"queuetimems":8561,"class":"HRegionServer","responsesize":17343,"method":"Multi"}
2014-07-01 16:36:18,260 DEBUG [MemStoreFlusher.1] regionserver.HRegion: Started memstore flush for usertable,user4,1404257714774.c831a0ac8e8f14e82fdaef2ba3a5e1f6., current region memstore size 257.1m
2014-07-01 16:36:18,285 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-01 16:36:18,324 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 21083 synced till here 21070
2014-07-01 16:36:18,861 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1404255227840/sceplus-vm48.almaden.ibm.com%2C60020%2C1404255227840.1404257770494 with entries=109, filesize=81.9m; new WAL /hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1404255227840/sceplus-vm48.almaden.ibm.com%2C60020%2C1404255227840.1404257778286
2014-07-01 16:36:19,057 DEBUG [MemStoreFlusher.1] util.FSUtils: DFS Client does not support most favored nodes create; using default create
2014-07-01 16:36:19,768 INFO  [MemStoreFlusher.0] regionserver.DefaultStoreFlusher: Flushed, sequenceid=182, memsize=60.3m, hasBloomFilter=true, into tmp file hdfs://master:54310/hbase/data/default/usertable/a59b89c303508bc329bc1095d0beb9e5/.tmp/2a22ccd36da747669a264b61847e9ec1
2014-07-01 16:36:19,777 DEBUG [MemStoreFlusher.0] regionserver.HRegionFileSystem: Committing store file hdfs://master:54310/hbase/data/default/usertable/a59b89c303508bc329bc1095d0beb9e5/.tmp/2a22ccd36da747669a264b61847e9ec1 as hdfs://master:54310/hbase/data/default/usertable/a59b89c303508bc329bc1095d0beb9e5/family/2a22ccd36da747669a264b61847e9ec1
2014-07-01 16:36:19,789 INFO  [MemStoreFlusher.0] regionserver.HStore: Added hdfs://master:54310/hbase/data/default/usertable/a59b89c303508bc329bc1095d0beb9e5/family/2a22ccd36da747669a264b61847e9ec1, entries=219490, sequenceid=182, filesize=15.6m
2014-07-01 16:36:19,789 INFO  [MemStoreFlusher.0] regionserver.HRegion: Finished memstore flush of ~276.6m/290055040, currentsize=25.0m/26224640 for region usertable,user3,1404257714774.a59b89c303508bc329bc1095d0beb9e5. in 4006ms, sequenceid=182, compaction requested=false
2014-07-01 16:36:20,278 DEBUG [RpcServer.handler=36,port=60020] regionserver.HRegion: Flush requested on usertable,user8,1404257714774.b6ae40892bbb2a1c545234ed91d62d1a.
2014-07-01 16:36:20,309 DEBUG [MemStoreFlusher.0] regionserver.HRegion: Started memstore flush for usertable,user8,1404257714774.b6ae40892bbb2a1c545234ed91d62d1a., current region memstore size 259.4m
2014-07-01 16:36:20,806 DEBUG [MemStoreFlusher.0] util.FSUtils: DFS Client does not support most favored nodes create; using default create
2014-07-01 16:36:22,186 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-01 16:36:22,187 WARN  [RpcServer.handler=23,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":13060,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.58:41353","starttimems":1404257769127,"queuetimems":11350,"class":"HRegionServer","responsesize":16692,"method":"Multi"}
2014-07-01 16:36:22,188 WARN  [RpcServer.handler=15,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":13068,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.58:41353","starttimems":1404257769120,"queuetimems":11365,"class":"HRegionServer","responsesize":16937,"method":"Multi"}
2014-07-01 16:36:22,188 WARN  [RpcServer.handler=6,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":13039,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.58:41353","starttimems":1404257769149,"queuetimems":11044,"class":"HRegionServer","responsesize":17332,"method":"Multi"}
2014-07-01 16:36:22,188 WARN  [RpcServer.handler=16,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":13034,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.58:41353","starttimems":1404257769154,"queuetimems":10865,"class":"HRegionServer","responsesize":16462,"method":"Multi"}
2014-07-01 16:36:22,188 WARN  [RpcServer.handler=20,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":13049,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.58:41353","starttimems":1404257769139,"queuetimems":11341,"class":"HRegionServer","responsesize":16929,"method":"Multi"}
2014-07-01 16:36:22,188 WARN  [RpcServer.handler=36,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":13069,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.58:41353","starttimems":1404257769119,"queuetimems":11441,"class":"HRegionServer","responsesize":16947,"method":"Multi"}
2014-07-01 16:36:22,188 WARN  [RpcServer.handler=34,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":13046,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.58:41353","starttimems":1404257769142,"queuetimems":11253,"class":"HRegionServer","responsesize":17205,"method":"Multi"}
2014-07-01 16:36:22,188 WARN  [RpcServer.handler=28,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":13045,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.58:41353","starttimems":1404257769143,"queuetimems":11055,"class":"HRegionServer","responsesize":17074,"method":"Multi"}
2014-07-01 16:36:22,211 WARN  [RpcServer.handler=12,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":13092,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.58:41353","starttimems":1404257769119,"queuetimems":11407,"class":"HRegionServer","responsesize":16743,"method":"Multi"}
2014-07-01 16:36:22,212 WARN  [RpcServer.handler=21,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":13070,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.58:41353","starttimems":1404257769142,"queuetimems":11276,"class":"HRegionServer","responsesize":16879,"method":"Multi"}
2014-07-01 16:36:22,211 WARN  [RpcServer.handler=49,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":13084,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.58:41353","starttimems":1404257769127,"queuetimems":11361,"class":"HRegionServer","responsesize":16846,"method":"Multi"}
2014-07-01 16:36:22,219 WARN  [RpcServer.handler=18,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":13078,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.58:41353","starttimems":1404257769140,"queuetimems":11326,"class":"HRegionServer","responsesize":16914,"method":"Multi"}
2014-07-01 16:36:22,212 WARN  [RpcServer.handler=14,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":13058,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.58:41353","starttimems":1404257769154,"queuetimems":10846,"class":"HRegionServer","responsesize":17343,"method":"Multi"}
2014-07-01 16:36:22,223 WARN  [RpcServer.handler=22,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":13103,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.58:41353","starttimems":1404257769119,"queuetimems":11376,"class":"HRegionServer","responsesize":17100,"method":"Multi"}
2014-07-01 16:36:22,223 WARN  [RpcServer.handler=27,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":13079,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.58:41353","starttimems":1404257769143,"queuetimems":11122,"class":"HRegionServer","responsesize":16731,"method":"Multi"}
2014-07-01 16:36:22,223 WARN  [RpcServer.handler=11,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":13105,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.58:41353","starttimems":1404257769117,"queuetimems":11479,"class":"HRegionServer","responsesize":17632,"method":"Multi"}
2014-07-01 16:36:22,223 WARN  [RpcServer.handler=13,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":13083,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.58:41353","starttimems":1404257769140,"queuetimems":11297,"class":"HRegionServer","responsesize":17255,"method":"Multi"}
2014-07-01 16:36:22,223 WARN  [RpcServer.handler=17,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":13084,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.58:41353","starttimems":1404257769138,"queuetimems":11349,"class":"HRegionServer","responsesize":17103,"method":"Multi"}
2014-07-01 16:36:22,249 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 21190 synced till here 21181
2014-07-01 16:36:22,378 WARN  [RpcServer.handler=29,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":11914,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.58:41353","starttimems":1404257770463,"queuetimems":11076,"class":"HRegionServer","responsesize":17632,"method":"Multi"}
2014-07-01 16:36:22,391 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1404255227840/sceplus-vm48.almaden.ibm.com%2C60020%2C1404255227840.1404257778286 with entries=107, filesize=78.0m; new WAL /hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1404255227840/sceplus-vm48.almaden.ibm.com%2C60020%2C1404255227840.1404257782186
2014-07-01 16:36:22,480 INFO  [MemStoreFlusher.1] regionserver.DefaultStoreFlusher: Flushed, sequenceid=183, memsize=60.5m, hasBloomFilter=true, into tmp file hdfs://master:54310/hbase/data/default/usertable/c831a0ac8e8f14e82fdaef2ba3a5e1f6/.tmp/70406c7a695b416b89698affdae61397
2014-07-01 16:36:22,539 DEBUG [MemStoreFlusher.1] regionserver.HRegionFileSystem: Committing store file hdfs://master:54310/hbase/data/default/usertable/c831a0ac8e8f14e82fdaef2ba3a5e1f6/.tmp/70406c7a695b416b89698affdae61397 as hdfs://master:54310/hbase/data/default/usertable/c831a0ac8e8f14e82fdaef2ba3a5e1f6/family/70406c7a695b416b89698affdae61397
2014-07-01 16:36:22,645 INFO  [MemStoreFlusher.1] regionserver.HStore: Added hdfs://master:54310/hbase/data/default/usertable/c831a0ac8e8f14e82fdaef2ba3a5e1f6/family/70406c7a695b416b89698affdae61397, entries=220340, sequenceid=183, filesize=15.7m
2014-07-01 16:36:22,645 INFO  [MemStoreFlusher.1] regionserver.HRegion: Finished memstore flush of ~279.2m/292756720, currentsize=26.5m/27747840 for region usertable,user4,1404257714774.c831a0ac8e8f14e82fdaef2ba3a5e1f6. in 4385ms, sequenceid=183, compaction requested=false
2014-07-01 16:36:24,042 INFO  [MemStoreFlusher.0] regionserver.DefaultStoreFlusher: Flushed, sequenceid=184, memsize=60.6m, hasBloomFilter=true, into tmp file hdfs://master:54310/hbase/data/default/usertable/b6ae40892bbb2a1c545234ed91d62d1a/.tmp/a0c25e6884e144168b0858b476cf17e9
2014-07-01 16:36:24,058 DEBUG [MemStoreFlusher.0] regionserver.HRegionFileSystem: Committing store file hdfs://master:54310/hbase/data/default/usertable/b6ae40892bbb2a1c545234ed91d62d1a/.tmp/a0c25e6884e144168b0858b476cf17e9 as hdfs://master:54310/hbase/data/default/usertable/b6ae40892bbb2a1c545234ed91d62d1a/family/a0c25e6884e144168b0858b476cf17e9
2014-07-01 16:36:24,066 INFO  [MemStoreFlusher.0] regionserver.HStore: Added hdfs://master:54310/hbase/data/default/usertable/b6ae40892bbb2a1c545234ed91d62d1a/family/a0c25e6884e144168b0858b476cf17e9, entries=220810, sequenceid=184, filesize=15.7m
2014-07-01 16:36:24,066 INFO  [MemStoreFlusher.0] regionserver.HRegion: Finished memstore flush of ~282.9m/296676960, currentsize=23.2m/24324080 for region usertable,user8,1404257714774.b6ae40892bbb2a1c545234ed91d62d1a. in 3757ms, sequenceid=184, compaction requested=false
2014-07-01 16:36:24,979 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-01 16:36:25,808 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 21304 synced till here 21277
2014-07-01 16:36:26,667 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1404255227840/sceplus-vm48.almaden.ibm.com%2C60020%2C1404255227840.1404257782186 with entries=114, filesize=84.8m; new WAL /hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1404255227840/sceplus-vm48.almaden.ibm.com%2C60020%2C1404255227840.1404257784980
2014-07-01 16:36:30,422 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-01 16:36:30,448 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 21411 synced till here 21389
2014-07-01 16:36:31,142 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1404255227840/sceplus-vm48.almaden.ibm.com%2C60020%2C1404255227840.1404257784980 with entries=107, filesize=86.3m; new WAL /hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1404255227840/sceplus-vm48.almaden.ibm.com%2C60020%2C1404255227840.1404257790423
2014-07-01 16:36:33,671 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-01 16:36:33,741 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 21537 synced till here 21516
2014-07-01 16:36:34,170 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1404255227840/sceplus-vm48.almaden.ibm.com%2C60020%2C1404255227840.1404257790423 with entries=126, filesize=83.8m; new WAL /hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1404255227840/sceplus-vm48.almaden.ibm.com%2C60020%2C1404255227840.1404257793671
2014-07-01 16:36:37,717 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-01 16:36:37,779 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 21638 synced till here 21616
2014-07-01 16:36:38,314 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1404255227840/sceplus-vm48.almaden.ibm.com%2C60020%2C1404255227840.1404257793671 with entries=101, filesize=79.8m; new WAL /hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1404255227840/sceplus-vm48.almaden.ibm.com%2C60020%2C1404255227840.1404257797717
2014-07-01 16:36:41,009 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-01 16:36:41,041 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 21765 synced till here 21741
2014-07-01 16:36:41,696 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1404255227840/sceplus-vm48.almaden.ibm.com%2C60020%2C1404255227840.1404257797717 with entries=127, filesize=85.4m; new WAL /hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1404255227840/sceplus-vm48.almaden.ibm.com%2C60020%2C1404255227840.1404257801009
2014-07-01 16:36:45,013 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-01 16:36:45,064 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 21862 synced till here 21851
2014-07-01 16:36:45,300 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1404255227840/sceplus-vm48.almaden.ibm.com%2C60020%2C1404255227840.1404257801009 with entries=97, filesize=75.1m; new WAL /hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1404255227840/sceplus-vm48.almaden.ibm.com%2C60020%2C1404255227840.1404257805013
2014-07-01 16:36:46,385 DEBUG [RpcServer.handler=16,port=60020] regionserver.HRegion: Flush requested on usertable,user2,1404257714774.2ed053f05f5eec791a57de24e16f0043.
2014-07-01 16:36:46,387 DEBUG [MemStoreFlusher.1] regionserver.HRegion: Started memstore flush for usertable,user2,1404257714774.2ed053f05f5eec791a57de24e16f0043., current region memstore size 257.2m
2014-07-01 16:36:46,546 DEBUG [MemStoreFlusher.1] util.FSUtils: DFS Client does not support most favored nodes create; using default create
2014-07-01 16:36:48,601 DEBUG [RpcServer.handler=8,port=60020] regionserver.HRegion: Flush requested on usertable,user3,1404257714774.a59b89c303508bc329bc1095d0beb9e5.
2014-07-01 16:36:48,601 DEBUG [MemStoreFlusher.0] regionserver.HRegion: Started memstore flush for usertable,user3,1404257714774.a59b89c303508bc329bc1095d0beb9e5., current region memstore size 256.2m
2014-07-01 16:36:48,789 DEBUG [RpcServer.handler=12,port=60020] regionserver.HRegion: Flush requested on usertable,user4,1404257714774.c831a0ac8e8f14e82fdaef2ba3a5e1f6.
2014-07-01 16:36:48,820 INFO  [MemStoreFlusher.1] regionserver.DefaultStoreFlusher: Flushed, sequenceid=346, memsize=56.4m, hasBloomFilter=true, into tmp file hdfs://master:54310/hbase/data/default/usertable/2ed053f05f5eec791a57de24e16f0043/.tmp/1376e983db1144179718d8c60ff56ed3
2014-07-01 16:36:48,830 DEBUG [MemStoreFlusher.1] regionserver.HRegionFileSystem: Committing store file hdfs://master:54310/hbase/data/default/usertable/2ed053f05f5eec791a57de24e16f0043/.tmp/1376e983db1144179718d8c60ff56ed3 as hdfs://master:54310/hbase/data/default/usertable/2ed053f05f5eec791a57de24e16f0043/family/1376e983db1144179718d8c60ff56ed3
2014-07-01 16:36:48,838 DEBUG [MemStoreFlusher.0] util.FSUtils: DFS Client does not support most favored nodes create; using default create
2014-07-01 16:36:48,862 INFO  [MemStoreFlusher.1] regionserver.HStore: Added hdfs://master:54310/hbase/data/default/usertable/2ed053f05f5eec791a57de24e16f0043/family/1376e983db1144179718d8c60ff56ed3, entries=205240, sequenceid=346, filesize=14.6m
2014-07-01 16:36:48,863 INFO  [MemStoreFlusher.1] regionserver.HRegion: Finished memstore flush of ~257.2m/269684640, currentsize=7.8m/8132880 for region usertable,user2,1404257714774.2ed053f05f5eec791a57de24e16f0043. in 2476ms, sequenceid=346, compaction requested=false
2014-07-01 16:36:48,864 DEBUG [MemStoreFlusher.1] regionserver.HRegion: Started memstore flush for usertable,user4,1404257714774.c831a0ac8e8f14e82fdaef2ba3a5e1f6., current region memstore size 260.5m
2014-07-01 16:36:48,989 DEBUG [RpcServer.handler=47,port=60020] regionserver.HRegion: Flush requested on usertable,user8,1404257714774.b6ae40892bbb2a1c545234ed91d62d1a.
2014-07-01 16:36:49,049 DEBUG [MemStoreFlusher.1] util.FSUtils: DFS Client does not support most favored nodes create; using default create
2014-07-01 16:36:51,673 INFO  [MemStoreFlusher.0] regionserver.DefaultStoreFlusher: Flushed, sequenceid=351, memsize=55.6m, hasBloomFilter=true, into tmp file hdfs://master:54310/hbase/data/default/usertable/a59b89c303508bc329bc1095d0beb9e5/.tmp/83fb257a032c495aacd11b459b0fbc0d
2014-07-01 16:36:51,712 DEBUG [MemStoreFlusher.0] regionserver.HRegionFileSystem: Committing store file hdfs://master:54310/hbase/data/default/usertable/a59b89c303508bc329bc1095d0beb9e5/.tmp/83fb257a032c495aacd11b459b0fbc0d as hdfs://master:54310/hbase/data/default/usertable/a59b89c303508bc329bc1095d0beb9e5/family/83fb257a032c495aacd11b459b0fbc0d
2014-07-01 16:36:51,763 INFO  [MemStoreFlusher.0] regionserver.HStore: Added hdfs://master:54310/hbase/data/default/usertable/a59b89c303508bc329bc1095d0beb9e5/family/83fb257a032c495aacd11b459b0fbc0d, entries=202430, sequenceid=351, filesize=14.4m
2014-07-01 16:36:51,763 INFO  [MemStoreFlusher.0] regionserver.HRegion: Finished memstore flush of ~259.2m/271813200, currentsize=6.1m/6355920 for region usertable,user3,1404257714774.a59b89c303508bc329bc1095d0beb9e5. in 3162ms, sequenceid=351, compaction requested=false
2014-07-01 16:36:51,764 DEBUG [MemStoreFlusher.0] regionserver.HRegion: Started memstore flush for usertable,user8,1404257714774.b6ae40892bbb2a1c545234ed91d62d1a., current region memstore size 258.2m
2014-07-01 16:36:51,883 INFO  [MemStoreFlusher.1] regionserver.DefaultStoreFlusher: Flushed, sequenceid=352, memsize=55.8m, hasBloomFilter=true, into tmp file hdfs://master:54310/hbase/data/default/usertable/c831a0ac8e8f14e82fdaef2ba3a5e1f6/.tmp/fe9d01bfcec2494b8ea61f4c916a3fff
2014-07-01 16:36:51,902 DEBUG [MemStoreFlusher.1] regionserver.HRegionFileSystem: Committing store file hdfs://master:54310/hbase/data/default/usertable/c831a0ac8e8f14e82fdaef2ba3a5e1f6/.tmp/fe9d01bfcec2494b8ea61f4c916a3fff as hdfs://master:54310/hbase/data/default/usertable/c831a0ac8e8f14e82fdaef2ba3a5e1f6/family/fe9d01bfcec2494b8ea61f4c916a3fff
2014-07-01 16:36:51,910 INFO  [MemStoreFlusher.1] regionserver.HStore: Added hdfs://master:54310/hbase/data/default/usertable/c831a0ac8e8f14e82fdaef2ba3a5e1f6/family/fe9d01bfcec2494b8ea61f4c916a3fff, entries=203160, sequenceid=352, filesize=14.5m
2014-07-01 16:36:51,910 INFO  [MemStoreFlusher.1] regionserver.HRegion: Finished memstore flush of ~260.5m/273175680, currentsize=3.2m/3383840 for region usertable,user4,1404257714774.c831a0ac8e8f14e82fdaef2ba3a5e1f6. in 3046ms, sequenceid=352, compaction requested=false
2014-07-01 16:36:51,932 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-01 16:36:51,952 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 21957 synced till here 21954
2014-07-01 16:36:52,041 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1404255227840/sceplus-vm48.almaden.ibm.com%2C60020%2C1404255227840.1404257805013 with entries=95, filesize=63.1m; new WAL /hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1404255227840/sceplus-vm48.almaden.ibm.com%2C60020%2C1404255227840.1404257811932
2014-07-01 16:36:52,207 DEBUG [MemStoreFlusher.0] util.FSUtils: DFS Client does not support most favored nodes create; using default create
2014-07-01 16:36:54,562 INFO  [MemStoreFlusher.0] regionserver.DefaultStoreFlusher: Flushed, sequenceid=353, memsize=56.0m, hasBloomFilter=true, into tmp file hdfs://master:54310/hbase/data/default/usertable/b6ae40892bbb2a1c545234ed91d62d1a/.tmp/ded3484f44374ff2802f94c532246f57
2014-07-01 16:36:54,604 DEBUG [MemStoreFlusher.0] regionserver.HRegionFileSystem: Committing store file hdfs://master:54310/hbase/data/default/usertable/b6ae40892bbb2a1c545234ed91d62d1a/.tmp/ded3484f44374ff2802f94c532246f57 as hdfs://master:54310/hbase/data/default/usertable/b6ae40892bbb2a1c545234ed91d62d1a/family/ded3484f44374ff2802f94c532246f57
2014-07-01 16:36:54,614 INFO  [MemStoreFlusher.0] regionserver.HStore: Added hdfs://master:54310/hbase/data/default/usertable/b6ae40892bbb2a1c545234ed91d62d1a/family/ded3484f44374ff2802f94c532246f57, entries=203960, sequenceid=353, filesize=14.5m
2014-07-01 16:36:54,614 INFO  [MemStoreFlusher.0] regionserver.HRegion: Finished memstore flush of ~261.2m/273924000, currentsize=16.6m/17394720 for region usertable,user8,1404257714774.b6ae40892bbb2a1c545234ed91d62d1a. in 2850ms, sequenceid=353, compaction requested=false
2014-07-01 16:36:56,213 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-01 16:36:56,261 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 22060 synced till here 22038
2014-07-01 16:36:56,609 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1404255227840/sceplus-vm48.almaden.ibm.com%2C60020%2C1404255227840.1404257811932 with entries=103, filesize=82.3m; new WAL /hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1404255227840/sceplus-vm48.almaden.ibm.com%2C60020%2C1404255227840.1404257816213
2014-07-01 16:37:00,001 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-01 16:37:00,037 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1404255227840/sceplus-vm48.almaden.ibm.com%2C60020%2C1404255227840.1404257816213 with entries=86, filesize=60.9m; new WAL /hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1404255227840/sceplus-vm48.almaden.ibm.com%2C60020%2C1404255227840.1404257820001
2014-07-01 16:37:04,000 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-01 16:37:04,058 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1404255227840/sceplus-vm48.almaden.ibm.com%2C60020%2C1404255227840.1404257820001 with entries=75, filesize=61.1m; new WAL /hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1404255227840/sceplus-vm48.almaden.ibm.com%2C60020%2C1404255227840.1404257824001
2014-07-01 16:37:07,825 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-01 16:37:07,911 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 22296 synced till here 22294
2014-07-01 16:37:08,073 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1404255227840/sceplus-vm48.almaden.ibm.com%2C60020%2C1404255227840.1404257824001 with entries=75, filesize=62.5m; new WAL /hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1404255227840/sceplus-vm48.almaden.ibm.com%2C60020%2C1404255227840.1404257827826
2014-07-01 16:37:14,116 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-01 16:37:14,158 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 22372 synced till here 22371
2014-07-01 16:37:14,231 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1404255227840/sceplus-vm48.almaden.ibm.com%2C60020%2C1404255227840.1404257827826 with entries=76, filesize=62.7m; new WAL /hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1404255227840/sceplus-vm48.almaden.ibm.com%2C60020%2C1404255227840.1404257834116
2014-07-01 16:37:19,035 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-01 16:37:19,083 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 22455 synced till here 22443
2014-07-01 16:37:19,325 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1404255227840/sceplus-vm48.almaden.ibm.com%2C60020%2C1404255227840.1404257834116 with entries=83, filesize=70.8m; new WAL /hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1404255227840/sceplus-vm48.almaden.ibm.com%2C60020%2C1404255227840.1404257839035
2014-07-01 16:37:22,589 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-01 16:37:22,624 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 22542 synced till here 22541
2014-07-01 16:37:22,652 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1404255227840/sceplus-vm48.almaden.ibm.com%2C60020%2C1404255227840.1404257839035 with entries=87, filesize=62.6m; new WAL /hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1404255227840/sceplus-vm48.almaden.ibm.com%2C60020%2C1404255227840.1404257842589
2014-07-01 16:37:25,769 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-01 16:37:25,812 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1404255227840/sceplus-vm48.almaden.ibm.com%2C60020%2C1404255227840.1404257842589 with entries=74, filesize=61.4m; new WAL /hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1404255227840/sceplus-vm48.almaden.ibm.com%2C60020%2C1404255227840.1404257845769
2014-07-01 16:37:28,136 DEBUG [RpcServer.handler=9,port=60020] regionserver.HRegion: Flush requested on usertable,user2,1404257714774.2ed053f05f5eec791a57de24e16f0043.
2014-07-01 16:37:28,136 DEBUG [MemStoreFlusher.1] regionserver.HRegion: Started memstore flush for usertable,user2,1404257714774.2ed053f05f5eec791a57de24e16f0043., current region memstore size 256.2m
2014-07-01 16:37:28,291 DEBUG [MemStoreFlusher.1] util.FSUtils: DFS Client does not support most favored nodes create; using default create
2014-07-01 16:37:28,656 DEBUG [RpcServer.handler=36,port=60020] regionserver.HRegion: Flush requested on usertable,user3,1404257714774.a59b89c303508bc329bc1095d0beb9e5.
2014-07-01 16:37:28,656 DEBUG [MemStoreFlusher.0] regionserver.HRegion: Started memstore flush for usertable,user3,1404257714774.a59b89c303508bc329bc1095d0beb9e5., current region memstore size 257.0m
2014-07-01 16:37:28,711 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-01 16:37:28,736 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1404255227840/sceplus-vm48.almaden.ibm.com%2C60020%2C1404255227840.1404257845769 with entries=72, filesize=61.5m; new WAL /hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1404255227840/sceplus-vm48.almaden.ibm.com%2C60020%2C1404255227840.1404257848711
2014-07-01 16:37:28,788 DEBUG [MemStoreFlusher.0] util.FSUtils: DFS Client does not support most favored nodes create; using default create
2014-07-01 16:37:29,045 DEBUG [RpcServer.handler=11,port=60020] regionserver.HRegion: Flush requested on usertable,user4,1404257714774.c831a0ac8e8f14e82fdaef2ba3a5e1f6.
2014-07-01 16:37:29,564 DEBUG [RpcServer.handler=47,port=60020] regionserver.HRegion: Flush requested on usertable,user8,1404257714774.b6ae40892bbb2a1c545234ed91d62d1a.
2014-07-01 16:37:32,288 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-01 16:37:32,313 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 22765 synced till here 22763
2014-07-01 16:37:32,617 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1404255227840/sceplus-vm48.almaden.ibm.com%2C60020%2C1404255227840.1404257848711 with entries=77, filesize=62.2m; new WAL /hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1404255227840/sceplus-vm48.almaden.ibm.com%2C60020%2C1404255227840.1404257852289
2014-07-01 16:37:35,682 INFO  [MemStoreFlusher.1] regionserver.DefaultStoreFlusher: Flushed, sequenceid=500, memsize=211.6m, hasBloomFilter=true, into tmp file hdfs://master:54310/hbase/data/default/usertable/2ed053f05f5eec791a57de24e16f0043/.tmp/763859d66def434fa03b2729227f837c
2014-07-01 16:37:35,712 DEBUG [MemStoreFlusher.1] regionserver.HRegionFileSystem: Committing store file hdfs://master:54310/hbase/data/default/usertable/2ed053f05f5eec791a57de24e16f0043/.tmp/763859d66def434fa03b2729227f837c as hdfs://master:54310/hbase/data/default/usertable/2ed053f05f5eec791a57de24e16f0043/family/763859d66def434fa03b2729227f837c
2014-07-01 16:37:35,739 INFO  [MemStoreFlusher.1] regionserver.HStore: Added hdfs://master:54310/hbase/data/default/usertable/2ed053f05f5eec791a57de24e16f0043/family/763859d66def434fa03b2729227f837c, entries=770490, sequenceid=500, filesize=54.9m
2014-07-01 16:37:35,746 INFO  [MemStoreFlusher.1] regionserver.HRegion: Finished memstore flush of ~257.9m/270472880, currentsize=55.6m/58312320 for region usertable,user2,1404257714774.2ed053f05f5eec791a57de24e16f0043. in 7610ms, sequenceid=500, compaction requested=true
2014-07-01 16:37:35,747 DEBUG [MemStoreFlusher.1] regionserver.CompactSplitThread: Small Compaction requested: system; Because: MemStoreFlusher.1; compaction_queue=(0:0), split_queue=0, merge_queue=0
2014-07-01 16:37:35,747 DEBUG [regionserver60020-smallCompactions-1404255751957] compactions.RatioBasedCompactionPolicy: Selecting compaction from 3 store files, 0 compacting, 3 eligible, 20 blocking
2014-07-01 16:37:35,747 DEBUG [regionserver60020-smallCompactions-1404255751957] compactions.RatioBasedCompactionPolicy: Some files are too large. Excluding 3 files from compaction candidates
2014-07-01 16:37:35,747 DEBUG [MemStoreFlusher.1] regionserver.HRegion: Started memstore flush for usertable,user4,1404257714774.c831a0ac8e8f14e82fdaef2ba3a5e1f6., current region memstore size 305.1m
2014-07-01 16:37:35,747 DEBUG [regionserver60020-smallCompactions-1404255751957] compactions.ExploringCompactionPolicy: Exploring compaction algorithm has selected 0 files of size 0 starting at candidate #-1 after considering 0 permutations with 0 in ratio
2014-07-01 16:37:35,747 DEBUG [regionserver60020-smallCompactions-1404255751957] compactions.RatioBasedCompactionPolicy: Not compacting files because we only have 0 files ready for compaction. Need 3 to initiate.
2014-07-01 16:37:35,747 DEBUG [regionserver60020-smallCompactions-1404255751957] regionserver.CompactSplitThread: Not compacting usertable,user2,1404257714774.2ed053f05f5eec791a57de24e16f0043. because compaction request was cancelled
2014-07-01 16:37:35,829 INFO  [MemStoreFlusher.0] regionserver.DefaultStoreFlusher: Flushed, sequenceid=504, memsize=211.0m, hasBloomFilter=true, into tmp file hdfs://master:54310/hbase/data/default/usertable/a59b89c303508bc329bc1095d0beb9e5/.tmp/7bc97bbf0e324888bd6a12cfe75d6e31
2014-07-01 16:37:35,856 DEBUG [MemStoreFlusher.0] regionserver.HRegionFileSystem: Committing store file hdfs://master:54310/hbase/data/default/usertable/a59b89c303508bc329bc1095d0beb9e5/.tmp/7bc97bbf0e324888bd6a12cfe75d6e31 as hdfs://master:54310/hbase/data/default/usertable/a59b89c303508bc329bc1095d0beb9e5/family/7bc97bbf0e324888bd6a12cfe75d6e31
2014-07-01 16:37:35,859 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-01 16:37:35,875 INFO  [MemStoreFlusher.0] regionserver.HStore: Added hdfs://master:54310/hbase/data/default/usertable/a59b89c303508bc329bc1095d0beb9e5/family/7bc97bbf0e324888bd6a12cfe75d6e31, entries=768350, sequenceid=504, filesize=54.8m
2014-07-01 16:37:35,875 INFO  [MemStoreFlusher.0] regionserver.HRegion: Finished memstore flush of ~257.0m/269508080, currentsize=51.7m/54231360 for region usertable,user3,1404257714774.a59b89c303508bc329bc1095d0beb9e5. in 7219ms, sequenceid=504, compaction requested=true
2014-07-01 16:37:35,876 DEBUG [MemStoreFlusher.0] regionserver.CompactSplitThread: Small Compaction requested: system; Because: MemStoreFlusher.0; compaction_queue=(0:0), split_queue=0, merge_queue=0
2014-07-01 16:37:35,876 DEBUG [regionserver60020-smallCompactions-1404255751957] compactions.RatioBasedCompactionPolicy: Selecting compaction from 3 store files, 0 compacting, 3 eligible, 20 blocking
2014-07-01 16:37:35,876 DEBUG [regionserver60020-smallCompactions-1404255751957] compactions.RatioBasedCompactionPolicy: Some files are too large. Excluding 3 files from compaction candidates
2014-07-01 16:37:35,876 DEBUG [MemStoreFlusher.0] regionserver.HRegion: Started memstore flush for usertable,user8,1404257714774.b6ae40892bbb2a1c545234ed91d62d1a., current region memstore size 301.5m
2014-07-01 16:37:35,876 DEBUG [regionserver60020-smallCompactions-1404255751957] compactions.ExploringCompactionPolicy: Exploring compaction algorithm has selected 0 files of size 0 starting at candidate #-1 after considering 0 permutations with 0 in ratio
2014-07-01 16:37:35,876 DEBUG [regionserver60020-smallCompactions-1404255751957] compactions.RatioBasedCompactionPolicy: Not compacting files because we only have 0 files ready for compaction. Need 3 to initiate.
2014-07-01 16:37:35,876 DEBUG [regionserver60020-smallCompactions-1404255751957] regionserver.CompactSplitThread: Not compacting usertable,user3,1404257714774.a59b89c303508bc329bc1095d0beb9e5. because compaction request was cancelled
2014-07-01 16:37:35,891 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1404255227840/sceplus-vm48.almaden.ibm.com%2C60020%2C1404255227840.1404257852289 with entries=74, filesize=61.3m; new WAL /hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1404255227840/sceplus-vm48.almaden.ibm.com%2C60020%2C1404255227840.1404257855859
2014-07-01 16:37:35,930 DEBUG [MemStoreFlusher.1] util.FSUtils: DFS Client does not support most favored nodes create; using default create
2014-07-01 16:37:36,059 DEBUG [RpcServer.handler=28,port=60020] regionserver.HRegion: Flush requested on usertable,user9,1404257714774.84635e5ca79006f09dceafa095992aab.
2014-07-01 16:37:36,127 DEBUG [MemStoreFlusher.0] util.FSUtils: DFS Client does not support most favored nodes create; using default create
2014-07-01 16:37:38,092 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-01 16:37:38,120 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1404255227840/sceplus-vm48.almaden.ibm.com%2C60020%2C1404255227840.1404257855859 with entries=73, filesize=61.2m; new WAL /hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1404255227840/sceplus-vm48.almaden.ibm.com%2C60020%2C1404255227840.1404257858093
2014-07-01 16:37:40,350 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-01 16:37:40,440 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 23008 synced till here 22989
2014-07-01 16:37:40,542 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1404255227840/sceplus-vm48.almaden.ibm.com%2C60020%2C1404255227840.1404257858093 with entries=96, filesize=78.6m; new WAL /hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1404255227840/sceplus-vm48.almaden.ibm.com%2C60020%2C1404255227840.1404257860351
2014-07-01 16:37:42,549 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-01 16:37:42,717 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 23101 synced till here 23088
2014-07-01 16:37:42,935 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1404255227840/sceplus-vm48.almaden.ibm.com%2C60020%2C1404255227840.1404257860351 with entries=93, filesize=73.2m; new WAL /hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1404255227840/sceplus-vm48.almaden.ibm.com%2C60020%2C1404255227840.1404257862549
2014-07-01 16:37:45,542 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-01 16:37:45,589 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 23187 synced till here 23178
2014-07-01 16:37:45,886 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1404255227840/sceplus-vm48.almaden.ibm.com%2C60020%2C1404255227840.1404257862549 with entries=86, filesize=70.9m; new WAL /hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1404255227840/sceplus-vm48.almaden.ibm.com%2C60020%2C1404255227840.1404257865542
2014-07-01 16:37:46,555 INFO  [MemStoreFlusher.1] regionserver.DefaultStoreFlusher: Flushed, sequenceid=533, memsize=259.2m, hasBloomFilter=true, into tmp file hdfs://master:54310/hbase/data/default/usertable/c831a0ac8e8f14e82fdaef2ba3a5e1f6/.tmp/d678949f2cd7450599841df912ad40c5
2014-07-01 16:37:46,577 DEBUG [MemStoreFlusher.1] regionserver.HRegionFileSystem: Committing store file hdfs://master:54310/hbase/data/default/usertable/c831a0ac8e8f14e82fdaef2ba3a5e1f6/.tmp/d678949f2cd7450599841df912ad40c5 as hdfs://master:54310/hbase/data/default/usertable/c831a0ac8e8f14e82fdaef2ba3a5e1f6/family/d678949f2cd7450599841df912ad40c5
2014-07-01 16:37:46,592 INFO  [MemStoreFlusher.1] regionserver.HStore: Added hdfs://master:54310/hbase/data/default/usertable/c831a0ac8e8f14e82fdaef2ba3a5e1f6/family/d678949f2cd7450599841df912ad40c5, entries=943600, sequenceid=533, filesize=67.3m
2014-07-01 16:37:46,592 INFO  [MemStoreFlusher.1] regionserver.HRegion: Finished memstore flush of ~305.1m/319871440, currentsize=129.2m/135509040 for region usertable,user4,1404257714774.c831a0ac8e8f14e82fdaef2ba3a5e1f6. in 10845ms, sequenceid=533, compaction requested=true
2014-07-01 16:37:46,593 DEBUG [MemStoreFlusher.1] regionserver.CompactSplitThread: Small Compaction requested: system; Because: MemStoreFlusher.1; compaction_queue=(0:0), split_queue=0, merge_queue=0
2014-07-01 16:37:46,593 DEBUG [regionserver60020-smallCompactions-1404255751957] compactions.RatioBasedCompactionPolicy: Selecting compaction from 3 store files, 0 compacting, 3 eligible, 20 blocking
2014-07-01 16:37:46,593 DEBUG [MemStoreFlusher.1] regionserver.HRegion: Started memstore flush for usertable,user9,1404257714774.84635e5ca79006f09dceafa095992aab., current region memstore size 290.7m
2014-07-01 16:37:46,593 DEBUG [regionserver60020-smallCompactions-1404255751957] compactions.RatioBasedCompactionPolicy: Some files are too large. Excluding 3 files from compaction candidates
2014-07-01 16:37:46,594 DEBUG [regionserver60020-smallCompactions-1404255751957] compactions.ExploringCompactionPolicy: Exploring compaction algorithm has selected 0 files of size 0 starting at candidate #-1 after considering 0 permutations with 0 in ratio
2014-07-01 16:37:46,594 DEBUG [regionserver60020-smallCompactions-1404255751957] compactions.RatioBasedCompactionPolicy: Not compacting files because we only have 0 files ready for compaction. Need 3 to initiate.
2014-07-01 16:37:46,594 DEBUG [regionserver60020-smallCompactions-1404255751957] regionserver.CompactSplitThread: Not compacting usertable,user4,1404257714774.c831a0ac8e8f14e82fdaef2ba3a5e1f6. because compaction request was cancelled
2014-07-01 16:37:46,680 INFO  [MemStoreFlusher.0] regionserver.DefaultStoreFlusher: Flushed, sequenceid=533, memsize=257.1m, hasBloomFilter=true, into tmp file hdfs://master:54310/hbase/data/default/usertable/b6ae40892bbb2a1c545234ed91d62d1a/.tmp/e2ae951201c84f84ab25b4cab4f26b68
2014-07-01 16:37:47,691 DEBUG [MemStoreFlusher.0] regionserver.HRegionFileSystem: Committing store file hdfs://master:54310/hbase/data/default/usertable/b6ae40892bbb2a1c545234ed91d62d1a/.tmp/e2ae951201c84f84ab25b4cab4f26b68 as hdfs://master:54310/hbase/data/default/usertable/b6ae40892bbb2a1c545234ed91d62d1a/family/e2ae951201c84f84ab25b4cab4f26b68
2014-07-01 16:37:47,701 INFO  [MemStoreFlusher.0] regionserver.HStore: Added hdfs://master:54310/hbase/data/default/usertable/b6ae40892bbb2a1c545234ed91d62d1a/family/e2ae951201c84f84ab25b4cab4f26b68, entries=935940, sequenceid=533, filesize=66.7m
2014-07-01 16:37:47,701 INFO  [MemStoreFlusher.0] regionserver.HRegion: Finished memstore flush of ~303.3m/318057520, currentsize=123.9m/129913840 for region usertable,user8,1404257714774.b6ae40892bbb2a1c545234ed91d62d1a. in 11825ms, sequenceid=533, compaction requested=true
2014-07-01 16:37:47,701 DEBUG [MemStoreFlusher.0] regionserver.CompactSplitThread: Small Compaction requested: system; Because: MemStoreFlusher.0; compaction_queue=(0:0), split_queue=0, merge_queue=0
2014-07-01 16:37:47,701 DEBUG [regionserver60020-smallCompactions-1404255751957] compactions.RatioBasedCompactionPolicy: Selecting compaction from 3 store files, 0 compacting, 3 eligible, 20 blocking
2014-07-01 16:37:47,701 DEBUG [regionserver60020-smallCompactions-1404255751957] compactions.RatioBasedCompactionPolicy: Some files are too large. Excluding 3 files from compaction candidates
2014-07-01 16:37:47,702 DEBUG [regionserver60020-smallCompactions-1404255751957] compactions.ExploringCompactionPolicy: Exploring compaction algorithm has selected 0 files of size 0 starting at candidate #-1 after considering 0 permutations with 0 in ratio
2014-07-01 16:37:47,702 DEBUG [regionserver60020-smallCompactions-1404255751957] compactions.RatioBasedCompactionPolicy: Not compacting files because we only have 0 files ready for compaction. Need 3 to initiate.
2014-07-01 16:37:47,702 DEBUG [regionserver60020-smallCompactions-1404255751957] regionserver.CompactSplitThread: Not compacting usertable,user8,1404257714774.b6ae40892bbb2a1c545234ed91d62d1a. because compaction request was cancelled
2014-07-01 16:37:47,851 DEBUG [MemStoreFlusher.1] util.FSUtils: DFS Client does not support most favored nodes create; using default create
2014-07-01 16:37:48,186 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-01 16:37:48,228 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 23277 synced till here 23271
2014-07-01 16:37:48,263 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1404255227840/sceplus-vm48.almaden.ibm.com%2C60020%2C1404255227840.1404257865542 with entries=90, filesize=72.7m; new WAL /hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1404255227840/sceplus-vm48.almaden.ibm.com%2C60020%2C1404255227840.1404257868186
2014-07-01 16:37:49,919 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-01 16:37:50,036 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 23372 synced till here 23366
2014-07-01 16:37:50,148 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1404255227840/sceplus-vm48.almaden.ibm.com%2C60020%2C1404255227840.1404257868186 with entries=95, filesize=77.7m; new WAL /hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1404255227840/sceplus-vm48.almaden.ibm.com%2C60020%2C1404255227840.1404257869919
2014-07-01 16:37:50,437 DEBUG [RpcServer.handler=12,port=60020] regionserver.HRegion: Flush requested on usertable,user2,1404257714774.2ed053f05f5eec791a57de24e16f0043.
2014-07-01 16:37:50,438 DEBUG [MemStoreFlusher.0] regionserver.HRegion: Started memstore flush for usertable,user2,1404257714774.2ed053f05f5eec791a57de24e16f0043., current region memstore size 256.5m
2014-07-01 16:37:50,851 DEBUG [MemStoreFlusher.0] util.FSUtils: DFS Client does not support most favored nodes create; using default create
2014-07-01 16:37:52,046 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-01 16:37:52,055 DEBUG [RpcServer.handler=36,port=60020] regionserver.HRegion: Flush requested on usertable,user3,1404257714774.a59b89c303508bc329bc1095d0beb9e5.
2014-07-01 16:37:52,111 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1404255227840/sceplus-vm48.almaden.ibm.com%2C60020%2C1404255227840.1404257869919 with entries=77, filesize=63.7m; new WAL /hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1404255227840/sceplus-vm48.almaden.ibm.com%2C60020%2C1404255227840.1404257872047
2014-07-01 16:37:53,620 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-01 16:37:54,135 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 23528 synced till here 23522
2014-07-01 16:37:54,209 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1404255227840/sceplus-vm48.almaden.ibm.com%2C60020%2C1404255227840.1404257872047 with entries=79, filesize=66.3m; new WAL /hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1404255227840/sceplus-vm48.almaden.ibm.com%2C60020%2C1404255227840.1404257873621
2014-07-01 16:37:54,864 INFO  [MemStoreFlusher.1] regionserver.DefaultStoreFlusher: Flushed, sequenceid=601, memsize=108.8m, hasBloomFilter=true, into tmp file hdfs://master:54310/hbase/data/default/usertable/84635e5ca79006f09dceafa095992aab/.tmp/382d0b87c95e48ebaa04b40d82719249
2014-07-01 16:37:54,879 DEBUG [MemStoreFlusher.1] regionserver.HRegionFileSystem: Committing store file hdfs://master:54310/hbase/data/default/usertable/84635e5ca79006f09dceafa095992aab/.tmp/382d0b87c95e48ebaa04b40d82719249 as hdfs://master:54310/hbase/data/default/usertable/84635e5ca79006f09dceafa095992aab/family/382d0b87c95e48ebaa04b40d82719249
2014-07-01 16:37:54,901 INFO  [MemStoreFlusher.1] regionserver.HStore: Added hdfs://master:54310/hbase/data/default/usertable/84635e5ca79006f09dceafa095992aab/family/382d0b87c95e48ebaa04b40d82719249, entries=396120, sequenceid=601, filesize=28.2m
2014-07-01 16:37:54,901 INFO  [MemStoreFlusher.1] regionserver.HRegion: Finished memstore flush of ~290.7m/304792640, currentsize=38.5m/40382640 for region usertable,user9,1404257714774.84635e5ca79006f09dceafa095992aab. in 8308ms, sequenceid=601, compaction requested=false
2014-07-01 16:37:54,901 DEBUG [MemStoreFlusher.1] regionserver.HRegion: Started memstore flush for usertable,user3,1404257714774.a59b89c303508bc329bc1095d0beb9e5., current region memstore size 306.3m
2014-07-01 16:37:54,957 DEBUG [RpcServer.handler=29,port=60020] regionserver.HRegion: Flush requested on usertable,user4,1404257714774.c831a0ac8e8f14e82fdaef2ba3a5e1f6.
2014-07-01 16:37:55,131 DEBUG [MemStoreFlusher.1] util.FSUtils: DFS Client does not support most favored nodes create; using default create
2014-07-01 16:37:55,336 DEBUG [RpcServer.handler=28,port=60020] regionserver.HRegion: Flush requested on usertable,user8,1404257714774.b6ae40892bbb2a1c545234ed91d62d1a.
2014-07-01 16:37:55,561 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-01 16:37:55,859 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1404255227840/sceplus-vm48.almaden.ibm.com%2C60020%2C1404255227840.1404257873621 with entries=86, filesize=67.5m; new WAL /hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1404255227840/sceplus-vm48.almaden.ibm.com%2C60020%2C1404255227840.1404257875561
2014-07-01 16:37:55,859 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1404255227840/sceplus-vm48.almaden.ibm.com%2C60020%2C1404255227840.1404257365840
2014-07-01 16:37:55,859 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1404255227840/sceplus-vm48.almaden.ibm.com%2C60020%2C1404255227840.1404257733475
2014-07-01 16:37:55,859 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1404255227840/sceplus-vm48.almaden.ibm.com%2C60020%2C1404255227840.1404257738103
2014-07-01 16:37:55,859 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1404255227840/sceplus-vm48.almaden.ibm.com%2C60020%2C1404255227840.1404257743000
2014-07-01 16:37:55,859 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1404255227840/sceplus-vm48.almaden.ibm.com%2C60020%2C1404255227840.1404257753902
2014-07-01 16:37:55,859 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1404255227840/sceplus-vm48.almaden.ibm.com%2C60020%2C1404255227840.1404257758671
2014-07-01 16:37:55,859 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1404255227840/sceplus-vm48.almaden.ibm.com%2C60020%2C1404255227840.1404257764193
2014-07-01 16:37:55,860 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1404255227840/sceplus-vm48.almaden.ibm.com%2C60020%2C1404255227840.1404257770494
2014-07-01 16:37:55,860 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1404255227840/sceplus-vm48.almaden.ibm.com%2C60020%2C1404255227840.1404257778286
2014-07-01 16:37:55,860 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1404255227840/sceplus-vm48.almaden.ibm.com%2C60020%2C1404255227840.1404257782186
2014-07-01 16:37:55,860 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1404255227840/sceplus-vm48.almaden.ibm.com%2C60020%2C1404255227840.1404257784980
2014-07-01 16:37:55,860 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1404255227840/sceplus-vm48.almaden.ibm.com%2C60020%2C1404255227840.1404257790423
2014-07-01 16:37:55,860 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1404255227840/sceplus-vm48.almaden.ibm.com%2C60020%2C1404255227840.1404257793671
2014-07-01 16:37:55,860 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1404255227840/sceplus-vm48.almaden.ibm.com%2C60020%2C1404255227840.1404257797717
2014-07-01 16:37:55,860 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1404255227840/sceplus-vm48.almaden.ibm.com%2C60020%2C1404255227840.1404257801009
2014-07-01 16:37:55,860 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1404255227840/sceplus-vm48.almaden.ibm.com%2C60020%2C1404255227840.1404257805013
2014-07-01 16:37:55,860 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1404255227840/sceplus-vm48.almaden.ibm.com%2C60020%2C1404255227840.1404257811932
2014-07-01 16:37:55,861 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1404255227840/sceplus-vm48.almaden.ibm.com%2C60020%2C1404255227840.1404257816213
2014-07-01 16:37:55,861 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1404255227840/sceplus-vm48.almaden.ibm.com%2C60020%2C1404255227840.1404257820001
2014-07-01 16:37:55,861 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1404255227840/sceplus-vm48.almaden.ibm.com%2C60020%2C1404255227840.1404257824001
2014-07-01 16:37:55,861 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1404255227840/sceplus-vm48.almaden.ibm.com%2C60020%2C1404255227840.1404257827826
2014-07-01 16:37:55,862 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1404255227840/sceplus-vm48.almaden.ibm.com%2C60020%2C1404255227840.1404257834116
2014-07-01 16:37:55,862 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1404255227840/sceplus-vm48.almaden.ibm.com%2C60020%2C1404255227840.1404257839035
2014-07-01 16:37:55,862 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1404255227840/sceplus-vm48.almaden.ibm.com%2C60020%2C1404255227840.1404257842589
2014-07-01 16:37:58,449 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-01 16:37:58,490 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1404255227840/sceplus-vm48.almaden.ibm.com%2C60020%2C1404255227840.1404257875561 with entries=83, filesize=60.9m; new WAL /hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1404255227840/sceplus-vm48.almaden.ibm.com%2C60020%2C1404255227840.1404257878449
2014-07-01 16:38:00,130 INFO  [MemStoreFlusher.0] regionserver.DefaultStoreFlusher: Flushed, sequenceid=652, memsize=217.7m, hasBloomFilter=true, into tmp file hdfs://master:54310/hbase/data/default/usertable/2ed053f05f5eec791a57de24e16f0043/.tmp/a8f174f332b847d4a48305e6f777f4bd
2014-07-01 16:38:00,141 DEBUG [MemStoreFlusher.0] regionserver.HRegionFileSystem: Committing store file hdfs://master:54310/hbase/data/default/usertable/2ed053f05f5eec791a57de24e16f0043/.tmp/a8f174f332b847d4a48305e6f777f4bd as hdfs://master:54310/hbase/data/default/usertable/2ed053f05f5eec791a57de24e16f0043/family/a8f174f332b847d4a48305e6f777f4bd
2014-07-01 16:38:00,151 INFO  [MemStoreFlusher.0] regionserver.HStore: Added hdfs://master:54310/hbase/data/default/usertable/2ed053f05f5eec791a57de24e16f0043/family/a8f174f332b847d4a48305e6f777f4bd, entries=792750, sequenceid=652, filesize=56.5m
2014-07-01 16:38:00,151 INFO  [MemStoreFlusher.0] regionserver.HRegion: Finished memstore flush of ~260.1m/272696400, currentsize=97.4m/102085200 for region usertable,user2,1404257714774.2ed053f05f5eec791a57de24e16f0043. in 9713ms, sequenceid=652, compaction requested=true
2014-07-01 16:38:00,151 DEBUG [MemStoreFlusher.0] regionserver.CompactSplitThread: Small Compaction requested: system; Because: MemStoreFlusher.0; compaction_queue=(0:0), split_queue=0, merge_queue=0
2014-07-01 16:38:00,152 DEBUG [regionserver60020-smallCompactions-1404255751957] compactions.RatioBasedCompactionPolicy: Selecting compaction from 4 store files, 0 compacting, 4 eligible, 20 blocking
2014-07-01 16:38:00,152 DEBUG [regionserver60020-smallCompactions-1404255751957] compactions.RatioBasedCompactionPolicy: Some files are too large. Excluding 4 files from compaction candidates
2014-07-01 16:38:00,152 DEBUG [regionserver60020-smallCompactions-1404255751957] compactions.ExploringCompactionPolicy: Exploring compaction algorithm has selected 0 files of size 0 starting at candidate #-1 after considering 0 permutations with 0 in ratio
2014-07-01 16:38:00,152 DEBUG [regionserver60020-smallCompactions-1404255751957] compactions.RatioBasedCompactionPolicy: Not compacting files because we only have 0 files ready for compaction. Need 3 to initiate.
2014-07-01 16:38:00,152 DEBUG [MemStoreFlusher.0] regionserver.HRegion: Started memstore flush for usertable,user4,1404257714774.c831a0ac8e8f14e82fdaef2ba3a5e1f6., current region memstore size 296.0m
2014-07-01 16:38:00,152 DEBUG [regionserver60020-smallCompactions-1404255751957] regionserver.CompactSplitThread: Not compacting usertable,user2,1404257714774.2ed053f05f5eec791a57de24e16f0043. because compaction request was cancelled
2014-07-01 16:38:00,418 DEBUG [MemStoreFlusher.0] util.FSUtils: DFS Client does not support most favored nodes create; using default create
2014-07-01 16:38:01,225 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-01 16:38:01,270 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 23778 synced till here 23775
2014-07-01 16:38:01,315 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1404255227840/sceplus-vm48.almaden.ibm.com%2C60020%2C1404255227840.1404257878449 with entries=81, filesize=66.7m; new WAL /hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1404255227840/sceplus-vm48.almaden.ibm.com%2C60020%2C1404255227840.1404257881225
2014-07-01 16:38:01,315 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1404255227840/sceplus-vm48.almaden.ibm.com%2C60020%2C1404255227840.1404257845769
2014-07-01 16:38:02,718 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-01 16:38:02,891 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1404255227840/sceplus-vm48.almaden.ibm.com%2C60020%2C1404255227840.1404257881225 with entries=88, filesize=65.5m; new WAL /hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1404255227840/sceplus-vm48.almaden.ibm.com%2C60020%2C1404255227840.1404257882719
2014-07-01 16:38:05,274 INFO  [MemStoreFlusher.1] regionserver.DefaultStoreFlusher: Flushed, sequenceid=683, memsize=260.7m, hasBloomFilter=true, into tmp file hdfs://master:54310/hbase/data/default/usertable/a59b89c303508bc329bc1095d0beb9e5/.tmp/5fd824a23dc34ecb9be6e116f498ba02
2014-07-01 16:38:05,289 DEBUG [MemStoreFlusher.1] regionserver.HRegionFileSystem: Committing store file hdfs://master:54310/hbase/data/default/usertable/a59b89c303508bc329bc1095d0beb9e5/.tmp/5fd824a23dc34ecb9be6e116f498ba02 as hdfs://master:54310/hbase/data/default/usertable/a59b89c303508bc329bc1095d0beb9e5/family/5fd824a23dc34ecb9be6e116f498ba02
2014-07-01 16:38:05,359 INFO  [MemStoreFlusher.1] regionserver.HStore: Added hdfs://master:54310/hbase/data/default/usertable/a59b89c303508bc329bc1095d0beb9e5/family/5fd824a23dc34ecb9be6e116f498ba02, entries=949310, sequenceid=683, filesize=67.6m
2014-07-01 16:38:05,360 INFO  [MemStoreFlusher.1] regionserver.HRegion: Finished memstore flush of ~306.3m/321166880, currentsize=112.8m/118266400 for region usertable,user3,1404257714774.a59b89c303508bc329bc1095d0beb9e5. in 10459ms, sequenceid=683, compaction requested=true
2014-07-01 16:38:05,361 DEBUG [MemStoreFlusher.1] regionserver.CompactSplitThread: Small Compaction requested: system; Because: MemStoreFlusher.1; compaction_queue=(0:0), split_queue=0, merge_queue=0
2014-07-01 16:38:05,361 DEBUG [regionserver60020-smallCompactions-1404255751957] compactions.RatioBasedCompactionPolicy: Selecting compaction from 4 store files, 0 compacting, 4 eligible, 20 blocking
2014-07-01 16:38:05,361 DEBUG [MemStoreFlusher.1] regionserver.HRegion: Started memstore flush for usertable,user8,1404257714774.b6ae40892bbb2a1c545234ed91d62d1a., current region memstore size 361.1m
2014-07-01 16:38:05,361 DEBUG [regionserver60020-smallCompactions-1404255751957] compactions.RatioBasedCompactionPolicy: Some files are too large. Excluding 4 files from compaction candidates
2014-07-01 16:38:05,361 DEBUG [regionserver60020-smallCompactions-1404255751957] compactions.ExploringCompactionPolicy: Exploring compaction algorithm has selected 0 files of size 0 starting at candidate #-1 after considering 0 permutations with 0 in ratio
2014-07-01 16:38:05,361 DEBUG [regionserver60020-smallCompactions-1404255751957] compactions.RatioBasedCompactionPolicy: Not compacting files because we only have 0 files ready for compaction. Need 3 to initiate.
2014-07-01 16:38:05,361 DEBUG [regionserver60020-smallCompactions-1404255751957] regionserver.CompactSplitThread: Not compacting usertable,user3,1404257714774.a59b89c303508bc329bc1095d0beb9e5. because compaction request was cancelled
2014-07-01 16:38:05,654 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-01 16:38:05,714 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 23953 synced till here 23939
2014-07-01 16:38:05,782 DEBUG [MemStoreFlusher.1] util.FSUtils: DFS Client does not support most favored nodes create; using default create
2014-07-01 16:38:05,865 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1404255227840/sceplus-vm48.almaden.ibm.com%2C60020%2C1404255227840.1404257882719 with entries=87, filesize=73.9m; new WAL /hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1404255227840/sceplus-vm48.almaden.ibm.com%2C60020%2C1404255227840.1404257885654
2014-07-01 16:38:05,865 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1404255227840/sceplus-vm48.almaden.ibm.com%2C60020%2C1404255227840.1404257848711
2014-07-01 16:38:07,764 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-01 16:38:09,266 INFO  [JvmPauseMonitor] util.JvmPauseMonitor: Detected pause in JVM or host machine (eg GC): pause of approximately 1220ms
GC pool 'ParNew' had collection(s): count=1 time=1373ms
2014-07-01 16:38:09,354 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 24052 synced till here 24028
2014-07-01 16:38:09,650 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1404255227840/sceplus-vm48.almaden.ibm.com%2C60020%2C1404255227840.1404257885654 with entries=99, filesize=82.7m; new WAL /hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1404255227840/sceplus-vm48.almaden.ibm.com%2C60020%2C1404255227840.1404257887764
2014-07-01 16:38:11,771 DEBUG [RpcServer.handler=47,port=60020] regionserver.HRegion: Flush requested on usertable,user2,1404257714774.2ed053f05f5eec791a57de24e16f0043.
2014-07-01 16:38:12,237 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-01 16:38:12,382 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 24167 synced till here 24165
2014-07-01 16:38:12,443 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1404255227840/sceplus-vm48.almaden.ibm.com%2C60020%2C1404255227840.1404257887764 with entries=115, filesize=94.3m; new WAL /hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1404255227840/sceplus-vm48.almaden.ibm.com%2C60020%2C1404255227840.1404257892237
2014-07-01 16:38:14,503 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-01 16:38:14,527 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 24283 synced till here 24244
2014-07-01 16:38:14,663 INFO  [MemStoreFlusher.0] regionserver.DefaultStoreFlusher: Flushed, sequenceid=709, memsize=248.9m, hasBloomFilter=true, into tmp file hdfs://master:54310/hbase/data/default/usertable/c831a0ac8e8f14e82fdaef2ba3a5e1f6/.tmp/b0b436bff1a845d390fe26e7d56258c5
2014-07-01 16:38:14,684 DEBUG [MemStoreFlusher.0] regionserver.HRegionFileSystem: Committing store file hdfs://master:54310/hbase/data/default/usertable/c831a0ac8e8f14e82fdaef2ba3a5e1f6/.tmp/b0b436bff1a845d390fe26e7d56258c5 as hdfs://master:54310/hbase/data/default/usertable/c831a0ac8e8f14e82fdaef2ba3a5e1f6/family/b0b436bff1a845d390fe26e7d56258c5
2014-07-01 16:38:14,694 INFO  [MemStoreFlusher.0] regionserver.HStore: Added hdfs://master:54310/hbase/data/default/usertable/c831a0ac8e8f14e82fdaef2ba3a5e1f6/family/b0b436bff1a845d390fe26e7d56258c5, entries=906170, sequenceid=709, filesize=64.6m
2014-07-01 16:38:14,694 INFO  [MemStoreFlusher.0] regionserver.HRegion: Finished memstore flush of ~297.7m/312125840, currentsize=180.0m/188759760 for region usertable,user4,1404257714774.c831a0ac8e8f14e82fdaef2ba3a5e1f6. in 14542ms, sequenceid=709, compaction requested=true
2014-07-01 16:38:14,695 DEBUG [MemStoreFlusher.0] regionserver.CompactSplitThread: Small Compaction requested: system; Because: MemStoreFlusher.0; compaction_queue=(0:0), split_queue=0, merge_queue=0
2014-07-01 16:38:14,695 DEBUG [regionserver60020-smallCompactions-1404255751957] compactions.RatioBasedCompactionPolicy: Selecting compaction from 4 store files, 0 compacting, 4 eligible, 20 blocking
2014-07-01 16:38:14,695 DEBUG [regionserver60020-smallCompactions-1404255751957] compactions.RatioBasedCompactionPolicy: Some files are too large. Excluding 4 files from compaction candidates
2014-07-01 16:38:14,695 DEBUG [regionserver60020-smallCompactions-1404255751957] compactions.ExploringCompactionPolicy: Exploring compaction algorithm has selected 0 files of size 0 starting at candidate #-1 after considering 0 permutations with 0 in ratio
2014-07-01 16:38:14,695 DEBUG [MemStoreFlusher.0] regionserver.HRegion: Started memstore flush for usertable,user2,1404257714774.2ed053f05f5eec791a57de24e16f0043., current region memstore size 312.2m
2014-07-01 16:38:14,695 DEBUG [regionserver60020-smallCompactions-1404255751957] compactions.RatioBasedCompactionPolicy: Not compacting files because we only have 0 files ready for compaction. Need 3 to initiate.
2014-07-01 16:38:14,695 DEBUG [regionserver60020-smallCompactions-1404255751957] regionserver.CompactSplitThread: Not compacting usertable,user4,1404257714774.c831a0ac8e8f14e82fdaef2ba3a5e1f6. because compaction request was cancelled
2014-07-01 16:38:16,142 INFO  [JvmPauseMonitor] util.JvmPauseMonitor: Detected pause in JVM or host machine (eg GC): pause of approximately 1252ms
GC pool 'ParNew' had collection(s): count=1 time=1434ms
2014-07-01 16:38:16,244 DEBUG [RpcServer.handler=19,port=60020] regionserver.HRegion: Flush requested on usertable,user3,1404257714774.a59b89c303508bc329bc1095d0beb9e5.
2014-07-01 16:38:16,270 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1404255227840/sceplus-vm48.almaden.ibm.com%2C60020%2C1404255227840.1404257892237 with entries=116, filesize=99.8m; new WAL /hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1404255227840/sceplus-vm48.almaden.ibm.com%2C60020%2C1404255227840.1404257894504
2014-07-01 16:38:16,270 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1404255227840/sceplus-vm48.almaden.ibm.com%2C60020%2C1404255227840.1404257852289
2014-07-01 16:38:16,477 DEBUG [MemStoreFlusher.0] util.FSUtils: DFS Client does not support most favored nodes create; using default create
2014-07-01 16:38:18,242 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-01 16:38:18,280 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 24403 synced till here 24371
2014-07-01 16:38:18,804 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1404255227840/sceplus-vm48.almaden.ibm.com%2C60020%2C1404255227840.1404257894504 with entries=120, filesize=92.8m; new WAL /hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1404255227840/sceplus-vm48.almaden.ibm.com%2C60020%2C1404255227840.1404257898243
2014-07-01 16:38:20,831 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-01 16:38:20,878 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 24511 synced till here 24486
2014-07-01 16:38:21,182 DEBUG [RpcServer.handler=25,port=60020] regionserver.HRegion: Flush requested on usertable,user4,1404257714774.c831a0ac8e8f14e82fdaef2ba3a5e1f6.
2014-07-01 16:38:21,250 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1404255227840/sceplus-vm48.almaden.ibm.com%2C60020%2C1404255227840.1404257898243 with entries=108, filesize=85.9m; new WAL /hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1404255227840/sceplus-vm48.almaden.ibm.com%2C60020%2C1404255227840.1404257900832
2014-07-01 16:38:22,475 INFO  [JvmPauseMonitor] util.JvmPauseMonitor: Detected pause in JVM or host machine (eg GC): pause of approximately 1081ms
GC pool 'ParNew' had collection(s): count=1 time=1183ms
2014-07-01 16:38:23,609 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-01 16:38:25,366 INFO  [JvmPauseMonitor] util.JvmPauseMonitor: Detected pause in JVM or host machine (eg GC): pause of approximately 1390ms
GC pool 'ParNew' had collection(s): count=1 time=1547ms
2014-07-01 16:38:25,404 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 24632 synced till here 24582
2014-07-01 16:38:26,033 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1404255227840/sceplus-vm48.almaden.ibm.com%2C60020%2C1404255227840.1404257900832 with entries=121, filesize=94.3m; new WAL /hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1404255227840/sceplus-vm48.almaden.ibm.com%2C60020%2C1404255227840.1404257903610
2014-07-01 16:38:27,842 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-01 16:38:27,942 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 24753 synced till here 24718
2014-07-01 16:38:28,263 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1404255227840/sceplus-vm48.almaden.ibm.com%2C60020%2C1404255227840.1404257903610 with entries=121, filesize=95.2m; new WAL /hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1404255227840/sceplus-vm48.almaden.ibm.com%2C60020%2C1404255227840.1404257907842
2014-07-01 16:38:30,715 INFO  [JvmPauseMonitor] util.JvmPauseMonitor: Detected pause in JVM or host machine (eg GC): pause of approximately 1520ms
GC pool 'ParNew' had collection(s): count=1 time=1842ms
2014-07-01 16:38:31,014 INFO  [MemStoreFlusher.1] regionserver.DefaultStoreFlusher: Flushed, sequenceid=749, memsize=300.1m, hasBloomFilter=true, into tmp file hdfs://master:54310/hbase/data/default/usertable/b6ae40892bbb2a1c545234ed91d62d1a/.tmp/36442fd3741842478a463d457cf9e5b3
2014-07-01 16:38:31,052 DEBUG [MemStoreFlusher.1] regionserver.HRegionFileSystem: Committing store file hdfs://master:54310/hbase/data/default/usertable/b6ae40892bbb2a1c545234ed91d62d1a/.tmp/36442fd3741842478a463d457cf9e5b3 as hdfs://master:54310/hbase/data/default/usertable/b6ae40892bbb2a1c545234ed91d62d1a/family/36442fd3741842478a463d457cf9e5b3
2014-07-01 16:38:31,068 INFO  [MemStoreFlusher.1] regionserver.HStore: Added hdfs://master:54310/hbase/data/default/usertable/b6ae40892bbb2a1c545234ed91d62d1a/family/36442fd3741842478a463d457cf9e5b3, entries=1092610, sequenceid=749, filesize=77.8m
2014-07-01 16:38:31,068 INFO  [MemStoreFlusher.1] regionserver.HRegion: Finished memstore flush of ~364.5m/382241120, currentsize=264.7m/277538880 for region usertable,user8,1404257714774.b6ae40892bbb2a1c545234ed91d62d1a. in 25707ms, sequenceid=749, compaction requested=true
2014-07-01 16:38:31,069 DEBUG [MemStoreFlusher.1] regionserver.CompactSplitThread: Small Compaction requested: system; Because: MemStoreFlusher.1; compaction_queue=(0:0), split_queue=0, merge_queue=0
2014-07-01 16:38:31,069 DEBUG [regionserver60020-smallCompactions-1404255751957] compactions.RatioBasedCompactionPolicy: Selecting compaction from 4 store files, 0 compacting, 4 eligible, 20 blocking
2014-07-01 16:38:31,069 DEBUG [regionserver60020-smallCompactions-1404255751957] compactions.RatioBasedCompactionPolicy: Some files are too large. Excluding 4 files from compaction candidates
2014-07-01 16:38:31,069 DEBUG [MemStoreFlusher.1] regionserver.HRegion: Started memstore flush for usertable,user3,1404257714774.a59b89c303508bc329bc1095d0beb9e5., current region memstore size 439.0m
2014-07-01 16:38:31,069 DEBUG [regionserver60020-smallCompactions-1404255751957] compactions.ExploringCompactionPolicy: Exploring compaction algorithm has selected 0 files of size 0 starting at candidate #-1 after considering 0 permutations with 0 in ratio
2014-07-01 16:38:31,069 DEBUG [regionserver60020-smallCompactions-1404255751957] compactions.RatioBasedCompactionPolicy: Not compacting files because we only have 0 files ready for compaction. Need 3 to initiate.
2014-07-01 16:38:31,069 DEBUG [regionserver60020-smallCompactions-1404255751957] regionserver.CompactSplitThread: Not compacting usertable,user8,1404257714774.b6ae40892bbb2a1c545234ed91d62d1a. because compaction request was cancelled
2014-07-01 16:38:31,073 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-01 16:38:31,077 DEBUG [RpcServer.handler=48,port=60020] regionserver.HRegion: Flush requested on usertable,user8,1404257714774.b6ae40892bbb2a1c545234ed91d62d1a.
2014-07-01 16:38:31,126 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 24868 synced till here 24828
2014-07-01 16:38:31,381 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1404255227840/sceplus-vm48.almaden.ibm.com%2C60020%2C1404255227840.1404257907842 with entries=115, filesize=95.8m; new WAL /hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1404255227840/sceplus-vm48.almaden.ibm.com%2C60020%2C1404255227840.1404257911073
2014-07-01 16:38:31,381 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1404255227840/sceplus-vm48.almaden.ibm.com%2C60020%2C1404255227840.1404257855859
2014-07-01 16:38:31,381 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1404255227840/sceplus-vm48.almaden.ibm.com%2C60020%2C1404255227840.1404257858093
2014-07-01 16:38:31,381 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1404255227840/sceplus-vm48.almaden.ibm.com%2C60020%2C1404255227840.1404257860351
2014-07-01 16:38:31,381 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1404255227840/sceplus-vm48.almaden.ibm.com%2C60020%2C1404255227840.1404257862549
2014-07-01 16:38:31,625 DEBUG [MemStoreFlusher.1] util.FSUtils: DFS Client does not support most favored nodes create; using default create
2014-07-01 16:38:33,801 INFO  [JvmPauseMonitor] util.JvmPauseMonitor: Detected pause in JVM or host machine (eg GC): pause of approximately 1585ms
GC pool 'ParNew' had collection(s): count=1 time=1971ms
2014-07-01 16:38:34,292 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-01 16:38:34,344 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 25000 synced till here 24973
2014-07-01 16:38:34,657 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1404255227840/sceplus-vm48.almaden.ibm.com%2C60020%2C1404255227840.1404257911073 with entries=132, filesize=94.6m; new WAL /hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1404255227840/sceplus-vm48.almaden.ibm.com%2C60020%2C1404255227840.1404257914292
2014-07-01 16:38:35,853 INFO  [JvmPauseMonitor] util.JvmPauseMonitor: Detected pause in JVM or host machine (eg GC): pause of approximately 1050ms
GC pool 'ParNew' had collection(s): count=1 time=1176ms
2014-07-01 16:38:36,180 INFO  [MemStoreFlusher.0] regionserver.DefaultStoreFlusher: Flushed, sequenceid=841, memsize=223.4m, hasBloomFilter=true, into tmp file hdfs://master:54310/hbase/data/default/usertable/2ed053f05f5eec791a57de24e16f0043/.tmp/47d7628b94d241f6bf11a1a974423782
2014-07-01 16:38:36,212 DEBUG [MemStoreFlusher.0] regionserver.HRegionFileSystem: Committing store file hdfs://master:54310/hbase/data/default/usertable/2ed053f05f5eec791a57de24e16f0043/.tmp/47d7628b94d241f6bf11a1a974423782 as hdfs://master:54310/hbase/data/default/usertable/2ed053f05f5eec791a57de24e16f0043/family/47d7628b94d241f6bf11a1a974423782
2014-07-01 16:38:36,222 INFO  [MemStoreFlusher.0] regionserver.HStore: Added hdfs://master:54310/hbase/data/default/usertable/2ed053f05f5eec791a57de24e16f0043/family/47d7628b94d241f6bf11a1a974423782, entries=813300, sequenceid=841, filesize=58.0m
2014-07-01 16:38:36,222 INFO  [MemStoreFlusher.0] regionserver.HRegion: Finished memstore flush of ~314.0m/329226640, currentsize=249.8m/261891760 for region usertable,user2,1404257714774.2ed053f05f5eec791a57de24e16f0043. in 21527ms, sequenceid=841, compaction requested=true
2014-07-01 16:38:36,222 DEBUG [MemStoreFlusher.0] regionserver.CompactSplitThread: Small Compaction requested: system; Because: MemStoreFlusher.0; compaction_queue=(0:0), split_queue=0, merge_queue=0
2014-07-01 16:38:36,222 DEBUG [regionserver60020-smallCompactions-1404255751957] compactions.RatioBasedCompactionPolicy: Selecting compaction from 5 store files, 0 compacting, 5 eligible, 20 blocking
2014-07-01 16:38:36,223 DEBUG [regionserver60020-smallCompactions-1404255751957] compactions.RatioBasedCompactionPolicy: Some files are too large. Excluding 5 files from compaction candidates
2014-07-01 16:38:36,223 DEBUG [regionserver60020-smallCompactions-1404255751957] compactions.ExploringCompactionPolicy: Exploring compaction algorithm has selected 0 files of size 0 starting at candidate #-1 after considering 0 permutations with 0 in ratio
2014-07-01 16:38:36,223 DEBUG [MemStoreFlusher.0] regionserver.HRegion: Started memstore flush for usertable,user4,1404257714774.c831a0ac8e8f14e82fdaef2ba3a5e1f6., current region memstore size 424.2m
2014-07-01 16:38:36,223 DEBUG [regionserver60020-smallCompactions-1404255751957] compactions.RatioBasedCompactionPolicy: Not compacting files because we only have 0 files ready for compaction. Need 3 to initiate.
2014-07-01 16:38:36,223 DEBUG [regionserver60020-smallCompactions-1404255751957] regionserver.CompactSplitThread: Not compacting usertable,user2,1404257714774.2ed053f05f5eec791a57de24e16f0043. because compaction request was cancelled
2014-07-01 16:38:36,265 DEBUG [RpcServer.handler=38,port=60020] regionserver.HRegion: Flush requested on usertable,user2,1404257714774.2ed053f05f5eec791a57de24e16f0043.
2014-07-01 16:38:36,666 DEBUG [MemStoreFlusher.0] util.FSUtils: DFS Client does not support most favored nodes create; using default create
2014-07-01 16:38:36,871 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-01 16:38:36,891 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 25114 synced till here 25083
2014-07-01 16:38:36,997 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1404255227840/sceplus-vm48.almaden.ibm.com%2C60020%2C1404255227840.1404257914292 with entries=114, filesize=98.7m; new WAL /hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1404255227840/sceplus-vm48.almaden.ibm.com%2C60020%2C1404255227840.1404257916872
2014-07-01 16:38:38,497 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-01 16:38:38,528 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 25229 synced till here 25196
2014-07-01 16:38:38,675 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1404255227840/sceplus-vm48.almaden.ibm.com%2C60020%2C1404255227840.1404257916872 with entries=115, filesize=89.2m; new WAL /hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1404255227840/sceplus-vm48.almaden.ibm.com%2C60020%2C1404255227840.1404257918498
2014-07-01 16:38:40,381 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-01 16:38:40,556 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 25356 synced till here 25345
2014-07-01 16:38:40,699 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1404255227840/sceplus-vm48.almaden.ibm.com%2C60020%2C1404255227840.1404257918498 with entries=127, filesize=99.2m; new WAL /hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1404255227840/sceplus-vm48.almaden.ibm.com%2C60020%2C1404255227840.1404257920381
2014-07-01 16:38:42,068 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-01 16:38:42,142 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 25462 synced till here 25452
2014-07-01 16:38:42,208 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1404255227840/sceplus-vm48.almaden.ibm.com%2C60020%2C1404255227840.1404257920381 with entries=106, filesize=84.1m; new WAL /hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1404255227840/sceplus-vm48.almaden.ibm.com%2C60020%2C1404255227840.1404257922068
2014-07-01 16:38:44,389 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-01 16:38:44,408 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 25556 synced till here 25551
2014-07-01 16:38:44,471 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1404255227840/sceplus-vm48.almaden.ibm.com%2C60020%2C1404255227840.1404257922068 with entries=94, filesize=66.6m; new WAL /hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1404255227840/sceplus-vm48.almaden.ibm.com%2C60020%2C1404255227840.1404257924390
2014-07-01 16:38:44,730 INFO  [MemStoreFlusher.1] regionserver.DefaultStoreFlusher: Flushed, sequenceid=956, memsize=174.8m, hasBloomFilter=true, into tmp file hdfs://master:54310/hbase/data/default/usertable/a59b89c303508bc329bc1095d0beb9e5/.tmp/9ca2ff2dc84d4c68aecd5f3ea45e31d7
2014-07-01 16:38:44,754 DEBUG [MemStoreFlusher.1] regionserver.HRegionFileSystem: Committing store file hdfs://master:54310/hbase/data/default/usertable/a59b89c303508bc329bc1095d0beb9e5/.tmp/9ca2ff2dc84d4c68aecd5f3ea45e31d7 as hdfs://master:54310/hbase/data/default/usertable/a59b89c303508bc329bc1095d0beb9e5/family/9ca2ff2dc84d4c68aecd5f3ea45e31d7
2014-07-01 16:38:44,765 INFO  [MemStoreFlusher.1] regionserver.HStore: Added hdfs://master:54310/hbase/data/default/usertable/a59b89c303508bc329bc1095d0beb9e5/family/9ca2ff2dc84d4c68aecd5f3ea45e31d7, entries=636470, sequenceid=956, filesize=45.4m
2014-07-01 16:38:44,765 INFO  [MemStoreFlusher.1] regionserver.HRegion: Finished memstore flush of ~452.4m/474354800, currentsize=220.1m/230818400 for region usertable,user3,1404257714774.a59b89c303508bc329bc1095d0beb9e5. in 13696ms, sequenceid=956, compaction requested=true
2014-07-01 16:38:44,766 DEBUG [MemStoreFlusher.1] regionserver.CompactSplitThread: Small Compaction requested: system; Because: MemStoreFlusher.1; compaction_queue=(0:0), split_queue=0, merge_queue=0
2014-07-01 16:38:44,766 DEBUG [regionserver60020-smallCompactions-1404255751957] compactions.RatioBasedCompactionPolicy: Selecting compaction from 5 store files, 0 compacting, 5 eligible, 20 blocking
2014-07-01 16:38:44,766 DEBUG [regionserver60020-smallCompactions-1404255751957] compactions.RatioBasedCompactionPolicy: Some files are too large. Excluding 5 files from compaction candidates
2014-07-01 16:38:44,766 DEBUG [regionserver60020-smallCompactions-1404255751957] compactions.ExploringCompactionPolicy: Exploring compaction algorithm has selected 0 files of size 0 starting at candidate #-1 after considering 0 permutations with 0 in ratio
2014-07-01 16:38:44,766 DEBUG [MemStoreFlusher.1] regionserver.HRegion: Started memstore flush for usertable,user8,1404257714774.b6ae40892bbb2a1c545234ed91d62d1a., current region memstore size 556.9m
2014-07-01 16:38:44,766 DEBUG [regionserver60020-smallCompactions-1404255751957] compactions.RatioBasedCompactionPolicy: Not compacting files because we only have 0 files ready for compaction. Need 3 to initiate.
2014-07-01 16:38:44,766 DEBUG [regionserver60020-smallCompactions-1404255751957] regionserver.CompactSplitThread: Not compacting usertable,user3,1404257714774.a59b89c303508bc329bc1095d0beb9e5. because compaction request was cancelled
2014-07-01 16:38:45,210 INFO  [MemStoreFlusher.0] regionserver.DefaultStoreFlusher: Flushed, sequenceid=967, memsize=133.7m, hasBloomFilter=true, into tmp file hdfs://master:54310/hbase/data/default/usertable/c831a0ac8e8f14e82fdaef2ba3a5e1f6/.tmp/37465557b5b64abf80a839016e49d94e
2014-07-01 16:38:45,220 DEBUG [MemStoreFlusher.0] regionserver.HRegionFileSystem: Committing store file hdfs://master:54310/hbase/data/default/usertable/c831a0ac8e8f14e82fdaef2ba3a5e1f6/.tmp/37465557b5b64abf80a839016e49d94e as hdfs://master:54310/hbase/data/default/usertable/c831a0ac8e8f14e82fdaef2ba3a5e1f6/family/37465557b5b64abf80a839016e49d94e
2014-07-01 16:38:45,271 INFO  [MemStoreFlusher.0] regionserver.HStore: Added hdfs://master:54310/hbase/data/default/usertable/c831a0ac8e8f14e82fdaef2ba3a5e1f6/family/37465557b5b64abf80a839016e49d94e, entries=486840, sequenceid=967, filesize=34.7m
2014-07-01 16:38:45,271 INFO  [MemStoreFlusher.0] regionserver.HRegion: Finished memstore flush of ~427.5m/448216480, currentsize=211.1m/221372640 for region usertable,user4,1404257714774.c831a0ac8e8f14e82fdaef2ba3a5e1f6. in 9048ms, sequenceid=967, compaction requested=true
2014-07-01 16:38:45,273 DEBUG [MemStoreFlusher.0] regionserver.CompactSplitThread: Small Compaction requested: system; Because: MemStoreFlusher.0; compaction_queue=(0:0), split_queue=0, merge_queue=0
2014-07-01 16:38:45,274 DEBUG [regionserver60020-smallCompactions-1404255751957] compactions.RatioBasedCompactionPolicy: Selecting compaction from 5 store files, 0 compacting, 5 eligible, 20 blocking
2014-07-01 16:38:45,274 DEBUG [regionserver60020-smallCompactions-1404255751957] compactions.RatioBasedCompactionPolicy: Some files are too large. Excluding 5 files from compaction candidates
2014-07-01 16:38:45,274 DEBUG [MemStoreFlusher.0] regionserver.HRegion: Started memstore flush for usertable,user2,1404257714774.2ed053f05f5eec791a57de24e16f0043., current region memstore size 424.0m
2014-07-01 16:38:45,274 DEBUG [regionserver60020-smallCompactions-1404255751957] compactions.ExploringCompactionPolicy: Exploring compaction algorithm has selected 0 files of size 0 starting at candidate #-1 after considering 0 permutations with 0 in ratio
2014-07-01 16:38:45,274 DEBUG [regionserver60020-smallCompactions-1404255751957] compactions.RatioBasedCompactionPolicy: Not compacting files because we only have 0 files ready for compaction. Need 3 to initiate.
2014-07-01 16:38:45,274 DEBUG [regionserver60020-smallCompactions-1404255751957] regionserver.CompactSplitThread: Not compacting usertable,user4,1404257714774.c831a0ac8e8f14e82fdaef2ba3a5e1f6. because compaction request was cancelled
2014-07-01 16:38:45,723 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-01 16:38:45,760 DEBUG [MemStoreFlusher.1] util.FSUtils: DFS Client does not support most favored nodes create; using default create
2014-07-01 16:38:45,783 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 25642 synced till here 25635
2014-07-01 16:38:45,840 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1404255227840/sceplus-vm48.almaden.ibm.com%2C60020%2C1404255227840.1404257924390 with entries=86, filesize=69.1m; new WAL /hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1404255227840/sceplus-vm48.almaden.ibm.com%2C60020%2C1404255227840.1404257925723
2014-07-01 16:38:45,951 DEBUG [MemStoreFlusher.0] util.FSUtils: DFS Client does not support most favored nodes create; using default create
2014-07-01 16:38:46,368 DEBUG [RpcServer.handler=10,port=60020] regionserver.HRegion: Flush requested on usertable,user3,1404257714774.a59b89c303508bc329bc1095d0beb9e5.
2014-07-01 16:38:46,584 DEBUG [RpcServer.handler=45,port=60020] regionserver.HRegion: Flush requested on usertable,user9,1404257714774.84635e5ca79006f09dceafa095992aab.
2014-07-01 16:38:47,032 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-01 16:38:47,827 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 25727 synced till here 25720
2014-07-01 16:38:47,916 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1404255227840/sceplus-vm48.almaden.ibm.com%2C60020%2C1404255227840.1404257925723 with entries=85, filesize=69.8m; new WAL /hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1404255227840/sceplus-vm48.almaden.ibm.com%2C60020%2C1404255227840.1404257927032
2014-07-01 16:38:47,929 DEBUG [LruStats #0] hfile.LruBlockCache: Total=4.16 MB, free=3.95 GB, max=3.96 GB, blocks=0, accesses=10456985, hits=10201085, hitRatio=97.55%, , cachingAccesses=10269046, cachingHits=10189981, cachingHitsRatio=99.23%, evictions=2, evicted=78407, evictedPerRun=39203.5
2014-07-01 16:38:48,247 DEBUG [RpcServer.handler=1,port=60020] regionserver.HRegion: Flush requested on usertable,user4,1404257714774.c831a0ac8e8f14e82fdaef2ba3a5e1f6.
2014-07-01 16:38:49,339 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-01 16:38:50,194 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 25817 synced till here 25808
2014-07-01 16:38:50,259 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1404255227840/sceplus-vm48.almaden.ibm.com%2C60020%2C1404255227840.1404257927032 with entries=90, filesize=72.7m; new WAL /hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1404255227840/sceplus-vm48.almaden.ibm.com%2C60020%2C1404255227840.1404257929339
2014-07-01 16:38:51,814 INFO  [MemStoreFlusher.0] regionserver.DefaultStoreFlusher: Flushed, sequenceid=1097, memsize=84.1m, hasBloomFilter=true, into tmp file hdfs://master:54310/hbase/data/default/usertable/2ed053f05f5eec791a57de24e16f0043/.tmp/7356a65663754bff9b8f4e940074e99a
2014-07-01 16:38:51,817 INFO  [MemStoreFlusher.1] regionserver.DefaultStoreFlusher: Flushed, sequenceid=1085, memsize=75.9m, hasBloomFilter=true, into tmp file hdfs://master:54310/hbase/data/default/usertable/b6ae40892bbb2a1c545234ed91d62d1a/.tmp/61f848da3a094b19bbb7a9c5d603a4a1
2014-07-01 16:38:51,836 DEBUG [MemStoreFlusher.1] regionserver.HRegionFileSystem: Committing store file hdfs://master:54310/hbase/data/default/usertable/b6ae40892bbb2a1c545234ed91d62d1a/.tmp/61f848da3a094b19bbb7a9c5d603a4a1 as hdfs://master:54310/hbase/data/default/usertable/b6ae40892bbb2a1c545234ed91d62d1a/family/61f848da3a094b19bbb7a9c5d603a4a1
2014-07-01 16:38:51,836 DEBUG [MemStoreFlusher.0] regionserver.HRegionFileSystem: Committing store file hdfs://master:54310/hbase/data/default/usertable/2ed053f05f5eec791a57de24e16f0043/.tmp/7356a65663754bff9b8f4e940074e99a as hdfs://master:54310/hbase/data/default/usertable/2ed053f05f5eec791a57de24e16f0043/family/7356a65663754bff9b8f4e940074e99a
2014-07-01 16:38:51,856 INFO  [MemStoreFlusher.1] regionserver.HStore: Added hdfs://master:54310/hbase/data/default/usertable/b6ae40892bbb2a1c545234ed91d62d1a/family/61f848da3a094b19bbb7a9c5d603a4a1, entries=276430, sequenceid=1085, filesize=19.7m
2014-07-01 16:38:51,856 INFO  [MemStoreFlusher.0] regionserver.HStore: Added hdfs://master:54310/hbase/data/default/usertable/2ed053f05f5eec791a57de24e16f0043/family/7356a65663754bff9b8f4e940074e99a, entries=306320, sequenceid=1097, filesize=21.8m
2014-07-01 16:38:51,856 INFO  [MemStoreFlusher.1] regionserver.HRegion: Finished memstore flush of ~564.1m/591525600, currentsize=100.8m/105732880 for region usertable,user8,1404257714774.b6ae40892bbb2a1c545234ed91d62d1a. in 7090ms, sequenceid=1085, compaction requested=true
2014-07-01 16:38:51,856 INFO  [MemStoreFlusher.0] regionserver.HRegion: Finished memstore flush of ~424.0m/444566720, currentsize=80.6m/84519440 for region usertable,user2,1404257714774.2ed053f05f5eec791a57de24e16f0043. in 6582ms, sequenceid=1097, compaction requested=true
2014-07-01 16:38:51,857 DEBUG [MemStoreFlusher.1] regionserver.CompactSplitThread: Small Compaction requested: system; Because: MemStoreFlusher.1; compaction_queue=(0:0), split_queue=0, merge_queue=0
2014-07-01 16:38:51,857 DEBUG [regionserver60020-smallCompactions-1404255751957] compactions.RatioBasedCompactionPolicy: Selecting compaction from 5 store files, 0 compacting, 5 eligible, 20 blocking
2014-07-01 16:38:51,857 DEBUG [regionserver60020-smallCompactions-1404255751957] compactions.RatioBasedCompactionPolicy: Some files are too large. Excluding 5 files from compaction candidates
2014-07-01 16:38:51,857 DEBUG [MemStoreFlusher.0] regionserver.CompactSplitThread: Small Compaction requested: system; Because: MemStoreFlusher.0; compaction_queue=(0:1), split_queue=0, merge_queue=0
2014-07-01 16:38:51,857 DEBUG [regionserver60020-smallCompactions-1404255751957] compactions.ExploringCompactionPolicy: Exploring compaction algorithm has selected 0 files of size 0 starting at candidate #-1 after considering 0 permutations with 0 in ratio
2014-07-01 16:38:51,858 DEBUG [regionserver60020-smallCompactions-1404255751957] compactions.RatioBasedCompactionPolicy: Not compacting files because we only have 0 files ready for compaction. Need 3 to initiate.
2014-07-01 16:38:51,858 DEBUG [regionserver60020-smallCompactions-1404255751957] regionserver.CompactSplitThread: Not compacting usertable,user8,1404257714774.b6ae40892bbb2a1c545234ed91d62d1a. because compaction request was cancelled
2014-07-01 16:38:51,858 DEBUG [MemStoreFlusher.1] regionserver.HRegion: Started memstore flush for usertable,user3,1404257714774.a59b89c303508bc329bc1095d0beb9e5., current region memstore size 318.1m
2014-07-01 16:38:51,858 DEBUG [regionserver60020-smallCompactions-1404255751957] compactions.RatioBasedCompactionPolicy: Selecting compaction from 6 store files, 0 compacting, 6 eligible, 20 blocking
2014-07-01 16:38:51,858 DEBUG [MemStoreFlusher.0] regionserver.HRegion: Started memstore flush for usertable,user9,1404257714774.84635e5ca79006f09dceafa095992aab., current region memstore size 276.3m
2014-07-01 16:38:51,858 DEBUG [regionserver60020-smallCompactions-1404255751957] compactions.RatioBasedCompactionPolicy: Some files are too large. Excluding 6 files from compaction candidates
2014-07-01 16:38:51,858 DEBUG [regionserver60020-smallCompactions-1404255751957] compactions.ExploringCompactionPolicy: Exploring compaction algorithm has selected 0 files of size 0 starting at candidate #-1 after considering 0 permutations with 0 in ratio
2014-07-01 16:38:51,859 DEBUG [regionserver60020-smallCompactions-1404255751957] compactions.RatioBasedCompactionPolicy: Not compacting files because we only have 0 files ready for compaction. Need 3 to initiate.
2014-07-01 16:38:51,859 DEBUG [regionserver60020-smallCompactions-1404255751957] regionserver.CompactSplitThread: Not compacting usertable,user2,1404257714774.2ed053f05f5eec791a57de24e16f0043. because compaction request was cancelled
2014-07-01 16:38:52,085 DEBUG [MemStoreFlusher.1] util.FSUtils: DFS Client does not support most favored nodes create; using default create
2014-07-01 16:38:52,178 DEBUG [MemStoreFlusher.0] util.FSUtils: DFS Client does not support most favored nodes create; using default create
2014-07-01 16:38:52,470 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-01 16:38:52,491 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 25898 synced till here 25893
2014-07-01 16:38:53,109 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1404255227840/sceplus-vm48.almaden.ibm.com%2C60020%2C1404255227840.1404257929339 with entries=81, filesize=66.2m; new WAL /hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1404255227840/sceplus-vm48.almaden.ibm.com%2C60020%2C1404255227840.1404257932471
2014-07-01 16:38:54,030 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-01 16:38:55,160 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 25995 synced till here 25972
2014-07-01 16:38:55,282 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1404255227840/sceplus-vm48.almaden.ibm.com%2C60020%2C1404255227840.1404257932471 with entries=97, filesize=84.9m; new WAL /hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1404255227840/sceplus-vm48.almaden.ibm.com%2C60020%2C1404255227840.1404257934031
2014-07-01 16:38:56,118 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-01 16:38:56,243 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 26093 synced till here 26079
2014-07-01 16:38:56,293 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1404255227840/sceplus-vm48.almaden.ibm.com%2C60020%2C1404255227840.1404257934031 with entries=98, filesize=74.5m; new WAL /hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1404255227840/sceplus-vm48.almaden.ibm.com%2C60020%2C1404255227840.1404257936119
2014-07-01 16:38:57,781 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-01 16:38:57,811 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 26173 synced till here 26171
2014-07-01 16:38:57,841 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1404255227840/sceplus-vm48.almaden.ibm.com%2C60020%2C1404255227840.1404257936119 with entries=80, filesize=63.3m; new WAL /hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1404255227840/sceplus-vm48.almaden.ibm.com%2C60020%2C1404255227840.1404257937781
2014-07-01 16:38:59,358 INFO  [MemStoreFlusher.0] regionserver.DefaultStoreFlusher: Flushed, sequenceid=1141, memsize=108.1m, hasBloomFilter=true, into tmp file hdfs://master:54310/hbase/data/default/usertable/84635e5ca79006f09dceafa095992aab/.tmp/364759b20af3434ba2798dcd0143eba9
2014-07-01 16:38:59,369 DEBUG [MemStoreFlusher.0] regionserver.HRegionFileSystem: Committing store file hdfs://master:54310/hbase/data/default/usertable/84635e5ca79006f09dceafa095992aab/.tmp/364759b20af3434ba2798dcd0143eba9 as hdfs://master:54310/hbase/data/default/usertable/84635e5ca79006f09dceafa095992aab/family/364759b20af3434ba2798dcd0143eba9
2014-07-01 16:38:59,386 INFO  [MemStoreFlusher.0] regionserver.HStore: Added hdfs://master:54310/hbase/data/default/usertable/84635e5ca79006f09dceafa095992aab/family/364759b20af3434ba2798dcd0143eba9, entries=393550, sequenceid=1141, filesize=28.1m
2014-07-01 16:38:59,387 INFO  [MemStoreFlusher.0] regionserver.HRegion: Finished memstore flush of ~276.3m/289759680, currentsize=33.9m/35540800 for region usertable,user9,1404257714774.84635e5ca79006f09dceafa095992aab. in 7528ms, sequenceid=1141, compaction requested=false
2014-07-01 16:38:59,387 DEBUG [MemStoreFlusher.0] regionserver.HRegion: Started memstore flush for usertable,user4,1404257714774.c831a0ac8e8f14e82fdaef2ba3a5e1f6., current region memstore size 410.6m
2014-07-01 16:38:59,696 DEBUG [MemStoreFlusher.0] util.FSUtils: DFS Client does not support most favored nodes create; using default create
2014-07-01 16:39:00,072 INFO  [MemStoreFlusher.1] regionserver.DefaultStoreFlusher: Flushed, sequenceid=1145, memsize=140.0m, hasBloomFilter=true, into tmp file hdfs://master:54310/hbase/data/default/usertable/a59b89c303508bc329bc1095d0beb9e5/.tmp/4bf96f57f0ff4e43bc80562146680418
2014-07-01 16:39:00,083 DEBUG [MemStoreFlusher.1] regionserver.HRegionFileSystem: Committing store file hdfs://master:54310/hbase/data/default/usertable/a59b89c303508bc329bc1095d0beb9e5/.tmp/4bf96f57f0ff4e43bc80562146680418 as hdfs://master:54310/hbase/data/default/usertable/a59b89c303508bc329bc1095d0beb9e5/family/4bf96f57f0ff4e43bc80562146680418
2014-07-01 16:39:00,091 INFO  [MemStoreFlusher.1] regionserver.HStore: Added hdfs://master:54310/hbase/data/default/usertable/a59b89c303508bc329bc1095d0beb9e5/family/4bf96f57f0ff4e43bc80562146680418, entries=509640, sequenceid=1145, filesize=36.3m
2014-07-01 16:39:00,092 INFO  [MemStoreFlusher.1] regionserver.HRegion: Finished memstore flush of ~318.1m/333508160, currentsize=116.0m/121639200 for region usertable,user3,1404257714774.a59b89c303508bc329bc1095d0beb9e5. in 8234ms, sequenceid=1145, compaction requested=true
2014-07-01 16:39:00,092 DEBUG [MemStoreFlusher.1] regionserver.CompactSplitThread: Small Compaction requested: system; Because: MemStoreFlusher.1; compaction_queue=(0:0), split_queue=0, merge_queue=0
2014-07-01 16:39:00,092 DEBUG [regionserver60020-smallCompactions-1404255751957] compactions.RatioBasedCompactionPolicy: Selecting compaction from 6 store files, 0 compacting, 6 eligible, 20 blocking
2014-07-01 16:39:00,092 DEBUG [regionserver60020-smallCompactions-1404255751957] compactions.RatioBasedCompactionPolicy: Some files are too large. Excluding 6 files from compaction candidates
2014-07-01 16:39:00,092 DEBUG [regionserver60020-smallCompactions-1404255751957] compactions.ExploringCompactionPolicy: Exploring compaction algorithm has selected 0 files of size 0 starting at candidate #-1 after considering 0 permutations with 0 in ratio
2014-07-01 16:39:00,092 DEBUG [regionserver60020-smallCompactions-1404255751957] compactions.RatioBasedCompactionPolicy: Not compacting files because we only have 0 files ready for compaction. Need 3 to initiate.
2014-07-01 16:39:00,092 DEBUG [regionserver60020-smallCompactions-1404255751957] regionserver.CompactSplitThread: Not compacting usertable,user3,1404257714774.a59b89c303508bc329bc1095d0beb9e5. because compaction request was cancelled
2014-07-01 16:39:00,418 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-01 16:39:00,438 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 26250 synced till here 26245
2014-07-01 16:39:00,479 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1404255227840/sceplus-vm48.almaden.ibm.com%2C60020%2C1404255227840.1404257937781 with entries=77, filesize=66.6m; new WAL /hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1404255227840/sceplus-vm48.almaden.ibm.com%2C60020%2C1404255227840.1404257940419
2014-07-01 16:39:00,479 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1404255227840/sceplus-vm48.almaden.ibm.com%2C60020%2C1404255227840.1404257865542
2014-07-01 16:39:00,479 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1404255227840/sceplus-vm48.almaden.ibm.com%2C60020%2C1404255227840.1404257868186
2014-07-01 16:39:00,479 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1404255227840/sceplus-vm48.almaden.ibm.com%2C60020%2C1404255227840.1404257869919
2014-07-01 16:39:00,479 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1404255227840/sceplus-vm48.almaden.ibm.com%2C60020%2C1404255227840.1404257872047
2014-07-01 16:39:00,479 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1404255227840/sceplus-vm48.almaden.ibm.com%2C60020%2C1404255227840.1404257873621
2014-07-01 16:39:00,479 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1404255227840/sceplus-vm48.almaden.ibm.com%2C60020%2C1404255227840.1404257875561
2014-07-01 16:39:00,479 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1404255227840/sceplus-vm48.almaden.ibm.com%2C60020%2C1404255227840.1404257878449
2014-07-01 16:39:00,480 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1404255227840/sceplus-vm48.almaden.ibm.com%2C60020%2C1404255227840.1404257881225
2014-07-01 16:39:00,480 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1404255227840/sceplus-vm48.almaden.ibm.com%2C60020%2C1404255227840.1404257882719
2014-07-01 16:39:00,480 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1404255227840/sceplus-vm48.almaden.ibm.com%2C60020%2C1404255227840.1404257885654
2014-07-01 16:39:00,480 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1404255227840/sceplus-vm48.almaden.ibm.com%2C60020%2C1404255227840.1404257887764
2014-07-01 16:39:00,480 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1404255227840/sceplus-vm48.almaden.ibm.com%2C60020%2C1404255227840.1404257892237
2014-07-01 16:39:00,480 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1404255227840/sceplus-vm48.almaden.ibm.com%2C60020%2C1404255227840.1404257894504
2014-07-01 16:39:00,480 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1404255227840/sceplus-vm48.almaden.ibm.com%2C60020%2C1404255227840.1404257898243
2014-07-01 16:39:00,480 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1404255227840/sceplus-vm48.almaden.ibm.com%2C60020%2C1404255227840.1404257900832
2014-07-01 16:39:00,480 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1404255227840/sceplus-vm48.almaden.ibm.com%2C60020%2C1404255227840.1404257903610
2014-07-01 16:39:00,480 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1404255227840/sceplus-vm48.almaden.ibm.com%2C60020%2C1404255227840.1404257907842
2014-07-01 16:39:00,480 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1404255227840/sceplus-vm48.almaden.ibm.com%2C60020%2C1404255227840.1404257911073
2014-07-01 16:39:02,115 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-01 16:39:02,135 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 26332 synced till here 26325
2014-07-01 16:39:02,184 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1404255227840/sceplus-vm48.almaden.ibm.com%2C60020%2C1404255227840.1404257940419 with entries=82, filesize=67.4m; new WAL /hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1404255227840/sceplus-vm48.almaden.ibm.com%2C60020%2C1404255227840.1404257942115
2014-07-01 16:39:02,231 DEBUG [RpcServer.handler=43,port=60020] regionserver.HRegion: Flush requested on usertable,user8,1404257714774.b6ae40892bbb2a1c545234ed91d62d1a.
2014-07-01 16:39:02,231 DEBUG [MemStoreFlusher.1] regionserver.HRegion: Started memstore flush for usertable,user8,1404257714774.b6ae40892bbb2a1c545234ed91d62d1a., current region memstore size 257.5m
2014-07-01 16:39:02,448 DEBUG [MemStoreFlusher.1] util.FSUtils: DFS Client does not support most favored nodes create; using default create
2014-07-01 16:39:03,636 DEBUG [RpcServer.handler=31,port=60020] regionserver.HRegion: Flush requested on usertable,user2,1404257714774.2ed053f05f5eec791a57de24e16f0043.
2014-07-01 16:39:03,943 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-01 16:39:03,972 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 26414 synced till here 26405
2014-07-01 16:39:04,189 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1404255227840/sceplus-vm48.almaden.ibm.com%2C60020%2C1404255227840.1404257942115 with entries=82, filesize=67.1m; new WAL /hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1404255227840/sceplus-vm48.almaden.ibm.com%2C60020%2C1404255227840.1404257943944
2014-07-01 16:39:06,867 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-01 16:39:06,981 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1404255227840/sceplus-vm48.almaden.ibm.com%2C60020%2C1404255227840.1404257943944 with entries=82, filesize=62.8m; new WAL /hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1404255227840/sceplus-vm48.almaden.ibm.com%2C60020%2C1404255227840.1404257946868
2014-07-01 16:39:07,896 INFO  [MemStoreFlusher.0] regionserver.DefaultStoreFlusher: Flushed, sequenceid=1210, memsize=191.9m, hasBloomFilter=true, into tmp file hdfs://master:54310/hbase/data/default/usertable/c831a0ac8e8f14e82fdaef2ba3a5e1f6/.tmp/b6cc78825589474c8e1580dddd3bce36
2014-07-01 16:39:07,925 DEBUG [MemStoreFlusher.0] regionserver.HRegionFileSystem: Committing store file hdfs://master:54310/hbase/data/default/usertable/c831a0ac8e8f14e82fdaef2ba3a5e1f6/.tmp/b6cc78825589474c8e1580dddd3bce36 as hdfs://master:54310/hbase/data/default/usertable/c831a0ac8e8f14e82fdaef2ba3a5e1f6/family/b6cc78825589474c8e1580dddd3bce36
2014-07-01 16:39:07,939 INFO  [MemStoreFlusher.0] regionserver.HStore: Added hdfs://master:54310/hbase/data/default/usertable/c831a0ac8e8f14e82fdaef2ba3a5e1f6/family/b6cc78825589474c8e1580dddd3bce36, entries=698870, sequenceid=1210, filesize=49.8m
2014-07-01 16:39:07,940 INFO  [MemStoreFlusher.0] regionserver.HRegion: Finished memstore flush of ~410.6m/430502000, currentsize=101.2m/106074000 for region usertable,user4,1404257714774.c831a0ac8e8f14e82fdaef2ba3a5e1f6. in 8553ms, sequenceid=1210, compaction requested=true
2014-07-01 16:39:07,940 DEBUG [MemStoreFlusher.0] regionserver.CompactSplitThread: Small Compaction requested: system; Because: MemStoreFlusher.0; compaction_queue=(0:0), split_queue=0, merge_queue=0
2014-07-01 16:39:07,940 DEBUG [regionserver60020-smallCompactions-1404255751957] compactions.RatioBasedCompactionPolicy: Selecting compaction from 6 store files, 0 compacting, 6 eligible, 20 blocking
2014-07-01 16:39:07,941 DEBUG [regionserver60020-smallCompactions-1404255751957] compactions.RatioBasedCompactionPolicy: Some files are too large. Excluding 6 files from compaction candidates
2014-07-01 16:39:07,941 DEBUG [regionserver60020-smallCompactions-1404255751957] compactions.ExploringCompactionPolicy: Exploring compaction algorithm has selected 0 files of size 0 starting at candidate #-1 after considering 0 permutations with 0 in ratio
2014-07-01 16:39:07,941 DEBUG [regionserver60020-smallCompactions-1404255751957] compactions.RatioBasedCompactionPolicy: Not compacting files because we only have 0 files ready for compaction. Need 3 to initiate.
2014-07-01 16:39:07,941 DEBUG [MemStoreFlusher.0] regionserver.HRegion: Started memstore flush for usertable,user2,1404257714774.2ed053f05f5eec791a57de24e16f0043., current region memstore size 303.5m
2014-07-01 16:39:07,941 DEBUG [regionserver60020-smallCompactions-1404255751957] regionserver.CompactSplitThread: Not compacting usertable,user4,1404257714774.c831a0ac8e8f14e82fdaef2ba3a5e1f6. because compaction request was cancelled
2014-07-01 16:39:08,868 DEBUG [MemStoreFlusher.0] util.FSUtils: DFS Client does not support most favored nodes create; using default create
2014-07-01 16:39:09,163 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-01 16:39:09,184 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 26585 synced till here 26578
2014-07-01 16:39:09,251 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1404255227840/sceplus-vm48.almaden.ibm.com%2C60020%2C1404255227840.1404257946868 with entries=89, filesize=71.3m; new WAL /hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1404255227840/sceplus-vm48.almaden.ibm.com%2C60020%2C1404255227840.1404257949163
2014-07-01 16:39:09,251 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1404255227840/sceplus-vm48.almaden.ibm.com%2C60020%2C1404255227840.1404257914292
2014-07-01 16:39:09,252 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1404255227840/sceplus-vm48.almaden.ibm.com%2C60020%2C1404255227840.1404257916872
2014-07-01 16:39:09,252 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1404255227840/sceplus-vm48.almaden.ibm.com%2C60020%2C1404255227840.1404257918498
2014-07-01 16:39:09,252 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1404255227840/sceplus-vm48.almaden.ibm.com%2C60020%2C1404255227840.1404257920381
2014-07-01 16:39:09,252 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1404255227840/sceplus-vm48.almaden.ibm.com%2C60020%2C1404255227840.1404257922068
2014-07-01 16:39:09,685 DEBUG [RpcServer.handler=28,port=60020] regionserver.HRegion: Flush requested on usertable,user3,1404257714774.a59b89c303508bc329bc1095d0beb9e5.
2014-07-01 16:39:10,834 INFO  [MemStoreFlusher.1] regionserver.DefaultStoreFlusher: Flushed, sequenceid=1236, memsize=186.1m, hasBloomFilter=true, into tmp file hdfs://master:54310/hbase/data/default/usertable/b6ae40892bbb2a1c545234ed91d62d1a/.tmp/c9a159ce9a404288bbbd3af0352ffe57
2014-07-01 16:39:10,844 DEBUG [MemStoreFlusher.1] regionserver.HRegionFileSystem: Committing store file hdfs://master:54310/hbase/data/default/usertable/b6ae40892bbb2a1c545234ed91d62d1a/.tmp/c9a159ce9a404288bbbd3af0352ffe57 as hdfs://master:54310/hbase/data/default/usertable/b6ae40892bbb2a1c545234ed91d62d1a/family/c9a159ce9a404288bbbd3af0352ffe57
2014-07-01 16:39:10,901 INFO  [MemStoreFlusher.1] regionserver.HStore: Added hdfs://master:54310/hbase/data/default/usertable/b6ae40892bbb2a1c545234ed91d62d1a/family/c9a159ce9a404288bbbd3af0352ffe57, entries=677490, sequenceid=1236, filesize=48.3m
2014-07-01 16:39:10,901 INFO  [MemStoreFlusher.1] regionserver.HRegion: Finished memstore flush of ~259.2m/271815200, currentsize=99.2m/104047920 for region usertable,user8,1404257714774.b6ae40892bbb2a1c545234ed91d62d1a. in 8670ms, sequenceid=1236, compaction requested=true
2014-07-01 16:39:10,902 DEBUG [regionserver60020-smallCompactions-1404255751957] compactions.RatioBasedCompactionPolicy: Selecting compaction from 6 store files, 0 compacting, 6 eligible, 20 blocking
2014-07-01 16:39:10,902 DEBUG [regionserver60020-smallCompactions-1404255751957] compactions.RatioBasedCompactionPolicy: Some files are too large. Excluding 6 files from compaction candidates
2014-07-01 16:39:10,902 DEBUG [regionserver60020-smallCompactions-1404255751957] compactions.ExploringCompactionPolicy: Exploring compaction algorithm has selected 0 files of size 0 starting at candidate #-1 after considering 0 permutations with 0 in ratio
2014-07-01 16:39:10,902 DEBUG [regionserver60020-smallCompactions-1404255751957] compactions.RatioBasedCompactionPolicy: Not compacting files because we only have 0 files ready for compaction. Need 3 to initiate.
2014-07-01 16:39:10,902 DEBUG [regionserver60020-smallCompactions-1404255751957] regionserver.CompactSplitThread: Not compacting usertable,user8,1404257714774.b6ae40892bbb2a1c545234ed91d62d1a. because compaction request was cancelled
2014-07-01 16:39:10,902 DEBUG [MemStoreFlusher.1] regionserver.CompactSplitThread: Small Compaction requested: system; Because: MemStoreFlusher.1; compaction_queue=(0:0), split_queue=0, merge_queue=0
2014-07-01 16:39:10,902 DEBUG [MemStoreFlusher.1] regionserver.HRegion: Started memstore flush for usertable,user3,1404257714774.a59b89c303508bc329bc1095d0beb9e5., current region memstore size 261.3m
2014-07-01 16:39:11,018 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-01 16:39:11,032 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 26676 synced till here 26668
2014-07-01 16:39:11,294 DEBUG [MemStoreFlusher.1] util.FSUtils: DFS Client does not support most favored nodes create; using default create
2014-07-01 16:39:11,325 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1404255227840/sceplus-vm48.almaden.ibm.com%2C60020%2C1404255227840.1404257949163 with entries=91, filesize=71.1m; new WAL /hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1404255227840/sceplus-vm48.almaden.ibm.com%2C60020%2C1404255227840.1404257951019
2014-07-01 16:39:12,788 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-01 16:39:12,811 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 26761 synced till here 26756
2014-07-01 16:39:13,270 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1404255227840/sceplus-vm48.almaden.ibm.com%2C60020%2C1404255227840.1404257951019 with entries=85, filesize=65.9m; new WAL /hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1404255227840/sceplus-vm48.almaden.ibm.com%2C60020%2C1404255227840.1404257952788
2014-07-01 16:39:15,092 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-01 16:39:15,139 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 26843 synced till here 26831
2014-07-01 16:39:15,318 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1404255227840/sceplus-vm48.almaden.ibm.com%2C60020%2C1404255227840.1404257952788 with entries=82, filesize=70.0m; new WAL /hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1404255227840/sceplus-vm48.almaden.ibm.com%2C60020%2C1404255227840.1404257955092
2014-07-01 16:39:17,285 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-01 16:39:17,320 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 26935 synced till here 26916
2014-07-01 16:39:17,625 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1404255227840/sceplus-vm48.almaden.ibm.com%2C60020%2C1404255227840.1404257955092 with entries=92, filesize=81.8m; new WAL /hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1404255227840/sceplus-vm48.almaden.ibm.com%2C60020%2C1404255227840.1404257957286
2014-07-01 16:39:19,303 DEBUG [RpcServer.handler=15,port=60020] regionserver.HRegion: Flush requested on usertable,user4,1404257714774.c831a0ac8e8f14e82fdaef2ba3a5e1f6.
2014-07-01 16:39:19,464 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-01 16:39:19,565 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 27047 synced till here 27009
2014-07-01 16:39:19,909 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1404255227840/sceplus-vm48.almaden.ibm.com%2C60020%2C1404255227840.1404257957286 with entries=112, filesize=98.2m; new WAL /hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1404255227840/sceplus-vm48.almaden.ibm.com%2C60020%2C1404255227840.1404257959464
2014-07-01 16:39:21,302 DEBUG [RpcServer.handler=4,port=60020] regionserver.HRegion: Flush requested on usertable,user8,1404257714774.b6ae40892bbb2a1c545234ed91d62d1a.
2014-07-01 16:39:21,508 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-01 16:39:21,700 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 27164 synced till here 27128
2014-07-01 16:39:23,103 INFO  [JvmPauseMonitor] util.JvmPauseMonitor: Detected pause in JVM or host machine (eg GC): pause of approximately 1091ms
GC pool 'ParNew' had collection(s): count=1 time=1198ms
2014-07-01 16:39:23,205 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1404255227840/sceplus-vm48.almaden.ibm.com%2C60020%2C1404255227840.1404257959464 with entries=117, filesize=87.8m; new WAL /hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1404255227840/sceplus-vm48.almaden.ibm.com%2C60020%2C1404255227840.1404257961509
2014-07-01 16:39:23,235 INFO  [MemStoreFlusher.0] regionserver.DefaultStoreFlusher: Flushed, sequenceid=1278, memsize=223.4m, hasBloomFilter=true, into tmp file hdfs://master:54310/hbase/data/default/usertable/2ed053f05f5eec791a57de24e16f0043/.tmp/a029b5aac3a94fb891bba4f784db8a27
2014-07-01 16:39:23,275 DEBUG [MemStoreFlusher.0] regionserver.HRegionFileSystem: Committing store file hdfs://master:54310/hbase/data/default/usertable/2ed053f05f5eec791a57de24e16f0043/.tmp/a029b5aac3a94fb891bba4f784db8a27 as hdfs://master:54310/hbase/data/default/usertable/2ed053f05f5eec791a57de24e16f0043/family/a029b5aac3a94fb891bba4f784db8a27
2014-07-01 16:39:23,287 INFO  [MemStoreFlusher.0] regionserver.HStore: Added hdfs://master:54310/hbase/data/default/usertable/2ed053f05f5eec791a57de24e16f0043/family/a029b5aac3a94fb891bba4f784db8a27, entries=813460, sequenceid=1278, filesize=58.0m
2014-07-01 16:39:23,287 INFO  [MemStoreFlusher.0] regionserver.HRegion: Finished memstore flush of ~308.0m/323006880, currentsize=241.2m/252934880 for region usertable,user2,1404257714774.2ed053f05f5eec791a57de24e16f0043. in 15346ms, sequenceid=1278, compaction requested=true
2014-07-01 16:39:23,288 DEBUG [regionserver60020-smallCompactions-1404255751957] compactions.RatioBasedCompactionPolicy: Selecting compaction from 7 store files, 0 compacting, 7 eligible, 20 blocking
2014-07-01 16:39:23,288 DEBUG [regionserver60020-smallCompactions-1404255751957] compactions.RatioBasedCompactionPolicy: Some files are too large. Excluding 7 files from compaction candidates
2014-07-01 16:39:23,288 DEBUG [regionserver60020-smallCompactions-1404255751957] compactions.ExploringCompactionPolicy: Exploring compaction algorithm has selected 0 files of size 0 starting at candidate #-1 after considering 0 permutations with 0 in ratio
2014-07-01 16:39:23,288 DEBUG [MemStoreFlusher.0] regionserver.CompactSplitThread: Small Compaction requested: system; Because: MemStoreFlusher.0; compaction_queue=(0:0), split_queue=0, merge_queue=0
2014-07-01 16:39:23,288 DEBUG [regionserver60020-smallCompactions-1404255751957] compactions.RatioBasedCompactionPolicy: Not compacting files because we only have 0 files ready for compaction. Need 3 to initiate.
2014-07-01 16:39:23,289 DEBUG [regionserver60020-smallCompactions-1404255751957] regionserver.CompactSplitThread: Not compacting usertable,user2,1404257714774.2ed053f05f5eec791a57de24e16f0043. because compaction request was cancelled
2014-07-01 16:39:23,289 DEBUG [MemStoreFlusher.0] regionserver.HRegion: Started memstore flush for usertable,user4,1404257714774.c831a0ac8e8f14e82fdaef2ba3a5e1f6., current region memstore size 316.5m
2014-07-01 16:39:23,807 DEBUG [RpcServer.handler=25,port=60020] regionserver.HRegion: Flush requested on usertable,user2,1404257714774.2ed053f05f5eec791a57de24e16f0043.
2014-07-01 16:39:24,031 DEBUG [MemStoreFlusher.0] util.FSUtils: DFS Client does not support most favored nodes create; using default create
2014-07-01 16:39:24,124 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-01 16:39:25,323 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 27279 synced till here 27245
2014-07-01 16:39:25,569 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1404255227840/sceplus-vm48.almaden.ibm.com%2C60020%2C1404255227840.1404257961509 with entries=115, filesize=99.6m; new WAL /hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1404255227840/sceplus-vm48.almaden.ibm.com%2C60020%2C1404255227840.1404257964124
2014-07-01 16:39:25,570 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1404255227840/sceplus-vm48.almaden.ibm.com%2C60020%2C1404255227840.1404257924390
2014-07-01 16:39:25,570 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1404255227840/sceplus-vm48.almaden.ibm.com%2C60020%2C1404255227840.1404257925723
2014-07-01 16:39:25,570 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1404255227840/sceplus-vm48.almaden.ibm.com%2C60020%2C1404255227840.1404257927032
2014-07-01 16:39:25,648 INFO  [MemStoreFlusher.1] regionserver.DefaultStoreFlusher: Flushed, sequenceid=1309, memsize=202.1m, hasBloomFilter=true, into tmp file hdfs://master:54310/hbase/data/default/usertable/a59b89c303508bc329bc1095d0beb9e5/.tmp/f9235135372140479465a0a3ec5e3fe0
2014-07-01 16:39:25,667 DEBUG [MemStoreFlusher.1] regionserver.HRegionFileSystem: Committing store file hdfs://master:54310/hbase/data/default/usertable/a59b89c303508bc329bc1095d0beb9e5/.tmp/f9235135372140479465a0a3ec5e3fe0 as hdfs://master:54310/hbase/data/default/usertable/a59b89c303508bc329bc1095d0beb9e5/family/f9235135372140479465a0a3ec5e3fe0
2014-07-01 16:39:25,675 INFO  [MemStoreFlusher.1] regionserver.HStore: Added hdfs://master:54310/hbase/data/default/usertable/a59b89c303508bc329bc1095d0beb9e5/family/f9235135372140479465a0a3ec5e3fe0, entries=735800, sequenceid=1309, filesize=52.5m
2014-07-01 16:39:25,676 INFO  [MemStoreFlusher.1] regionserver.HRegion: Finished memstore flush of ~276.6m/290069760, currentsize=211.5m/221767200 for region usertable,user3,1404257714774.a59b89c303508bc329bc1095d0beb9e5. in 14774ms, sequenceid=1309, compaction requested=true
2014-07-01 16:39:25,676 DEBUG [MemStoreFlusher.1] regionserver.CompactSplitThread: Small Compaction requested: system; Because: MemStoreFlusher.1; compaction_queue=(0:0), split_queue=0, merge_queue=0
2014-07-01 16:39:25,676 DEBUG [regionserver60020-smallCompactions-1404255751957] compactions.RatioBasedCompactionPolicy: Selecting compaction from 7 store files, 0 compacting, 7 eligible, 20 blocking
2014-07-01 16:39:25,676 DEBUG [regionserver60020-smallCompactions-1404255751957] compactions.RatioBasedCompactionPolicy: Some files are too large. Excluding 7 files from compaction candidates
2014-07-01 16:39:25,676 DEBUG [MemStoreFlusher.1] regionserver.HRegion: Started memstore flush for usertable,user8,1404257714774.b6ae40892bbb2a1c545234ed91d62d1a., current region memstore size 308.1m
2014-07-01 16:39:25,676 DEBUG [regionserver60020-smallCompactions-1404255751957] compactions.ExploringCompactionPolicy: Exploring compaction algorithm has selected 0 files of size 0 starting at candidate #-1 after considering 0 permutations with 0 in ratio
2014-07-01 16:39:25,676 DEBUG [regionserver60020-smallCompactions-1404255751957] compactions.RatioBasedCompactionPolicy: Not compacting files because we only have 0 files ready for compaction. Need 3 to initiate.
2014-07-01 16:39:25,677 DEBUG [regionserver60020-smallCompactions-1404255751957] regionserver.CompactSplitThread: Not compacting usertable,user3,1404257714774.a59b89c303508bc329bc1095d0beb9e5. because compaction request was cancelled
2014-07-01 16:39:27,252 DEBUG [MemStoreFlusher.1] util.FSUtils: DFS Client does not support most favored nodes create; using default create
2014-07-01 16:39:27,317 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-01 16:39:27,390 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 27391 synced till here 27363
2014-07-01 16:39:27,502 DEBUG [RpcServer.handler=31,port=60020] regionserver.HRegion: Flush requested on usertable,user3,1404257714774.a59b89c303508bc329bc1095d0beb9e5.
2014-07-01 16:39:27,673 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1404255227840/sceplus-vm48.almaden.ibm.com%2C60020%2C1404255227840.1404257964124 with entries=112, filesize=95.9m; new WAL /hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1404255227840/sceplus-vm48.almaden.ibm.com%2C60020%2C1404255227840.1404257967317
2014-07-01 16:39:27,674 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1404255227840/sceplus-vm48.almaden.ibm.com%2C60020%2C1404255227840.1404257929339
2014-07-01 16:39:29,775 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-01 16:39:30,040 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 27538 synced till here 27525
2014-07-01 16:39:30,173 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1404255227840/sceplus-vm48.almaden.ibm.com%2C60020%2C1404255227840.1404257967317 with entries=147, filesize=121.5m; new WAL /hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1404255227840/sceplus-vm48.almaden.ibm.com%2C60020%2C1404255227840.1404257969775
2014-07-01 16:39:33,251 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-01 16:39:33,310 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 27655 synced till here 27626
2014-07-01 16:39:33,900 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1404255227840/sceplus-vm48.almaden.ibm.com%2C60020%2C1404255227840.1404257969775 with entries=117, filesize=97.4m; new WAL /hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1404255227840/sceplus-vm48.almaden.ibm.com%2C60020%2C1404255227840.1404257973251
2014-07-01 16:39:34,812 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-01 16:39:34,983 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 27755 synced till here 27734
2014-07-01 16:39:35,715 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1404255227840/sceplus-vm48.almaden.ibm.com%2C60020%2C1404255227840.1404257973251 with entries=100, filesize=78.3m; new WAL /hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1404255227840/sceplus-vm48.almaden.ibm.com%2C60020%2C1404255227840.1404257974813
2014-07-01 16:39:37,249 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-01 16:39:37,284 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 27838 synced till here 27831
2014-07-01 16:39:37,321 INFO  [MemStoreFlusher.1] regionserver.DefaultStoreFlusher: Flushed, sequenceid=1426, memsize=179.9m, hasBloomFilter=true, into tmp file hdfs://master:54310/hbase/data/default/usertable/b6ae40892bbb2a1c545234ed91d62d1a/.tmp/a6c33f9aa6274fc0b3f04d983195b877
2014-07-01 16:39:37,333 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1404255227840/sceplus-vm48.almaden.ibm.com%2C60020%2C1404255227840.1404257974813 with entries=83, filesize=64.8m; new WAL /hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1404255227840/sceplus-vm48.almaden.ibm.com%2C60020%2C1404255227840.1404257977250
2014-07-01 16:39:37,338 DEBUG [MemStoreFlusher.1] regionserver.HRegionFileSystem: Committing store file hdfs://master:54310/hbase/data/default/usertable/b6ae40892bbb2a1c545234ed91d62d1a/.tmp/a6c33f9aa6274fc0b3f04d983195b877 as hdfs://master:54310/hbase/data/default/usertable/b6ae40892bbb2a1c545234ed91d62d1a/family/a6c33f9aa6274fc0b3f04d983195b877
2014-07-01 16:39:37,356 INFO  [MemStoreFlusher.1] regionserver.HStore: Added hdfs://master:54310/hbase/data/default/usertable/b6ae40892bbb2a1c545234ed91d62d1a/family/a6c33f9aa6274fc0b3f04d983195b877, entries=655020, sequenceid=1426, filesize=46.7m
2014-07-01 16:39:37,357 INFO  [MemStoreFlusher.1] regionserver.HRegion: Finished memstore flush of ~321.9m/337534080, currentsize=187.2m/196264080 for region usertable,user8,1404257714774.b6ae40892bbb2a1c545234ed91d62d1a. in 11681ms, sequenceid=1426, compaction requested=true
2014-07-01 16:39:37,357 DEBUG [MemStoreFlusher.1] regionserver.CompactSplitThread: Small Compaction requested: system; Because: MemStoreFlusher.1; compaction_queue=(0:0), split_queue=0, merge_queue=0
2014-07-01 16:39:37,358 DEBUG [MemStoreFlusher.1] regionserver.HRegion: Started memstore flush for usertable,user2,1404257714774.2ed053f05f5eec791a57de24e16f0043., current region memstore size 460.1m
2014-07-01 16:39:37,952 DEBUG [regionserver60020-smallCompactions-1404255751957] compactions.RatioBasedCompactionPolicy: Selecting compaction from 7 store files, 0 compacting, 7 eligible, 20 blocking
2014-07-01 16:39:37,953 DEBUG [regionserver60020-smallCompactions-1404255751957] compactions.RatioBasedCompactionPolicy: Some files are too large. Excluding 7 files from compaction candidates
2014-07-01 16:39:37,953 DEBUG [regionserver60020-smallCompactions-1404255751957] compactions.ExploringCompactionPolicy: Exploring compaction algorithm has selected 0 files of size 0 starting at candidate #-1 after considering 0 permutations with 0 in ratio
2014-07-01 16:39:37,953 DEBUG [regionserver60020-smallCompactions-1404255751957] compactions.RatioBasedCompactionPolicy: Not compacting files because we only have 0 files ready for compaction. Need 3 to initiate.
2014-07-01 16:39:37,953 DEBUG [regionserver60020-smallCompactions-1404255751957] regionserver.CompactSplitThread: Not compacting usertable,user8,1404257714774.b6ae40892bbb2a1c545234ed91d62d1a. because compaction request was cancelled
2014-07-01 16:39:38,112 INFO  [MemStoreFlusher.0] regionserver.DefaultStoreFlusher: Flushed, sequenceid=1407, memsize=220.3m, hasBloomFilter=true, into tmp file hdfs://master:54310/hbase/data/default/usertable/c831a0ac8e8f14e82fdaef2ba3a5e1f6/.tmp/78bc3777ac92482ba953cb3cd8dc62a6
2014-07-01 16:39:38,127 DEBUG [MemStoreFlusher.0] regionserver.HRegionFileSystem: Committing store file hdfs://master:54310/hbase/data/default/usertable/c831a0ac8e8f14e82fdaef2ba3a5e1f6/.tmp/78bc3777ac92482ba953cb3cd8dc62a6 as hdfs://master:54310/hbase/data/default/usertable/c831a0ac8e8f14e82fdaef2ba3a5e1f6/family/78bc3777ac92482ba953cb3cd8dc62a6
2014-07-01 16:39:38,150 INFO  [MemStoreFlusher.0] regionserver.HStore: Added hdfs://master:54310/hbase/data/default/usertable/c831a0ac8e8f14e82fdaef2ba3a5e1f6/family/78bc3777ac92482ba953cb3cd8dc62a6, entries=801950, sequenceid=1407, filesize=57.2m
2014-07-01 16:39:38,150 INFO  [MemStoreFlusher.0] regionserver.HRegion: Finished memstore flush of ~336.2m/352521600, currentsize=235.1m/246472800 for region usertable,user4,1404257714774.c831a0ac8e8f14e82fdaef2ba3a5e1f6. in 14861ms, sequenceid=1407, compaction requested=true
2014-07-01 16:39:38,151 DEBUG [MemStoreFlusher.0] regionserver.CompactSplitThread: Small Compaction requested: system; Because: MemStoreFlusher.0; compaction_queue=(0:0), split_queue=0, merge_queue=0
2014-07-01 16:39:38,151 DEBUG [regionserver60020-smallCompactions-1404255751957] compactions.RatioBasedCompactionPolicy: Selecting compaction from 7 store files, 0 compacting, 7 eligible, 20 blocking
2014-07-01 16:39:38,151 DEBUG [regionserver60020-smallCompactions-1404255751957] compactions.RatioBasedCompactionPolicy: Some files are too large. Excluding 7 files from compaction candidates
2014-07-01 16:39:38,151 DEBUG [MemStoreFlusher.0] regionserver.HRegion: Started memstore flush for usertable,user3,1404257714774.a59b89c303508bc329bc1095d0beb9e5., current region memstore size 410.5m
2014-07-01 16:39:38,151 DEBUG [regionserver60020-smallCompactions-1404255751957] compactions.ExploringCompactionPolicy: Exploring compaction algorithm has selected 0 files of size 0 starting at candidate #-1 after considering 0 permutations with 0 in ratio
2014-07-01 16:39:38,151 DEBUG [regionserver60020-smallCompactions-1404255751957] compactions.RatioBasedCompactionPolicy: Not compacting files because we only have 0 files ready for compaction. Need 3 to initiate.
2014-07-01 16:39:38,151 DEBUG [regionserver60020-smallCompactions-1404255751957] regionserver.CompactSplitThread: Not compacting usertable,user4,1404257714774.c831a0ac8e8f14e82fdaef2ba3a5e1f6. because compaction request was cancelled
2014-07-01 16:39:38,350 DEBUG [MemStoreFlusher.1] util.FSUtils: DFS Client does not support most favored nodes create; using default create
2014-07-01 16:39:38,575 DEBUG [RpcServer.handler=23,port=60020] regionserver.HRegion: Flush requested on usertable,user4,1404257714774.c831a0ac8e8f14e82fdaef2ba3a5e1f6.
2014-07-01 16:39:38,603 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-01 16:39:38,645 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 27920 synced till here 27914
2014-07-01 16:39:38,654 DEBUG [MemStoreFlusher.0] util.FSUtils: DFS Client does not support most favored nodes create; using default create
2014-07-01 16:39:38,715 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1404255227840/sceplus-vm48.almaden.ibm.com%2C60020%2C1404255227840.1404257977250 with entries=82, filesize=67.5m; new WAL /hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1404255227840/sceplus-vm48.almaden.ibm.com%2C60020%2C1404255227840.1404257978603
2014-07-01 16:39:40,381 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-01 16:39:41,735 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 28033 synced till here 28030
2014-07-01 16:39:41,782 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1404255227840/sceplus-vm48.almaden.ibm.com%2C60020%2C1404255227840.1404257978603 with entries=113, filesize=93.8m; new WAL /hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1404255227840/sceplus-vm48.almaden.ibm.com%2C60020%2C1404255227840.1404257980382
2014-07-01 16:39:41,963 DEBUG [RpcServer.handler=46,port=60020] regionserver.HRegion: Flush requested on usertable,user8,1404257714774.b6ae40892bbb2a1c545234ed91d62d1a.
2014-07-01 16:39:44,652 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-01 16:39:44,722 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 28124 synced till here 28111
2014-07-01 16:39:44,892 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1404255227840/sceplus-vm48.almaden.ibm.com%2C60020%2C1404255227840.1404257980382 with entries=91, filesize=75.1m; new WAL /hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1404255227840/sceplus-vm48.almaden.ibm.com%2C60020%2C1404255227840.1404257984655
2014-07-01 16:39:45,475 INFO  [MemStoreFlusher.0] regionserver.DefaultStoreFlusher: Flushed, sequenceid=1549, memsize=90.5m, hasBloomFilter=true, into tmp file hdfs://master:54310/hbase/data/default/usertable/a59b89c303508bc329bc1095d0beb9e5/.tmp/b379dc3098f5491781306b05fb07e90b
2014-07-01 16:39:45,535 DEBUG [MemStoreFlusher.0] regionserver.HRegionFileSystem: Committing store file hdfs://master:54310/hbase/data/default/usertable/a59b89c303508bc329bc1095d0beb9e5/.tmp/b379dc3098f5491781306b05fb07e90b as hdfs://master:54310/hbase/data/default/usertable/a59b89c303508bc329bc1095d0beb9e5/family/b379dc3098f5491781306b05fb07e90b
2014-07-01 16:39:45,558 INFO  [MemStoreFlusher.0] regionserver.HStore: Added hdfs://master:54310/hbase/data/default/usertable/a59b89c303508bc329bc1095d0beb9e5/family/b379dc3098f5491781306b05fb07e90b, entries=329460, sequenceid=1549, filesize=23.5m
2014-07-01 16:39:45,559 INFO  [MemStoreFlusher.0] regionserver.HRegion: Finished memstore flush of ~417.6m/437924800, currentsize=100.8m/105704640 for region usertable,user3,1404257714774.a59b89c303508bc329bc1095d0beb9e5. in 7408ms, sequenceid=1549, compaction requested=true
2014-07-01 16:39:45,559 DEBUG [MemStoreFlusher.0] regionserver.CompactSplitThread: Small Compaction requested: system; Because: MemStoreFlusher.0; compaction_queue=(0:0), split_queue=0, merge_queue=0
2014-07-01 16:39:45,559 DEBUG [regionserver60020-smallCompactions-1404255751957] compactions.RatioBasedCompactionPolicy: Selecting compaction from 8 store files, 0 compacting, 8 eligible, 20 blocking
2014-07-01 16:39:45,559 DEBUG [regionserver60020-smallCompactions-1404255751957] compactions.RatioBasedCompactionPolicy: Some files are too large. Excluding 8 files from compaction candidates
2014-07-01 16:39:45,560 DEBUG [MemStoreFlusher.0] regionserver.HRegion: Started memstore flush for usertable,user4,1404257714774.c831a0ac8e8f14e82fdaef2ba3a5e1f6., current region memstore size 332.4m
2014-07-01 16:39:45,560 DEBUG [regionserver60020-smallCompactions-1404255751957] compactions.ExploringCompactionPolicy: Exploring compaction algorithm has selected 0 files of size 0 starting at candidate #-1 after considering 0 permutations with 0 in ratio
2014-07-01 16:39:45,560 DEBUG [regionserver60020-smallCompactions-1404255751957] compactions.RatioBasedCompactionPolicy: Not compacting files because we only have 0 files ready for compaction. Need 3 to initiate.
2014-07-01 16:39:45,560 DEBUG [regionserver60020-smallCompactions-1404255751957] regionserver.CompactSplitThread: Not compacting usertable,user3,1404257714774.a59b89c303508bc329bc1095d0beb9e5. because compaction request was cancelled
2014-07-01 16:39:46,814 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-01 16:39:46,856 DEBUG [MemStoreFlusher.0] util.FSUtils: DFS Client does not support most favored nodes create; using default create
2014-07-01 16:39:47,167 INFO  [MemStoreFlusher.1] regionserver.DefaultStoreFlusher: Flushed, sequenceid=1544, memsize=126.0m, hasBloomFilter=true, into tmp file hdfs://master:54310/hbase/data/default/usertable/2ed053f05f5eec791a57de24e16f0043/.tmp/f75e612bbe2a43518800d163ca6b58cb
2014-07-01 16:39:47,190 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 28264 synced till here 28241
2014-07-01 16:39:47,250 DEBUG [MemStoreFlusher.1] regionserver.HRegionFileSystem: Committing store file hdfs://master:54310/hbase/data/default/usertable/2ed053f05f5eec791a57de24e16f0043/.tmp/f75e612bbe2a43518800d163ca6b58cb as hdfs://master:54310/hbase/data/default/usertable/2ed053f05f5eec791a57de24e16f0043/family/f75e612bbe2a43518800d163ca6b58cb
2014-07-01 16:39:47,261 INFO  [MemStoreFlusher.1] regionserver.HStore: Added hdfs://master:54310/hbase/data/default/usertable/2ed053f05f5eec791a57de24e16f0043/family/f75e612bbe2a43518800d163ca6b58cb, entries=458790, sequenceid=1544, filesize=32.7m
2014-07-01 16:39:47,261 INFO  [MemStoreFlusher.1] regionserver.HRegion: Finished memstore flush of ~460.1m/482472720, currentsize=151.2m/158591680 for region usertable,user2,1404257714774.2ed053f05f5eec791a57de24e16f0043. in 9903ms, sequenceid=1544, compaction requested=true
2014-07-01 16:39:47,261 DEBUG [MemStoreFlusher.1] regionserver.CompactSplitThread: Small Compaction requested: system; Because: MemStoreFlusher.1; compaction_queue=(0:0), split_queue=0, merge_queue=0
2014-07-01 16:39:47,262 DEBUG [regionserver60020-smallCompactions-1404255751957] compactions.RatioBasedCompactionPolicy: Selecting compaction from 8 store files, 0 compacting, 8 eligible, 20 blocking
2014-07-01 16:39:47,262 DEBUG [regionserver60020-smallCompactions-1404255751957] compactions.RatioBasedCompactionPolicy: Some files are too large. Excluding 8 files from compaction candidates
2014-07-01 16:39:47,262 DEBUG [MemStoreFlusher.1] regionserver.HRegion: Started memstore flush for usertable,user8,1404257714774.b6ae40892bbb2a1c545234ed91d62d1a., current region memstore size 322.4m
2014-07-01 16:39:47,262 DEBUG [regionserver60020-smallCompactions-1404255751957] compactions.ExploringCompactionPolicy: Exploring compaction algorithm has selected 0 files of size 0 starting at candidate #-1 after considering 0 permutations with 0 in ratio
2014-07-01 16:39:47,262 DEBUG [regionserver60020-smallCompactions-1404255751957] compactions.RatioBasedCompactionPolicy: Not compacting files because we only have 0 files ready for compaction. Need 3 to initiate.
2014-07-01 16:39:47,262 DEBUG [regionserver60020-smallCompactions-1404255751957] regionserver.CompactSplitThread: Not compacting usertable,user2,1404257714774.2ed053f05f5eec791a57de24e16f0043. because compaction request was cancelled
2014-07-01 16:39:47,360 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1404255227840/sceplus-vm48.almaden.ibm.com%2C60020%2C1404255227840.1404257984655 with entries=140, filesize=113.5m; new WAL /hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1404255227840/sceplus-vm48.almaden.ibm.com%2C60020%2C1404255227840.1404257986814
2014-07-01 16:39:47,777 DEBUG [MemStoreFlusher.1] util.FSUtils: DFS Client does not support most favored nodes create; using default create
2014-07-01 16:39:49,209 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-01 16:39:49,269 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 28365 synced till here 28347
2014-07-01 16:39:49,412 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1404255227840/sceplus-vm48.almaden.ibm.com%2C60020%2C1404255227840.1404257986814 with entries=101, filesize=80.8m; new WAL /hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1404255227840/sceplus-vm48.almaden.ibm.com%2C60020%2C1404255227840.1404257989210
2014-07-01 16:39:49,440 DEBUG [RpcServer.handler=4,port=60020] regionserver.HRegion: Flush requested on usertable,user9,1404257714774.84635e5ca79006f09dceafa095992aab.
2014-07-01 16:39:51,166 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-01 16:39:51,229 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 28468 synced till here 28447
2014-07-01 16:39:51,323 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1404255227840/sceplus-vm48.almaden.ibm.com%2C60020%2C1404255227840.1404257989210 with entries=103, filesize=82.4m; new WAL /hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1404255227840/sceplus-vm48.almaden.ibm.com%2C60020%2C1404255227840.1404257991166
2014-07-01 16:39:53,298 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-01 16:39:53,322 DEBUG [RpcServer.handler=13,port=60020] regionserver.HRegion: Flush requested on usertable,user2,1404257714774.2ed053f05f5eec791a57de24e16f0043.
2014-07-01 16:39:53,352 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 28576 synced till here 28559
2014-07-01 16:39:53,439 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1404255227840/sceplus-vm48.almaden.ibm.com%2C60020%2C1404255227840.1404257991166 with entries=108, filesize=86.8m; new WAL /hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1404255227840/sceplus-vm48.almaden.ibm.com%2C60020%2C1404255227840.1404257993298
2014-07-01 16:39:54,630 DEBUG [RpcServer.handler=35,port=60020] regionserver.HRegion: Flush requested on usertable,user3,1404257714774.a59b89c303508bc329bc1095d0beb9e5.
2014-07-01 16:39:54,977 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-01 16:39:55,179 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 28684 synced till here 28670
2014-07-01 16:39:56,705 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1404255227840/sceplus-vm48.almaden.ibm.com%2C60020%2C1404255227840.1404257993298 with entries=108, filesize=86.7m; new WAL /hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1404255227840/sceplus-vm48.almaden.ibm.com%2C60020%2C1404255227840.1404257994977
2014-07-01 16:39:57,400 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-01 16:39:57,435 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 28769 synced till here 28762
2014-07-01 16:39:57,628 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1404255227840/sceplus-vm48.almaden.ibm.com%2C60020%2C1404255227840.1404257994977 with entries=85, filesize=67.9m; new WAL /hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1404255227840/sceplus-vm48.almaden.ibm.com%2C60020%2C1404255227840.1404257997401
2014-07-01 16:39:58,656 INFO  [MemStoreFlusher.0] regionserver.DefaultStoreFlusher: Flushed, sequenceid=1608, memsize=118.0m, hasBloomFilter=true, into tmp file hdfs://master:54310/hbase/data/default/usertable/c831a0ac8e8f14e82fdaef2ba3a5e1f6/.tmp/00c02ac1dba74017a02e5160884fce80
2014-07-01 16:39:58,667 DEBUG [MemStoreFlusher.0] regionserver.HRegionFileSystem: Committing store file hdfs://master:54310/hbase/data/default/usertable/c831a0ac8e8f14e82fdaef2ba3a5e1f6/.tmp/00c02ac1dba74017a02e5160884fce80 as hdfs://master:54310/hbase/data/default/usertable/c831a0ac8e8f14e82fdaef2ba3a5e1f6/family/00c02ac1dba74017a02e5160884fce80
2014-07-01 16:39:58,680 INFO  [MemStoreFlusher.0] regionserver.HStore: Added hdfs://master:54310/hbase/data/default/usertable/c831a0ac8e8f14e82fdaef2ba3a5e1f6/family/00c02ac1dba74017a02e5160884fce80, entries=429500, sequenceid=1608, filesize=30.6m
2014-07-01 16:39:58,680 INFO  [MemStoreFlusher.0] regionserver.HRegion: Finished memstore flush of ~348.6m/365526320, currentsize=182.9m/191784080 for region usertable,user4,1404257714774.c831a0ac8e8f14e82fdaef2ba3a5e1f6. in 13120ms, sequenceid=1608, compaction requested=true
2014-07-01 16:39:58,681 DEBUG [MemStoreFlusher.0] regionserver.CompactSplitThread: Small Compaction requested: system; Because: MemStoreFlusher.0; compaction_queue=(0:0), split_queue=0, merge_queue=0
2014-07-01 16:39:58,681 DEBUG [regionserver60020-smallCompactions-1404255751957] compactions.RatioBasedCompactionPolicy: Selecting compaction from 8 store files, 0 compacting, 8 eligible, 20 blocking
2014-07-01 16:39:58,681 DEBUG [regionserver60020-smallCompactions-1404255751957] compactions.RatioBasedCompactionPolicy: Some files are too large. Excluding 8 files from compaction candidates
2014-07-01 16:39:58,681 DEBUG [MemStoreFlusher.0] regionserver.HRegion: Started memstore flush for usertable,user9,1404257714774.84635e5ca79006f09dceafa095992aab., current region memstore size 293.5m
2014-07-01 16:39:58,681 DEBUG [regionserver60020-smallCompactions-1404255751957] compactions.ExploringCompactionPolicy: Exploring compaction algorithm has selected 0 files of size 0 starting at candidate #-1 after considering 0 permutations with 0 in ratio
2014-07-01 16:39:58,681 DEBUG [regionserver60020-smallCompactions-1404255751957] compactions.RatioBasedCompactionPolicy: Not compacting files because we only have 0 files ready for compaction. Need 3 to initiate.
2014-07-01 16:39:58,681 DEBUG [regionserver60020-smallCompactions-1404255751957] regionserver.CompactSplitThread: Not compacting usertable,user4,1404257714774.c831a0ac8e8f14e82fdaef2ba3a5e1f6. because compaction request was cancelled
2014-07-01 16:39:59,131 DEBUG [MemStoreFlusher.0] util.FSUtils: DFS Client does not support most favored nodes create; using default create
2014-07-01 16:39:59,147 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-01 16:39:59,178 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 28867 synced till here 28843
2014-07-01 16:39:59,344 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1404255227840/sceplus-vm48.almaden.ibm.com%2C60020%2C1404255227840.1404257997401 with entries=98, filesize=84.1m; new WAL /hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1404255227840/sceplus-vm48.almaden.ibm.com%2C60020%2C1404255227840.1404257999147
2014-07-01 16:39:59,639 INFO  [MemStoreFlusher.1] regionserver.DefaultStoreFlusher: Flushed, sequenceid=1619, memsize=135.5m, hasBloomFilter=true, into tmp file hdfs://master:54310/hbase/data/default/usertable/b6ae40892bbb2a1c545234ed91d62d1a/.tmp/c5b4f262b10d41018221231a5a0311c8
2014-07-01 16:39:59,650 DEBUG [MemStoreFlusher.1] regionserver.HRegionFileSystem: Committing store file hdfs://master:54310/hbase/data/default/usertable/b6ae40892bbb2a1c545234ed91d62d1a/.tmp/c5b4f262b10d41018221231a5a0311c8 as hdfs://master:54310/hbase/data/default/usertable/b6ae40892bbb2a1c545234ed91d62d1a/family/c5b4f262b10d41018221231a5a0311c8
2014-07-01 16:39:59,661 INFO  [MemStoreFlusher.1] regionserver.HStore: Added hdfs://master:54310/hbase/data/default/usertable/b6ae40892bbb2a1c545234ed91d62d1a/family/c5b4f262b10d41018221231a5a0311c8, entries=493460, sequenceid=1619, filesize=35.2m
2014-07-01 16:39:59,661 INFO  [MemStoreFlusher.1] regionserver.HRegion: Finished memstore flush of ~331.0m/347070240, currentsize=214.6m/225044720 for region usertable,user8,1404257714774.b6ae40892bbb2a1c545234ed91d62d1a. in 12399ms, sequenceid=1619, compaction requested=true
2014-07-01 16:39:59,661 DEBUG [MemStoreFlusher.1] regionserver.CompactSplitThread: Small Compaction requested: system; Because: MemStoreFlusher.1; compaction_queue=(0:0), split_queue=0, merge_queue=0
2014-07-01 16:39:59,661 DEBUG [regionserver60020-smallCompactions-1404255751957] compactions.RatioBasedCompactionPolicy: Selecting compaction from 8 store files, 0 compacting, 8 eligible, 20 blocking
2014-07-01 16:39:59,662 DEBUG [MemStoreFlusher.1] regionserver.HRegion: Started memstore flush for usertable,user2,1404257714774.2ed053f05f5eec791a57de24e16f0043., current region memstore size 358.8m
2014-07-01 16:39:59,662 DEBUG [regionserver60020-smallCompactions-1404255751957] compactions.RatioBasedCompactionPolicy: Some files are too large. Excluding 8 files from compaction candidates
2014-07-01 16:39:59,662 DEBUG [regionserver60020-smallCompactions-1404255751957] compactions.ExploringCompactionPolicy: Exploring compaction algorithm has selected 0 files of size 0 starting at candidate #-1 after considering 0 permutations with 0 in ratio
2014-07-01 16:39:59,662 DEBUG [regionserver60020-smallCompactions-1404255751957] compactions.RatioBasedCompactionPolicy: Not compacting files because we only have 0 files ready for compaction. Need 3 to initiate.
2014-07-01 16:39:59,662 DEBUG [regionserver60020-smallCompactions-1404255751957] regionserver.CompactSplitThread: Not compacting usertable,user8,1404257714774.b6ae40892bbb2a1c545234ed91d62d1a. because compaction request was cancelled
2014-07-01 16:40:00,438 DEBUG [MemStoreFlusher.1] util.FSUtils: DFS Client does not support most favored nodes create; using default create
2014-07-01 16:40:01,080 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-01 16:40:01,084 DEBUG [RpcServer.handler=41,port=60020] regionserver.HRegion: Flush requested on usertable,user4,1404257714774.c831a0ac8e8f14e82fdaef2ba3a5e1f6.
2014-07-01 16:40:01,128 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 28965 synced till here 28958
2014-07-01 16:40:01,172 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1404255227840/sceplus-vm48.almaden.ibm.com%2C60020%2C1404255227840.1404257999147 with entries=98, filesize=69.3m; new WAL /hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1404255227840/sceplus-vm48.almaden.ibm.com%2C60020%2C1404255227840.1404258001080
2014-07-01 16:40:02,299 DEBUG [RpcServer.handler=18,port=60020] regionserver.HRegion: Flush requested on usertable,user8,1404257714774.b6ae40892bbb2a1c545234ed91d62d1a.
2014-07-01 16:40:02,554 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-01 16:40:02,963 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 29074 synced till here 29065
2014-07-01 16:40:03,059 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1404255227840/sceplus-vm48.almaden.ibm.com%2C60020%2C1404255227840.1404258001080 with entries=109, filesize=87.5m; new WAL /hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1404255227840/sceplus-vm48.almaden.ibm.com%2C60020%2C1404255227840.1404258002554
2014-07-01 16:40:04,450 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-01 16:40:04,877 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1404255227840/sceplus-vm48.almaden.ibm.com%2C60020%2C1404255227840.1404258002554 with entries=77, filesize=61.2m; new WAL /hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1404255227840/sceplus-vm48.almaden.ibm.com%2C60020%2C1404255227840.1404258004451
2014-07-01 16:40:05,840 INFO  [MemStoreFlusher.0] regionserver.DefaultStoreFlusher: Flushed, sequenceid=1712, memsize=109.4m, hasBloomFilter=true, into tmp file hdfs://master:54310/hbase/data/default/usertable/84635e5ca79006f09dceafa095992aab/.tmp/423a2cfc9ccc4d99ae40172beab6f462
2014-07-01 16:40:05,853 DEBUG [MemStoreFlusher.0] regionserver.HRegionFileSystem: Committing store file hdfs://master:54310/hbase/data/default/usertable/84635e5ca79006f09dceafa095992aab/.tmp/423a2cfc9ccc4d99ae40172beab6f462 as hdfs://master:54310/hbase/data/default/usertable/84635e5ca79006f09dceafa095992aab/family/423a2cfc9ccc4d99ae40172beab6f462
2014-07-01 16:40:05,890 INFO  [MemStoreFlusher.0] regionserver.HStore: Added hdfs://master:54310/hbase/data/default/usertable/84635e5ca79006f09dceafa095992aab/family/423a2cfc9ccc4d99ae40172beab6f462, entries=398210, sequenceid=1712, filesize=28.4m
2014-07-01 16:40:05,891 INFO  [MemStoreFlusher.0] regionserver.HRegion: Finished memstore flush of ~296.1m/310495760, currentsize=44.6m/46720400 for region usertable,user9,1404257714774.84635e5ca79006f09dceafa095992aab. in 7210ms, sequenceid=1712, compaction requested=true
2014-07-01 16:40:05,891 DEBUG [MemStoreFlusher.0] regionserver.CompactSplitThread: Small Compaction requested: system; Because: MemStoreFlusher.0; compaction_queue=(0:0), split_queue=0, merge_queue=0
2014-07-01 16:40:05,891 DEBUG [regionserver60020-smallCompactions-1404255751957] compactions.RatioBasedCompactionPolicy: Selecting compaction from 3 store files, 0 compacting, 3 eligible, 20 blocking
2014-07-01 16:40:05,891 DEBUG [regionserver60020-smallCompactions-1404255751957] compactions.RatioBasedCompactionPolicy: Some files are too large. Excluding 3 files from compaction candidates
2014-07-01 16:40:05,891 DEBUG [MemStoreFlusher.0] regionserver.HRegion: Started memstore flush for usertable,user3,1404257714774.a59b89c303508bc329bc1095d0beb9e5., current region memstore size 443.2m
2014-07-01 16:40:05,891 DEBUG [regionserver60020-smallCompactions-1404255751957] compactions.ExploringCompactionPolicy: Exploring compaction algorithm has selected 0 files of size 0 starting at candidate #-1 after considering 0 permutations with 0 in ratio
2014-07-01 16:40:05,892 DEBUG [regionserver60020-smallCompactions-1404255751957] compactions.RatioBasedCompactionPolicy: Not compacting files because we only have 0 files ready for compaction. Need 3 to initiate.
2014-07-01 16:40:05,892 DEBUG [regionserver60020-smallCompactions-1404255751957] regionserver.CompactSplitThread: Not compacting usertable,user9,1404257714774.84635e5ca79006f09dceafa095992aab. because compaction request was cancelled
2014-07-01 16:40:06,794 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-01 16:40:06,811 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 29226 synced till here 29222
2014-07-01 16:40:06,832 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1404255227840/sceplus-vm48.almaden.ibm.com%2C60020%2C1404255227840.1404258004451 with entries=75, filesize=63.4m; new WAL /hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1404255227840/sceplus-vm48.almaden.ibm.com%2C60020%2C1404255227840.1404258006795
2014-07-01 16:40:06,832 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1404255227840/sceplus-vm48.almaden.ibm.com%2C60020%2C1404255227840.1404257932471
2014-07-01 16:40:06,832 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1404255227840/sceplus-vm48.almaden.ibm.com%2C60020%2C1404255227840.1404257934031
2014-07-01 16:40:06,832 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1404255227840/sceplus-vm48.almaden.ibm.com%2C60020%2C1404255227840.1404257936119
2014-07-01 16:40:06,832 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1404255227840/sceplus-vm48.almaden.ibm.com%2C60020%2C1404255227840.1404257937781
2014-07-01 16:40:06,832 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1404255227840/sceplus-vm48.almaden.ibm.com%2C60020%2C1404255227840.1404257940419
2014-07-01 16:40:06,832 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1404255227840/sceplus-vm48.almaden.ibm.com%2C60020%2C1404255227840.1404257942115
2014-07-01 16:40:06,832 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1404255227840/sceplus-vm48.almaden.ibm.com%2C60020%2C1404255227840.1404257943944
2014-07-01 16:40:06,832 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1404255227840/sceplus-vm48.almaden.ibm.com%2C60020%2C1404255227840.1404257946868
2014-07-01 16:40:06,832 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1404255227840/sceplus-vm48.almaden.ibm.com%2C60020%2C1404255227840.1404257949163
2014-07-01 16:40:06,833 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1404255227840/sceplus-vm48.almaden.ibm.com%2C60020%2C1404255227840.1404257951019
2014-07-01 16:40:06,833 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1404255227840/sceplus-vm48.almaden.ibm.com%2C60020%2C1404255227840.1404257952788
2014-07-01 16:40:06,833 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1404255227840/sceplus-vm48.almaden.ibm.com%2C60020%2C1404255227840.1404257955092
2014-07-01 16:40:06,833 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1404255227840/sceplus-vm48.almaden.ibm.com%2C60020%2C1404255227840.1404257957286
2014-07-01 16:40:06,833 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1404255227840/sceplus-vm48.almaden.ibm.com%2C60020%2C1404255227840.1404257959464
2014-07-01 16:40:06,833 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1404255227840/sceplus-vm48.almaden.ibm.com%2C60020%2C1404255227840.1404257961509
2014-07-01 16:40:06,833 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1404255227840/sceplus-vm48.almaden.ibm.com%2C60020%2C1404255227840.1404257964124
2014-07-01 16:40:06,833 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1404255227840/sceplus-vm48.almaden.ibm.com%2C60020%2C1404255227840.1404257967317
2014-07-01 16:40:06,833 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1404255227840/sceplus-vm48.almaden.ibm.com%2C60020%2C1404255227840.1404257969775
2014-07-01 16:40:06,833 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1404255227840/sceplus-vm48.almaden.ibm.com%2C60020%2C1404255227840.1404257973251
2014-07-01 16:40:06,833 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1404255227840/sceplus-vm48.almaden.ibm.com%2C60020%2C1404255227840.1404257974813
2014-07-01 16:40:06,862 DEBUG [MemStoreFlusher.0] util.FSUtils: DFS Client does not support most favored nodes create; using default create
2014-07-01 16:40:06,901 INFO  [MemStoreFlusher.1] regionserver.DefaultStoreFlusher: Flushed, sequenceid=1756, memsize=115.0m, hasBloomFilter=true, into tmp file hdfs://master:54310/hbase/data/default/usertable/2ed053f05f5eec791a57de24e16f0043/.tmp/06f7a8bafedc4ab6b223fae898d67dd0
2014-07-01 16:40:06,921 DEBUG [MemStoreFlusher.1] regionserver.HRegionFileSystem: Committing store file hdfs://master:54310/hbase/data/default/usertable/2ed053f05f5eec791a57de24e16f0043/.tmp/06f7a8bafedc4ab6b223fae898d67dd0 as hdfs://master:54310/hbase/data/default/usertable/2ed053f05f5eec791a57de24e16f0043/family/06f7a8bafedc4ab6b223fae898d67dd0
2014-07-01 16:40:06,930 INFO  [MemStoreFlusher.1] regionserver.HStore: Added hdfs://master:54310/hbase/data/default/usertable/2ed053f05f5eec791a57de24e16f0043/family/06f7a8bafedc4ab6b223fae898d67dd0, entries=418700, sequenceid=1756, filesize=29.8m
2014-07-01 16:40:06,931 INFO  [MemStoreFlusher.1] regionserver.HRegion: Finished memstore flush of ~358.8m/376213120, currentsize=107.0m/112245200 for region usertable,user2,1404257714774.2ed053f05f5eec791a57de24e16f0043. in 7268ms, sequenceid=1756, compaction requested=true
2014-07-01 16:40:06,931 DEBUG [MemStoreFlusher.1] regionserver.CompactSplitThread: Small Compaction requested: system; Because: MemStoreFlusher.1; compaction_queue=(0:0), split_queue=0, merge_queue=0
2014-07-01 16:40:06,931 DEBUG [regionserver60020-smallCompactions-1404255751957] compactions.RatioBasedCompactionPolicy: Selecting compaction from 9 store files, 0 compacting, 9 eligible, 20 blocking
2014-07-01 16:40:06,931 DEBUG [regionserver60020-smallCompactions-1404255751957] compactions.RatioBasedCompactionPolicy: Some files are too large. Excluding 9 files from compaction candidates
2014-07-01 16:40:06,931 DEBUG [MemStoreFlusher.1] regionserver.HRegion: Started memstore flush for usertable,user4,1404257714774.c831a0ac8e8f14e82fdaef2ba3a5e1f6., current region memstore size 356.2m
2014-07-01 16:40:06,931 DEBUG [regionserver60020-smallCompactions-1404255751957] compactions.ExploringCompactionPolicy: Exploring compaction algorithm has selected 0 files of size 0 starting at candidate #-1 after considering 0 permutations with 0 in ratio
2014-07-01 16:40:06,931 DEBUG [regionserver60020-smallCompactions-1404255751957] compactions.RatioBasedCompactionPolicy: Not compacting files because we only have 0 files ready for compaction. Need 3 to initiate.
2014-07-01 16:40:06,931 DEBUG [regionserver60020-smallCompactions-1404255751957] regionserver.CompactSplitThread: Not compacting usertable,user2,1404257714774.2ed053f05f5eec791a57de24e16f0043. because compaction request was cancelled
2014-07-01 16:40:07,232 DEBUG [MemStoreFlusher.1] util.FSUtils: DFS Client does not support most favored nodes create; using default create
2014-07-01 16:40:08,398 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-01 16:40:08,418 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 29301 synced till here 29300
2014-07-01 16:40:08,423 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1404255227840/sceplus-vm48.almaden.ibm.com%2C60020%2C1404255227840.1404258006795 with entries=75, filesize=61.6m; new WAL /hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1404255227840/sceplus-vm48.almaden.ibm.com%2C60020%2C1404255227840.1404258008398
2014-07-01 16:40:11,563 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-01 16:40:11,589 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 29383 synced till here 29374
2014-07-01 16:40:11,699 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1404255227840/sceplus-vm48.almaden.ibm.com%2C60020%2C1404255227840.1404258008398 with entries=82, filesize=69.1m; new WAL /hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1404255227840/sceplus-vm48.almaden.ibm.com%2C60020%2C1404255227840.1404258011563
2014-07-01 16:40:12,471 INFO  [MemStoreFlusher.1] regionserver.DefaultStoreFlusher: Flushed, sequenceid=1819, memsize=107.7m, hasBloomFilter=true, into tmp file hdfs://master:54310/hbase/data/default/usertable/c831a0ac8e8f14e82fdaef2ba3a5e1f6/.tmp/c60a9a7f1bb049deaaf514e8944e0705
2014-07-01 16:40:12,499 DEBUG [MemStoreFlusher.1] regionserver.HRegionFileSystem: Committing store file hdfs://master:54310/hbase/data/default/usertable/c831a0ac8e8f14e82fdaef2ba3a5e1f6/.tmp/c60a9a7f1bb049deaaf514e8944e0705 as hdfs://master:54310/hbase/data/default/usertable/c831a0ac8e8f14e82fdaef2ba3a5e1f6/family/c60a9a7f1bb049deaaf514e8944e0705
2014-07-01 16:40:12,525 INFO  [MemStoreFlusher.1] regionserver.HStore: Added hdfs://master:54310/hbase/data/default/usertable/c831a0ac8e8f14e82fdaef2ba3a5e1f6/family/c60a9a7f1bb049deaaf514e8944e0705, entries=392160, sequenceid=1819, filesize=28.0m
2014-07-01 16:40:12,526 INFO  [MemStoreFlusher.1] regionserver.HRegion: Finished memstore flush of ~356.2m/373477040, currentsize=73.3m/76839280 for region usertable,user4,1404257714774.c831a0ac8e8f14e82fdaef2ba3a5e1f6. in 5595ms, sequenceid=1819, compaction requested=true
2014-07-01 16:40:12,527 DEBUG [MemStoreFlusher.1] regionserver.CompactSplitThread: Small Compaction requested: system; Because: MemStoreFlusher.1; compaction_queue=(0:0), split_queue=0, merge_queue=0
2014-07-01 16:40:12,527 DEBUG [regionserver60020-smallCompactions-1404255751957] compactions.RatioBasedCompactionPolicy: Selecting compaction from 9 store files, 0 compacting, 9 eligible, 20 blocking
2014-07-01 16:40:12,527 DEBUG [regionserver60020-smallCompactions-1404255751957] compactions.RatioBasedCompactionPolicy: Some files are too large. Excluding 9 files from compaction candidates
2014-07-01 16:40:12,528 DEBUG [regionserver60020-smallCompactions-1404255751957] compactions.ExploringCompactionPolicy: Exploring compaction algorithm has selected 0 files of size 0 starting at candidate #-1 after considering 0 permutations with 0 in ratio
2014-07-01 16:40:12,528 DEBUG [MemStoreFlusher.1] regionserver.HRegion: Started memstore flush for usertable,user8,1404257714774.b6ae40892bbb2a1c545234ed91d62d1a., current region memstore size 399.2m
2014-07-01 16:40:12,528 DEBUG [regionserver60020-smallCompactions-1404255751957] compactions.RatioBasedCompactionPolicy: Not compacting files because we only have 0 files ready for compaction. Need 3 to initiate.
2014-07-01 16:40:12,529 DEBUG [regionserver60020-smallCompactions-1404255751957] regionserver.CompactSplitThread: Not compacting usertable,user4,1404257714774.c831a0ac8e8f14e82fdaef2ba3a5e1f6. because compaction request was cancelled
2014-07-01 16:40:12,681 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-01 16:40:12,711 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 29469 synced till here 29463
2014-07-01 16:40:13,210 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1404255227840/sceplus-vm48.almaden.ibm.com%2C60020%2C1404255227840.1404258011563 with entries=86, filesize=66.4m; new WAL /hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1404255227840/sceplus-vm48.almaden.ibm.com%2C60020%2C1404255227840.1404258012682
2014-07-01 16:40:13,420 DEBUG [MemStoreFlusher.1] util.FSUtils: DFS Client does not support most favored nodes create; using default create
2014-07-01 16:40:14,216 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-01 16:40:14,233 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 29548 synced till here 29541
2014-07-01 16:40:14,280 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1404255227840/sceplus-vm48.almaden.ibm.com%2C60020%2C1404255227840.1404258012682 with entries=79, filesize=64.8m; new WAL /hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1404255227840/sceplus-vm48.almaden.ibm.com%2C60020%2C1404255227840.1404258014217
2014-07-01 16:40:14,327 INFO  [MemStoreFlusher.0] regionserver.DefaultStoreFlusher: Flushed, sequenceid=1814, memsize=148.2m, hasBloomFilter=true, into tmp file hdfs://master:54310/hbase/data/default/usertable/a59b89c303508bc329bc1095d0beb9e5/.tmp/2e035a004b10466a8aec2ffd78232fca
2014-07-01 16:40:14,340 DEBUG [MemStoreFlusher.0] regionserver.HRegionFileSystem: Committing store file hdfs://master:54310/hbase/data/default/usertable/a59b89c303508bc329bc1095d0beb9e5/.tmp/2e035a004b10466a8aec2ffd78232fca as hdfs://master:54310/hbase/data/default/usertable/a59b89c303508bc329bc1095d0beb9e5/family/2e035a004b10466a8aec2ffd78232fca
2014-07-01 16:40:14,351 INFO  [MemStoreFlusher.0] regionserver.HStore: Added hdfs://master:54310/hbase/data/default/usertable/a59b89c303508bc329bc1095d0beb9e5/family/2e035a004b10466a8aec2ffd78232fca, entries=539510, sequenceid=1814, filesize=38.4m
2014-07-01 16:40:14,352 INFO  [MemStoreFlusher.0] regionserver.HRegion: Finished memstore flush of ~448.2m/469945760, currentsize=119.4m/125215520 for region usertable,user3,1404257714774.a59b89c303508bc329bc1095d0beb9e5. in 8461ms, sequenceid=1814, compaction requested=true
2014-07-01 16:40:14,358 DEBUG [MemStoreFlusher.0] regionserver.CompactSplitThread: Small Compaction requested: system; Because: MemStoreFlusher.0; compaction_queue=(0:0), split_queue=0, merge_queue=0
2014-07-01 16:40:14,358 DEBUG [regionserver60020-smallCompactions-1404255751957] compactions.RatioBasedCompactionPolicy: Selecting compaction from 9 store files, 0 compacting, 9 eligible, 20 blocking
2014-07-01 16:40:14,358 DEBUG [regionserver60020-smallCompactions-1404255751957] compactions.RatioBasedCompactionPolicy: Some files are too large. Excluding 9 files from compaction candidates
2014-07-01 16:40:14,359 DEBUG [regionserver60020-smallCompactions-1404255751957] compactions.ExploringCompactionPolicy: Exploring compaction algorithm has selected 0 files of size 0 starting at candidate #-1 after considering 0 permutations with 0 in ratio
2014-07-01 16:40:14,359 DEBUG [regionserver60020-smallCompactions-1404255751957] compactions.RatioBasedCompactionPolicy: Not compacting files because we only have 0 files ready for compaction. Need 3 to initiate.
2014-07-01 16:40:14,359 DEBUG [regionserver60020-smallCompactions-1404255751957] regionserver.CompactSplitThread: Not compacting usertable,user3,1404257714774.a59b89c303508bc329bc1095d0beb9e5. because compaction request was cancelled
2014-07-01 16:40:16,222 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-01 16:40:16,254 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 29634 synced till here 29628
2014-07-01 16:40:16,279 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1404255227840/sceplus-vm48.almaden.ibm.com%2C60020%2C1404255227840.1404258014217 with entries=86, filesize=69.5m; new WAL /hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1404255227840/sceplus-vm48.almaden.ibm.com%2C60020%2C1404255227840.1404258016223
2014-07-01 16:40:16,280 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1404255227840/sceplus-vm48.almaden.ibm.com%2C60020%2C1404255227840.1404257977250
2014-07-01 16:40:16,280 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1404255227840/sceplus-vm48.almaden.ibm.com%2C60020%2C1404255227840.1404257978603
2014-07-01 16:40:16,280 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1404255227840/sceplus-vm48.almaden.ibm.com%2C60020%2C1404255227840.1404257980382
2014-07-01 16:40:16,280 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1404255227840/sceplus-vm48.almaden.ibm.com%2C60020%2C1404255227840.1404257984655
2014-07-01 16:40:17,555 DEBUG [RpcServer.handler=23,port=60020] regionserver.HRegion: Flush requested on usertable,user2,1404257714774.2ed053f05f5eec791a57de24e16f0043.
2014-07-01 16:40:17,556 DEBUG [MemStoreFlusher.0] regionserver.HRegion: Started memstore flush for usertable,user2,1404257714774.2ed053f05f5eec791a57de24e16f0043., current region memstore size 256.9m
2014-07-01 16:40:17,974 DEBUG [MemStoreFlusher.0] util.FSUtils: DFS Client does not support most favored nodes create; using default create
2014-07-01 16:40:18,263 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-01 16:40:18,292 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 29721 synced till here 29710
2014-07-01 16:40:18,416 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1404255227840/sceplus-vm48.almaden.ibm.com%2C60020%2C1404255227840.1404258016223 with entries=87, filesize=68.6m; new WAL /hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1404255227840/sceplus-vm48.almaden.ibm.com%2C60020%2C1404255227840.1404258018264
2014-07-01 16:40:20,162 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-01 16:40:20,261 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1404255227840/sceplus-vm48.almaden.ibm.com%2C60020%2C1404255227840.1404258018264 with entries=79, filesize=62.9m; new WAL /hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1404255227840/sceplus-vm48.almaden.ibm.com%2C60020%2C1404255227840.1404258020163
2014-07-01 16:40:22,463 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-01 16:40:22,479 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 29876 synced till here 29874
2014-07-01 16:40:22,503 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1404255227840/sceplus-vm48.almaden.ibm.com%2C60020%2C1404255227840.1404258020163 with entries=76, filesize=63.2m; new WAL /hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1404255227840/sceplus-vm48.almaden.ibm.com%2C60020%2C1404255227840.1404258022463
2014-07-01 16:40:22,930 INFO  [MemStoreFlusher.1] regionserver.DefaultStoreFlusher: Flushed, sequenceid=1861, memsize=167.7m, hasBloomFilter=true, into tmp file hdfs://master:54310/hbase/data/default/usertable/b6ae40892bbb2a1c545234ed91d62d1a/.tmp/c9efb1656af7432ab278a0ad33bb4d72
2014-07-01 16:40:22,959 DEBUG [MemStoreFlusher.1] regionserver.HRegionFileSystem: Committing store file hdfs://master:54310/hbase/data/default/usertable/b6ae40892bbb2a1c545234ed91d62d1a/.tmp/c9efb1656af7432ab278a0ad33bb4d72 as hdfs://master:54310/hbase/data/default/usertable/b6ae40892bbb2a1c545234ed91d62d1a/family/c9efb1656af7432ab278a0ad33bb4d72
2014-07-01 16:40:23,048 INFO  [MemStoreFlusher.1] regionserver.HStore: Added hdfs://master:54310/hbase/data/default/usertable/b6ae40892bbb2a1c545234ed91d62d1a/family/c9efb1656af7432ab278a0ad33bb4d72, entries=610620, sequenceid=1861, filesize=43.5m
2014-07-01 16:40:23,048 INFO  [MemStoreFlusher.1] regionserver.HRegion: Finished memstore flush of ~410.7m/430678640, currentsize=146.9m/153986880 for region usertable,user8,1404257714774.b6ae40892bbb2a1c545234ed91d62d1a. in 10520ms, sequenceid=1861, compaction requested=true
2014-07-01 16:40:23,049 DEBUG [regionserver60020-smallCompactions-1404255751957] compactions.RatioBasedCompactionPolicy: Selecting compaction from 9 store files, 0 compacting, 9 eligible, 20 blocking
2014-07-01 16:40:23,049 DEBUG [MemStoreFlusher.1] regionserver.CompactSplitThread: Small Compaction requested: system; Because: MemStoreFlusher.1; compaction_queue=(0:0), split_queue=0, merge_queue=0
2014-07-01 16:40:23,049 DEBUG [regionserver60020-smallCompactions-1404255751957] compactions.RatioBasedCompactionPolicy: Some files are too large. Excluding 9 files from compaction candidates
2014-07-01 16:40:23,049 DEBUG [regionserver60020-smallCompactions-1404255751957] compactions.ExploringCompactionPolicy: Exploring compaction algorithm has selected 0 files of size 0 starting at candidate #-1 after considering 0 permutations with 0 in ratio
2014-07-01 16:40:23,049 DEBUG [regionserver60020-smallCompactions-1404255751957] compactions.RatioBasedCompactionPolicy: Not compacting files because we only have 0 files ready for compaction. Need 3 to initiate.
2014-07-01 16:40:23,049 DEBUG [regionserver60020-smallCompactions-1404255751957] regionserver.CompactSplitThread: Not compacting usertable,user8,1404257714774.b6ae40892bbb2a1c545234ed91d62d1a. because compaction request was cancelled
2014-07-01 16:40:24,776 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-01 16:40:24,779 DEBUG [RpcServer.handler=1,port=60020] regionserver.HRegion: Flush requested on usertable,user3,1404257714774.a59b89c303508bc329bc1095d0beb9e5.
2014-07-01 16:40:24,782 DEBUG [MemStoreFlusher.1] regionserver.HRegion: Started memstore flush for usertable,user3,1404257714774.a59b89c303508bc329bc1095d0beb9e5., current region memstore size 257.0m
2014-07-01 16:40:24,809 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 29962 synced till here 29948
2014-07-01 16:40:25,043 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1404255227840/sceplus-vm48.almaden.ibm.com%2C60020%2C1404255227840.1404258022463 with entries=86, filesize=75.0m; new WAL /hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1404255227840/sceplus-vm48.almaden.ibm.com%2C60020%2C1404255227840.1404258024777
2014-07-01 16:40:25,044 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1404255227840/sceplus-vm48.almaden.ibm.com%2C60020%2C1404255227840.1404257986814
2014-07-01 16:40:25,044 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1404255227840/sceplus-vm48.almaden.ibm.com%2C60020%2C1404255227840.1404257989210
2014-07-01 16:40:25,044 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1404255227840/sceplus-vm48.almaden.ibm.com%2C60020%2C1404255227840.1404257991166
2014-07-01 16:40:25,044 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1404255227840/sceplus-vm48.almaden.ibm.com%2C60020%2C1404255227840.1404257993298
2014-07-01 16:40:25,044 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1404255227840/sceplus-vm48.almaden.ibm.com%2C60020%2C1404255227840.1404257994977
2014-07-01 16:40:25,249 DEBUG [MemStoreFlusher.1] util.FSUtils: DFS Client does not support most favored nodes create; using default create
2014-07-01 16:40:25,627 DEBUG [RpcServer.handler=47,port=60020] regionserver.HRegion: Flush requested on usertable,user4,1404257714774.c831a0ac8e8f14e82fdaef2ba3a5e1f6.
2014-07-01 16:40:27,136 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-01 16:40:27,176 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 30055 synced till here 30048
2014-07-01 16:40:27,272 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1404255227840/sceplus-vm48.almaden.ibm.com%2C60020%2C1404255227840.1404258024777 with entries=93, filesize=77.4m; new WAL /hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1404255227840/sceplus-vm48.almaden.ibm.com%2C60020%2C1404255227840.1404258027137
2014-07-01 16:40:29,480 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-01 16:40:29,572 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 30165 synced till here 30141
2014-07-01 16:40:29,770 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1404255227840/sceplus-vm48.almaden.ibm.com%2C60020%2C1404255227840.1404258027137 with entries=110, filesize=87.9m; new WAL /hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1404255227840/sceplus-vm48.almaden.ibm.com%2C60020%2C1404255227840.1404258029481
2014-07-01 16:40:31,910 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-01 16:40:33,418 INFO  [JvmPauseMonitor] util.JvmPauseMonitor: Detected pause in JVM or host machine (eg GC): pause of approximately 1464ms
GC pool 'ParNew' had collection(s): count=1 time=1490ms
2014-07-01 16:40:33,470 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 30278 synced till here 30253
2014-07-01 16:40:33,603 DEBUG [RpcServer.handler=41,port=60020] regionserver.HRegion: Flush requested on usertable,user8,1404257714774.b6ae40892bbb2a1c545234ed91d62d1a.
2014-07-01 16:40:33,854 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1404255227840/sceplus-vm48.almaden.ibm.com%2C60020%2C1404255227840.1404258029481 with entries=113, filesize=98.1m; new WAL /hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1404255227840/sceplus-vm48.almaden.ibm.com%2C60020%2C1404255227840.1404258031911
2014-07-01 16:40:33,891 INFO  [MemStoreFlusher.0] regionserver.DefaultStoreFlusher: Flushed, sequenceid=1912, memsize=227.4m, hasBloomFilter=true, into tmp file hdfs://master:54310/hbase/data/default/usertable/2ed053f05f5eec791a57de24e16f0043/.tmp/6ca72eea2a2d47e3858ecb263e5210f1
2014-07-01 16:40:33,912 DEBUG [MemStoreFlusher.0] regionserver.HRegionFileSystem: Committing store file hdfs://master:54310/hbase/data/default/usertable/2ed053f05f5eec791a57de24e16f0043/.tmp/6ca72eea2a2d47e3858ecb263e5210f1 as hdfs://master:54310/hbase/data/default/usertable/2ed053f05f5eec791a57de24e16f0043/family/6ca72eea2a2d47e3858ecb263e5210f1
2014-07-01 16:40:33,922 INFO  [MemStoreFlusher.0] regionserver.HStore: Added hdfs://master:54310/hbase/data/default/usertable/2ed053f05f5eec791a57de24e16f0043/family/6ca72eea2a2d47e3858ecb263e5210f1, entries=828070, sequenceid=1912, filesize=59.0m
2014-07-01 16:40:33,922 INFO  [MemStoreFlusher.0] regionserver.HRegion: Finished memstore flush of ~264.9m/277817920, currentsize=239.8m/251470720 for region usertable,user2,1404257714774.2ed053f05f5eec791a57de24e16f0043. in 16366ms, sequenceid=1912, compaction requested=true
2014-07-01 16:40:33,922 DEBUG [MemStoreFlusher.0] regionserver.CompactSplitThread: Small Compaction requested: system; Because: MemStoreFlusher.0; compaction_queue=(0:0), split_queue=0, merge_queue=0
2014-07-01 16:40:33,923 DEBUG [regionserver60020-smallCompactions-1404255751957] compactions.RatioBasedCompactionPolicy: Selecting compaction from 10 store files, 0 compacting, 10 eligible, 20 blocking
2014-07-01 16:40:33,923 DEBUG [regionserver60020-smallCompactions-1404255751957] compactions.RatioBasedCompactionPolicy: Some files are too large. Excluding 10 files from compaction candidates
2014-07-01 16:40:33,923 DEBUG [MemStoreFlusher.0] regionserver.HRegion: Started memstore flush for usertable,user4,1404257714774.c831a0ac8e8f14e82fdaef2ba3a5e1f6., current region memstore size 345.1m
2014-07-01 16:40:33,923 DEBUG [regionserver60020-smallCompactions-1404255751957] compactions.ExploringCompactionPolicy: Exploring compaction algorithm has selected 0 files of size 0 starting at candidate #-1 after considering 0 permutations with 0 in ratio
2014-07-01 16:40:33,923 DEBUG [regionserver60020-smallCompactions-1404255751957] compactions.RatioBasedCompactionPolicy: Not compacting files because we only have 0 files ready for compaction. Need 3 to initiate.
2014-07-01 16:40:33,923 DEBUG [regionserver60020-smallCompactions-1404255751957] regionserver.CompactSplitThread: Not compacting usertable,user2,1404257714774.2ed053f05f5eec791a57de24e16f0043. because compaction request was cancelled
2014-07-01 16:40:36,440 INFO  [JvmPauseMonitor] util.JvmPauseMonitor: Detected pause in JVM or host machine (eg GC): pause of approximately 2021ms
GC pool 'ParNew' had collection(s): count=1 time=2147ms
2014-07-01 16:40:36,667 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-01 16:40:36,681 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 30385 synced till here 30354
2014-07-01 16:40:36,708 DEBUG [MemStoreFlusher.0] util.FSUtils: DFS Client does not support most favored nodes create; using default create
2014-07-01 16:40:36,975 WARN  [RpcServer.handler=47,port=60020] ipc.RpcServer: RpcServer.respondercallId: 4728 service: ClientService methodName: Multi size: 3.4m connection: 9.1.143.58:41353: output error
2014-07-01 16:40:36,983 WARN  [RpcServer.handler=47,port=60020] ipc.RpcServer: RpcServer.handler=47,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-07-01 16:40:36,983 WARN  [RpcServer.handler=34,port=60020] ipc.RpcServer: RpcServer.respondercallId: 4731 service: ClientService methodName: Multi size: 3.3m connection: 9.1.143.58:41353: output error
2014-07-01 16:40:36,983 WARN  [RpcServer.handler=34,port=60020] ipc.RpcServer: RpcServer.handler=34,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-07-01 16:40:37,230 DEBUG [RpcServer.handler=17,port=60020] regionserver.HRegion: Flush requested on usertable,user2,1404257714774.2ed053f05f5eec791a57de24e16f0043.
2014-07-01 16:40:37,295 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1404255227840/sceplus-vm48.almaden.ibm.com%2C60020%2C1404255227840.1404258031911 with entries=107, filesize=89.7m; new WAL /hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1404255227840/sceplus-vm48.almaden.ibm.com%2C60020%2C1404255227840.1404258036667
2014-07-01 16:40:37,523 WARN  [RpcServer.handler=21,port=60020] ipc.RpcServer: RpcServer.respondercallId: 4725 service: ClientService methodName: Multi size: 3.4m connection: 9.1.143.58:41353: output error
2014-07-01 16:40:37,524 WARN  [RpcServer.handler=21,port=60020] ipc.RpcServer: RpcServer.handler=21,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-07-01 16:40:37,531 WARN  [RpcServer.handler=6,port=60020] ipc.RpcServer: RpcServer.respondercallId: 4762 service: ClientService methodName: Multi size: 3.3m connection: 9.1.143.58:41353: output error
2014-07-01 16:40:37,531 WARN  [RpcServer.handler=6,port=60020] ipc.RpcServer: RpcServer.handler=6,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-07-01 16:40:37,531 WARN  [RpcServer.handler=26,port=60020] ipc.RpcServer: RpcServer.respondercallId: 4763 service: ClientService methodName: Multi size: 3.3m connection: 9.1.143.58:41353: output error
2014-07-01 16:40:37,531 WARN  [RpcServer.handler=26,port=60020] ipc.RpcServer: RpcServer.handler=26,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-07-01 16:40:37,539 WARN  [RpcServer.handler=19,port=60020] ipc.RpcServer: RpcServer.respondercallId: 4760 service: ClientService methodName: Multi size: 2.9m connection: 9.1.143.58:41353: output error
2014-07-01 16:40:37,539 WARN  [RpcServer.handler=19,port=60020] ipc.RpcServer: RpcServer.handler=19,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-07-01 16:40:38,720 WARN  [RpcServer.handler=12,port=60020] ipc.RpcServer: RpcServer.respondercallId: 4782 service: ClientService methodName: Multi size: 3.3m connection: 9.1.143.58:41353: output error
2014-07-01 16:40:38,721 WARN  [RpcServer.handler=12,port=60020] ipc.RpcServer: RpcServer.handler=12,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-07-01 16:40:38,721 WARN  [RpcServer.handler=28,port=60020] ipc.RpcServer: RpcServer.respondercallId: 4759 service: ClientService methodName: Multi size: 3.4m connection: 9.1.143.58:41353: output error
2014-07-01 16:40:38,721 WARN  [RpcServer.handler=28,port=60020] ipc.RpcServer: RpcServer.handler=28,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-07-01 16:40:38,723 WARN  [RpcServer.handler=40,port=60020] ipc.RpcServer: RpcServer.respondercallId: 4775 service: ClientService methodName: Multi size: 3.3m connection: 9.1.143.58:41353: output error
2014-07-01 16:40:38,723 WARN  [RpcServer.handler=40,port=60020] ipc.RpcServer: RpcServer.handler=40,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-07-01 16:40:38,727 WARN  [RpcServer.handler=7,port=60020] ipc.RpcServer: RpcServer.respondercallId: 4761 service: ClientService methodName: Multi size: 3.4m connection: 9.1.143.58:41353: output error
2014-07-01 16:40:38,727 WARN  [RpcServer.handler=7,port=60020] ipc.RpcServer: RpcServer.handler=7,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-07-01 16:40:38,727 WARN  [RpcServer.handler=15,port=60020] ipc.RpcServer: RpcServer.respondercallId: 4776 service: ClientService methodName: Multi size: 3.3m connection: 9.1.143.58:41353: output error
2014-07-01 16:40:38,728 WARN  [RpcServer.handler=15,port=60020] ipc.RpcServer: RpcServer.handler=15,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-07-01 16:40:38,728 WARN  [RpcServer.handler=44,port=60020] ipc.RpcServer: RpcServer.respondercallId: 4765 service: ClientService methodName: Multi size: 3.4m connection: 9.1.143.58:41353: output error
2014-07-01 16:40:38,728 WARN  [RpcServer.handler=44,port=60020] ipc.RpcServer: RpcServer.handler=44,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-07-01 16:40:38,728 WARN  [RpcServer.handler=27,port=60020] ipc.RpcServer: RpcServer.respondercallId: 4769 service: ClientService methodName: Multi size: 3.0m connection: 9.1.143.58:41353: output error
2014-07-01 16:40:38,728 WARN  [RpcServer.handler=27,port=60020] ipc.RpcServer: RpcServer.handler=27,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-07-01 16:40:38,729 WARN  [RpcServer.handler=18,port=60020] ipc.RpcServer: RpcServer.respondercallId: 4704 service: ClientService methodName: Multi size: 3.3m connection: 9.1.143.58:41353: output error
2014-07-01 16:40:38,729 WARN  [RpcServer.handler=18,port=60020] ipc.RpcServer: RpcServer.handler=18,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-07-01 16:40:38,729 WARN  [RpcServer.handler=46,port=60020] ipc.RpcServer: RpcServer.respondercallId: 4721 service: ClientService methodName: Multi size: 3.4m connection: 9.1.143.58:41353: output error
2014-07-01 16:40:38,732 WARN  [RpcServer.handler=46,port=60020] ipc.RpcServer: RpcServer.handler=46,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-07-01 16:40:38,732 WARN  [RpcServer.handler=2,port=60020] ipc.RpcServer: RpcServer.respondercallId: 4698 service: ClientService methodName: Multi size: 3.0m connection: 9.1.143.58:41353: output error
2014-07-01 16:40:38,732 WARN  [RpcServer.handler=2,port=60020] ipc.RpcServer: RpcServer.handler=2,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-07-01 16:40:38,739 WARN  [RpcServer.handler=14,port=60020] ipc.RpcServer: RpcServer.respondercallId: 4774 service: ClientService methodName: Multi size: 3.3m connection: 9.1.143.58:41353: output error
2014-07-01 16:40:38,739 WARN  [RpcServer.handler=14,port=60020] ipc.RpcServer: RpcServer.handler=14,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-07-01 16:40:38,739 WARN  [RpcServer.handler=4,port=60020] ipc.RpcServer: RpcServer.respondercallId: 4771 service: ClientService methodName: Multi size: 3.4m connection: 9.1.143.58:41353: output error
2014-07-01 16:40:38,740 WARN  [RpcServer.handler=4,port=60020] ipc.RpcServer: RpcServer.handler=4,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-07-01 16:40:38,743 WARN  [RpcServer.handler=0,port=60020] ipc.RpcServer: RpcServer.respondercallId: 4773 service: ClientService methodName: Multi size: 3.3m connection: 9.1.143.58:41353: output error
2014-07-01 16:40:38,743 WARN  [RpcServer.handler=0,port=60020] ipc.RpcServer: RpcServer.handler=0,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-07-01 16:40:38,747 WARN  [RpcServer.handler=32,port=60020] ipc.RpcServer: RpcServer.respondercallId: 4766 service: ClientService methodName: Multi size: 3.3m connection: 9.1.143.58:41353: output error
2014-07-01 16:40:38,747 WARN  [RpcServer.handler=32,port=60020] ipc.RpcServer: RpcServer.handler=32,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-07-01 16:40:38,752 WARN  [RpcServer.handler=33,port=60020] ipc.RpcServer: RpcServer.respondercallId: 4770 service: ClientService methodName: Multi size: 3.3m connection: 9.1.143.58:41353: output error
2014-07-01 16:40:38,752 WARN  [RpcServer.handler=33,port=60020] ipc.RpcServer: RpcServer.handler=33,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-07-01 16:40:38,754 WARN  [RpcServer.handler=35,port=60020] ipc.RpcServer: RpcServer.respondercallId: 4764 service: ClientService methodName: Multi size: 3.4m connection: 9.1.143.58:41353: output error
2014-07-01 16:40:38,754 WARN  [RpcServer.handler=35,port=60020] ipc.RpcServer: RpcServer.handler=35,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-07-01 16:40:38,755 WARN  [RpcServer.handler=37,port=60020] ipc.RpcServer: RpcServer.respondercallId: 4772 service: ClientService methodName: Multi size: 3.4m connection: 9.1.143.58:41353: output error
2014-07-01 16:40:38,755 WARN  [RpcServer.handler=37,port=60020] ipc.RpcServer: RpcServer.handler=37,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-07-01 16:40:38,852 WARN  [RpcServer.handler=30,port=60020] ipc.RpcServer: RpcServer.respondercallId: 4758 service: ClientService methodName: Multi size: 3.0m connection: 9.1.143.58:41353: output error
2014-07-01 16:40:38,852 WARN  [RpcServer.handler=30,port=60020] ipc.RpcServer: RpcServer.handler=30,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-07-01 16:40:38,859 WARN  [RpcServer.handler=11,port=60020] ipc.RpcServer: RpcServer.respondercallId: 4757 service: ClientService methodName: Multi size: 3.3m connection: 9.1.143.58:41353: output error
2014-07-01 16:40:38,859 WARN  [RpcServer.handler=11,port=60020] ipc.RpcServer: RpcServer.handler=11,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-07-01 16:40:38,861 WARN  [RpcServer.handler=10,port=60020] ipc.RpcServer: RpcServer.respondercallId: 4796 service: ClientService methodName: Multi size: 3.3m connection: 9.1.143.58:41353: output error
2014-07-01 16:40:38,861 WARN  [RpcServer.handler=10,port=60020] ipc.RpcServer: RpcServer.handler=10,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-07-01 16:40:38,861 WARN  [RpcServer.handler=16,port=60020] ipc.RpcServer: RpcServer.respondercallId: 4794 service: ClientService methodName: Multi size: 2.7m connection: 9.1.143.58:41353: output error
2014-07-01 16:40:38,861 WARN  [RpcServer.handler=16,port=60020] ipc.RpcServer: RpcServer.handler=16,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-07-01 16:40:39,120 WARN  [RpcServer.handler=49,port=60020] ipc.RpcServer: RpcServer.respondercallId: 4798 service: ClientService methodName: Multi size: 3.3m connection: 9.1.143.58:41353: output error
2014-07-01 16:40:39,120 WARN  [RpcServer.handler=49,port=60020] ipc.RpcServer: RpcServer.handler=49,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-07-01 16:40:39,120 WARN  [RpcServer.handler=31,port=60020] ipc.RpcServer: RpcServer.respondercallId: 4695 service: ClientService methodName: Multi size: 3.3m connection: 9.1.143.58:41353: output error
2014-07-01 16:40:39,120 WARN  [RpcServer.handler=31,port=60020] ipc.RpcServer: RpcServer.handler=31,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-07-01 16:40:39,143 WARN  [RpcServer.handler=36,port=60020] ipc.RpcServer: RpcServer.respondercallId: 4756 service: ClientService methodName: Multi size: 3.3m connection: 9.1.143.58:41353: output error
2014-07-01 16:40:39,143 WARN  [RpcServer.handler=36,port=60020] ipc.RpcServer: RpcServer.handler=36,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-07-01 16:40:39,143 WARN  [RpcServer.handler=20,port=60020] ipc.RpcServer: RpcServer.respondercallId: 4795 service: ClientService methodName: Multi size: 3.4m connection: 9.1.143.58:41353: output error
2014-07-01 16:40:39,143 WARN  [RpcServer.handler=20,port=60020] ipc.RpcServer: RpcServer.handler=20,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-07-01 16:40:39,148 WARN  [RpcServer.handler=8,port=60020] ipc.RpcServer: RpcServer.respondercallId: 4797 service: ClientService methodName: Multi size: 3.4m connection: 9.1.143.58:41353: output error
2014-07-01 16:40:39,148 WARN  [RpcServer.handler=8,port=60020] ipc.RpcServer: RpcServer.handler=8,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-07-01 16:40:39,563 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-01 16:40:39,567 WARN  [RpcServer.handler=48,port=60020] ipc.RpcServer: RpcServer.respondercallId: 4793 service: ClientService methodName: Multi size: 2.9m connection: 9.1.143.58:41353: output error
2014-07-01 16:40:39,567 WARN  [RpcServer.handler=48,port=60020] ipc.RpcServer: RpcServer.handler=48,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-07-01 16:40:39,567 WARN  [RpcServer.handler=3,port=60020] ipc.RpcServer: RpcServer.respondercallId: 4790 service: ClientService methodName: Multi size: 3.4m connection: 9.1.143.58:41353: output error
2014-07-01 16:40:39,567 WARN  [RpcServer.handler=3,port=60020] ipc.RpcServer: RpcServer.handler=3,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-07-01 16:40:39,568 WARN  [RpcServer.handler=42,port=60020] ipc.RpcServer: RpcServer.respondercallId: 4792 service: ClientService methodName: Multi size: 3.0m connection: 9.1.143.58:41353: output error
2014-07-01 16:40:39,568 WARN  [RpcServer.handler=42,port=60020] ipc.RpcServer: RpcServer.handler=42,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-07-01 16:40:39,653 WARN  [RpcServer.handler=29,port=60020] ipc.RpcServer: RpcServer.respondercallId: 4791 service: ClientService methodName: Multi size: 3.4m connection: 9.1.143.58:41353: output error
2014-07-01 16:40:39,653 WARN  [RpcServer.handler=29,port=60020] ipc.RpcServer: RpcServer.handler=29,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-07-01 16:40:39,744 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 30512 synced till here 30509
2014-07-01 16:40:39,761 WARN  [RpcServer.handler=39,port=60020] ipc.RpcServer: RpcServer.respondercallId: 4787 service: ClientService methodName: Multi size: 3.4m connection: 9.1.143.58:41353: output error
2014-07-01 16:40:39,762 WARN  [RpcServer.handler=39,port=60020] ipc.RpcServer: RpcServer.handler=39,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-07-01 16:40:39,768 WARN  [RpcServer.handler=41,port=60020] ipc.RpcServer: RpcServer.respondercallId: 4783 service: ClientService methodName: Multi size: 3.3m connection: 9.1.143.58:41353: output error
2014-07-01 16:40:39,768 WARN  [RpcServer.handler=41,port=60020] ipc.RpcServer: RpcServer.handler=41,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-07-01 16:40:39,775 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1404255227840/sceplus-vm48.almaden.ibm.com%2C60020%2C1404255227840.1404258036667 with entries=127, filesize=95.4m; new WAL /hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1404255227840/sceplus-vm48.almaden.ibm.com%2C60020%2C1404255227840.1404258039564
2014-07-01 16:40:40,926 WARN  [RpcServer.handler=5,port=60020] ipc.RpcServer: RpcServer.respondercallId: 4816 service: ClientService methodName: Multi size: 2.9m connection: 9.1.143.58:41353: output error
2014-07-01 16:40:40,927 WARN  [RpcServer.handler=5,port=60020] ipc.RpcServer: RpcServer.handler=5,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-07-01 16:40:41,183 WARN  [RpcServer.handler=9,port=60020] ipc.RpcServer: RpcServer.respondercallId: 4788 service: ClientService methodName: Multi size: 3.4m connection: 9.1.143.58:41353: output error
2014-07-01 16:40:41,184 WARN  [RpcServer.handler=9,port=60020] ipc.RpcServer: RpcServer.handler=9,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-07-01 16:40:41,259 WARN  [RpcServer.handler=13,port=60020] ipc.RpcServer: RpcServer.respondercallId: 4781 service: ClientService methodName: Multi size: 3.0m connection: 9.1.143.58:41353: output error
2014-07-01 16:40:41,259 WARN  [RpcServer.handler=13,port=60020] ipc.RpcServer: RpcServer.handler=13,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-07-01 16:40:41,319 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-01 16:40:41,323 WARN  [RpcServer.handler=24,port=60020] ipc.RpcServer: RpcServer.respondercallId: 4818 service: ClientService methodName: Multi size: 3.3m connection: 9.1.143.58:41353: output error
2014-07-01 16:40:41,323 WARN  [RpcServer.handler=24,port=60020] ipc.RpcServer: RpcServer.handler=24,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-07-01 16:40:41,324 WARN  [RpcServer.handler=1,port=60020] ipc.RpcServer: RpcServer.respondercallId: 4780 service: ClientService methodName: Multi size: 3.3m connection: 9.1.143.58:41353: output error
2014-07-01 16:40:41,324 WARN  [RpcServer.handler=1,port=60020] ipc.RpcServer: RpcServer.handler=1,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-07-01 16:40:41,326 WARN  [RpcServer.handler=38,port=60020] ipc.RpcServer: RpcServer.respondercallId: 4819 service: ClientService methodName: Multi size: 3.3m connection: 9.1.143.58:41353: output error
2014-07-01 16:40:41,326 WARN  [RpcServer.handler=38,port=60020] ipc.RpcServer: RpcServer.handler=38,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-07-01 16:40:41,327 WARN  [RpcServer.handler=43,port=60020] ipc.RpcServer: RpcServer.respondercallId: 4786 service: ClientService methodName: Multi size: 2.9m connection: 9.1.143.58:41353: output error
2014-07-01 16:40:41,327 WARN  [RpcServer.handler=43,port=60020] ipc.RpcServer: RpcServer.handler=43,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-07-01 16:40:41,327 WARN  [RpcServer.handler=22,port=60020] ipc.RpcServer: RpcServer.respondercallId: 4785 service: ClientService methodName: Multi size: 3.4m connection: 9.1.143.58:41353: output error
2014-07-01 16:40:41,328 WARN  [RpcServer.handler=22,port=60020] ipc.RpcServer: RpcServer.handler=22,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-07-01 16:40:41,331 WARN  [RpcServer.handler=25,port=60020] ipc.RpcServer: RpcServer.respondercallId: 4779 service: ClientService methodName: Multi size: 3.4m connection: 9.1.143.58:41353: output error
2014-07-01 16:40:41,332 WARN  [RpcServer.handler=25,port=60020] ipc.RpcServer: RpcServer.handler=25,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-07-01 16:40:41,332 WARN  [RpcServer.handler=45,port=60020] ipc.RpcServer: RpcServer.respondercallId: 4817 service: ClientService methodName: Multi size: 3.3m connection: 9.1.143.58:41353: output error
2014-07-01 16:40:41,332 WARN  [RpcServer.handler=45,port=60020] ipc.RpcServer: RpcServer.handler=45,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-07-01 16:40:41,333 WARN  [RpcServer.handler=17,port=60020] ipc.RpcServer: RpcServer.respondercallId: 4789 service: ClientService methodName: Multi size: 3.5m connection: 9.1.143.58:41353: output error
2014-07-01 16:40:41,333 WARN  [RpcServer.handler=17,port=60020] ipc.RpcServer: RpcServer.handler=17,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-07-01 16:40:41,370 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 30607 synced till here 30592
2014-07-01 16:40:41,462 WARN  [RpcServer.handler=23,port=60020] ipc.RpcServer: RpcServer.respondercallId: 4784 service: ClientService methodName: Multi size: 3.3m connection: 9.1.143.58:41353: output error
2014-07-01 16:40:41,462 WARN  [RpcServer.handler=23,port=60020] ipc.RpcServer: RpcServer.handler=23,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-07-01 16:40:41,519 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1404255227840/sceplus-vm48.almaden.ibm.com%2C60020%2C1404255227840.1404258039564 with entries=95, filesize=75.1m; new WAL /hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1404255227840/sceplus-vm48.almaden.ibm.com%2C60020%2C1404255227840.1404258041320
2014-07-01 16:40:43,305 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-01 16:40:43,452 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 30707 synced till here 30693
2014-07-01 16:40:43,741 INFO  [MemStoreFlusher.1] regionserver.DefaultStoreFlusher: Flushed, sequenceid=1968, memsize=218.4m, hasBloomFilter=true, into tmp file hdfs://master:54310/hbase/data/default/usertable/a59b89c303508bc329bc1095d0beb9e5/.tmp/740f76eaa1ca43a5bee141711b36878d
2014-07-01 16:40:43,752 DEBUG [MemStoreFlusher.1] regionserver.HRegionFileSystem: Committing store file hdfs://master:54310/hbase/data/default/usertable/a59b89c303508bc329bc1095d0beb9e5/.tmp/740f76eaa1ca43a5bee141711b36878d as hdfs://master:54310/hbase/data/default/usertable/a59b89c303508bc329bc1095d0beb9e5/family/740f76eaa1ca43a5bee141711b36878d
2014-07-01 16:40:43,760 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1404255227840/sceplus-vm48.almaden.ibm.com%2C60020%2C1404255227840.1404258041320 with entries=100, filesize=88.0m; new WAL /hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1404255227840/sceplus-vm48.almaden.ibm.com%2C60020%2C1404255227840.1404258043305
2014-07-01 16:40:43,762 INFO  [MemStoreFlusher.1] regionserver.HStore: Added hdfs://master:54310/hbase/data/default/usertable/a59b89c303508bc329bc1095d0beb9e5/family/740f76eaa1ca43a5bee141711b36878d, entries=795220, sequenceid=1968, filesize=56.7m
2014-07-01 16:40:43,763 INFO  [MemStoreFlusher.1] regionserver.HRegion: Finished memstore flush of ~260.6m/273211600, currentsize=256.4m/268838720 for region usertable,user3,1404257714774.a59b89c303508bc329bc1095d0beb9e5. in 18982ms, sequenceid=1968, compaction requested=true
2014-07-01 16:40:43,764 DEBUG [MemStoreFlusher.1] regionserver.CompactSplitThread: Small Compaction requested: system; Because: MemStoreFlusher.1; compaction_queue=(0:0), split_queue=0, merge_queue=0
2014-07-01 16:40:43,764 DEBUG [regionserver60020-smallCompactions-1404255751957] compactions.RatioBasedCompactionPolicy: Selecting compaction from 10 store files, 0 compacting, 10 eligible, 20 blocking
2014-07-01 16:40:43,764 DEBUG [regionserver60020-smallCompactions-1404255751957] compactions.RatioBasedCompactionPolicy: Some files are too large. Excluding 10 files from compaction candidates
2014-07-01 16:40:43,764 DEBUG [MemStoreFlusher.1] regionserver.HRegion: Started memstore flush for usertable,user8,1404257714774.b6ae40892bbb2a1c545234ed91d62d1a., current region memstore size 427.1m
2014-07-01 16:40:43,764 DEBUG [regionserver60020-smallCompactions-1404255751957] compactions.ExploringCompactionPolicy: Exploring compaction algorithm has selected 0 files of size 0 starting at candidate #-1 after considering 0 permutations with 0 in ratio
2014-07-01 16:40:43,764 DEBUG [regionserver60020-smallCompactions-1404255751957] compactions.RatioBasedCompactionPolicy: Not compacting files because we only have 0 files ready for compaction. Need 3 to initiate.
2014-07-01 16:40:43,764 DEBUG [regionserver60020-smallCompactions-1404255751957] regionserver.CompactSplitThread: Not compacting usertable,user3,1404257714774.a59b89c303508bc329bc1095d0beb9e5. because compaction request was cancelled
2014-07-01 16:40:43,917 DEBUG [RpcServer.handler=21,port=60020] regionserver.HRegion: Flush requested on usertable,user3,1404257714774.a59b89c303508bc329bc1095d0beb9e5.
2014-07-01 16:40:45,495 DEBUG [MemStoreFlusher.1] util.FSUtils: DFS Client does not support most favored nodes create; using default create
2014-07-01 16:40:45,575 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-01 16:40:45,649 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 30826 synced till here 30813
2014-07-01 16:40:45,751 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1404255227840/sceplus-vm48.almaden.ibm.com%2C60020%2C1404255227840.1404258043305 with entries=119, filesize=84.0m; new WAL /hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1404255227840/sceplus-vm48.almaden.ibm.com%2C60020%2C1404255227840.1404258045576
2014-07-01 16:40:47,571 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-01 16:40:47,717 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 30927 synced till here 30904
2014-07-01 16:40:47,826 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1404255227840/sceplus-vm48.almaden.ibm.com%2C60020%2C1404255227840.1404258045576 with entries=101, filesize=80.1m; new WAL /hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1404255227840/sceplus-vm48.almaden.ibm.com%2C60020%2C1404255227840.1404258047571
2014-07-01 16:40:49,552 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-01 16:40:49,674 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 31016 synced till here 31014
2014-07-01 16:40:49,748 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1404255227840/sceplus-vm48.almaden.ibm.com%2C60020%2C1404255227840.1404258047571 with entries=89, filesize=69.4m; new WAL /hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1404255227840/sceplus-vm48.almaden.ibm.com%2C60020%2C1404255227840.1404258049552
2014-07-01 16:40:52,066 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-01 16:40:52,087 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 31092 synced till here 31091
2014-07-01 16:40:52,100 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1404255227840/sceplus-vm48.almaden.ibm.com%2C60020%2C1404255227840.1404258049552 with entries=76, filesize=62.2m; new WAL /hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1404255227840/sceplus-vm48.almaden.ibm.com%2C60020%2C1404255227840.1404258052067
2014-07-01 16:40:53,467 INFO  [MemStoreFlusher.0] regionserver.DefaultStoreFlusher: Flushed, sequenceid=2043, memsize=250.4m, hasBloomFilter=true, into tmp file hdfs://master:54310/hbase/data/default/usertable/c831a0ac8e8f14e82fdaef2ba3a5e1f6/.tmp/324e31d59fc346ecac15e58b3a2586c1
2014-07-01 16:40:53,522 DEBUG [MemStoreFlusher.0] regionserver.HRegionFileSystem: Committing store file hdfs://master:54310/hbase/data/default/usertable/c831a0ac8e8f14e82fdaef2ba3a5e1f6/.tmp/324e31d59fc346ecac15e58b3a2586c1 as hdfs://master:54310/hbase/data/default/usertable/c831a0ac8e8f14e82fdaef2ba3a5e1f6/family/324e31d59fc346ecac15e58b3a2586c1
2014-07-01 16:40:53,545 INFO  [MemStoreFlusher.0] regionserver.HStore: Added hdfs://master:54310/hbase/data/default/usertable/c831a0ac8e8f14e82fdaef2ba3a5e1f6/family/324e31d59fc346ecac15e58b3a2586c1, entries=911710, sequenceid=2043, filesize=65.0m
2014-07-01 16:40:53,546 INFO  [MemStoreFlusher.0] regionserver.HRegion: Finished memstore flush of ~379.4m/397879280, currentsize=262.9m/275658240 for region usertable,user4,1404257714774.c831a0ac8e8f14e82fdaef2ba3a5e1f6. in 19623ms, sequenceid=2043, compaction requested=true
2014-07-01 16:40:53,546 DEBUG [MemStoreFlusher.0] regionserver.CompactSplitThread: Small Compaction requested: system; Because: MemStoreFlusher.0; compaction_queue=(0:0), split_queue=0, merge_queue=0
2014-07-01 16:40:53,546 DEBUG [regionserver60020-smallCompactions-1404255751957] compactions.RatioBasedCompactionPolicy: Selecting compaction from 10 store files, 0 compacting, 10 eligible, 20 blocking
2014-07-01 16:40:53,547 DEBUG [regionserver60020-smallCompactions-1404255751957] compactions.RatioBasedCompactionPolicy: Some files are too large. Excluding 10 files from compaction candidates
2014-07-01 16:40:53,547 DEBUG [MemStoreFlusher.0] regionserver.HRegion: Started memstore flush for usertable,user2,1404257714774.2ed053f05f5eec791a57de24e16f0043., current region memstore size 493.1m
2014-07-01 16:40:53,547 DEBUG [regionserver60020-smallCompactions-1404255751957] compactions.ExploringCompactionPolicy: Exploring compaction algorithm has selected 0 files of size 0 starting at candidate #-1 after considering 0 permutations with 0 in ratio
2014-07-01 16:40:53,547 DEBUG [regionserver60020-smallCompactions-1404255751957] compactions.RatioBasedCompactionPolicy: Not compacting files because we only have 0 files ready for compaction. Need 3 to initiate.
2014-07-01 16:40:53,547 DEBUG [regionserver60020-smallCompactions-1404255751957] regionserver.CompactSplitThread: Not compacting usertable,user4,1404257714774.c831a0ac8e8f14e82fdaef2ba3a5e1f6. because compaction request was cancelled
2014-07-01 16:40:53,586 DEBUG [RpcServer.handler=20,port=60020] regionserver.HRegion: Flush requested on usertable,user4,1404257714774.c831a0ac8e8f14e82fdaef2ba3a5e1f6.
2014-07-01 16:40:54,830 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-01 16:40:54,857 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 31172 synced till here 31165
2014-07-01 16:40:54,864 DEBUG [MemStoreFlusher.0] util.FSUtils: DFS Client does not support most favored nodes create; using default create
2014-07-01 16:40:55,005 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1404255227840/sceplus-vm48.almaden.ibm.com%2C60020%2C1404255227840.1404258052067 with entries=80, filesize=68.5m; new WAL /hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1404255227840/sceplus-vm48.almaden.ibm.com%2C60020%2C1404255227840.1404258054831
2014-07-01 16:40:56,108 DEBUG [RpcServer.handler=34,port=60020] regionserver.HRegion: Flush requested on usertable,user9,1404257714774.84635e5ca79006f09dceafa095992aab.
2014-07-01 16:40:56,971 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-01 16:40:56,994 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1404255227840/sceplus-vm48.almaden.ibm.com%2C60020%2C1404255227840.1404258054831 with entries=77, filesize=61.9m; new WAL /hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1404255227840/sceplus-vm48.almaden.ibm.com%2C60020%2C1404255227840.1404258056971
2014-07-01 16:40:58,604 INFO  [MemStoreFlusher.1] regionserver.DefaultStoreFlusher: Flushed, sequenceid=2120, memsize=196.5m, hasBloomFilter=true, into tmp file hdfs://master:54310/hbase/data/default/usertable/b6ae40892bbb2a1c545234ed91d62d1a/.tmp/cb8d746f46c44aaab8662f1c936f21ae
2014-07-01 16:40:58,613 DEBUG [MemStoreFlusher.1] regionserver.HRegionFileSystem: Committing store file hdfs://master:54310/hbase/data/default/usertable/b6ae40892bbb2a1c545234ed91d62d1a/.tmp/cb8d746f46c44aaab8662f1c936f21ae as hdfs://master:54310/hbase/data/default/usertable/b6ae40892bbb2a1c545234ed91d62d1a/family/cb8d746f46c44aaab8662f1c936f21ae
2014-07-01 16:40:58,622 INFO  [MemStoreFlusher.1] regionserver.HStore: Added hdfs://master:54310/hbase/data/default/usertable/b6ae40892bbb2a1c545234ed91d62d1a/family/cb8d746f46c44aaab8662f1c936f21ae, entries=715540, sequenceid=2120, filesize=51.0m
2014-07-01 16:40:58,622 INFO  [MemStoreFlusher.1] regionserver.HRegion: Finished memstore flush of ~437.4m/458668400, currentsize=183.7m/192638160 for region usertable,user8,1404257714774.b6ae40892bbb2a1c545234ed91d62d1a. in 14858ms, sequenceid=2120, compaction requested=true
2014-07-01 16:40:58,622 DEBUG [MemStoreFlusher.1] regionserver.CompactSplitThread: Small Compaction requested: system; Because: MemStoreFlusher.1; compaction_queue=(0:0), split_queue=0, merge_queue=0
2014-07-01 16:40:58,623 DEBUG [regionserver60020-smallCompactions-1404255751957] compactions.RatioBasedCompactionPolicy: Selecting compaction from 10 store files, 0 compacting, 10 eligible, 20 blocking
2014-07-01 16:40:58,623 DEBUG [MemStoreFlusher.1] regionserver.HRegion: Started memstore flush for usertable,user3,1404257714774.a59b89c303508bc329bc1095d0beb9e5., current region memstore size 437.1m
2014-07-01 16:40:58,623 DEBUG [regionserver60020-smallCompactions-1404255751957] compactions.RatioBasedCompactionPolicy: Some files are too large. Excluding 10 files from compaction candidates
2014-07-01 16:40:58,623 DEBUG [regionserver60020-smallCompactions-1404255751957] compactions.ExploringCompactionPolicy: Exploring compaction algorithm has selected 0 files of size 0 starting at candidate #-1 after considering 0 permutations with 0 in ratio
2014-07-01 16:40:58,623 DEBUG [regionserver60020-smallCompactions-1404255751957] compactions.RatioBasedCompactionPolicy: Not compacting files because we only have 0 files ready for compaction. Need 3 to initiate.
2014-07-01 16:40:58,623 DEBUG [regionserver60020-smallCompactions-1404255751957] regionserver.CompactSplitThread: Not compacting usertable,user8,1404257714774.b6ae40892bbb2a1c545234ed91d62d1a. because compaction request was cancelled
2014-07-01 16:40:59,000 DEBUG [MemStoreFlusher.1] util.FSUtils: DFS Client does not support most favored nodes create; using default create
2014-07-01 16:40:59,728 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-01 16:41:00,450 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 31332 synced till here 31321
2014-07-01 16:41:00,715 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1404255227840/sceplus-vm48.almaden.ibm.com%2C60020%2C1404255227840.1404258056971 with entries=83, filesize=72.1m; new WAL /hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1404255227840/sceplus-vm48.almaden.ibm.com%2C60020%2C1404255227840.1404258060404
2014-07-01 16:41:01,612 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-01 16:41:01,686 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 31421 synced till here 31413
2014-07-01 16:41:02,351 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1404255227840/sceplus-vm48.almaden.ibm.com%2C60020%2C1404255227840.1404258060404 with entries=89, filesize=72.8m; new WAL /hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1404255227840/sceplus-vm48.almaden.ibm.com%2C60020%2C1404255227840.1404258061612
2014-07-01 16:41:03,457 DEBUG [RpcServer.handler=22,port=60020] regionserver.HRegion: Flush requested on usertable,user8,1404257714774.b6ae40892bbb2a1c545234ed91d62d1a.
2014-07-01 16:41:03,713 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-01 16:41:03,757 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 31503 synced till here 31501
2014-07-01 16:41:03,774 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1404255227840/sceplus-vm48.almaden.ibm.com%2C60020%2C1404255227840.1404258061612 with entries=82, filesize=65.3m; new WAL /hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1404255227840/sceplus-vm48.almaden.ibm.com%2C60020%2C1404255227840.1404258063713
2014-07-01 16:41:05,775 INFO  [MemStoreFlusher.0] regionserver.DefaultStoreFlusher: Flushed, sequenceid=2205, memsize=138.5m, hasBloomFilter=true, into tmp file hdfs://master:54310/hbase/data/default/usertable/2ed053f05f5eec791a57de24e16f0043/.tmp/607086314d2749d29985822ac5b26898
2014-07-01 16:41:05,787 DEBUG [MemStoreFlusher.0] regionserver.HRegionFileSystem: Committing store file hdfs://master:54310/hbase/data/default/usertable/2ed053f05f5eec791a57de24e16f0043/.tmp/607086314d2749d29985822ac5b26898 as hdfs://master:54310/hbase/data/default/usertable/2ed053f05f5eec791a57de24e16f0043/family/607086314d2749d29985822ac5b26898
2014-07-01 16:41:05,835 INFO  [MemStoreFlusher.0] regionserver.HStore: Added hdfs://master:54310/hbase/data/default/usertable/2ed053f05f5eec791a57de24e16f0043/family/607086314d2749d29985822ac5b26898, entries=504220, sequenceid=2205, filesize=35.9m
2014-07-01 16:41:05,836 INFO  [MemStoreFlusher.0] regionserver.HRegion: Finished memstore flush of ~499.9m/524192080, currentsize=139.7m/146475280 for region usertable,user2,1404257714774.2ed053f05f5eec791a57de24e16f0043. in 12289ms, sequenceid=2205, compaction requested=true
2014-07-01 16:41:05,837 DEBUG [MemStoreFlusher.0] regionserver.CompactSplitThread: Small Compaction requested: system; Because: MemStoreFlusher.0; compaction_queue=(0:0), split_queue=0, merge_queue=0
2014-07-01 16:41:05,837 DEBUG [regionserver60020-smallCompactions-1404255751957] compactions.RatioBasedCompactionPolicy: Selecting compaction from 11 store files, 0 compacting, 11 eligible, 20 blocking
2014-07-01 16:41:05,838 DEBUG [regionserver60020-smallCompactions-1404255751957] compactions.RatioBasedCompactionPolicy: Some files are too large. Excluding 11 files from compaction candidates
2014-07-01 16:41:05,838 DEBUG [MemStoreFlusher.0] regionserver.HRegion: Started memstore flush for usertable,user4,1404257714774.c831a0ac8e8f14e82fdaef2ba3a5e1f6., current region memstore size 417.5m
2014-07-01 16:41:05,838 DEBUG [regionserver60020-smallCompactions-1404255751957] compactions.ExploringCompactionPolicy: Exploring compaction algorithm has selected 0 files of size 0 starting at candidate #-1 after considering 0 permutations with 0 in ratio
2014-07-01 16:41:05,838 DEBUG [regionserver60020-smallCompactions-1404255751957] compactions.RatioBasedCompactionPolicy: Not compacting files because we only have 0 files ready for compaction. Need 3 to initiate.
2014-07-01 16:41:05,838 DEBUG [regionserver60020-smallCompactions-1404255751957] regionserver.CompactSplitThread: Not compacting usertable,user2,1404257714774.2ed053f05f5eec791a57de24e16f0043. because compaction request was cancelled
2014-07-01 16:41:06,207 DEBUG [MemStoreFlusher.0] util.FSUtils: DFS Client does not support most favored nodes create; using default create
2014-07-01 16:41:06,534 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-01 16:41:06,556 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 31581 synced till here 31577
2014-07-01 16:41:06,647 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1404255227840/sceplus-vm48.almaden.ibm.com%2C60020%2C1404255227840.1404258063713 with entries=78, filesize=65.6m; new WAL /hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1404255227840/sceplus-vm48.almaden.ibm.com%2C60020%2C1404255227840.1404258066534
2014-07-01 16:41:06,903 INFO  [MemStoreFlusher.1] regionserver.DefaultStoreFlusher: Flushed, sequenceid=2225, memsize=120.8m, hasBloomFilter=true, into tmp file hdfs://master:54310/hbase/data/default/usertable/a59b89c303508bc329bc1095d0beb9e5/.tmp/6653b0555a8b4437ba8d62b9697f4ec4
2014-07-01 16:41:06,954 DEBUG [MemStoreFlusher.1] regionserver.HRegionFileSystem: Committing store file hdfs://master:54310/hbase/data/default/usertable/a59b89c303508bc329bc1095d0beb9e5/.tmp/6653b0555a8b4437ba8d62b9697f4ec4 as hdfs://master:54310/hbase/data/default/usertable/a59b89c303508bc329bc1095d0beb9e5/family/6653b0555a8b4437ba8d62b9697f4ec4
2014-07-01 16:41:06,994 INFO  [MemStoreFlusher.1] regionserver.HStore: Added hdfs://master:54310/hbase/data/default/usertable/a59b89c303508bc329bc1095d0beb9e5/family/6653b0555a8b4437ba8d62b9697f4ec4, entries=439740, sequenceid=2225, filesize=31.3m
2014-07-01 16:41:06,998 INFO  [MemStoreFlusher.1] regionserver.HRegion: Finished memstore flush of ~437.1m/458322640, currentsize=121.3m/127166160 for region usertable,user3,1404257714774.a59b89c303508bc329bc1095d0beb9e5. in 8375ms, sequenceid=2225, compaction requested=true
2014-07-01 16:41:06,998 DEBUG [MemStoreFlusher.1] regionserver.CompactSplitThread: Small Compaction requested: system; Because: MemStoreFlusher.1; compaction_queue=(0:0), split_queue=0, merge_queue=0
2014-07-01 16:41:06,998 DEBUG [regionserver60020-smallCompactions-1404255751957] compactions.RatioBasedCompactionPolicy: Selecting compaction from 11 store files, 0 compacting, 11 eligible, 20 blocking
2014-07-01 16:41:06,999 DEBUG [regionserver60020-smallCompactions-1404255751957] compactions.RatioBasedCompactionPolicy: Some files are too large. Excluding 11 files from compaction candidates
2014-07-01 16:41:06,999 DEBUG [MemStoreFlusher.1] regionserver.HRegion: Started memstore flush for usertable,user9,1404257714774.84635e5ca79006f09dceafa095992aab., current region memstore size 293.9m
2014-07-01 16:41:06,999 DEBUG [regionserver60020-smallCompactions-1404255751957] compactions.ExploringCompactionPolicy: Exploring compaction algorithm has selected 0 files of size 0 starting at candidate #-1 after considering 0 permutations with 0 in ratio
2014-07-01 16:41:06,999 DEBUG [regionserver60020-smallCompactions-1404255751957] compactions.RatioBasedCompactionPolicy: Not compacting files because we only have 0 files ready for compaction. Need 3 to initiate.
2014-07-01 16:41:06,999 DEBUG [regionserver60020-smallCompactions-1404255751957] regionserver.CompactSplitThread: Not compacting usertable,user3,1404257714774.a59b89c303508bc329bc1095d0beb9e5. because compaction request was cancelled
2014-07-01 16:41:08,469 DEBUG [MemStoreFlusher.1] util.FSUtils: DFS Client does not support most favored nodes create; using default create
2014-07-01 16:41:08,618 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-01 16:41:08,706 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 31668 synced till here 31652
2014-07-01 16:41:08,882 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1404255227840/sceplus-vm48.almaden.ibm.com%2C60020%2C1404255227840.1404258066534 with entries=87, filesize=72.9m; new WAL /hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1404255227840/sceplus-vm48.almaden.ibm.com%2C60020%2C1404255227840.1404258068618
2014-07-01 16:41:10,992 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-01 16:41:11,142 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 31768 synced till here 31753
2014-07-01 16:41:12,617 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1404255227840/sceplus-vm48.almaden.ibm.com%2C60020%2C1404255227840.1404258068618 with entries=100, filesize=84.1m; new WAL /hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1404255227840/sceplus-vm48.almaden.ibm.com%2C60020%2C1404255227840.1404258070993
2014-07-01 16:41:13,511 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-01 16:41:13,579 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 31858 synced till here 31848
2014-07-01 16:41:13,674 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1404255227840/sceplus-vm48.almaden.ibm.com%2C60020%2C1404255227840.1404258070993 with entries=90, filesize=71.7m; new WAL /hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1404255227840/sceplus-vm48.almaden.ibm.com%2C60020%2C1404255227840.1404258073511
2014-07-01 16:41:15,014 DEBUG [RpcServer.handler=49,port=60020] regionserver.HRegion: Flush requested on usertable,user2,1404257714774.2ed053f05f5eec791a57de24e16f0043.
2014-07-01 16:41:15,752 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-01 16:41:15,809 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 31959 synced till here 31946
2014-07-01 16:41:15,959 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1404255227840/sceplus-vm48.almaden.ibm.com%2C60020%2C1404255227840.1404258073511 with entries=101, filesize=80.2m; new WAL /hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1404255227840/sceplus-vm48.almaden.ibm.com%2C60020%2C1404255227840.1404258075753
2014-07-01 16:41:17,487 DEBUG [RpcServer.handler=12,port=60020] regionserver.HRegion: Flush requested on usertable,user3,1404257714774.a59b89c303508bc329bc1095d0beb9e5.
2014-07-01 16:41:18,018 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-01 16:41:18,076 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 32059 synced till here 32046
2014-07-01 16:41:18,344 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1404255227840/sceplus-vm48.almaden.ibm.com%2C60020%2C1404255227840.1404258075753 with entries=100, filesize=79.9m; new WAL /hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1404255227840/sceplus-vm48.almaden.ibm.com%2C60020%2C1404255227840.1404258078018
2014-07-01 16:41:20,420 INFO  [JvmPauseMonitor] util.JvmPauseMonitor: Detected pause in JVM or host machine (eg GC): pause of approximately 1182ms
GC pool 'ParNew' had collection(s): count=1 time=1251ms
2014-07-01 16:41:21,248 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-01 16:41:21,344 INFO  [MemStoreFlusher.1] regionserver.DefaultStoreFlusher: Flushed, sequenceid=2286, memsize=142.8m, hasBloomFilter=true, into tmp file hdfs://master:54310/hbase/data/default/usertable/84635e5ca79006f09dceafa095992aab/.tmp/6e530ac59355455ca06513c48c00452f
2014-07-01 16:41:22,401 DEBUG [MemStoreFlusher.1] regionserver.HRegionFileSystem: Committing store file hdfs://master:54310/hbase/data/default/usertable/84635e5ca79006f09dceafa095992aab/.tmp/6e530ac59355455ca06513c48c00452f as hdfs://master:54310/hbase/data/default/usertable/84635e5ca79006f09dceafa095992aab/family/6e530ac59355455ca06513c48c00452f
2014-07-01 16:41:22,402 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 32174 synced till here 32159
2014-07-01 16:41:22,475 INFO  [MemStoreFlusher.1] regionserver.HStore: Added hdfs://master:54310/hbase/data/default/usertable/84635e5ca79006f09dceafa095992aab/family/6e530ac59355455ca06513c48c00452f, entries=520130, sequenceid=2286, filesize=37.1m
2014-07-01 16:41:22,475 INFO  [MemStoreFlusher.1] regionserver.HRegion: Finished memstore flush of ~294.5m/308757840, currentsize=53.5m/56053920 for region usertable,user9,1404257714774.84635e5ca79006f09dceafa095992aab. in 15476ms, sequenceid=2286, compaction requested=true
2014-07-01 16:41:22,476 DEBUG [regionserver60020-smallCompactions-1404255751957] compactions.RatioBasedCompactionPolicy: Selecting compaction from 4 store files, 0 compacting, 4 eligible, 20 blocking
2014-07-01 16:41:22,476 DEBUG [regionserver60020-smallCompactions-1404255751957] compactions.RatioBasedCompactionPolicy: Some files are too large. Excluding 4 files from compaction candidates
2014-07-01 16:41:22,476 DEBUG [regionserver60020-smallCompactions-1404255751957] compactions.ExploringCompactionPolicy: Exploring compaction algorithm has selected 0 files of size 0 starting at candidate #-1 after considering 0 permutations with 0 in ratio
2014-07-01 16:41:22,476 DEBUG [regionserver60020-smallCompactions-1404255751957] compactions.RatioBasedCompactionPolicy: Not compacting files because we only have 0 files ready for compaction. Need 3 to initiate.
2014-07-01 16:41:22,476 DEBUG [regionserver60020-smallCompactions-1404255751957] regionserver.CompactSplitThread: Not compacting usertable,user9,1404257714774.84635e5ca79006f09dceafa095992aab. because compaction request was cancelled
2014-07-01 16:41:22,477 DEBUG [MemStoreFlusher.1] regionserver.CompactSplitThread: Small Compaction requested: system; Because: MemStoreFlusher.1; compaction_queue=(0:0), split_queue=0, merge_queue=0
2014-07-01 16:41:22,479 DEBUG [MemStoreFlusher.1] regionserver.HRegion: Started memstore flush for usertable,user8,1404257714774.b6ae40892bbb2a1c545234ed91d62d1a., current region memstore size 478.4m
2014-07-01 16:41:22,539 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1404255227840/sceplus-vm48.almaden.ibm.com%2C60020%2C1404255227840.1404258078018 with entries=115, filesize=91.3m; new WAL /hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1404255227840/sceplus-vm48.almaden.ibm.com%2C60020%2C1404255227840.1404258081249
2014-07-01 16:41:22,540 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1404255227840/sceplus-vm48.almaden.ibm.com%2C60020%2C1404255227840.1404257997401
2014-07-01 16:41:22,540 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1404255227840/sceplus-vm48.almaden.ibm.com%2C60020%2C1404255227840.1404257999147
2014-07-01 16:41:22,540 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1404255227840/sceplus-vm48.almaden.ibm.com%2C60020%2C1404255227840.1404258001080
2014-07-01 16:41:22,540 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1404255227840/sceplus-vm48.almaden.ibm.com%2C60020%2C1404255227840.1404258002554
2014-07-01 16:41:22,540 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1404255227840/sceplus-vm48.almaden.ibm.com%2C60020%2C1404255227840.1404258004451
2014-07-01 16:41:22,540 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1404255227840/sceplus-vm48.almaden.ibm.com%2C60020%2C1404255227840.1404258006795
2014-07-01 16:41:22,540 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1404255227840/sceplus-vm48.almaden.ibm.com%2C60020%2C1404255227840.1404258008398
2014-07-01 16:41:22,540 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1404255227840/sceplus-vm48.almaden.ibm.com%2C60020%2C1404255227840.1404258011563
2014-07-01 16:41:22,540 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1404255227840/sceplus-vm48.almaden.ibm.com%2C60020%2C1404255227840.1404258012682
2014-07-01 16:41:22,541 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1404255227840/sceplus-vm48.almaden.ibm.com%2C60020%2C1404255227840.1404258014217
2014-07-01 16:41:22,541 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1404255227840/sceplus-vm48.almaden.ibm.com%2C60020%2C1404255227840.1404258016223
2014-07-01 16:41:22,541 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1404255227840/sceplus-vm48.almaden.ibm.com%2C60020%2C1404255227840.1404258018264
2014-07-01 16:41:22,541 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1404255227840/sceplus-vm48.almaden.ibm.com%2C60020%2C1404255227840.1404258020163
2014-07-01 16:41:22,541 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1404255227840/sceplus-vm48.almaden.ibm.com%2C60020%2C1404255227840.1404258022463
2014-07-01 16:41:22,541 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1404255227840/sceplus-vm48.almaden.ibm.com%2C60020%2C1404255227840.1404258024777
2014-07-01 16:41:22,541 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1404255227840/sceplus-vm48.almaden.ibm.com%2C60020%2C1404255227840.1404258027137
2014-07-01 16:41:22,541 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1404255227840/sceplus-vm48.almaden.ibm.com%2C60020%2C1404255227840.1404258029481
2014-07-01 16:41:23,203 INFO  [MemStoreFlusher.0] regionserver.DefaultStoreFlusher: Flushed, sequenceid=2286, memsize=192.9m, hasBloomFilter=true, into tmp file hdfs://master:54310/hbase/data/default/usertable/c831a0ac8e8f14e82fdaef2ba3a5e1f6/.tmp/a9b894d5ed8841cabd8ae32e34093070
2014-07-01 16:41:23,223 DEBUG [MemStoreFlusher.1] util.FSUtils: DFS Client does not support most favored nodes create; using default create
2014-07-01 16:41:23,239 DEBUG [MemStoreFlusher.0] regionserver.HRegionFileSystem: Committing store file hdfs://master:54310/hbase/data/default/usertable/c831a0ac8e8f14e82fdaef2ba3a5e1f6/.tmp/a9b894d5ed8841cabd8ae32e34093070 as hdfs://master:54310/hbase/data/default/usertable/c831a0ac8e8f14e82fdaef2ba3a5e1f6/family/a9b894d5ed8841cabd8ae32e34093070
2014-07-01 16:41:23,253 INFO  [MemStoreFlusher.0] regionserver.HStore: Added hdfs://master:54310/hbase/data/default/usertable/c831a0ac8e8f14e82fdaef2ba3a5e1f6/family/a9b894d5ed8841cabd8ae32e34093070, entries=702240, sequenceid=2286, filesize=50.1m
2014-07-01 16:41:23,253 INFO  [MemStoreFlusher.0] regionserver.HRegion: Finished memstore flush of ~417.5m/437798160, currentsize=231.5m/242782480 for region usertable,user4,1404257714774.c831a0ac8e8f14e82fdaef2ba3a5e1f6. in 17415ms, sequenceid=2286, compaction requested=true
2014-07-01 16:41:23,254 DEBUG [MemStoreFlusher.0] regionserver.CompactSplitThread: Small Compaction requested: system; Because: MemStoreFlusher.0; compaction_queue=(0:0), split_queue=0, merge_queue=0
2014-07-01 16:41:23,254 DEBUG [regionserver60020-smallCompactions-1404255751957] compactions.RatioBasedCompactionPolicy: Selecting compaction from 11 store files, 0 compacting, 11 eligible, 20 blocking
2014-07-01 16:41:23,255 DEBUG [MemStoreFlusher.0] regionserver.HRegion: Started memstore flush for usertable,user2,1404257714774.2ed053f05f5eec791a57de24e16f0043., current region memstore size 369.3m
2014-07-01 16:41:23,255 DEBUG [regionserver60020-smallCompactions-1404255751957] compactions.RatioBasedCompactionPolicy: Some files are too large. Excluding 11 files from compaction candidates
2014-07-01 16:41:23,255 DEBUG [regionserver60020-smallCompactions-1404255751957] compactions.ExploringCompactionPolicy: Exploring compaction algorithm has selected 0 files of size 0 starting at candidate #-1 after considering 0 permutations with 0 in ratio
2014-07-01 16:41:23,255 DEBUG [regionserver60020-smallCompactions-1404255751957] compactions.RatioBasedCompactionPolicy: Not compacting files because we only have 0 files ready for compaction. Need 3 to initiate.
2014-07-01 16:41:23,255 DEBUG [regionserver60020-smallCompactions-1404255751957] regionserver.CompactSplitThread: Not compacting usertable,user4,1404257714774.c831a0ac8e8f14e82fdaef2ba3a5e1f6. because compaction request was cancelled
2014-07-01 16:41:23,456 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-01 16:41:23,495 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 32262 synced till here 32260
2014-07-01 16:41:23,558 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1404255227840/sceplus-vm48.almaden.ibm.com%2C60020%2C1404255227840.1404258081249 with entries=88, filesize=65.3m; new WAL /hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1404255227840/sceplus-vm48.almaden.ibm.com%2C60020%2C1404255227840.1404258083456
2014-07-01 16:41:23,559 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1404255227840/sceplus-vm48.almaden.ibm.com%2C60020%2C1404255227840.1404258031911
2014-07-01 16:41:23,559 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1404255227840/sceplus-vm48.almaden.ibm.com%2C60020%2C1404255227840.1404258036667
2014-07-01 16:41:23,559 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1404255227840/sceplus-vm48.almaden.ibm.com%2C60020%2C1404255227840.1404258039564
2014-07-01 16:41:23,559 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1404255227840/sceplus-vm48.almaden.ibm.com%2C60020%2C1404255227840.1404258041320
2014-07-01 16:41:24,434 DEBUG [MemStoreFlusher.0] util.FSUtils: DFS Client does not support most favored nodes create; using default create
2014-07-01 16:41:24,809 DEBUG [RpcServer.handler=48,port=60020] regionserver.HRegion: Flush requested on usertable,user4,1404257714774.c831a0ac8e8f14e82fdaef2ba3a5e1f6.
2014-07-01 16:41:25,231 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-01 16:41:25,311 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 32343 synced till here 32341
2014-07-01 16:41:25,369 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1404255227840/sceplus-vm48.almaden.ibm.com%2C60020%2C1404255227840.1404258083456 with entries=81, filesize=64.0m; new WAL /hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1404255227840/sceplus-vm48.almaden.ibm.com%2C60020%2C1404255227840.1404258085232
2014-07-01 16:41:27,686 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-01 16:41:27,843 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 32433 synced till here 32411
2014-07-01 16:41:27,998 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1404255227840/sceplus-vm48.almaden.ibm.com%2C60020%2C1404255227840.1404258085232 with entries=90, filesize=80.6m; new WAL /hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1404255227840/sceplus-vm48.almaden.ibm.com%2C60020%2C1404255227840.1404258087687
2014-07-01 16:41:29,973 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-01 16:41:30,008 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1404255227840/sceplus-vm48.almaden.ibm.com%2C60020%2C1404255227840.1404258087687 with entries=82, filesize=62.7m; new WAL /hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1404255227840/sceplus-vm48.almaden.ibm.com%2C60020%2C1404255227840.1404258089974
2014-07-01 16:41:31,412 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-01 16:41:31,460 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 32594 synced till here 32591
2014-07-01 16:41:31,567 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1404255227840/sceplus-vm48.almaden.ibm.com%2C60020%2C1404255227840.1404258089974 with entries=79, filesize=66.5m; new WAL /hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1404255227840/sceplus-vm48.almaden.ibm.com%2C60020%2C1404255227840.1404258091412
2014-07-01 16:41:33,429 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-01 16:41:33,455 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 32674 synced till here 32670
2014-07-01 16:41:33,544 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1404255227840/sceplus-vm48.almaden.ibm.com%2C60020%2C1404255227840.1404258091412 with entries=80, filesize=67.3m; new WAL /hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1404255227840/sceplus-vm48.almaden.ibm.com%2C60020%2C1404255227840.1404258093430
2014-07-01 16:41:34,611 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-01 16:41:34,717 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 32757 synced till here 32753
2014-07-01 16:41:35,490 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1404255227840/sceplus-vm48.almaden.ibm.com%2C60020%2C1404255227840.1404258093430 with entries=83, filesize=67.3m; new WAL /hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1404255227840/sceplus-vm48.almaden.ibm.com%2C60020%2C1404255227840.1404258094611
2014-07-01 16:41:35,917 INFO  [MemStoreFlusher.0] regionserver.DefaultStoreFlusher: Flushed, sequenceid=2426, memsize=175.8m, hasBloomFilter=true, into tmp file hdfs://master:54310/hbase/data/default/usertable/2ed053f05f5eec791a57de24e16f0043/.tmp/33b134314edb40ff85f0be80208a56ec
2014-07-01 16:41:35,947 DEBUG [MemStoreFlusher.0] regionserver.HRegionFileSystem: Committing store file hdfs://master:54310/hbase/data/default/usertable/2ed053f05f5eec791a57de24e16f0043/.tmp/33b134314edb40ff85f0be80208a56ec as hdfs://master:54310/hbase/data/default/usertable/2ed053f05f5eec791a57de24e16f0043/family/33b134314edb40ff85f0be80208a56ec
2014-07-01 16:41:35,957 INFO  [MemStoreFlusher.0] regionserver.HStore: Added hdfs://master:54310/hbase/data/default/usertable/2ed053f05f5eec791a57de24e16f0043/family/33b134314edb40ff85f0be80208a56ec, entries=640210, sequenceid=2426, filesize=45.6m
2014-07-01 16:41:35,957 INFO  [MemStoreFlusher.0] regionserver.HRegion: Finished memstore flush of ~374.3m/392484800, currentsize=186.8m/195906160 for region usertable,user2,1404257714774.2ed053f05f5eec791a57de24e16f0043. in 12702ms, sequenceid=2426, compaction requested=true
2014-07-01 16:41:35,966 DEBUG [MemStoreFlusher.0] regionserver.CompactSplitThread: Small Compaction requested: system; Because: MemStoreFlusher.0; compaction_queue=(0:0), split_queue=0, merge_queue=0
2014-07-01 16:41:35,966 DEBUG [regionserver60020-smallCompactions-1404255751957] compactions.RatioBasedCompactionPolicy: Selecting compaction from 12 store files, 0 compacting, 12 eligible, 20 blocking
2014-07-01 16:41:35,966 DEBUG [regionserver60020-smallCompactions-1404255751957] compactions.RatioBasedCompactionPolicy: Some files are too large. Excluding 12 files from compaction candidates
2014-07-01 16:41:35,967 DEBUG [regionserver60020-smallCompactions-1404255751957] compactions.ExploringCompactionPolicy: Exploring compaction algorithm has selected 0 files of size 0 starting at candidate #-1 after considering 0 permutations with 0 in ratio
2014-07-01 16:41:35,967 DEBUG [regionserver60020-smallCompactions-1404255751957] compactions.RatioBasedCompactionPolicy: Not compacting files because we only have 0 files ready for compaction. Need 3 to initiate.
2014-07-01 16:41:35,967 DEBUG [MemStoreFlusher.0] regionserver.HRegion: Started memstore flush for usertable,user3,1404257714774.a59b89c303508bc329bc1095d0beb9e5., current region memstore size 532.9m
2014-07-01 16:41:35,967 DEBUG [regionserver60020-smallCompactions-1404255751957] regionserver.CompactSplitThread: Not compacting usertable,user2,1404257714774.2ed053f05f5eec791a57de24e16f0043. because compaction request was cancelled
2014-07-01 16:41:36,521 DEBUG [MemStoreFlusher.0] util.FSUtils: DFS Client does not support most favored nodes create; using default create
2014-07-01 16:41:39,137 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-01 16:41:39,592 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 32840 synced till here 32838
2014-07-01 16:41:39,682 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1404255227840/sceplus-vm48.almaden.ibm.com%2C60020%2C1404255227840.1404258094611 with entries=83, filesize=65.1m; new WAL /hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1404255227840/sceplus-vm48.almaden.ibm.com%2C60020%2C1404255227840.1404258099138
2014-07-01 16:41:39,792 INFO  [MemStoreFlusher.1] regionserver.DefaultStoreFlusher: Flushed, sequenceid=2406, memsize=242.1m, hasBloomFilter=true, into tmp file hdfs://master:54310/hbase/data/default/usertable/b6ae40892bbb2a1c545234ed91d62d1a/.tmp/4c13c0d744d74aa4bd39d5b1b6a181bd
2014-07-01 16:41:39,804 DEBUG [MemStoreFlusher.1] regionserver.HRegionFileSystem: Committing store file hdfs://master:54310/hbase/data/default/usertable/b6ae40892bbb2a1c545234ed91d62d1a/.tmp/4c13c0d744d74aa4bd39d5b1b6a181bd as hdfs://master:54310/hbase/data/default/usertable/b6ae40892bbb2a1c545234ed91d62d1a/family/4c13c0d744d74aa4bd39d5b1b6a181bd
2014-07-01 16:41:39,814 INFO  [MemStoreFlusher.1] regionserver.HStore: Added hdfs://master:54310/hbase/data/default/usertable/b6ae40892bbb2a1c545234ed91d62d1a/family/4c13c0d744d74aa4bd39d5b1b6a181bd, entries=881550, sequenceid=2406, filesize=62.8m
2014-07-01 16:41:39,814 INFO  [MemStoreFlusher.1] regionserver.HRegion: Finished memstore flush of ~490.1m/513891280, currentsize=233.0m/244278560 for region usertable,user8,1404257714774.b6ae40892bbb2a1c545234ed91d62d1a. in 17335ms, sequenceid=2406, compaction requested=true
2014-07-01 16:41:39,815 DEBUG [MemStoreFlusher.1] regionserver.CompactSplitThread: Small Compaction requested: system; Because: MemStoreFlusher.1; compaction_queue=(0:0), split_queue=0, merge_queue=0
2014-07-01 16:41:39,815 DEBUG [regionserver60020-smallCompactions-1404255751957] compactions.RatioBasedCompactionPolicy: Selecting compaction from 11 store files, 0 compacting, 11 eligible, 20 blocking
2014-07-01 16:41:39,815 DEBUG [regionserver60020-smallCompactions-1404255751957] compactions.RatioBasedCompactionPolicy: Some files are too large. Excluding 11 files from compaction candidates
2014-07-01 16:41:39,815 DEBUG [regionserver60020-smallCompactions-1404255751957] compactions.ExploringCompactionPolicy: Exploring compaction algorithm has selected 0 files of size 0 starting at candidate #-1 after considering 0 permutations with 0 in ratio
2014-07-01 16:41:39,815 DEBUG [MemStoreFlusher.1] regionserver.HRegion: Started memstore flush for usertable,user4,1404257714774.c831a0ac8e8f14e82fdaef2ba3a5e1f6., current region memstore size 433.6m
2014-07-01 16:41:39,815 DEBUG [regionserver60020-smallCompactions-1404255751957] compactions.RatioBasedCompactionPolicy: Not compacting files because we only have 0 files ready for compaction. Need 3 to initiate.
2014-07-01 16:41:39,815 DEBUG [regionserver60020-smallCompactions-1404255751957] regionserver.CompactSplitThread: Not compacting usertable,user8,1404257714774.b6ae40892bbb2a1c545234ed91d62d1a. because compaction request was cancelled
2014-07-01 16:41:40,312 DEBUG [MemStoreFlusher.1] util.FSUtils: DFS Client does not support most favored nodes create; using default create
2014-07-01 16:41:40,702 DEBUG [RpcServer.handler=40,port=60020] regionserver.HRegion: Flush requested on usertable,user8,1404257714774.b6ae40892bbb2a1c545234ed91d62d1a.
2014-07-01 16:41:40,708 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-01 16:41:40,725 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 32919 synced till here 32914
2014-07-01 16:41:42,068 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1404255227840/sceplus-vm48.almaden.ibm.com%2C60020%2C1404255227840.1404258099138 with entries=79, filesize=65.0m; new WAL /hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1404255227840/sceplus-vm48.almaden.ibm.com%2C60020%2C1404255227840.1404258100708
2014-07-01 16:41:42,068 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1404255227840/sceplus-vm48.almaden.ibm.com%2C60020%2C1404255227840.1404258043305
2014-07-01 16:41:42,068 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1404255227840/sceplus-vm48.almaden.ibm.com%2C60020%2C1404255227840.1404258045576
2014-07-01 16:41:42,068 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1404255227840/sceplus-vm48.almaden.ibm.com%2C60020%2C1404255227840.1404258047571
2014-07-01 16:41:42,068 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1404255227840/sceplus-vm48.almaden.ibm.com%2C60020%2C1404255227840.1404258049552
2014-07-01 16:41:42,069 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1404255227840/sceplus-vm48.almaden.ibm.com%2C60020%2C1404255227840.1404258052067
2014-07-01 16:41:42,069 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1404255227840/sceplus-vm48.almaden.ibm.com%2C60020%2C1404255227840.1404258054831
2014-07-01 16:41:42,774 DEBUG [RpcServer.handler=12,port=60020] regionserver.HRegion: Flush requested on usertable,user2,1404257714774.2ed053f05f5eec791a57de24e16f0043.
2014-07-01 16:41:42,953 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-01 16:41:42,987 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 33008 synced till here 32991
2014-07-01 16:41:43,141 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1404255227840/sceplus-vm48.almaden.ibm.com%2C60020%2C1404255227840.1404258100708 with entries=89, filesize=75.9m; new WAL /hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1404255227840/sceplus-vm48.almaden.ibm.com%2C60020%2C1404255227840.1404258102953
2014-07-01 16:41:44,576 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-01 16:41:44,601 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 33094 synced till here 33080
2014-07-01 16:41:44,692 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1404255227840/sceplus-vm48.almaden.ibm.com%2C60020%2C1404255227840.1404258102953 with entries=86, filesize=71.3m; new WAL /hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1404255227840/sceplus-vm48.almaden.ibm.com%2C60020%2C1404255227840.1404258104576
2014-07-01 16:41:46,255 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-01 16:41:46,330 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 33178 synced till here 33173
2014-07-01 16:41:46,455 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1404255227840/sceplus-vm48.almaden.ibm.com%2C60020%2C1404255227840.1404258104576 with entries=84, filesize=66.7m; new WAL /hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1404255227840/sceplus-vm48.almaden.ibm.com%2C60020%2C1404255227840.1404258106255
2014-07-01 16:41:48,229 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-01 16:41:48,331 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 33255 synced till here 33251
2014-07-01 16:41:48,441 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1404255227840/sceplus-vm48.almaden.ibm.com%2C60020%2C1404255227840.1404258106255 with entries=77, filesize=66.9m; new WAL /hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1404255227840/sceplus-vm48.almaden.ibm.com%2C60020%2C1404255227840.1404258108230
2014-07-01 16:41:53,185 INFO  [JvmPauseMonitor] util.JvmPauseMonitor: Detected pause in JVM or host machine (eg GC): pause of approximately 1260ms
GC pool 'ParNew' had collection(s): count=1 time=1456ms
2014-07-01 16:41:53,186 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-01 16:41:53,294 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 33361 synced till here 33354
2014-07-01 16:41:53,391 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1404255227840/sceplus-vm48.almaden.ibm.com%2C60020%2C1404255227840.1404258108230 with entries=106, filesize=92.9m; new WAL /hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1404255227840/sceplus-vm48.almaden.ibm.com%2C60020%2C1404255227840.1404258113186
2014-07-01 16:41:55,304 INFO  [MemStoreFlusher.1] regionserver.DefaultStoreFlusher: Flushed, sequenceid=2543, memsize=182.7m, hasBloomFilter=true, into tmp file hdfs://master:54310/hbase/data/default/usertable/c831a0ac8e8f14e82fdaef2ba3a5e1f6/.tmp/1990de860e7548d190cacbaaddf95e6b
2014-07-01 16:41:55,317 DEBUG [MemStoreFlusher.1] regionserver.HRegionFileSystem: Committing store file hdfs://master:54310/hbase/data/default/usertable/c831a0ac8e8f14e82fdaef2ba3a5e1f6/.tmp/1990de860e7548d190cacbaaddf95e6b as hdfs://master:54310/hbase/data/default/usertable/c831a0ac8e8f14e82fdaef2ba3a5e1f6/family/1990de860e7548d190cacbaaddf95e6b
2014-07-01 16:41:55,328 INFO  [MemStoreFlusher.1] regionserver.HStore: Added hdfs://master:54310/hbase/data/default/usertable/c831a0ac8e8f14e82fdaef2ba3a5e1f6/family/1990de860e7548d190cacbaaddf95e6b, entries=665130, sequenceid=2543, filesize=47.4m
2014-07-01 16:41:55,329 INFO  [MemStoreFlusher.1] regionserver.HRegion: Finished memstore flush of ~436.9m/458135920, currentsize=204.5m/214442000 for region usertable,user4,1404257714774.c831a0ac8e8f14e82fdaef2ba3a5e1f6. in 15513ms, sequenceid=2543, compaction requested=true
2014-07-01 16:41:55,329 DEBUG [MemStoreFlusher.1] regionserver.CompactSplitThread: Small Compaction requested: system; Because: MemStoreFlusher.1; compaction_queue=(0:0), split_queue=0, merge_queue=0
2014-07-01 16:41:55,329 DEBUG [regionserver60020-smallCompactions-1404255751957] compactions.RatioBasedCompactionPolicy: Selecting compaction from 12 store files, 0 compacting, 12 eligible, 20 blocking
2014-07-01 16:41:55,329 DEBUG [regionserver60020-smallCompactions-1404255751957] compactions.RatioBasedCompactionPolicy: Some files are too large. Excluding 12 files from compaction candidates
2014-07-01 16:41:55,329 DEBUG [regionserver60020-smallCompactions-1404255751957] compactions.ExploringCompactionPolicy: Exploring compaction algorithm has selected 0 files of size 0 starting at candidate #-1 after considering 0 permutations with 0 in ratio
2014-07-01 16:41:55,329 DEBUG [MemStoreFlusher.1] regionserver.HRegion: Started memstore flush for usertable,user8,1404257714774.b6ae40892bbb2a1c545234ed91d62d1a., current region memstore size 401.2m
2014-07-01 16:41:55,329 DEBUG [regionserver60020-smallCompactions-1404255751957] compactions.RatioBasedCompactionPolicy: Not compacting files because we only have 0 files ready for compaction. Need 3 to initiate.
2014-07-01 16:41:55,330 DEBUG [regionserver60020-smallCompactions-1404255751957] regionserver.CompactSplitThread: Not compacting usertable,user4,1404257714774.c831a0ac8e8f14e82fdaef2ba3a5e1f6. because compaction request was cancelled
2014-07-01 16:41:55,562 INFO  [MemStoreFlusher.0] regionserver.DefaultStoreFlusher: Flushed, sequenceid=2535, memsize=261.9m, hasBloomFilter=true, into tmp file hdfs://master:54310/hbase/data/default/usertable/a59b89c303508bc329bc1095d0beb9e5/.tmp/784ac2ea82234e3e8d9a5576e965b528
2014-07-01 16:41:55,583 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-01 16:41:55,584 DEBUG [MemStoreFlusher.0] regionserver.HRegionFileSystem: Committing store file hdfs://master:54310/hbase/data/default/usertable/a59b89c303508bc329bc1095d0beb9e5/.tmp/784ac2ea82234e3e8d9a5576e965b528 as hdfs://master:54310/hbase/data/default/usertable/a59b89c303508bc329bc1095d0beb9e5/family/784ac2ea82234e3e8d9a5576e965b528
2014-07-01 16:41:55,637 INFO  [MemStoreFlusher.0] regionserver.HStore: Added hdfs://master:54310/hbase/data/default/usertable/a59b89c303508bc329bc1095d0beb9e5/family/784ac2ea82234e3e8d9a5576e965b528, entries=953620, sequenceid=2535, filesize=68.0m
2014-07-01 16:41:55,671 INFO  [MemStoreFlusher.0] regionserver.HRegion: Finished memstore flush of ~532.9m/558803040, currentsize=256.5m/268983360 for region usertable,user3,1404257714774.a59b89c303508bc329bc1095d0beb9e5. in 19704ms, sequenceid=2535, compaction requested=true
2014-07-01 16:41:55,672 DEBUG [regionserver60020-smallCompactions-1404255751957] compactions.RatioBasedCompactionPolicy: Selecting compaction from 12 store files, 0 compacting, 12 eligible, 20 blocking
2014-07-01 16:41:55,672 DEBUG [regionserver60020-smallCompactions-1404255751957] compactions.RatioBasedCompactionPolicy: Some files are too large. Excluding 12 files from compaction candidates
2014-07-01 16:41:55,672 DEBUG [regionserver60020-smallCompactions-1404255751957] compactions.ExploringCompactionPolicy: Exploring compaction algorithm has selected 0 files of size 0 starting at candidate #-1 after considering 0 permutations with 0 in ratio
2014-07-01 16:41:55,672 DEBUG [regionserver60020-smallCompactions-1404255751957] compactions.RatioBasedCompactionPolicy: Not compacting files because we only have 0 files ready for compaction. Need 3 to initiate.
2014-07-01 16:41:55,672 DEBUG [regionserver60020-smallCompactions-1404255751957] regionserver.CompactSplitThread: Not compacting usertable,user3,1404257714774.a59b89c303508bc329bc1095d0beb9e5. because compaction request was cancelled
2014-07-01 16:41:55,672 DEBUG [MemStoreFlusher.0] regionserver.CompactSplitThread: Small Compaction requested: system; Because: MemStoreFlusher.0; compaction_queue=(0:0), split_queue=0, merge_queue=0
2014-07-01 16:41:55,673 DEBUG [MemStoreFlusher.0] regionserver.HRegion: Started memstore flush for usertable,user2,1404257714774.2ed053f05f5eec791a57de24e16f0043., current region memstore size 450.6m
2014-07-01 16:41:55,687 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 33483 synced till here 33471
2014-07-01 16:41:55,885 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1404255227840/sceplus-vm48.almaden.ibm.com%2C60020%2C1404255227840.1404258113186 with entries=122, filesize=108.7m; new WAL /hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1404255227840/sceplus-vm48.almaden.ibm.com%2C60020%2C1404255227840.1404258115583
2014-07-01 16:41:55,886 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1404255227840/sceplus-vm48.almaden.ibm.com%2C60020%2C1404255227840.1404258056971
2014-07-01 16:41:55,886 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1404255227840/sceplus-vm48.almaden.ibm.com%2C60020%2C1404255227840.1404258060404
2014-07-01 16:41:55,886 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1404255227840/sceplus-vm48.almaden.ibm.com%2C60020%2C1404255227840.1404258061612
2014-07-01 16:41:55,886 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1404255227840/sceplus-vm48.almaden.ibm.com%2C60020%2C1404255227840.1404258063713
2014-07-01 16:41:56,035 DEBUG [RpcServer.handler=29,port=60020] regionserver.HRegion: Flush requested on usertable,user3,1404257714774.a59b89c303508bc329bc1095d0beb9e5.
2014-07-01 16:41:57,311 DEBUG [MemStoreFlusher.1] util.FSUtils: DFS Client does not support most favored nodes create; using default create
2014-07-01 16:41:57,737 DEBUG [MemStoreFlusher.0] util.FSUtils: DFS Client does not support most favored nodes create; using default create
2014-07-01 16:41:57,779 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-01 16:41:57,834 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 33605 synced till here 33581
2014-07-01 16:41:58,101 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1404255227840/sceplus-vm48.almaden.ibm.com%2C60020%2C1404255227840.1404258115583 with entries=122, filesize=89.8m; new WAL /hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1404255227840/sceplus-vm48.almaden.ibm.com%2C60020%2C1404255227840.1404258117779
2014-07-01 16:41:59,358 INFO  [JvmPauseMonitor] util.JvmPauseMonitor: Detected pause in JVM or host machine (eg GC): pause of approximately 1046ms
GC pool 'ParNew' had collection(s): count=1 time=1103ms
2014-07-01 16:41:59,820 DEBUG [RpcServer.handler=7,port=60020] regionserver.HRegion: Flush requested on usertable,user4,1404257714774.c831a0ac8e8f14e82fdaef2ba3a5e1f6.
2014-07-01 16:42:00,096 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-01 16:42:01,146 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 33730 synced till here 33688
2014-07-01 16:42:01,442 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1404255227840/sceplus-vm48.almaden.ibm.com%2C60020%2C1404255227840.1404258117779 with entries=125, filesize=117.3m; new WAL /hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1404255227840/sceplus-vm48.almaden.ibm.com%2C60020%2C1404255227840.1404258120096
2014-07-01 16:42:03,116 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-01 16:42:03,164 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 33851 synced till here 33815
2014-07-01 16:42:03,731 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1404255227840/sceplus-vm48.almaden.ibm.com%2C60020%2C1404255227840.1404258120096 with entries=121, filesize=83.4m; new WAL /hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1404255227840/sceplus-vm48.almaden.ibm.com%2C60020%2C1404255227840.1404258123116
2014-07-01 16:42:05,147 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-01 16:42:05,220 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 33953 synced till here 33918
2014-07-01 16:42:05,571 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1404255227840/sceplus-vm48.almaden.ibm.com%2C60020%2C1404255227840.1404258123116 with entries=102, filesize=89.6m; new WAL /hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1404255227840/sceplus-vm48.almaden.ibm.com%2C60020%2C1404255227840.1404258125147
2014-07-01 16:42:07,418 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-01 16:42:07,474 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 34049 synced till here 34029
2014-07-01 16:42:07,829 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1404255227840/sceplus-vm48.almaden.ibm.com%2C60020%2C1404255227840.1404258125147 with entries=96, filesize=86.6m; new WAL /hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1404255227840/sceplus-vm48.almaden.ibm.com%2C60020%2C1404255227840.1404258127418
2014-07-01 16:42:09,635 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-01 16:42:09,707 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 34152 synced till here 34132
2014-07-01 16:42:09,978 DEBUG [RpcServer.handler=5,port=60020] regionserver.HRegion: Flush requested on usertable,user9,1404257714774.84635e5ca79006f09dceafa095992aab.
2014-07-01 16:42:10,026 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1404255227840/sceplus-vm48.almaden.ibm.com%2C60020%2C1404255227840.1404258127418 with entries=103, filesize=76.2m; new WAL /hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1404255227840/sceplus-vm48.almaden.ibm.com%2C60020%2C1404255227840.1404258129635
2014-07-01 16:42:11,475 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-01 16:42:11,528 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 34239 synced till here 34238
2014-07-01 16:42:11,611 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1404255227840/sceplus-vm48.almaden.ibm.com%2C60020%2C1404255227840.1404258129635 with entries=87, filesize=72.3m; new WAL /hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1404255227840/sceplus-vm48.almaden.ibm.com%2C60020%2C1404255227840.1404258131475
2014-07-01 16:42:12,280 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-01 16:42:12,297 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 34323 synced till here 34320
2014-07-01 16:42:12,332 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1404255227840/sceplus-vm48.almaden.ibm.com%2C60020%2C1404255227840.1404258131475 with entries=84, filesize=64.9m; new WAL /hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1404255227840/sceplus-vm48.almaden.ibm.com%2C60020%2C1404255227840.1404258132280
2014-07-01 16:42:13,759 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-01 16:42:13,774 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 34403 synced till here 34401
2014-07-01 16:42:13,803 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1404255227840/sceplus-vm48.almaden.ibm.com%2C60020%2C1404255227840.1404258132280 with entries=80, filesize=63.7m; new WAL /hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1404255227840/sceplus-vm48.almaden.ibm.com%2C60020%2C1404255227840.1404258133759
2014-07-01 16:42:15,326 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-01 16:42:15,905 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 34501 synced till here 34492
2014-07-01 16:42:15,943 INFO  [MemStoreFlusher.1] regionserver.DefaultStoreFlusher: Flushed, sequenceid=2662, memsize=298.1m, hasBloomFilter=true, into tmp file hdfs://master:54310/hbase/data/default/usertable/b6ae40892bbb2a1c545234ed91d62d1a/.tmp/d224d7ca140445e1a63f46845f8ce1b1
2014-07-01 16:42:15,954 DEBUG [MemStoreFlusher.1] regionserver.HRegionFileSystem: Committing store file hdfs://master:54310/hbase/data/default/usertable/b6ae40892bbb2a1c545234ed91d62d1a/.tmp/d224d7ca140445e1a63f46845f8ce1b1 as hdfs://master:54310/hbase/data/default/usertable/b6ae40892bbb2a1c545234ed91d62d1a/family/d224d7ca140445e1a63f46845f8ce1b1
2014-07-01 16:42:15,965 INFO  [MemStoreFlusher.1] regionserver.HStore: Added hdfs://master:54310/hbase/data/default/usertable/b6ae40892bbb2a1c545234ed91d62d1a/family/d224d7ca140445e1a63f46845f8ce1b1, entries=1085450, sequenceid=2662, filesize=77.3m
2014-07-01 16:42:15,965 INFO  [MemStoreFlusher.1] regionserver.HRegion: Finished memstore flush of ~439.5m/460870080, currentsize=367.8m/385636400 for region usertable,user8,1404257714774.b6ae40892bbb2a1c545234ed91d62d1a. in 20636ms, sequenceid=2662, compaction requested=true
2014-07-01 16:42:15,966 DEBUG [MemStoreFlusher.1] regionserver.CompactSplitThread: Small Compaction requested: system; Because: MemStoreFlusher.1; compaction_queue=(0:0), split_queue=0, merge_queue=0
2014-07-01 16:42:15,966 DEBUG [regionserver60020-smallCompactions-1404255751957] compactions.RatioBasedCompactionPolicy: Selecting compaction from 12 store files, 0 compacting, 12 eligible, 20 blocking
2014-07-01 16:42:15,967 DEBUG [MemStoreFlusher.1] regionserver.HRegion: Started memstore flush for usertable,user3,1404257714774.a59b89c303508bc329bc1095d0beb9e5., current region memstore size 588.7m
2014-07-01 16:42:15,967 DEBUG [regionserver60020-smallCompactions-1404255751957] compactions.RatioBasedCompactionPolicy: Some files are too large. Excluding 12 files from compaction candidates
2014-07-01 16:42:15,967 DEBUG [regionserver60020-smallCompactions-1404255751957] compactions.ExploringCompactionPolicy: Exploring compaction algorithm has selected 0 files of size 0 starting at candidate #-1 after considering 0 permutations with 0 in ratio
2014-07-01 16:42:15,967 DEBUG [regionserver60020-smallCompactions-1404255751957] compactions.RatioBasedCompactionPolicy: Not compacting files because we only have 0 files ready for compaction. Need 3 to initiate.
2014-07-01 16:42:15,967 DEBUG [regionserver60020-smallCompactions-1404255751957] regionserver.CompactSplitThread: Not compacting usertable,user8,1404257714774.b6ae40892bbb2a1c545234ed91d62d1a. because compaction request was cancelled
2014-07-01 16:42:16,079 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1404255227840/sceplus-vm48.almaden.ibm.com%2C60020%2C1404255227840.1404258133759 with entries=98, filesize=82.3m; new WAL /hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1404255227840/sceplus-vm48.almaden.ibm.com%2C60020%2C1404255227840.1404258135327
2014-07-01 16:42:17,182 DEBUG [RpcServer.handler=14,port=60020] regionserver.HRegion: Flush requested on usertable,user8,1404257714774.b6ae40892bbb2a1c545234ed91d62d1a.
2014-07-01 16:42:17,399 INFO  [MemStoreFlusher.0] regionserver.DefaultStoreFlusher: Flushed, sequenceid=2688, memsize=296.3m, hasBloomFilter=true, into tmp file hdfs://master:54310/hbase/data/default/usertable/2ed053f05f5eec791a57de24e16f0043/.tmp/c29d7dd7636b4b62afec5e151fce4b94
2014-07-01 16:42:17,414 DEBUG [MemStoreFlusher.0] regionserver.HRegionFileSystem: Committing store file hdfs://master:54310/hbase/data/default/usertable/2ed053f05f5eec791a57de24e16f0043/.tmp/c29d7dd7636b4b62afec5e151fce4b94 as hdfs://master:54310/hbase/data/default/usertable/2ed053f05f5eec791a57de24e16f0043/family/c29d7dd7636b4b62afec5e151fce4b94
2014-07-01 16:42:17,441 INFO  [MemStoreFlusher.0] regionserver.HStore: Added hdfs://master:54310/hbase/data/default/usertable/2ed053f05f5eec791a57de24e16f0043/family/c29d7dd7636b4b62afec5e151fce4b94, entries=1078870, sequenceid=2688, filesize=76.8m
2014-07-01 16:42:17,441 INFO  [MemStoreFlusher.0] regionserver.HRegion: Finished memstore flush of ~452.4m/474423680, currentsize=356.6m/373899680 for region usertable,user2,1404257714774.2ed053f05f5eec791a57de24e16f0043. in 21768ms, sequenceid=2688, compaction requested=true
2014-07-01 16:42:17,453 DEBUG [MemStoreFlusher.0] regionserver.CompactSplitThread: Small Compaction requested: system; Because: MemStoreFlusher.0; compaction_queue=(0:0), split_queue=0, merge_queue=0
2014-07-01 16:42:17,453 DEBUG [MemStoreFlusher.0] regionserver.HRegion: Started memstore flush for usertable,user4,1404257714774.c831a0ac8e8f14e82fdaef2ba3a5e1f6., current region memstore size 580.2m
2014-07-01 16:42:17,454 DEBUG [regionserver60020-smallCompactions-1404255751957] compactions.RatioBasedCompactionPolicy: Selecting compaction from 13 store files, 0 compacting, 13 eligible, 20 blocking
2014-07-01 16:42:17,454 DEBUG [regionserver60020-smallCompactions-1404255751957] compactions.RatioBasedCompactionPolicy: Some files are too large. Excluding 13 files from compaction candidates
2014-07-01 16:42:17,454 DEBUG [regionserver60020-smallCompactions-1404255751957] compactions.ExploringCompactionPolicy: Exploring compaction algorithm has selected 0 files of size 0 starting at candidate #-1 after considering 0 permutations with 0 in ratio
2014-07-01 16:42:17,459 DEBUG [regionserver60020-smallCompactions-1404255751957] compactions.RatioBasedCompactionPolicy: Not compacting files because we only have 0 files ready for compaction. Need 3 to initiate.
2014-07-01 16:42:17,459 DEBUG [regionserver60020-smallCompactions-1404255751957] regionserver.CompactSplitThread: Not compacting usertable,user2,1404257714774.2ed053f05f5eec791a57de24e16f0043. because compaction request was cancelled
2014-07-01 16:42:17,455 DEBUG [RpcServer.handler=44,port=60020] regionserver.HRegion: Flush requested on usertable,user2,1404257714774.2ed053f05f5eec791a57de24e16f0043.
2014-07-01 16:42:17,509 DEBUG [MemStoreFlusher.1] util.FSUtils: DFS Client does not support most favored nodes create; using default create
2014-07-01 16:42:17,821 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-01 16:42:18,044 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 34598 synced till here 34567
2014-07-01 16:42:19,179 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1404255227840/sceplus-vm48.almaden.ibm.com%2C60020%2C1404255227840.1404258135327 with entries=97, filesize=89.9m; new WAL /hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1404255227840/sceplus-vm48.almaden.ibm.com%2C60020%2C1404255227840.1404258137822
2014-07-01 16:42:19,208 DEBUG [MemStoreFlusher.0] util.FSUtils: DFS Client does not support most favored nodes create; using default create
2014-07-01 16:42:20,256 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-01 16:42:20,288 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 34690 synced till here 34682
2014-07-01 16:42:20,376 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1404255227840/sceplus-vm48.almaden.ibm.com%2C60020%2C1404255227840.1404258137822 with entries=92, filesize=67.5m; new WAL /hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1404255227840/sceplus-vm48.almaden.ibm.com%2C60020%2C1404255227840.1404258140256
2014-07-01 16:42:20,376 INFO  [regionserver60020.logRoller] wal.FSHLog: Too many hlogs: logs=33, maxlogs=32; forcing flush of 1 regions(s): 84635e5ca79006f09dceafa095992aab
2014-07-01 16:42:22,102 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-01 16:42:22,150 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 34767 synced till here 34765
2014-07-01 16:42:22,181 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1404255227840/sceplus-vm48.almaden.ibm.com%2C60020%2C1404255227840.1404258140256 with entries=77, filesize=63.3m; new WAL /hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1404255227840/sceplus-vm48.almaden.ibm.com%2C60020%2C1404255227840.1404258142102
2014-07-01 16:42:22,181 INFO  [regionserver60020.logRoller] wal.FSHLog: Too many hlogs: logs=34, maxlogs=32; forcing flush of 1 regions(s): 84635e5ca79006f09dceafa095992aab
2014-07-01 16:42:24,180 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-01 16:42:24,375 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 34858 synced till here 34839
2014-07-01 16:42:24,517 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1404255227840/sceplus-vm48.almaden.ibm.com%2C60020%2C1404255227840.1404258142102 with entries=91, filesize=80.0m; new WAL /hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1404255227840/sceplus-vm48.almaden.ibm.com%2C60020%2C1404255227840.1404258144180
2014-07-01 16:42:24,517 INFO  [regionserver60020.logRoller] wal.FSHLog: Too many hlogs: logs=35, maxlogs=32; forcing flush of 1 regions(s): 84635e5ca79006f09dceafa095992aab
2014-07-01 16:42:26,413 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-01 16:42:26,722 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 34966 synced till here 34958
2014-07-01 16:42:26,926 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1404255227840/sceplus-vm48.almaden.ibm.com%2C60020%2C1404255227840.1404258144180 with entries=108, filesize=88.9m; new WAL /hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1404255227840/sceplus-vm48.almaden.ibm.com%2C60020%2C1404255227840.1404258146413
2014-07-01 16:42:26,926 INFO  [regionserver60020.logRoller] wal.FSHLog: Too many hlogs: logs=36, maxlogs=32; forcing flush of 1 regions(s): 84635e5ca79006f09dceafa095992aab
2014-07-01 16:42:28,462 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-01 16:42:28,509 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 35047 synced till here 35043
2014-07-01 16:42:28,578 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1404255227840/sceplus-vm48.almaden.ibm.com%2C60020%2C1404255227840.1404258146413 with entries=81, filesize=67.6m; new WAL /hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1404255227840/sceplus-vm48.almaden.ibm.com%2C60020%2C1404255227840.1404258148462
2014-07-01 16:42:28,578 INFO  [regionserver60020.logRoller] wal.FSHLog: Too many hlogs: logs=37, maxlogs=32; forcing flush of 1 regions(s): 84635e5ca79006f09dceafa095992aab
2014-07-01 16:42:31,056 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-01 16:42:31,121 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 35144 synced till here 35133
2014-07-01 16:42:31,169 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1404255227840/sceplus-vm48.almaden.ibm.com%2C60020%2C1404255227840.1404258148462 with entries=97, filesize=82.6m; new WAL /hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1404255227840/sceplus-vm48.almaden.ibm.com%2C60020%2C1404255227840.1404258151057
2014-07-01 16:42:31,170 INFO  [regionserver60020.logRoller] wal.FSHLog: Too many hlogs: logs=38, maxlogs=32; forcing flush of 1 regions(s): 84635e5ca79006f09dceafa095992aab
2014-07-01 16:42:32,030 INFO  [MemStoreFlusher.1] regionserver.DefaultStoreFlusher: Flushed, sequenceid=2876, memsize=192.8m, hasBloomFilter=true, into tmp file hdfs://master:54310/hbase/data/default/usertable/a59b89c303508bc329bc1095d0beb9e5/.tmp/5a0d459c74574283800d2b73e528f451
2014-07-01 16:42:32,047 DEBUG [MemStoreFlusher.1] regionserver.HRegionFileSystem: Committing store file hdfs://master:54310/hbase/data/default/usertable/a59b89c303508bc329bc1095d0beb9e5/.tmp/5a0d459c74574283800d2b73e528f451 as hdfs://master:54310/hbase/data/default/usertable/a59b89c303508bc329bc1095d0beb9e5/family/5a0d459c74574283800d2b73e528f451
2014-07-01 16:42:32,069 INFO  [MemStoreFlusher.1] regionserver.HStore: Added hdfs://master:54310/hbase/data/default/usertable/a59b89c303508bc329bc1095d0beb9e5/family/5a0d459c74574283800d2b73e528f451, entries=702030, sequenceid=2876, filesize=50.0m
2014-07-01 16:42:32,069 INFO  [MemStoreFlusher.1] regionserver.HRegion: Finished memstore flush of ~592.3m/621115200, currentsize=241.0m/252683760 for region usertable,user3,1404257714774.a59b89c303508bc329bc1095d0beb9e5. in 16103ms, sequenceid=2876, compaction requested=true
2014-07-01 16:42:32,070 DEBUG [MemStoreFlusher.1] regionserver.CompactSplitThread: Small Compaction requested: system; Because: MemStoreFlusher.1; compaction_queue=(0:0), split_queue=0, merge_queue=0
2014-07-01 16:42:32,070 DEBUG [MemStoreFlusher.1] regionserver.HRegion: Started memstore flush for usertable,user9,1404257714774.84635e5ca79006f09dceafa095992aab., current region memstore size 365.5m
2014-07-01 16:42:32,071 DEBUG [regionserver60020-smallCompactions-1404255751957] compactions.RatioBasedCompactionPolicy: Selecting compaction from 13 store files, 0 compacting, 13 eligible, 20 blocking
2014-07-01 16:42:32,071 DEBUG [regionserver60020-smallCompactions-1404255751957] compactions.RatioBasedCompactionPolicy: Some files are too large. Excluding 13 files from compaction candidates
2014-07-01 16:42:32,071 DEBUG [regionserver60020-smallCompactions-1404255751957] compactions.ExploringCompactionPolicy: Exploring compaction algorithm has selected 0 files of size 0 starting at candidate #-1 after considering 0 permutations with 0 in ratio
2014-07-01 16:42:32,071 DEBUG [regionserver60020-smallCompactions-1404255751957] compactions.RatioBasedCompactionPolicy: Not compacting files because we only have 0 files ready for compaction. Need 3 to initiate.
2014-07-01 16:42:32,071 DEBUG [regionserver60020-smallCompactions-1404255751957] regionserver.CompactSplitThread: Not compacting usertable,user3,1404257714774.a59b89c303508bc329bc1095d0beb9e5. because compaction request was cancelled
2014-07-01 16:42:32,432 DEBUG [MemStoreFlusher.1] util.FSUtils: DFS Client does not support most favored nodes create; using default create
2014-07-01 16:42:32,576 INFO  [MemStoreFlusher.0] regionserver.DefaultStoreFlusher: Flushed, sequenceid=2884, memsize=196.2m, hasBloomFilter=true, into tmp file hdfs://master:54310/hbase/data/default/usertable/c831a0ac8e8f14e82fdaef2ba3a5e1f6/.tmp/38c4aaf0c4fe48889f01c2ed28828995
2014-07-01 16:42:32,595 DEBUG [MemStoreFlusher.0] regionserver.HRegionFileSystem: Committing store file hdfs://master:54310/hbase/data/default/usertable/c831a0ac8e8f14e82fdaef2ba3a5e1f6/.tmp/38c4aaf0c4fe48889f01c2ed28828995 as hdfs://master:54310/hbase/data/default/usertable/c831a0ac8e8f14e82fdaef2ba3a5e1f6/family/38c4aaf0c4fe48889f01c2ed28828995
2014-07-01 16:42:32,611 INFO  [MemStoreFlusher.0] regionserver.HStore: Added hdfs://master:54310/hbase/data/default/usertable/c831a0ac8e8f14e82fdaef2ba3a5e1f6/family/38c4aaf0c4fe48889f01c2ed28828995, entries=714520, sequenceid=2884, filesize=50.9m
2014-07-01 16:42:32,611 INFO  [MemStoreFlusher.0] regionserver.HRegion: Finished memstore flush of ~594.4m/623238640, currentsize=232.2m/243475920 for region usertable,user4,1404257714774.c831a0ac8e8f14e82fdaef2ba3a5e1f6. in 15158ms, sequenceid=2884, compaction requested=true
2014-07-01 16:42:32,612 DEBUG [MemStoreFlusher.0] regionserver.CompactSplitThread: Small Compaction requested: system; Because: MemStoreFlusher.0; compaction_queue=(0:0), split_queue=0, merge_queue=0
2014-07-01 16:42:32,612 DEBUG [regionserver60020-smallCompactions-1404255751957] compactions.RatioBasedCompactionPolicy: Selecting compaction from 13 store files, 0 compacting, 13 eligible, 20 blocking
2014-07-01 16:42:32,612 DEBUG [regionserver60020-smallCompactions-1404255751957] compactions.RatioBasedCompactionPolicy: Some files are too large. Excluding 13 files from compaction candidates
2014-07-01 16:42:32,612 DEBUG [MemStoreFlusher.0] regionserver.HRegion: Started memstore flush for usertable,user8,1404257714774.b6ae40892bbb2a1c545234ed91d62d1a., current region memstore size 617.4m
2014-07-01 16:42:32,612 DEBUG [regionserver60020-smallCompactions-1404255751957] compactions.ExploringCompactionPolicy: Exploring compaction algorithm has selected 0 files of size 0 starting at candidate #-1 after considering 0 permutations with 0 in ratio
2014-07-01 16:42:32,612 DEBUG [regionserver60020-smallCompactions-1404255751957] compactions.RatioBasedCompactionPolicy: Not compacting files because we only have 0 files ready for compaction. Need 3 to initiate.
2014-07-01 16:42:32,612 DEBUG [regionserver60020-smallCompactions-1404255751957] regionserver.CompactSplitThread: Not compacting usertable,user4,1404257714774.c831a0ac8e8f14e82fdaef2ba3a5e1f6. because compaction request was cancelled
2014-07-01 16:42:32,767 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-01 16:42:32,820 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 35230 synced till here 35225
2014-07-01 16:42:32,922 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1404255227840/sceplus-vm48.almaden.ibm.com%2C60020%2C1404255227840.1404258151057 with entries=86, filesize=66.3m; new WAL /hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1404255227840/sceplus-vm48.almaden.ibm.com%2C60020%2C1404255227840.1404258152768
2014-07-01 16:42:32,963 DEBUG [RpcServer.handler=9,port=60020] regionserver.HRegion: Flush requested on usertable,user3,1404257714774.a59b89c303508bc329bc1095d0beb9e5.
2014-07-01 16:42:33,456 DEBUG [MemStoreFlusher.0] util.FSUtils: DFS Client does not support most favored nodes create; using default create
2014-07-01 16:42:34,575 DEBUG [RpcServer.handler=36,port=60020] regionserver.HRegion: Flush requested on usertable,user4,1404257714774.c831a0ac8e8f14e82fdaef2ba3a5e1f6.
2014-07-01 16:42:34,682 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-01 16:42:34,711 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 35309 synced till here 35303
2014-07-01 16:42:34,769 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1404255227840/sceplus-vm48.almaden.ibm.com%2C60020%2C1404255227840.1404258152768 with entries=79, filesize=66.6m; new WAL /hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1404255227840/sceplus-vm48.almaden.ibm.com%2C60020%2C1404255227840.1404258154682
2014-07-01 16:42:36,534 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-01 16:42:36,582 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 35392 synced till here 35385
2014-07-01 16:42:36,656 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1404255227840/sceplus-vm48.almaden.ibm.com%2C60020%2C1404255227840.1404258154682 with entries=83, filesize=70.5m; new WAL /hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1404255227840/sceplus-vm48.almaden.ibm.com%2C60020%2C1404255227840.1404258156535
2014-07-01 16:42:37,937 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-01 16:42:37,970 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 35475 synced till here 35468
2014-07-01 16:42:38,149 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1404255227840/sceplus-vm48.almaden.ibm.com%2C60020%2C1404255227840.1404258156535 with entries=83, filesize=67.7m; new WAL /hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1404255227840/sceplus-vm48.almaden.ibm.com%2C60020%2C1404255227840.1404258157937
2014-07-01 16:42:40,023 INFO  [MemStoreFlusher.1] regionserver.DefaultStoreFlusher: Flushed, sequenceid=2989, memsize=138.7m, hasBloomFilter=true, into tmp file hdfs://master:54310/hbase/data/default/usertable/84635e5ca79006f09dceafa095992aab/.tmp/643b193257a447f288a952821e3d9419
2014-07-01 16:42:40,034 DEBUG [MemStoreFlusher.1] regionserver.HRegionFileSystem: Committing store file hdfs://master:54310/hbase/data/default/usertable/84635e5ca79006f09dceafa095992aab/.tmp/643b193257a447f288a952821e3d9419 as hdfs://master:54310/hbase/data/default/usertable/84635e5ca79006f09dceafa095992aab/family/643b193257a447f288a952821e3d9419
2014-07-01 16:42:40,273 INFO  [MemStoreFlusher.1] regionserver.HStore: Added hdfs://master:54310/hbase/data/default/usertable/84635e5ca79006f09dceafa095992aab/family/643b193257a447f288a952821e3d9419, entries=505100, sequenceid=2989, filesize=36.0m
2014-07-01 16:42:40,273 INFO  [MemStoreFlusher.1] regionserver.HRegion: Finished memstore flush of ~365.5m/383233840, currentsize=45.9m/48178160 for region usertable,user9,1404257714774.84635e5ca79006f09dceafa095992aab. in 8203ms, sequenceid=2989, compaction requested=true
2014-07-01 16:42:40,274 DEBUG [MemStoreFlusher.1] regionserver.CompactSplitThread: Small Compaction requested: system; Because: MemStoreFlusher.1; compaction_queue=(0:0), split_queue=0, merge_queue=0
2014-07-01 16:42:40,274 DEBUG [regionserver60020-smallCompactions-1404255751957] compactions.RatioBasedCompactionPolicy: Selecting compaction from 5 store files, 0 compacting, 5 eligible, 20 blocking
2014-07-01 16:42:40,274 DEBUG [regionserver60020-smallCompactions-1404255751957] compactions.RatioBasedCompactionPolicy: Some files are too large. Excluding 5 files from compaction candidates
2014-07-01 16:42:40,274 DEBUG [regionserver60020-smallCompactions-1404255751957] compactions.ExploringCompactionPolicy: Exploring compaction algorithm has selected 0 files of size 0 starting at candidate #-1 after considering 0 permutations with 0 in ratio
2014-07-01 16:42:40,274 DEBUG [regionserver60020-smallCompactions-1404255751957] compactions.RatioBasedCompactionPolicy: Not compacting files because we only have 0 files ready for compaction. Need 3 to initiate.
2014-07-01 16:42:40,274 DEBUG [MemStoreFlusher.1] regionserver.HRegion: Started memstore flush for usertable,user2,1404257714774.2ed053f05f5eec791a57de24e16f0043., current region memstore size 687.9m
2014-07-01 16:42:40,274 DEBUG [regionserver60020-smallCompactions-1404255751957] regionserver.CompactSplitThread: Not compacting usertable,user9,1404257714774.84635e5ca79006f09dceafa095992aab. because compaction request was cancelled
2014-07-01 16:42:40,813 DEBUG [MemStoreFlusher.1] util.FSUtils: DFS Client does not support most favored nodes create; using default create
2014-07-01 16:42:41,425 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-01 16:42:41,467 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 35559 synced till here 35552
2014-07-01 16:42:41,567 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1404255227840/sceplus-vm48.almaden.ibm.com%2C60020%2C1404255227840.1404258157937 with entries=84, filesize=68.4m; new WAL /hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1404255227840/sceplus-vm48.almaden.ibm.com%2C60020%2C1404255227840.1404258161426
2014-07-01 16:42:41,567 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1404255227840/sceplus-vm48.almaden.ibm.com%2C60020%2C1404255227840.1404258066534
2014-07-01 16:42:41,567 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1404255227840/sceplus-vm48.almaden.ibm.com%2C60020%2C1404255227840.1404258068618
2014-07-01 16:42:41,567 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1404255227840/sceplus-vm48.almaden.ibm.com%2C60020%2C1404255227840.1404258070993
2014-07-01 16:42:41,568 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1404255227840/sceplus-vm48.almaden.ibm.com%2C60020%2C1404255227840.1404258073511
2014-07-01 16:42:41,568 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1404255227840/sceplus-vm48.almaden.ibm.com%2C60020%2C1404255227840.1404258075753
2014-07-01 16:42:41,568 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1404255227840/sceplus-vm48.almaden.ibm.com%2C60020%2C1404255227840.1404258078018
2014-07-01 16:42:41,568 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1404255227840/sceplus-vm48.almaden.ibm.com%2C60020%2C1404255227840.1404258081249
2014-07-01 16:42:41,568 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1404255227840/sceplus-vm48.almaden.ibm.com%2C60020%2C1404255227840.1404258083456
2014-07-01 16:42:41,568 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1404255227840/sceplus-vm48.almaden.ibm.com%2C60020%2C1404255227840.1404258085232
2014-07-01 16:42:41,568 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1404255227840/sceplus-vm48.almaden.ibm.com%2C60020%2C1404255227840.1404258087687
2014-07-01 16:42:41,568 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1404255227840/sceplus-vm48.almaden.ibm.com%2C60020%2C1404255227840.1404258089974
2014-07-01 16:42:41,568 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1404255227840/sceplus-vm48.almaden.ibm.com%2C60020%2C1404255227840.1404258091412
2014-07-01 16:42:41,568 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1404255227840/sceplus-vm48.almaden.ibm.com%2C60020%2C1404255227840.1404258093430
2014-07-01 16:42:41,568 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1404255227840/sceplus-vm48.almaden.ibm.com%2C60020%2C1404255227840.1404258094611
2014-07-01 16:42:41,568 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1404255227840/sceplus-vm48.almaden.ibm.com%2C60020%2C1404255227840.1404258099138
2014-07-01 16:42:41,568 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1404255227840/sceplus-vm48.almaden.ibm.com%2C60020%2C1404255227840.1404258100708
2014-07-01 16:42:41,568 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1404255227840/sceplus-vm48.almaden.ibm.com%2C60020%2C1404255227840.1404258102953
2014-07-01 16:42:41,568 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1404255227840/sceplus-vm48.almaden.ibm.com%2C60020%2C1404255227840.1404258104576
2014-07-01 16:42:41,568 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1404255227840/sceplus-vm48.almaden.ibm.com%2C60020%2C1404255227840.1404258106255
2014-07-01 16:42:41,568 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1404255227840/sceplus-vm48.almaden.ibm.com%2C60020%2C1404255227840.1404258108230
2014-07-01 16:42:43,473 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-01 16:42:43,507 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 35643 synced till here 35629
2014-07-01 16:42:43,827 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1404255227840/sceplus-vm48.almaden.ibm.com%2C60020%2C1404255227840.1404258161426 with entries=84, filesize=72.6m; new WAL /hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1404255227840/sceplus-vm48.almaden.ibm.com%2C60020%2C1404255227840.1404258163473
2014-07-01 16:42:45,729 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-01 16:42:45,761 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 35754 synced till here 35731
2014-07-01 16:42:45,966 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1404255227840/sceplus-vm48.almaden.ibm.com%2C60020%2C1404255227840.1404258163473 with entries=111, filesize=91.1m; new WAL /hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1404255227840/sceplus-vm48.almaden.ibm.com%2C60020%2C1404255227840.1404258165730
2014-07-01 16:42:47,738 INFO  [JvmPauseMonitor] util.JvmPauseMonitor: Detected pause in JVM or host machine (eg GC): pause of approximately 1073ms
GC pool 'ParNew' had collection(s): count=1 time=1365ms
2014-07-01 16:42:48,208 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-01 16:42:48,279 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 35856 synced till here 35839
2014-07-01 16:42:48,602 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1404255227840/sceplus-vm48.almaden.ibm.com%2C60020%2C1404255227840.1404258165730 with entries=102, filesize=81.6m; new WAL /hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1404255227840/sceplus-vm48.almaden.ibm.com%2C60020%2C1404255227840.1404258168208
2014-07-01 16:42:48,653 INFO  [MemStoreFlusher.0] regionserver.DefaultStoreFlusher: Flushed, sequenceid=3021, memsize=226.6m, hasBloomFilter=true, into tmp file hdfs://master:54310/hbase/data/default/usertable/b6ae40892bbb2a1c545234ed91d62d1a/.tmp/4c01659ecdee411a920a2b688c54c0d6
2014-07-01 16:42:48,670 DEBUG [MemStoreFlusher.0] regionserver.HRegionFileSystem: Committing store file hdfs://master:54310/hbase/data/default/usertable/b6ae40892bbb2a1c545234ed91d62d1a/.tmp/4c01659ecdee411a920a2b688c54c0d6 as hdfs://master:54310/hbase/data/default/usertable/b6ae40892bbb2a1c545234ed91d62d1a/family/4c01659ecdee411a920a2b688c54c0d6
2014-07-01 16:42:48,679 INFO  [MemStoreFlusher.0] regionserver.HStore: Added hdfs://master:54310/hbase/data/default/usertable/b6ae40892bbb2a1c545234ed91d62d1a/family/4c01659ecdee411a920a2b688c54c0d6, entries=824870, sequenceid=3021, filesize=58.8m
2014-07-01 16:42:48,679 INFO  [MemStoreFlusher.0] regionserver.HRegion: Finished memstore flush of ~624.6m/654980480, currentsize=203.6m/213510720 for region usertable,user8,1404257714774.b6ae40892bbb2a1c545234ed91d62d1a. in 16067ms, sequenceid=3021, compaction requested=true
2014-07-01 16:42:48,679 DEBUG [MemStoreFlusher.0] regionserver.CompactSplitThread: Small Compaction requested: system; Because: MemStoreFlusher.0; compaction_queue=(0:0), split_queue=0, merge_queue=0
2014-07-01 16:42:48,679 DEBUG [regionserver60020-smallCompactions-1404255751957] compactions.RatioBasedCompactionPolicy: Selecting compaction from 13 store files, 0 compacting, 13 eligible, 20 blocking
2014-07-01 16:42:48,680 DEBUG [MemStoreFlusher.0] regionserver.HRegion: Started memstore flush for usertable,user3,1404257714774.a59b89c303508bc329bc1095d0beb9e5., current region memstore size 487.8m
2014-07-01 16:42:48,680 DEBUG [regionserver60020-smallCompactions-1404255751957] compactions.RatioBasedCompactionPolicy: Some files are too large. Excluding 13 files from compaction candidates
2014-07-01 16:42:48,680 DEBUG [regionserver60020-smallCompactions-1404255751957] compactions.ExploringCompactionPolicy: Exploring compaction algorithm has selected 0 files of size 0 starting at candidate #-1 after considering 0 permutations with 0 in ratio
2014-07-01 16:42:48,680 DEBUG [regionserver60020-smallCompactions-1404255751957] compactions.RatioBasedCompactionPolicy: Not compacting files because we only have 0 files ready for compaction. Need 3 to initiate.
2014-07-01 16:42:48,680 DEBUG [regionserver60020-smallCompactions-1404255751957] regionserver.CompactSplitThread: Not compacting usertable,user8,1404257714774.b6ae40892bbb2a1c545234ed91d62d1a. because compaction request was cancelled
2014-07-01 16:42:50,371 INFO  [JvmPauseMonitor] util.JvmPauseMonitor: Detected pause in JVM or host machine (eg GC): pause of approximately 1631ms
GC pool 'ParNew' had collection(s): count=1 time=1650ms
2014-07-01 16:42:51,088 DEBUG [MemStoreFlusher.0] util.FSUtils: DFS Client does not support most favored nodes create; using default create
2014-07-01 16:42:51,130 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-01 16:42:51,288 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 35965 synced till here 35933
2014-07-01 16:42:52,978 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1404255227840/sceplus-vm48.almaden.ibm.com%2C60020%2C1404255227840.1404258168208 with entries=109, filesize=94.3m; new WAL /hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1404255227840/sceplus-vm48.almaden.ibm.com%2C60020%2C1404255227840.1404258171130
2014-07-01 16:42:52,978 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1404255227840/sceplus-vm48.almaden.ibm.com%2C60020%2C1404255227840.1404258113186
2014-07-01 16:42:53,662 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-01 16:42:53,675 DEBUG [RpcServer.handler=24,port=60020] regionserver.HRegion: Flush requested on usertable,user8,1404257714774.b6ae40892bbb2a1c545234ed91d62d1a.
2014-07-01 16:42:56,246 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 36096 synced till here 36048
2014-07-01 16:42:56,246 INFO  [JvmPauseMonitor] util.JvmPauseMonitor: Detected pause in JVM or host machine (eg GC): pause of approximately 2389ms
GC pool 'ParNew' had collection(s): count=1 time=2348ms
2014-07-01 16:42:56,343 WARN  [RpcServer.handler=28,port=60020] ipc.RpcServer: RpcServer.respondercallId: 7584 service: ClientService methodName: Multi size: 3.4m connection: 9.1.143.58:41983: output error
2014-07-01 16:42:56,353 WARN  [RpcServer.handler=28,port=60020] ipc.RpcServer: RpcServer.handler=28,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-07-01 16:42:56,522 WARN  [RpcServer.handler=1,port=60020] ipc.RpcServer: RpcServer.respondercallId: 7603 service: ClientService methodName: Multi size: 3.3m connection: 9.1.143.58:41983: output error
2014-07-01 16:42:56,522 WARN  [RpcServer.handler=1,port=60020] ipc.RpcServer: RpcServer.handler=1,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-07-01 16:42:56,522 WARN  [RpcServer.handler=24,port=60020] ipc.RpcServer: RpcServer.respondercallId: 7583 service: ClientService methodName: Multi size: 3.0m connection: 9.1.143.58:41983: output error
2014-07-01 16:42:56,523 WARN  [RpcServer.handler=24,port=60020] ipc.RpcServer: RpcServer.handler=24,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-07-01 16:42:56,523 WARN  [RpcServer.handler=18,port=60020] ipc.RpcServer: RpcServer.respondercallId: 7590 service: ClientService methodName: Multi size: 3.4m connection: 9.1.143.58:41983: output error
2014-07-01 16:42:56,523 WARN  [RpcServer.handler=18,port=60020] ipc.RpcServer: RpcServer.handler=18,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-07-01 16:42:56,523 WARN  [RpcServer.handler=41,port=60020] ipc.RpcServer: RpcServer.respondercallId: 7600 service: ClientService methodName: Multi size: 3.0m connection: 9.1.143.58:41983: output error
2014-07-01 16:42:56,523 WARN  [RpcServer.handler=41,port=60020] ipc.RpcServer: RpcServer.handler=41,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-07-01 16:42:56,523 WARN  [RpcServer.handler=21,port=60020] ipc.RpcServer: RpcServer.respondercallId: 7589 service: ClientService methodName: Multi size: 2.6m connection: 9.1.143.58:41983: output error
2014-07-01 16:42:56,523 WARN  [RpcServer.handler=21,port=60020] ipc.RpcServer: RpcServer.handler=21,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-07-01 16:42:56,523 WARN  [RpcServer.handler=48,port=60020] ipc.RpcServer: RpcServer.respondercallId: 7593 service: ClientService methodName: Multi size: 3.3m connection: 9.1.143.58:41983: output error
2014-07-01 16:42:56,523 WARN  [RpcServer.handler=48,port=60020] ipc.RpcServer: RpcServer.handler=48,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-07-01 16:42:56,525 WARN  [RpcServer.handler=3,port=60020] ipc.RpcServer: RpcServer.respondercallId: 7602 service: ClientService methodName: Multi size: 3.3m connection: 9.1.143.58:41983: output error
2014-07-01 16:42:56,525 WARN  [RpcServer.handler=3,port=60020] ipc.RpcServer: RpcServer.handler=3,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-07-01 16:42:56,525 WARN  [RpcServer.handler=14,port=60020] ipc.RpcServer: RpcServer.respondercallId: 7604 service: ClientService methodName: Multi size: 3.4m connection: 9.1.143.58:41983: output error
2014-07-01 16:42:56,525 WARN  [RpcServer.handler=14,port=60020] ipc.RpcServer: RpcServer.handler=14,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-07-01 16:42:56,525 WARN  [RpcServer.handler=17,port=60020] ipc.RpcServer: RpcServer.respondercallId: 7601 service: ClientService methodName: Multi size: 3.3m connection: 9.1.143.58:41983: output error
2014-07-01 16:42:56,526 WARN  [RpcServer.handler=17,port=60020] ipc.RpcServer: RpcServer.handler=17,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-07-01 16:42:56,526 WARN  [RpcServer.handler=34,port=60020] ipc.RpcServer: RpcServer.respondercallId: 7599 service: ClientService methodName: Multi size: 3.4m connection: 9.1.143.58:41983: output error
2014-07-01 16:42:56,526 WARN  [RpcServer.handler=34,port=60020] ipc.RpcServer: RpcServer.handler=34,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-07-01 16:42:56,560 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1404255227840/sceplus-vm48.almaden.ibm.com%2C60020%2C1404255227840.1404258171130 with entries=131, filesize=105.6m; new WAL /hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1404255227840/sceplus-vm48.almaden.ibm.com%2C60020%2C1404255227840.1404258173662
2014-07-01 16:42:56,723 WARN  [RpcServer.handler=4,port=60020] ipc.RpcServer: RpcServer.respondercallId: 7610 service: ClientService methodName: Multi size: 3.0m connection: 9.1.143.58:41983: output error
2014-07-01 16:42:56,724 WARN  [RpcServer.handler=4,port=60020] ipc.RpcServer: RpcServer.handler=4,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-07-01 16:42:56,724 WARN  [RpcServer.handler=5,port=60020] ipc.RpcServer: RpcServer.respondercallId: 7615 service: ClientService methodName: Multi size: 3.3m connection: 9.1.143.58:41983: output error
2014-07-01 16:42:56,724 WARN  [RpcServer.handler=5,port=60020] ipc.RpcServer: RpcServer.handler=5,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-07-01 16:42:56,724 WARN  [RpcServer.handler=26,port=60020] ipc.RpcServer: RpcServer.respondercallId: 7617 service: ClientService methodName: Multi size: 3.5m connection: 9.1.143.58:41983: output error
2014-07-01 16:42:56,724 WARN  [RpcServer.handler=26,port=60020] ipc.RpcServer: RpcServer.handler=26,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-07-01 16:42:56,725 WARN  [RpcServer.handler=42,port=60020] ipc.RpcServer: RpcServer.respondercallId: 7616 service: ClientService methodName: Multi size: 3.4m connection: 9.1.143.58:41983: output error
2014-07-01 16:42:56,725 WARN  [RpcServer.handler=42,port=60020] ipc.RpcServer: RpcServer.handler=42,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-07-01 16:42:56,725 WARN  [RpcServer.handler=37,port=60020] ipc.RpcServer: RpcServer.respondercallId: 7594 service: ClientService methodName: Multi size: 3.3m connection: 9.1.143.58:41983: output error
2014-07-01 16:42:56,725 WARN  [RpcServer.handler=37,port=60020] ipc.RpcServer: RpcServer.handler=37,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-07-01 16:42:56,725 WARN  [RpcServer.handler=9,port=60020] ipc.RpcServer: RpcServer.respondercallId: 7598 service: ClientService methodName: Multi size: 3.3m connection: 9.1.143.58:41983: output error
2014-07-01 16:42:56,725 WARN  [RpcServer.handler=9,port=60020] ipc.RpcServer: RpcServer.handler=9,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-07-01 16:42:56,726 WARN  [RpcServer.handler=38,port=60020] ipc.RpcServer: RpcServer.respondercallId: 7606 service: ClientService methodName: Multi size: 2.9m connection: 9.1.143.58:41983: output error
2014-07-01 16:42:56,726 WARN  [RpcServer.handler=38,port=60020] ipc.RpcServer: RpcServer.handler=38,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-07-01 16:42:56,726 WARN  [RpcServer.handler=15,port=60020] ipc.RpcServer: RpcServer.respondercallId: 7609 service: ClientService methodName: Multi size: 3.4m connection: 9.1.143.58:41983: output error
2014-07-01 16:42:56,726 WARN  [RpcServer.handler=15,port=60020] ipc.RpcServer: RpcServer.handler=15,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-07-01 16:42:56,726 WARN  [RpcServer.handler=16,port=60020] ipc.RpcServer: RpcServer.respondercallId: 7597 service: ClientService methodName: Multi size: 3.4m connection: 9.1.143.58:41983: output error
2014-07-01 16:42:56,726 WARN  [RpcServer.handler=16,port=60020] ipc.RpcServer: RpcServer.handler=16,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-07-01 16:42:56,727 WARN  [RpcServer.handler=22,port=60020] ipc.RpcServer: RpcServer.respondercallId: 7614 service: ClientService methodName: Multi size: 3.3m connection: 9.1.143.58:41983: output error
2014-07-01 16:42:56,727 WARN  [RpcServer.handler=22,port=60020] ipc.RpcServer: RpcServer.handler=22,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-07-01 16:42:56,727 WARN  [RpcServer.handler=36,port=60020] ipc.RpcServer: RpcServer.respondercallId: 7595 service: ClientService methodName: Multi size: 3.3m connection: 9.1.143.58:41983: output error
2014-07-01 16:42:56,727 WARN  [RpcServer.handler=36,port=60020] ipc.RpcServer: RpcServer.handler=36,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-07-01 16:42:56,728 WARN  [RpcServer.handler=39,port=60020] ipc.RpcServer: RpcServer.respondercallId: 7596 service: ClientService methodName: Multi size: 3.4m connection: 9.1.143.58:41983: output error
2014-07-01 16:42:56,728 WARN  [RpcServer.handler=39,port=60020] ipc.RpcServer: RpcServer.handler=39,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-07-01 16:42:56,728 WARN  [RpcServer.handler=7,port=60020] ipc.RpcServer: RpcServer.respondercallId: 7605 service: ClientService methodName: Multi size: 3.4m connection: 9.1.143.58:41983: output error
2014-07-01 16:42:56,728 WARN  [RpcServer.handler=7,port=60020] ipc.RpcServer: RpcServer.handler=7,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-07-01 16:42:56,949 WARN  [RpcServer.handler=20,port=60020] ipc.RpcServer: RpcServer.respondercallId: 7612 service: ClientService methodName: Multi size: 3.0m connection: 9.1.143.58:41983: output error
2014-07-01 16:42:56,949 WARN  [RpcServer.handler=20,port=60020] ipc.RpcServer: RpcServer.handler=20,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-07-01 16:42:56,950 WARN  [RpcServer.handler=44,port=60020] ipc.RpcServer: RpcServer.respondercallId: 7618 service: ClientService methodName: Multi size: 3.0m connection: 9.1.143.58:41983: output error
2014-07-01 16:42:56,950 WARN  [RpcServer.handler=44,port=60020] ipc.RpcServer: RpcServer.handler=44,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-07-01 16:42:56,951 WARN  [RpcServer.handler=33,port=60020] ipc.RpcServer: RpcServer.respondercallId: 7613 service: ClientService methodName: Multi size: 3.4m connection: 9.1.143.58:41983: output error
2014-07-01 16:42:56,951 WARN  [RpcServer.handler=33,port=60020] ipc.RpcServer: RpcServer.handler=33,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-07-01 16:42:56,954 WARN  [RpcServer.handler=0,port=60020] ipc.RpcServer: RpcServer.respondercallId: 7611 service: ClientService methodName: Multi size: 3.3m connection: 9.1.143.58:41983: output error
2014-07-01 16:42:56,954 WARN  [RpcServer.handler=0,port=60020] ipc.RpcServer: RpcServer.handler=0,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-07-01 16:42:57,013 WARN  [RpcServer.handler=8,port=60020] ipc.RpcServer: RpcServer.respondercallId: 7622 service: ClientService methodName: Multi size: 2.6m connection: 9.1.143.58:41983: output error
2014-07-01 16:42:57,013 WARN  [RpcServer.handler=8,port=60020] ipc.RpcServer: RpcServer.handler=8,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-07-01 16:42:57,013 WARN  [RpcServer.handler=43,port=60020] ipc.RpcServer: RpcServer.respondercallId: 7626 service: ClientService methodName: Multi size: 3.3m connection: 9.1.143.58:41983: output error
2014-07-01 16:42:57,014 WARN  [RpcServer.handler=43,port=60020] ipc.RpcServer: RpcServer.handler=43,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-07-01 16:42:57,014 WARN  [RpcServer.handler=40,port=60020] ipc.RpcServer: RpcServer.respondercallId: 7621 service: ClientService methodName: Multi size: 3.3m connection: 9.1.143.58:41983: output error
2014-07-01 16:42:57,014 WARN  [RpcServer.handler=40,port=60020] ipc.RpcServer: RpcServer.handler=40,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-07-01 16:42:57,015 WARN  [RpcServer.handler=31,port=60020] ipc.RpcServer: RpcServer.respondercallId: 7623 service: ClientService methodName: Multi size: 3.4m connection: 9.1.143.58:41983: output error
2014-07-01 16:42:57,015 WARN  [RpcServer.handler=31,port=60020] ipc.RpcServer: RpcServer.handler=31,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-07-01 16:42:57,015 WARN  [RpcServer.handler=11,port=60020] ipc.RpcServer: RpcServer.respondercallId: 7624 service: ClientService methodName: Multi size: 3.4m connection: 9.1.143.58:41983: output error
2014-07-01 16:42:57,015 WARN  [RpcServer.handler=11,port=60020] ipc.RpcServer: RpcServer.handler=11,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-07-01 16:42:57,019 WARN  [RpcServer.handler=12,port=60020] ipc.RpcServer: RpcServer.respondercallId: 7625 service: ClientService methodName: Multi size: 3.4m connection: 9.1.143.58:41983: output error
2014-07-01 16:42:57,019 WARN  [RpcServer.handler=12,port=60020] ipc.RpcServer: RpcServer.handler=12,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-07-01 16:42:57,126 WARN  [RpcServer.handler=35,port=60020] ipc.RpcServer: RpcServer.respondercallId: 7631 service: ClientService methodName: Multi size: 3.4m connection: 9.1.143.58:41983: output error
2014-07-01 16:42:57,126 WARN  [RpcServer.handler=35,port=60020] ipc.RpcServer: RpcServer.handler=35,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-07-01 16:42:57,172 WARN  [RpcServer.handler=32,port=60020] ipc.RpcServer: RpcServer.respondercallId: 7632 service: ClientService methodName: Multi size: 2.9m connection: 9.1.143.58:41983: output error
2014-07-01 16:42:57,172 WARN  [RpcServer.handler=32,port=60020] ipc.RpcServer: RpcServer.handler=32,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-07-01 16:42:57,175 WARN  [RpcServer.handler=30,port=60020] ipc.RpcServer: RpcServer.respondercallId: 7627 service: ClientService methodName: Multi size: 3.3m connection: 9.1.143.58:41983: output error
2014-07-01 16:42:57,175 WARN  [RpcServer.handler=30,port=60020] ipc.RpcServer: RpcServer.handler=30,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-07-01 16:42:57,175 WARN  [RpcServer.handler=45,port=60020] ipc.RpcServer: RpcServer.respondercallId: 7620 service: ClientService methodName: Multi size: 3.3m connection: 9.1.143.58:41983: output error
2014-07-01 16:42:57,175 WARN  [RpcServer.handler=45,port=60020] ipc.RpcServer: RpcServer.handler=45,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-07-01 16:42:57,175 WARN  [RpcServer.handler=23,port=60020] ipc.RpcServer: RpcServer.respondercallId: 7629 service: ClientService methodName: Multi size: 3.4m connection: 9.1.143.58:41983: output error
2014-07-01 16:42:57,175 WARN  [RpcServer.handler=23,port=60020] ipc.RpcServer: RpcServer.handler=23,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-07-01 16:42:57,176 WARN  [RpcServer.handler=19,port=60020] ipc.RpcServer: RpcServer.respondercallId: 7628 service: ClientService methodName: Multi size: 3.3m connection: 9.1.143.58:41983: output error
2014-07-01 16:42:57,176 WARN  [RpcServer.handler=19,port=60020] ipc.RpcServer: RpcServer.handler=19,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-07-01 16:42:57,176 WARN  [RpcServer.handler=6,port=60020] ipc.RpcServer: RpcServer.respondercallId: 7630 service: ClientService methodName: Multi size: 3.4m connection: 9.1.143.58:41983: output error
2014-07-01 16:42:57,176 WARN  [RpcServer.handler=6,port=60020] ipc.RpcServer: RpcServer.handler=6,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-07-01 16:42:57,195 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-01 16:42:57,199 WARN  [RpcServer.handler=10,port=60020] ipc.RpcServer: RpcServer.respondercallId: 7648 service: ClientService methodName: Multi size: 3.3m connection: 9.1.143.58:41983: output error
2014-07-01 16:42:57,199 WARN  [RpcServer.handler=10,port=60020] ipc.RpcServer: RpcServer.handler=10,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-07-01 16:42:57,199 WARN  [RpcServer.handler=47,port=60020] ipc.RpcServer: RpcServer.respondercallId: 7646 service: ClientService methodName: Multi size: 3.4m connection: 9.1.143.58:41983: output error
2014-07-01 16:42:57,200 WARN  [RpcServer.handler=47,port=60020] ipc.RpcServer: RpcServer.handler=47,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-07-01 16:42:57,200 WARN  [RpcServer.handler=13,port=60020] ipc.RpcServer: RpcServer.respondercallId: 7635 service: ClientService methodName: Multi size: 3.3m connection: 9.1.143.58:41983: output error
2014-07-01 16:42:57,200 WARN  [RpcServer.handler=13,port=60020] ipc.RpcServer: RpcServer.handler=13,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-07-01 16:42:57,200 WARN  [RpcServer.handler=46,port=60020] ipc.RpcServer: RpcServer.respondercallId: 7647 service: ClientService methodName: Multi size: 3.4m connection: 9.1.143.58:41983: output error
2014-07-01 16:42:57,200 WARN  [RpcServer.handler=46,port=60020] ipc.RpcServer: RpcServer.handler=46,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-07-01 16:42:57,201 WARN  [RpcServer.handler=49,port=60020] ipc.RpcServer: RpcServer.respondercallId: 7619 service: ClientService methodName: Multi size: 3.3m connection: 9.1.143.58:41983: output error
2014-07-01 16:42:57,201 WARN  [RpcServer.handler=49,port=60020] ipc.RpcServer: RpcServer.handler=49,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-07-01 16:42:57,210 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1404255227840/sceplus-vm48.almaden.ibm.com%2C60020%2C1404255227840.1404258173662 with entries=91, filesize=63.3m; new WAL /hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1404255227840/sceplus-vm48.almaden.ibm.com%2C60020%2C1404255227840.1404258177195
2014-07-01 16:42:57,307 WARN  [RpcServer.handler=27,port=60020] ipc.RpcServer: RpcServer.respondercallId: 7642 service: ClientService methodName: Multi size: 3.5m connection: 9.1.143.58:41983: output error
2014-07-01 16:42:57,308 WARN  [RpcServer.handler=27,port=60020] ipc.RpcServer: RpcServer.handler=27,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-07-01 16:42:57,328 WARN  [RpcServer.handler=25,port=60020] ipc.RpcServer: RpcServer.respondercallId: 7644 service: ClientService methodName: Multi size: 3.0m connection: 9.1.143.58:41983: output error
2014-07-01 16:42:57,328 WARN  [RpcServer.handler=25,port=60020] ipc.RpcServer: RpcServer.handler=25,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-07-01 16:42:57,328 WARN  [RpcServer.handler=2,port=60020] ipc.RpcServer: RpcServer.respondercallId: 7643 service: ClientService methodName: Multi size: 3.3m connection: 9.1.143.58:41983: output error
2014-07-01 16:42:57,328 WARN  [RpcServer.handler=2,port=60020] ipc.RpcServer: RpcServer.handler=2,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-07-01 16:42:57,329 WARN  [RpcServer.handler=29,port=60020] ipc.RpcServer: RpcServer.respondercallId: 7645 service: ClientService methodName: Multi size: 3.3m connection: 9.1.143.58:41983: output error
2014-07-01 16:42:57,330 WARN  [RpcServer.handler=29,port=60020] ipc.RpcServer: RpcServer.handler=29,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-07-01 16:42:59,219 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-01 16:42:59,266 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 36269 synced till here 36260
2014-07-01 16:42:59,319 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1404255227840/sceplus-vm48.almaden.ibm.com%2C60020%2C1404255227840.1404258177195 with entries=82, filesize=69.1m; new WAL /hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1404255227840/sceplus-vm48.almaden.ibm.com%2C60020%2C1404255227840.1404258179220
2014-07-01 16:43:00,861 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-01 16:43:00,967 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1404255227840/sceplus-vm48.almaden.ibm.com%2C60020%2C1404255227840.1404258179220 with entries=80, filesize=62.1m; new WAL /hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1404255227840/sceplus-vm48.almaden.ibm.com%2C60020%2C1404255227840.1404258180862
2014-07-01 16:43:02,658 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-01 16:43:03,227 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1404255227840/sceplus-vm48.almaden.ibm.com%2C60020%2C1404255227840.1404258180862 with entries=98, filesize=82.1m; new WAL /hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1404255227840/sceplus-vm48.almaden.ibm.com%2C60020%2C1404255227840.1404258182658
2014-07-01 16:43:05,224 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-01 16:43:05,264 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 36529 synced till here 36522
2014-07-01 16:43:05,407 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1404255227840/sceplus-vm48.almaden.ibm.com%2C60020%2C1404255227840.1404258182658 with entries=82, filesize=70.1m; new WAL /hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1404255227840/sceplus-vm48.almaden.ibm.com%2C60020%2C1404255227840.1404258185224
2014-07-01 16:43:07,088 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-01 16:43:07,122 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 36614 synced till here 36604
2014-07-01 16:43:07,373 INFO  [MemStoreFlusher.1] regionserver.DefaultStoreFlusher: Flushed, sequenceid=3082, memsize=308.2m, hasBloomFilter=true, into tmp file hdfs://master:54310/hbase/data/default/usertable/2ed053f05f5eec791a57de24e16f0043/.tmp/9a735ca23d544dfb8fac4e4e8d744fea
2014-07-01 16:43:07,375 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1404255227840/sceplus-vm48.almaden.ibm.com%2C60020%2C1404255227840.1404258185224 with entries=85, filesize=70.8m; new WAL /hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1404255227840/sceplus-vm48.almaden.ibm.com%2C60020%2C1404255227840.1404258187089
2014-07-01 16:43:07,444 DEBUG [MemStoreFlusher.1] regionserver.HRegionFileSystem: Committing store file hdfs://master:54310/hbase/data/default/usertable/2ed053f05f5eec791a57de24e16f0043/.tmp/9a735ca23d544dfb8fac4e4e8d744fea as hdfs://master:54310/hbase/data/default/usertable/2ed053f05f5eec791a57de24e16f0043/family/9a735ca23d544dfb8fac4e4e8d744fea
2014-07-01 16:43:07,579 INFO  [MemStoreFlusher.1] regionserver.HStore: Added hdfs://master:54310/hbase/data/default/usertable/2ed053f05f5eec791a57de24e16f0043/family/9a735ca23d544dfb8fac4e4e8d744fea, entries=1122090, sequenceid=3082, filesize=79.9m
2014-07-01 16:43:07,579 INFO  [MemStoreFlusher.1] regionserver.HRegion: Finished memstore flush of ~687.9m/721266880, currentsize=384.6m/403256640 for region usertable,user2,1404257714774.2ed053f05f5eec791a57de24e16f0043. in 27305ms, sequenceid=3082, compaction requested=true
2014-07-01 16:43:07,580 DEBUG [regionserver60020-smallCompactions-1404255751957] compactions.RatioBasedCompactionPolicy: Selecting compaction from 14 store files, 0 compacting, 14 eligible, 20 blocking
2014-07-01 16:43:07,580 DEBUG [regionserver60020-smallCompactions-1404255751957] compactions.RatioBasedCompactionPolicy: Some files are too large. Excluding 14 files from compaction candidates
2014-07-01 16:43:07,580 DEBUG [regionserver60020-smallCompactions-1404255751957] compactions.ExploringCompactionPolicy: Exploring compaction algorithm has selected 0 files of size 0 starting at candidate #-1 after considering 0 permutations with 0 in ratio
2014-07-01 16:43:07,580 DEBUG [regionserver60020-smallCompactions-1404255751957] compactions.RatioBasedCompactionPolicy: Not compacting files because we only have 0 files ready for compaction. Need 3 to initiate.
2014-07-01 16:43:07,580 DEBUG [regionserver60020-smallCompactions-1404255751957] regionserver.CompactSplitThread: Not compacting usertable,user2,1404257714774.2ed053f05f5eec791a57de24e16f0043. because compaction request was cancelled
2014-07-01 16:43:07,581 DEBUG [MemStoreFlusher.1] regionserver.CompactSplitThread: Small Compaction requested: system; Because: MemStoreFlusher.1; compaction_queue=(0:0), split_queue=0, merge_queue=0
2014-07-01 16:43:07,582 DEBUG [MemStoreFlusher.1] regionserver.HRegion: Started memstore flush for usertable,user4,1404257714774.c831a0ac8e8f14e82fdaef2ba3a5e1f6., current region memstore size 722.8m
2014-07-01 16:43:07,830 DEBUG [RpcServer.handler=47,port=60020] regionserver.HRegion: Flush requested on usertable,user2,1404257714774.2ed053f05f5eec791a57de24e16f0043.
2014-07-01 16:43:09,129 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-01 16:43:09,301 DEBUG [MemStoreFlusher.1] util.FSUtils: DFS Client does not support most favored nodes create; using default create
2014-07-01 16:43:09,302 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 36712 synced till here 36696
2014-07-01 16:43:09,439 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1404255227840/sceplus-vm48.almaden.ibm.com%2C60020%2C1404255227840.1404258187089 with entries=98, filesize=81.3m; new WAL /hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1404255227840/sceplus-vm48.almaden.ibm.com%2C60020%2C1404255227840.1404258189130
2014-07-01 16:43:09,440 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1404255227840/sceplus-vm48.almaden.ibm.com%2C60020%2C1404255227840.1404258115583
2014-07-01 16:43:09,440 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1404255227840/sceplus-vm48.almaden.ibm.com%2C60020%2C1404255227840.1404258117779
2014-07-01 16:43:09,440 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1404255227840/sceplus-vm48.almaden.ibm.com%2C60020%2C1404255227840.1404258120096
2014-07-01 16:43:09,440 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1404255227840/sceplus-vm48.almaden.ibm.com%2C60020%2C1404255227840.1404258123116
2014-07-01 16:43:09,440 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1404255227840/sceplus-vm48.almaden.ibm.com%2C60020%2C1404255227840.1404258125147
2014-07-01 16:43:09,440 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1404255227840/sceplus-vm48.almaden.ibm.com%2C60020%2C1404255227840.1404258127418
2014-07-01 16:43:09,440 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1404255227840/sceplus-vm48.almaden.ibm.com%2C60020%2C1404255227840.1404258129635
2014-07-01 16:43:09,440 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1404255227840/sceplus-vm48.almaden.ibm.com%2C60020%2C1404255227840.1404258131475
2014-07-01 16:43:09,440 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1404255227840/sceplus-vm48.almaden.ibm.com%2C60020%2C1404255227840.1404258132280
2014-07-01 16:43:09,440 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1404255227840/sceplus-vm48.almaden.ibm.com%2C60020%2C1404255227840.1404258133759
2014-07-01 16:43:10,912 INFO  [MemStoreFlusher.0] regionserver.DefaultStoreFlusher: Flushed, sequenceid=3161, memsize=279.9m, hasBloomFilter=true, into tmp file hdfs://master:54310/hbase/data/default/usertable/a59b89c303508bc329bc1095d0beb9e5/.tmp/48401d65966d4b80ba4dc29230ab424a
2014-07-01 16:43:10,931 DEBUG [MemStoreFlusher.0] regionserver.HRegionFileSystem: Committing store file hdfs://master:54310/hbase/data/default/usertable/a59b89c303508bc329bc1095d0beb9e5/.tmp/48401d65966d4b80ba4dc29230ab424a as hdfs://master:54310/hbase/data/default/usertable/a59b89c303508bc329bc1095d0beb9e5/family/48401d65966d4b80ba4dc29230ab424a
2014-07-01 16:43:10,968 INFO  [MemStoreFlusher.0] regionserver.HStore: Added hdfs://master:54310/hbase/data/default/usertable/a59b89c303508bc329bc1095d0beb9e5/family/48401d65966d4b80ba4dc29230ab424a, entries=1018980, sequenceid=3161, filesize=72.6m
2014-07-01 16:43:10,969 INFO  [MemStoreFlusher.0] regionserver.HRegion: Finished memstore flush of ~492.9m/516867600, currentsize=296.2m/310629760 for region usertable,user3,1404257714774.a59b89c303508bc329bc1095d0beb9e5. in 22289ms, sequenceid=3161, compaction requested=true
2014-07-01 16:43:10,969 DEBUG [regionserver60020-smallCompactions-1404255751957] compactions.RatioBasedCompactionPolicy: Selecting compaction from 14 store files, 0 compacting, 14 eligible, 20 blocking
2014-07-01 16:43:10,969 DEBUG [regionserver60020-smallCompactions-1404255751957] compactions.RatioBasedCompactionPolicy: Some files are too large. Excluding 14 files from compaction candidates
2014-07-01 16:43:10,969 DEBUG [regionserver60020-smallCompactions-1404255751957] compactions.ExploringCompactionPolicy: Exploring compaction algorithm has selected 0 files of size 0 starting at candidate #-1 after considering 0 permutations with 0 in ratio
2014-07-01 16:43:10,969 DEBUG [regionserver60020-smallCompactions-1404255751957] compactions.RatioBasedCompactionPolicy: Not compacting files because we only have 0 files ready for compaction. Need 3 to initiate.
2014-07-01 16:43:10,969 DEBUG [regionserver60020-smallCompactions-1404255751957] regionserver.CompactSplitThread: Not compacting usertable,user3,1404257714774.a59b89c303508bc329bc1095d0beb9e5. because compaction request was cancelled
2014-07-01 16:43:10,970 DEBUG [MemStoreFlusher.0] regionserver.CompactSplitThread: Small Compaction requested: system; Because: MemStoreFlusher.0; compaction_queue=(0:0), split_queue=0, merge_queue=0
2014-07-01 16:43:10,970 DEBUG [MemStoreFlusher.0] regionserver.HRegion: Started memstore flush for usertable,user8,1404257714774.b6ae40892bbb2a1c545234ed91d62d1a., current region memstore size 536.8m
2014-07-01 16:43:10,975 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-01 16:43:10,977 DEBUG [RpcServer.handler=33,port=60020] regionserver.HRegion: Flush requested on usertable,user3,1404257714774.a59b89c303508bc329bc1095d0beb9e5.
2014-07-01 16:43:11,032 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 36805 synced till here 36792
2014-07-01 16:43:11,129 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1404255227840/sceplus-vm48.almaden.ibm.com%2C60020%2C1404255227840.1404258189130 with entries=93, filesize=74.2m; new WAL /hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1404255227840/sceplus-vm48.almaden.ibm.com%2C60020%2C1404255227840.1404258190975
2014-07-01 16:43:11,520 DEBUG [MemStoreFlusher.0] util.FSUtils: DFS Client does not support most favored nodes create; using default create
2014-07-01 16:43:13,678 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-01 16:43:14,144 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 36925 synced till here 36923
2014-07-01 16:43:14,176 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1404255227840/sceplus-vm48.almaden.ibm.com%2C60020%2C1404255227840.1404258190975 with entries=120, filesize=94.4m; new WAL /hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1404255227840/sceplus-vm48.almaden.ibm.com%2C60020%2C1404255227840.1404258193678
2014-07-01 16:43:15,802 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-01 16:43:15,847 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1404255227840/sceplus-vm48.almaden.ibm.com%2C60020%2C1404255227840.1404258193678 with entries=78, filesize=62.5m; new WAL /hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1404255227840/sceplus-vm48.almaden.ibm.com%2C60020%2C1404255227840.1404258195803
2014-07-01 16:43:18,098 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-01 16:43:18,130 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 37089 synced till here 37078
2014-07-01 16:43:18,220 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1404255227840/sceplus-vm48.almaden.ibm.com%2C60020%2C1404255227840.1404258195803 with entries=86, filesize=73.0m; new WAL /hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1404255227840/sceplus-vm48.almaden.ibm.com%2C60020%2C1404255227840.1404258198098
2014-07-01 16:43:20,316 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-01 16:43:20,372 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 37176 synced till here 37167
2014-07-01 16:43:20,508 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1404255227840/sceplus-vm48.almaden.ibm.com%2C60020%2C1404255227840.1404258198098 with entries=87, filesize=69.7m; new WAL /hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1404255227840/sceplus-vm48.almaden.ibm.com%2C60020%2C1404255227840.1404258200316
2014-07-01 16:43:21,722 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-01 16:43:21,785 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 37258 synced till here 37254
2014-07-01 16:43:21,915 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1404255227840/sceplus-vm48.almaden.ibm.com%2C60020%2C1404255227840.1404258200316 with entries=82, filesize=65.4m; new WAL /hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1404255227840/sceplus-vm48.almaden.ibm.com%2C60020%2C1404255227840.1404258201723
2014-07-01 16:43:24,134 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-01 16:43:24,160 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 37347 synced till here 37334
2014-07-01 16:43:24,280 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1404255227840/sceplus-vm48.almaden.ibm.com%2C60020%2C1404255227840.1404258201723 with entries=89, filesize=74.3m; new WAL /hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1404255227840/sceplus-vm48.almaden.ibm.com%2C60020%2C1404255227840.1404258204135
2014-07-01 16:43:26,810 INFO  [JvmPauseMonitor] util.JvmPauseMonitor: Detected pause in JVM or host machine (eg GC): pause of approximately 1430ms
GC pool 'ParNew' had collection(s): count=1 time=1713ms
2014-07-01 16:43:27,227 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-01 16:43:27,262 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 37442 synced till here 37422
2014-07-01 16:43:27,491 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1404255227840/sceplus-vm48.almaden.ibm.com%2C60020%2C1404255227840.1404258204135 with entries=95, filesize=80.1m; new WAL /hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1404255227840/sceplus-vm48.almaden.ibm.com%2C60020%2C1404255227840.1404258207227
2014-07-01 16:43:29,667 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-01 16:43:29,738 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 37549 synced till here 37515
2014-07-01 16:43:30,040 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1404255227840/sceplus-vm48.almaden.ibm.com%2C60020%2C1404255227840.1404258207227 with entries=107, filesize=91.7m; new WAL /hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1404255227840/sceplus-vm48.almaden.ibm.com%2C60020%2C1404255227840.1404258209667
2014-07-01 16:43:30,155 INFO  [MemStoreFlusher.0] regionserver.DefaultStoreFlusher: Flushed, sequenceid=3335, memsize=291.6m, hasBloomFilter=true, into tmp file hdfs://master:54310/hbase/data/default/usertable/b6ae40892bbb2a1c545234ed91d62d1a/.tmp/d80f22c1eb694ba9928e8a352543d069
2014-07-01 16:43:32,370 INFO  [JvmPauseMonitor] util.JvmPauseMonitor: Detected pause in JVM or host machine (eg GC): pause of approximately 1827ms
GC pool 'ParNew' had collection(s): count=1 time=2196ms
2014-07-01 16:43:32,398 DEBUG [MemStoreFlusher.0] regionserver.HRegionFileSystem: Committing store file hdfs://master:54310/hbase/data/default/usertable/b6ae40892bbb2a1c545234ed91d62d1a/.tmp/d80f22c1eb694ba9928e8a352543d069 as hdfs://master:54310/hbase/data/default/usertable/b6ae40892bbb2a1c545234ed91d62d1a/family/d80f22c1eb694ba9928e8a352543d069
2014-07-01 16:43:32,417 INFO  [MemStoreFlusher.0] regionserver.HStore: Added hdfs://master:54310/hbase/data/default/usertable/b6ae40892bbb2a1c545234ed91d62d1a/family/d80f22c1eb694ba9928e8a352543d069, entries=1061900, sequenceid=3335, filesize=75.7m
2014-07-01 16:43:32,417 INFO  [MemStoreFlusher.0] regionserver.HRegion: Finished memstore flush of ~538.5m/564704560, currentsize=245.6m/257532400 for region usertable,user8,1404257714774.b6ae40892bbb2a1c545234ed91d62d1a. in 21447ms, sequenceid=3335, compaction requested=true
2014-07-01 16:43:32,418 DEBUG [MemStoreFlusher.0] regionserver.CompactSplitThread: Small Compaction requested: system; Because: MemStoreFlusher.0; compaction_queue=(0:0), split_queue=0, merge_queue=0
2014-07-01 16:43:32,418 DEBUG [regionserver60020-smallCompactions-1404255751957] compactions.RatioBasedCompactionPolicy: Selecting compaction from 14 store files, 0 compacting, 14 eligible, 20 blocking
2014-07-01 16:43:32,418 DEBUG [regionserver60020-smallCompactions-1404255751957] compactions.RatioBasedCompactionPolicy: Some files are too large. Excluding 14 files from compaction candidates
2014-07-01 16:43:32,418 DEBUG [regionserver60020-smallCompactions-1404255751957] compactions.ExploringCompactionPolicy: Exploring compaction algorithm has selected 0 files of size 0 starting at candidate #-1 after considering 0 permutations with 0 in ratio
2014-07-01 16:43:32,418 DEBUG [MemStoreFlusher.0] regionserver.HRegion: Started memstore flush for usertable,user2,1404257714774.2ed053f05f5eec791a57de24e16f0043., current region memstore size 726.1m
2014-07-01 16:43:32,418 DEBUG [regionserver60020-smallCompactions-1404255751957] compactions.RatioBasedCompactionPolicy: Not compacting files because we only have 0 files ready for compaction. Need 3 to initiate.
2014-07-01 16:43:32,418 DEBUG [regionserver60020-smallCompactions-1404255751957] regionserver.CompactSplitThread: Not compacting usertable,user8,1404257714774.b6ae40892bbb2a1c545234ed91d62d1a. because compaction request was cancelled
2014-07-01 16:43:32,602 WARN  [RpcServer.reader=2,port=60020] ipc.RpcServer: RpcServer.listener,port=60020: count of bytes read: 0
java.io.IOException: Connection reset by peer
	at sun.nio.ch.FileDispatcher.read0(Native Method)
	at sun.nio.ch.SocketDispatcher.read(SocketDispatcher.java:39)
	at sun.nio.ch.IOUtil.readIntoNativeBuffer(IOUtil.java:251)
	at sun.nio.ch.IOUtil.read(IOUtil.java:224)
	at sun.nio.ch.SocketChannelImpl.read(SocketChannelImpl.java:254)
	at org.apache.hadoop.hbase.ipc.RpcServer.channelIO(RpcServer.java:2263)
	at org.apache.hadoop.hbase.ipc.RpcServer.channelRead(RpcServer.java:2229)
	at org.apache.hadoop.hbase.ipc.RpcServer$Connection.readAndProcess(RpcServer.java:1488)
	at org.apache.hadoop.hbase.ipc.RpcServer$Listener.doRead(RpcServer.java:790)
	at org.apache.hadoop.hbase.ipc.RpcServer$Listener$Reader.doRunLoop(RpcServer.java:581)
	at org.apache.hadoop.hbase.ipc.RpcServer$Listener$Reader.run(RpcServer.java:556)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1146)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:701)
2014-07-01 16:43:32,760 DEBUG [RpcServer.handler=36,port=60020] regionserver.HRegion: Flush requested on usertable,user8,1404257714774.b6ae40892bbb2a1c545234ed91d62d1a.
2014-07-01 16:43:32,766 WARN  [RpcServer.handler=24,port=60020] ipc.RpcServer: RpcServer.respondercallId: 8533 service: ClientService methodName: Multi size: 3.0m connection: 9.1.143.58:42328: output error
2014-07-01 16:43:32,768 WARN  [RpcServer.handler=24,port=60020] ipc.RpcServer: RpcServer.handler=24,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-07-01 16:43:32,770 WARN  [RpcServer.handler=3,port=60020] ipc.RpcServer: RpcServer.respondercallId: 8519 service: ClientService methodName: Multi size: 3.4m connection: 9.1.143.58:42328: output error
2014-07-01 16:43:32,770 WARN  [RpcServer.handler=3,port=60020] ipc.RpcServer: RpcServer.handler=3,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-07-01 16:43:32,770 WARN  [RpcServer.handler=18,port=60020] ipc.RpcServer: RpcServer.respondercallId: 8505 service: ClientService methodName: Multi size: 3.4m connection: 9.1.143.58:42328: output error
2014-07-01 16:43:32,770 WARN  [RpcServer.handler=18,port=60020] ipc.RpcServer: RpcServer.handler=18,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-07-01 16:43:32,970 WARN  [RpcServer.handler=38,port=60020] ipc.RpcServer: RpcServer.respondercallId: 8575 service: ClientService methodName: Multi size: 3.4m connection: 9.1.143.58:42328: output error
2014-07-01 16:43:32,970 WARN  [RpcServer.handler=38,port=60020] ipc.RpcServer: RpcServer.handler=38,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-07-01 16:43:32,971 WARN  [RpcServer.handler=17,port=60020] ipc.RpcServer: RpcServer.respondercallId: 8582 service: ClientService methodName: Multi size: 3.4m connection: 9.1.143.58:42328: output error
2014-07-01 16:43:32,971 WARN  [RpcServer.handler=17,port=60020] ipc.RpcServer: RpcServer.handler=17,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-07-01 16:43:32,973 WARN  [RpcServer.handler=26,port=60020] ipc.RpcServer: RpcServer.respondercallId: 8581 service: ClientService methodName: Multi size: 3.3m connection: 9.1.143.58:42328: output error
2014-07-01 16:43:32,973 WARN  [RpcServer.handler=26,port=60020] ipc.RpcServer: RpcServer.handler=26,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-07-01 16:43:32,973 WARN  [RpcServer.handler=22,port=60020] ipc.RpcServer: RpcServer.respondercallId: 8580 service: ClientService methodName: Multi size: 3.4m connection: 9.1.143.58:42328: output error
2014-07-01 16:43:32,973 WARN  [RpcServer.handler=22,port=60020] ipc.RpcServer: RpcServer.handler=22,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-07-01 16:43:33,281 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-01 16:43:33,283 WARN  [RpcServer.handler=33,port=60020] ipc.RpcServer: RpcServer.respondercallId: 8577 service: ClientService methodName: Multi size: 2.9m connection: 9.1.143.58:42328: output error
2014-07-01 16:43:33,283 WARN  [RpcServer.handler=33,port=60020] ipc.RpcServer: RpcServer.handler=33,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-07-01 16:43:33,283 WARN  [RpcServer.handler=36,port=60020] ipc.RpcServer: RpcServer.respondercallId: 8595 service: ClientService methodName: Multi size: 3.3m connection: 9.1.143.58:42328: output error
2014-07-01 16:43:33,283 WARN  [RpcServer.handler=36,port=60020] ipc.RpcServer: RpcServer.handler=36,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-07-01 16:43:33,283 WARN  [RpcServer.handler=39,port=60020] ipc.RpcServer: RpcServer.respondercallId: 8579 service: ClientService methodName: Multi size: 3.4m connection: 9.1.143.58:42328: output error
2014-07-01 16:43:33,284 WARN  [RpcServer.handler=39,port=60020] ipc.RpcServer: RpcServer.handler=39,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-07-01 16:43:33,284 WARN  [RpcServer.handler=42,port=60020] ipc.RpcServer: RpcServer.respondercallId: 8578 service: ClientService methodName: Multi size: 3.4m connection: 9.1.143.58:42328: output error
2014-07-01 16:43:33,284 WARN  [RpcServer.handler=42,port=60020] ipc.RpcServer: RpcServer.handler=42,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-07-01 16:43:33,363 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 37674 synced till here 37645
2014-07-01 16:43:37,204 INFO  [JvmPauseMonitor] util.JvmPauseMonitor: Detected pause in JVM or host machine (eg GC): pause of approximately 3332ms
GC pool 'ParNew' had collection(s): count=1 time=3809ms
2014-07-01 16:43:37,234 WARN  [RpcServer.handler=44,port=60020] ipc.RpcServer: RpcServer.respondercallId: 8590 service: ClientService methodName: Multi size: 3.0m connection: 9.1.143.58:42328: output error
2014-07-01 16:43:37,235 WARN  [RpcServer.handler=44,port=60020] ipc.RpcServer: RpcServer.handler=44,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-07-01 16:43:37,239 WARN  [RpcServer.handler=15,port=60020] ipc.RpcServer: RpcServer.respondercallId: 8592 service: ClientService methodName: Multi size: 3.0m connection: 9.1.143.58:42328: output error
2014-07-01 16:43:37,239 WARN  [RpcServer.handler=15,port=60020] ipc.RpcServer: RpcServer.handler=15,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-07-01 16:43:37,246 DEBUG [RpcServer.handler=37,port=60020] regionserver.HRegion: Flush requested on usertable,user9,1404257714774.84635e5ca79006f09dceafa095992aab.
2014-07-01 16:43:37,247 WARN  [RpcServer.handler=16,port=60020] ipc.RpcServer: RpcServer.respondercallId: 8576 service: ClientService methodName: Multi size: 3.4m connection: 9.1.143.58:42328: output error
2014-07-01 16:43:37,247 WARN  [RpcServer.handler=16,port=60020] ipc.RpcServer: RpcServer.handler=16,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-07-01 16:43:37,247 WARN  [RpcServer.handler=37,port=60020] ipc.RpcServer: RpcServer.respondercallId: 8593 service: ClientService methodName: Multi size: 3.3m connection: 9.1.143.58:42328: output error
2014-07-01 16:43:37,247 WARN  [RpcServer.handler=37,port=60020] ipc.RpcServer: RpcServer.handler=37,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-07-01 16:43:37,247 WARN  [RpcServer.handler=20,port=60020] ipc.RpcServer: RpcServer.respondercallId: 8594 service: ClientService methodName: Multi size: 3.0m connection: 9.1.143.58:42328: output error
2014-07-01 16:43:37,248 WARN  [RpcServer.handler=20,port=60020] ipc.RpcServer: RpcServer.handler=20,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-07-01 16:43:37,336 WARN  [RpcServer.handler=9,port=60020] ipc.RpcServer: RpcServer.respondercallId: 8589 service: ClientService methodName: Multi size: 3.4m connection: 9.1.143.58:42328: output error
2014-07-01 16:43:37,336 WARN  [RpcServer.handler=9,port=60020] ipc.RpcServer: RpcServer.handler=9,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-07-01 16:43:37,344 WARN  [RpcServer.handler=35,port=60020] ipc.RpcServer: RpcServer.respondercallId: 8586 service: ClientService methodName: Multi size: 3.0m connection: 9.1.143.58:42328: output error
2014-07-01 16:43:37,344 WARN  [RpcServer.handler=35,port=60020] ipc.RpcServer: RpcServer.handler=35,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-07-01 16:43:37,352 WARN  [RpcServer.handler=43,port=60020] ipc.RpcServer: RpcServer.respondercallId: 8585 service: ClientService methodName: Multi size: 3.3m connection: 9.1.143.58:42328: output error
2014-07-01 16:43:37,353 WARN  [RpcServer.handler=43,port=60020] ipc.RpcServer: RpcServer.handler=43,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-07-01 16:43:37,369 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1404255227840/sceplus-vm48.almaden.ibm.com%2C60020%2C1404255227840.1404258209667 with entries=125, filesize=101.0m; new WAL /hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1404255227840/sceplus-vm48.almaden.ibm.com%2C60020%2C1404255227840.1404258213281
2014-07-01 16:43:37,512 DEBUG [MemStoreFlusher.0] util.FSUtils: DFS Client does not support most favored nodes create; using default create
2014-07-01 16:43:37,771 WARN  [RpcServer.handler=8,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":10099,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.58:42328","starttimems":1404258207672,"queuetimems":0,"class":"HRegionServer","responsesize":19368,"method":"Multi"}
2014-07-01 16:43:37,772 WARN  [RpcServer.handler=8,port=60020] ipc.RpcServer: RpcServer.respondercallId: 8598 service: ClientService methodName: Multi size: 3.4m connection: 9.1.143.58:42328: output error
2014-07-01 16:43:37,772 WARN  [RpcServer.handler=8,port=60020] ipc.RpcServer: RpcServer.handler=8,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-07-01 16:43:37,775 WARN  [RpcServer.handler=0,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":10187,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.58:42328","starttimems":1404258207587,"queuetimems":1,"class":"HRegionServer","responsesize":19542,"method":"Multi"}
2014-07-01 16:43:37,775 WARN  [RpcServer.handler=0,port=60020] ipc.RpcServer: RpcServer.respondercallId: 8588 service: ClientService methodName: Multi size: 3.4m connection: 9.1.143.58:42328: output error
2014-07-01 16:43:37,775 WARN  [RpcServer.handler=0,port=60020] ipc.RpcServer: RpcServer.handler=0,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-07-01 16:43:37,776 WARN  [RpcServer.handler=7,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":10163,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.58:42328","starttimems":1404258207613,"queuetimems":1,"class":"HRegionServer","responsesize":19354,"method":"Multi"}
2014-07-01 16:43:37,777 WARN  [RpcServer.handler=7,port=60020] ipc.RpcServer: RpcServer.respondercallId: 8587 service: ClientService methodName: Multi size: 3.4m connection: 9.1.143.58:42328: output error
2014-07-01 16:43:37,777 WARN  [RpcServer.handler=7,port=60020] ipc.RpcServer: RpcServer.handler=7,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-07-01 16:43:37,783 WARN  [RpcServer.handler=12,port=60020] ipc.RpcServer: RpcServer.respondercallId: 8602 service: ClientService methodName: Multi size: 3.4m connection: 9.1.143.58:42328: output error
2014-07-01 16:43:37,783 WARN  [RpcServer.handler=12,port=60020] ipc.RpcServer: RpcServer.handler=12,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-07-01 16:43:37,787 WARN  [RpcServer.handler=32,port=60020] ipc.RpcServer: RpcServer.respondercallId: 8599 service: ClientService methodName: Multi size: 3.4m connection: 9.1.143.58:42328: output error
2014-07-01 16:43:37,787 WARN  [RpcServer.handler=32,port=60020] ipc.RpcServer: RpcServer.handler=32,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-07-01 16:43:37,795 WARN  [RpcServer.handler=40,port=60020] ipc.RpcServer: RpcServer.respondercallId: 8606 service: ClientService methodName: Multi size: 3.3m connection: 9.1.143.58:42328: output error
2014-07-01 16:43:37,795 WARN  [RpcServer.handler=40,port=60020] ipc.RpcServer: RpcServer.handler=40,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-07-01 16:43:37,836 WARN  [RpcServer.handler=19,port=60020] ipc.RpcServer: RpcServer.respondercallId: 8613 service: ClientService methodName: Multi size: 2.9m connection: 9.1.143.58:42328: output error
2014-07-01 16:43:37,836 WARN  [RpcServer.handler=19,port=60020] ipc.RpcServer: RpcServer.handler=19,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-07-01 16:43:37,863 WARN  [RpcServer.handler=45,port=60020] ipc.RpcServer: RpcServer.respondercallId: 8608 service: ClientService methodName: Multi size: 3.3m connection: 9.1.143.58:42328: output error
2014-07-01 16:43:37,863 WARN  [RpcServer.handler=45,port=60020] ipc.RpcServer: RpcServer.handler=45,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-07-01 16:43:37,879 WARN  [RpcServer.handler=11,port=60020] ipc.RpcServer: RpcServer.respondercallId: 8607 service: ClientService methodName: Multi size: 3.0m connection: 9.1.143.58:42328: output error
2014-07-01 16:43:37,879 WARN  [RpcServer.handler=11,port=60020] ipc.RpcServer: RpcServer.handler=11,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-07-01 16:43:37,919 WARN  [RpcServer.handler=31,port=60020] ipc.RpcServer: RpcServer.respondercallId: 8603 service: ClientService methodName: Multi size: 3.3m connection: 9.1.143.58:42328: output error
2014-07-01 16:43:37,919 WARN  [RpcServer.handler=31,port=60020] ipc.RpcServer: RpcServer.handler=31,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-07-01 16:43:37,920 WARN  [RpcServer.handler=6,port=60020] ipc.RpcServer: RpcServer.respondercallId: 8614 service: ClientService methodName: Multi size: 3.4m connection: 9.1.143.58:42328: output error
2014-07-01 16:43:37,920 WARN  [RpcServer.handler=6,port=60020] ipc.RpcServer: RpcServer.handler=6,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-07-01 16:43:37,923 WARN  [RpcServer.handler=13,port=60020] ipc.RpcServer: RpcServer.respondercallId: 8604 service: ClientService methodName: Multi size: 3.3m connection: 9.1.143.58:42328: output error
2014-07-01 16:43:37,923 WARN  [RpcServer.handler=13,port=60020] ipc.RpcServer: RpcServer.handler=13,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-07-01 16:43:37,923 WARN  [RpcServer.handler=23,port=60020] ipc.RpcServer: RpcServer.respondercallId: 8612 service: ClientService methodName: Multi size: 3.3m connection: 9.1.143.58:42328: output error
2014-07-01 16:43:37,923 WARN  [RpcServer.handler=23,port=60020] ipc.RpcServer: RpcServer.handler=23,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-07-01 16:43:37,926 WARN  [RpcServer.handler=25,port=60020] ipc.RpcServer: RpcServer.respondercallId: 8609 service: ClientService methodName: Multi size: 3.4m connection: 9.1.143.58:42328: output error
2014-07-01 16:43:37,926 WARN  [RpcServer.handler=25,port=60020] ipc.RpcServer: RpcServer.handler=25,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-07-01 16:43:37,927 WARN  [RpcServer.handler=46,port=60020] ipc.RpcServer: RpcServer.respondercallId: 8616 service: ClientService methodName: Multi size: 3.4m connection: 9.1.143.58:42328: output error
2014-07-01 16:43:37,927 WARN  [RpcServer.handler=46,port=60020] ipc.RpcServer: RpcServer.handler=46,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-07-01 16:43:37,934 WARN  [RpcServer.handler=30,port=60020] ipc.RpcServer: RpcServer.respondercallId: 8605 service: ClientService methodName: Multi size: 3.4m connection: 9.1.143.58:42328: output error
2014-07-01 16:43:37,934 WARN  [RpcServer.handler=30,port=60020] ipc.RpcServer: RpcServer.handler=30,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-07-01 16:43:37,935 WARN  [RpcServer.handler=47,port=60020] ipc.RpcServer: RpcServer.respondercallId: 8611 service: ClientService methodName: Multi size: 3.4m connection: 9.1.143.58:42328: output error
2014-07-01 16:43:37,935 WARN  [RpcServer.handler=47,port=60020] ipc.RpcServer: RpcServer.handler=47,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-07-01 16:43:37,935 WARN  [RpcServer.handler=10,port=60020] ipc.RpcServer: RpcServer.respondercallId: 8615 service: ClientService methodName: Multi size: 3.4m connection: 9.1.143.58:42328: output error
2014-07-01 16:43:37,935 WARN  [RpcServer.handler=10,port=60020] ipc.RpcServer: RpcServer.handler=10,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-07-01 16:43:37,936 WARN  [RpcServer.handler=14,port=60020] ipc.RpcServer: RpcServer.respondercallId: 8610 service: ClientService methodName: Multi size: 3.4m connection: 9.1.143.58:42328: output error
2014-07-01 16:43:37,936 WARN  [RpcServer.handler=14,port=60020] ipc.RpcServer: RpcServer.handler=14,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-07-01 16:43:42,859 INFO  [JvmPauseMonitor] util.JvmPauseMonitor: Detected pause in JVM or host machine (eg GC): pause of approximately 4654ms
GC pool 'ParNew' had collection(s): count=1 time=4769ms
2014-07-01 16:43:42,926 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-01 16:43:42,940 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 37784 synced till here 37759
2014-07-01 16:43:43,167 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1404255227840/sceplus-vm48.almaden.ibm.com%2C60020%2C1404255227840.1404258213281 with entries=110, filesize=92.5m; new WAL /hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1404255227840/sceplus-vm48.almaden.ibm.com%2C60020%2C1404255227840.1404258222927
2014-07-01 16:43:43,394 WARN  [RpcServer.handler=41,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":10991,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.58:42328","starttimems":1404258212402,"queuetimems":1,"class":"HRegionServer","responsesize":19097,"method":"Multi"}
2014-07-01 16:43:43,395 WARN  [RpcServer.handler=41,port=60020] ipc.RpcServer: RpcServer.respondercallId: 8627 service: ClientService methodName: Multi size: 3.3m connection: 9.1.143.58:42328: output error
2014-07-01 16:43:43,395 WARN  [RpcServer.handler=41,port=60020] ipc.RpcServer: RpcServer.handler=41,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-07-01 16:43:43,441 WARN  [RpcServer.handler=24,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":10438,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.58:42413","starttimems":1404258213002,"queuetimems":0,"class":"HRegionServer","responsesize":17009,"method":"Multi"}
2014-07-01 16:43:43,441 WARN  [RpcServer.handler=29,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":13337,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.58:42328","starttimems":1404258210103,"queuetimems":0,"class":"HRegionServer","responsesize":19132,"method":"Multi"}
2014-07-01 16:43:43,441 WARN  [RpcServer.handler=24,port=60020] ipc.RpcServer: RpcServer.respondercallId: 8655 service: ClientService methodName: Multi size: 3.0m connection: 9.1.143.58:42413: output error
2014-07-01 16:43:43,441 WARN  [RpcServer.handler=29,port=60020] ipc.RpcServer: RpcServer.respondercallId: 8620 service: ClientService methodName: Multi size: 3.3m connection: 9.1.143.58:42328: output error
2014-07-01 16:43:43,442 WARN  [RpcServer.handler=48,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":10562,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.58:42413","starttimems":1404258212879,"queuetimems":0,"class":"HRegionServer","responsesize":19542,"method":"Multi"}
2014-07-01 16:43:43,445 WARN  [RpcServer.handler=24,port=60020] ipc.RpcServer: RpcServer.handler=24,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-07-01 16:43:43,446 WARN  [RpcServer.handler=48,port=60020] ipc.RpcServer: RpcServer.respondercallId: 8654 service: ClientService methodName: Multi size: 3.4m connection: 9.1.143.58:42413: output error
2014-07-01 16:43:43,446 WARN  [RpcServer.handler=48,port=60020] ipc.RpcServer: RpcServer.handler=48,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-07-01 16:43:43,446 WARN  [RpcServer.handler=5,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":10530,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.58:42413","starttimems":1404258212915,"queuetimems":0,"class":"HRegionServer","responsesize":19152,"method":"Multi"}
2014-07-01 16:43:43,446 WARN  [RpcServer.handler=49,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":13303,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.58:42328","starttimems":1404258210143,"queuetimems":0,"class":"HRegionServer","responsesize":17128,"method":"Multi"}
2014-07-01 16:43:43,446 WARN  [RpcServer.handler=5,port=60020] ipc.RpcServer: RpcServer.respondercallId: 8658 service: ClientService methodName: Multi size: 3.3m connection: 9.1.143.58:42413: output error
2014-07-01 16:43:43,446 WARN  [RpcServer.handler=5,port=60020] ipc.RpcServer: RpcServer.handler=5,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-07-01 16:43:43,447 WARN  [RpcServer.handler=49,port=60020] ipc.RpcServer: RpcServer.respondercallId: 8628 service: ClientService methodName: Multi size: 3.0m connection: 9.1.143.58:42328: output error
2014-07-01 16:43:43,447 WARN  [RpcServer.handler=49,port=60020] ipc.RpcServer: RpcServer.handler=49,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-07-01 16:43:43,449 WARN  [RpcServer.handler=4,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":10841,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.58:42413","starttimems":1404258212607,"queuetimems":0,"class":"HRegionServer","responsesize":19289,"method":"Multi"}
2014-07-01 16:43:43,449 WARN  [RpcServer.handler=4,port=60020] ipc.RpcServer: RpcServer.respondercallId: 8650 service: ClientService methodName: Multi size: 3.4m connection: 9.1.143.58:42413: output error
2014-07-01 16:43:43,449 WARN  [RpcServer.handler=4,port=60020] ipc.RpcServer: RpcServer.handler=4,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-07-01 16:43:43,452 WARN  [RpcServer.handler=21,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":10917,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.58:42328","starttimems":1404258212533,"queuetimems":100,"class":"HRegionServer","responsesize":19133,"method":"Multi"}
2014-07-01 16:43:43,452 WARN  [RpcServer.handler=21,port=60020] ipc.RpcServer: RpcServer.respondercallId: 8625 service: ClientService methodName: Multi size: 3.3m connection: 9.1.143.58:42328: output error
2014-07-01 16:43:43,452 WARN  [RpcServer.handler=21,port=60020] ipc.RpcServer: RpcServer.handler=21,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-07-01 16:43:43,446 WARN  [RpcServer.handler=34,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":10800,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.58:42413","starttimems":1404258212645,"queuetimems":0,"class":"HRegionServer","responsesize":19354,"method":"Multi"}
2014-07-01 16:43:43,446 WARN  [RpcServer.handler=1,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":11030,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.58:42328","starttimems":1404258212415,"queuetimems":0,"class":"HRegionServer","responsesize":17101,"method":"Multi"}
2014-07-01 16:43:43,445 WARN  [RpcServer.handler=29,port=60020] ipc.RpcServer: RpcServer.handler=29,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-07-01 16:43:43,455 WARN  [RpcServer.handler=1,port=60020] ipc.RpcServer: RpcServer.respondercallId: 8626 service: ClientService methodName: Multi size: 3.0m connection: 9.1.143.58:42328: output error
2014-07-01 16:43:43,455 WARN  [RpcServer.handler=1,port=60020] ipc.RpcServer: RpcServer.handler=1,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-07-01 16:43:43,458 WARN  [RpcServer.handler=34,port=60020] ipc.RpcServer: RpcServer.respondercallId: 8651 service: ClientService methodName: Multi size: 3.4m connection: 9.1.143.58:42413: output error
2014-07-01 16:43:43,458 WARN  [RpcServer.handler=34,port=60020] ipc.RpcServer: RpcServer.handler=34,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-07-01 16:43:43,739 WARN  [RpcServer.handler=28,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":13788,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.58:42328","starttimems":1404258209950,"queuetimems":1,"class":"HRegionServer","responsesize":19354,"method":"Multi"}
2014-07-01 16:43:43,740 WARN  [RpcServer.handler=28,port=60020] ipc.RpcServer: RpcServer.respondercallId: 8619 service: ClientService methodName: Multi size: 3.4m connection: 9.1.143.58:42328: output error
2014-07-01 16:43:43,740 WARN  [RpcServer.handler=28,port=60020] ipc.RpcServer: RpcServer.handler=28,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-07-01 16:43:46,402 INFO  [JvmPauseMonitor] util.JvmPauseMonitor: Detected pause in JVM or host machine (eg GC): pause of approximately 2542ms
GC pool 'ParNew' had collection(s): count=1 time=2556ms
2014-07-01 16:43:46,505 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-01 16:43:46,567 WARN  [RpcServer.handler=3,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":13536,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.58:42413","starttimems":1404258213031,"queuetimems":0,"class":"HRegionServer","responsesize":18968,"method":"Multi"}
2014-07-01 16:43:46,568 WARN  [RpcServer.handler=3,port=60020] ipc.RpcServer: RpcServer.respondercallId: 8662 service: ClientService methodName: Multi size: 3.3m connection: 9.1.143.58:42413: output error
2014-07-01 16:43:46,568 WARN  [RpcServer.handler=3,port=60020] ipc.RpcServer: RpcServer.handler=3,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-07-01 16:43:46,608 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 37879 synced till here 37867
2014-07-01 16:43:46,630 WARN  [RpcServer.handler=26,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":13479,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.58:42413","starttimems":1404258213151,"queuetimems":1,"class":"HRegionServer","responsesize":17128,"method":"Multi"}
2014-07-01 16:43:46,631 WARN  [RpcServer.handler=26,port=60020] ipc.RpcServer: RpcServer.respondercallId: 8674 service: ClientService methodName: Multi size: 3.0m connection: 9.1.143.58:42413: output error
2014-07-01 16:43:46,631 WARN  [RpcServer.handler=26,port=60020] ipc.RpcServer: RpcServer.handler=26,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-07-01 16:43:46,630 WARN  [RpcServer.handler=38,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":13527,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.58:42413","starttimems":1404258213103,"queuetimems":0,"class":"HRegionServer","responsesize":19099,"method":"Multi"}
2014-07-01 16:43:46,631 WARN  [RpcServer.handler=18,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":13556,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.58:42413","starttimems":1404258213075,"queuetimems":0,"class":"HRegionServer","responsesize":19416,"method":"Multi"}
2014-07-01 16:43:46,631 WARN  [RpcServer.handler=38,port=60020] ipc.RpcServer: RpcServer.respondercallId: 8660 service: ClientService methodName: Multi size: 3.3m connection: 9.1.143.58:42413: output error
2014-07-01 16:43:46,631 WARN  [RpcServer.handler=38,port=60020] ipc.RpcServer: RpcServer.handler=38,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-07-01 16:43:46,631 WARN  [RpcServer.handler=2,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":13691,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.58:42413","starttimems":1404258212940,"queuetimems":0,"class":"HRegionServer","responsesize":16876,"method":"Multi"}
2014-07-01 16:43:46,631 WARN  [RpcServer.handler=22,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":13449,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.58:42413","starttimems":1404258213182,"queuetimems":0,"class":"HRegionServer","responsesize":19097,"method":"Multi"}
2014-07-01 16:43:46,631 WARN  [RpcServer.handler=27,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":13647,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.58:42413","starttimems":1404258212983,"queuetimems":0,"class":"HRegionServer","responsesize":19137,"method":"Multi"}
2014-07-01 16:43:46,631 WARN  [RpcServer.handler=18,port=60020] ipc.RpcServer: RpcServer.respondercallId: 8661 service: ClientService methodName: Multi size: 3.4m connection: 9.1.143.58:42413: output error
2014-07-01 16:43:46,632 WARN  [RpcServer.handler=18,port=60020] ipc.RpcServer: RpcServer.handler=18,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-07-01 16:43:46,632 WARN  [RpcServer.handler=22,port=60020] ipc.RpcServer: RpcServer.respondercallId: 8673 service: ClientService methodName: Multi size: 3.3m connection: 9.1.143.58:42413: output error
2014-07-01 16:43:46,632 WARN  [RpcServer.handler=22,port=60020] ipc.RpcServer: RpcServer.handler=22,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-07-01 16:43:46,632 WARN  [RpcServer.handler=27,port=60020] ipc.RpcServer: RpcServer.respondercallId: 8656 service: ClientService methodName: Multi size: 3.3m connection: 9.1.143.58:42413: output error
2014-07-01 16:43:46,632 WARN  [RpcServer.handler=27,port=60020] ipc.RpcServer: RpcServer.handler=27,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-07-01 16:43:46,632 WARN  [RpcServer.handler=2,port=60020] ipc.RpcServer: RpcServer.respondercallId: 8657 service: ClientService methodName: Multi size: 3.0m connection: 9.1.143.58:42413: output error
2014-07-01 16:43:46,632 WARN  [RpcServer.handler=2,port=60020] ipc.RpcServer: RpcServer.handler=2,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-07-01 16:43:46,631 WARN  [RpcServer.handler=39,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":13347,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.58:42413","starttimems":1404258213284,"queuetimems":15,"class":"HRegionServer","responsesize":17101,"method":"Multi"}
2014-07-01 16:43:46,636 WARN  [RpcServer.handler=39,port=60020] ipc.RpcServer: RpcServer.respondercallId: 8670 service: ClientService methodName: Multi size: 3.0m connection: 9.1.143.58:42413: output error
2014-07-01 16:43:46,636 WARN  [RpcServer.handler=39,port=60020] ipc.RpcServer: RpcServer.handler=39,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-07-01 16:43:46,632 WARN  [RpcServer.handler=17,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":13507,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.58:42413","starttimems":1404258213125,"queuetimems":0,"class":"HRegionServer","responsesize":17228,"method":"Multi"}
2014-07-01 16:43:46,637 WARN  [RpcServer.handler=17,port=60020] ipc.RpcServer: RpcServer.respondercallId: 8659 service: ClientService methodName: Multi size: 3.0m connection: 9.1.143.58:42413: output error
2014-07-01 16:43:46,637 WARN  [RpcServer.handler=17,port=60020] ipc.RpcServer: RpcServer.handler=17,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-07-01 16:43:46,661 WARN  [RpcServer.handler=8,port=60020] ipc.RpcServer: RpcServer.respondercallId: 8709 service: ClientService methodName: Multi size: 3.3m connection: 9.1.143.58:42418: output error
2014-07-01 16:43:46,664 WARN  [RpcServer.handler=8,port=60020] ipc.RpcServer: RpcServer.handler=8,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-07-01 16:43:46,686 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1404255227840/sceplus-vm48.almaden.ibm.com%2C60020%2C1404255227840.1404258222927 with entries=95, filesize=71.1m; new WAL /hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1404255227840/sceplus-vm48.almaden.ibm.com%2C60020%2C1404255227840.1404258226506
2014-07-01 16:43:46,742 WARN  [RpcServer.handler=9,port=60020] ipc.RpcServer: RpcServer.respondercallId: 8703 service: ClientService methodName: Multi size: 3.3m connection: 9.1.143.58:42418: output error
2014-07-01 16:43:46,742 WARN  [RpcServer.handler=16,port=60020] ipc.RpcServer: RpcServer.respondercallId: 8666 service: ClientService methodName: Multi size: 3.4m connection: 9.1.143.58:42413: output error
2014-07-01 16:43:46,742 WARN  [RpcServer.handler=16,port=60020] ipc.RpcServer: RpcServer.handler=16,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-07-01 16:43:46,743 WARN  [RpcServer.handler=37,port=60020] ipc.RpcServer: RpcServer.respondercallId: 8665 service: ClientService methodName: Multi size: 3.4m connection: 9.1.143.58:42413: output error
2014-07-01 16:43:46,742 WARN  [RpcServer.handler=9,port=60020] ipc.RpcServer: RpcServer.handler=9,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-07-01 16:43:46,743 WARN  [RpcServer.handler=44,port=60020] ipc.RpcServer: RpcServer.respondercallId: 8668 service: ClientService methodName: Multi size: 3.4m connection: 9.1.143.58:42413: output error
2014-07-01 16:43:46,743 WARN  [RpcServer.handler=37,port=60020] ipc.RpcServer: RpcServer.handler=37,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-07-01 16:43:46,743 WARN  [RpcServer.handler=33,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":13459,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.58:42413","starttimems":1404258213283,"queuetimems":73,"class":"HRegionServer","responsesize":19341,"method":"Multi"}
2014-07-01 16:43:46,743 WARN  [RpcServer.handler=44,port=60020] ipc.RpcServer: RpcServer.handler=44,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-07-01 16:43:46,743 WARN  [RpcServer.handler=33,port=60020] ipc.RpcServer: RpcServer.respondercallId: 8672 service: ClientService methodName: Multi size: 3.4m connection: 9.1.143.58:42413: output error
2014-07-01 16:43:46,744 WARN  [RpcServer.handler=33,port=60020] ipc.RpcServer: RpcServer.handler=33,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-07-01 16:43:47,103 WARN  [RpcServer.handler=15,port=60020] ipc.RpcServer: RpcServer.respondercallId: 8667 service: ClientService methodName: Multi size: 3.4m connection: 9.1.143.58:42413: output error
2014-07-01 16:43:47,103 WARN  [RpcServer.handler=15,port=60020] ipc.RpcServer: RpcServer.handler=15,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-07-01 16:43:47,167 WARN  [RpcServer.handler=0,port=60020] ipc.RpcServer: RpcServer.respondercallId: 8708 service: ClientService methodName: Multi size: 3.0m connection: 9.1.143.58:42418: output error
2014-07-01 16:43:47,167 WARN  [RpcServer.handler=0,port=60020] ipc.RpcServer: RpcServer.handler=0,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-07-01 16:43:47,242 WARN  [RpcServer.handler=42,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":13903,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.58:42413","starttimems":1404258213338,"queuetimems":0,"class":"HRegionServer","responsesize":19110,"method":"Multi"}
2014-07-01 16:43:47,243 WARN  [RpcServer.handler=42,port=60020] ipc.RpcServer: RpcServer.respondercallId: 8669 service: ClientService methodName: Multi size: 3.3m connection: 9.1.143.58:42413: output error
2014-07-01 16:43:47,243 WARN  [RpcServer.handler=42,port=60020] ipc.RpcServer: RpcServer.handler=42,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-07-01 16:43:47,270 WARN  [RpcServer.handler=20,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":10022,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.58:42418","starttimems":1404258217248,"queuetimems":2,"class":"HRegionServer","responsesize":19416,"method":"Multi"}
2014-07-01 16:43:47,271 WARN  [RpcServer.handler=20,port=60020] ipc.RpcServer: RpcServer.respondercallId: 8702 service: ClientService methodName: Multi size: 3.4m connection: 9.1.143.58:42418: output error
2014-07-01 16:43:47,271 WARN  [RpcServer.handler=20,port=60020] ipc.RpcServer: RpcServer.handler=20,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-07-01 16:43:47,271 WARN  [RpcServer.handler=45,port=60020] ipc.RpcServer: RpcServer.respondercallId: 8716 service: ClientService methodName: Multi size: 3.4m connection: 9.1.143.58:42418: output error
2014-07-01 16:43:47,271 WARN  [RpcServer.handler=36,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":13988,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.58:42413","starttimems":1404258213283,"queuetimems":39,"class":"HRegionServer","responsesize":19133,"method":"Multi"}
2014-07-01 16:43:47,271 WARN  [RpcServer.handler=43,port=60020] ipc.RpcServer: RpcServer.respondercallId: 8710 service: ClientService methodName: Multi size: 3.4m connection: 9.1.143.58:42418: output error
2014-07-01 16:43:47,271 WARN  [RpcServer.handler=43,port=60020] ipc.RpcServer: RpcServer.handler=43,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-07-01 16:43:47,272 WARN  [RpcServer.handler=36,port=60020] ipc.RpcServer: RpcServer.respondercallId: 8671 service: ClientService methodName: Multi size: 3.3m connection: 9.1.143.58:42413: output error
2014-07-01 16:43:47,272 WARN  [RpcServer.handler=36,port=60020] ipc.RpcServer: RpcServer.handler=36,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-07-01 16:43:47,271 WARN  [RpcServer.handler=45,port=60020] ipc.RpcServer: RpcServer.handler=45,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-07-01 16:43:47,284 WARN  [RpcServer.handler=46,port=60020] ipc.RpcServer: RpcServer.respondercallId: 8719 service: ClientService methodName: Multi size: 3.4m connection: 9.1.143.58:42418: output error
2014-07-01 16:43:47,284 WARN  [RpcServer.handler=46,port=60020] ipc.RpcServer: RpcServer.handler=46,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-07-01 16:43:47,288 WARN  [RpcServer.handler=35,port=60020] ipc.RpcServer: RpcServer.respondercallId: 8711 service: ClientService methodName: Multi size: 3.4m connection: 9.1.143.58:42418: output error
2014-07-01 16:43:47,288 WARN  [RpcServer.handler=35,port=60020] ipc.RpcServer: RpcServer.handler=35,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-07-01 16:43:47,295 WARN  [RpcServer.handler=13,port=60020] ipc.RpcServer: RpcServer.respondercallId: 8712 service: ClientService methodName: Multi size: 3.3m connection: 9.1.143.58:42418: output error
2014-07-01 16:43:47,295 WARN  [RpcServer.handler=13,port=60020] ipc.RpcServer: RpcServer.handler=13,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-07-01 16:43:47,515 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-01 16:43:47,533 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 37972 synced till here 37957
2014-07-01 16:43:47,644 WARN  [RpcServer.handler=30,port=60020] ipc.RpcServer: RpcServer.respondercallId: 8718 service: ClientService methodName: Multi size: 3.3m connection: 9.1.143.58:42418: output error
2014-07-01 16:43:47,644 WARN  [RpcServer.handler=30,port=60020] ipc.RpcServer: RpcServer.handler=30,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-07-01 16:43:47,645 WARN  [RpcServer.handler=6,port=60020] ipc.RpcServer: RpcServer.respondercallId: 8713 service: ClientService methodName: Multi size: 3.0m connection: 9.1.143.58:42418: output error
2014-07-01 16:43:47,645 WARN  [RpcServer.handler=6,port=60020] ipc.RpcServer: RpcServer.handler=6,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-07-01 16:43:47,688 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1404255227840/sceplus-vm48.almaden.ibm.com%2C60020%2C1404255227840.1404258226506 with entries=93, filesize=79.4m; new WAL /hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1404255227840/sceplus-vm48.almaden.ibm.com%2C60020%2C1404255227840.1404258227516
2014-07-01 16:43:47,743 WARN  [RpcServer.handler=10,port=60020] ipc.RpcServer: RpcServer.respondercallId: 8722 service: ClientService methodName: Multi size: 3.3m connection: 9.1.143.58:42418: output error
2014-07-01 16:43:47,743 WARN  [RpcServer.handler=10,port=60020] ipc.RpcServer: RpcServer.handler=10,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-07-01 16:43:49,431 INFO  [JvmPauseMonitor] util.JvmPauseMonitor: Detected pause in JVM or host machine (eg GC): pause of approximately 1528ms
GC pool 'ParNew' had collection(s): count=1 time=1629ms
2014-07-01 16:43:49,443 DEBUG [LruStats #0] hfile.LruBlockCache: Total=4.16 MB, free=3.95 GB, max=3.96 GB, blocks=0, accesses=10456985, hits=10201085, hitRatio=97.55%, , cachingAccesses=10269046, cachingHits=10189981, cachingHitsRatio=99.23%, evictions=2, evicted=78407, evictedPerRun=39203.5
2014-07-01 16:43:49,523 WARN  [RpcServer.handler=40,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":11727,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.58:42418","starttimems":1404258217795,"queuetimems":328,"class":"HRegionServer","responsesize":17228,"method":"Multi"}
2014-07-01 16:43:49,523 WARN  [RpcServer.handler=11,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":11643,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.58:42418","starttimems":1404258217879,"queuetimems":326,"class":"HRegionServer","responsesize":19184,"method":"Multi"}
2014-07-01 16:43:49,523 WARN  [RpcServer.handler=40,port=60020] ipc.RpcServer: RpcServer.respondercallId: 8704 service: ClientService methodName: Multi size: 3.0m connection: 9.1.143.58:42418: output error
2014-07-01 16:43:49,523 WARN  [RpcServer.handler=40,port=60020] ipc.RpcServer: RpcServer.handler=40,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-07-01 16:43:49,523 WARN  [RpcServer.handler=11,port=60020] ipc.RpcServer: RpcServer.respondercallId: 8715 service: ClientService methodName: Multi size: 3.4m connection: 9.1.143.58:42418: output error
2014-07-01 16:43:49,523 WARN  [RpcServer.handler=11,port=60020] ipc.RpcServer: RpcServer.handler=11,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-07-01 16:43:49,524 WARN  [RpcServer.handler=7,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":11746,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.58:42418","starttimems":1404258217777,"queuetimems":385,"class":"HRegionServer","responsesize":17009,"method":"Multi"}
2014-07-01 16:43:49,524 WARN  [RpcServer.handler=32,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":11736,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.58:42418","starttimems":1404258217787,"queuetimems":342,"class":"HRegionServer","responsesize":19542,"method":"Multi"}
2014-07-01 16:43:49,524 WARN  [RpcServer.handler=7,port=60020] ipc.RpcServer: RpcServer.respondercallId: 8707 service: ClientService methodName: Multi size: 3.0m connection: 9.1.143.58:42418: output error
2014-07-01 16:43:49,524 WARN  [RpcServer.handler=7,port=60020] ipc.RpcServer: RpcServer.handler=7,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-07-01 16:43:49,524 WARN  [RpcServer.handler=32,port=60020] ipc.RpcServer: RpcServer.respondercallId: 8705 service: ClientService methodName: Multi size: 3.4m connection: 9.1.143.58:42418: output error
2014-07-01 16:43:49,524 WARN  [RpcServer.handler=32,port=60020] ipc.RpcServer: RpcServer.handler=32,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-07-01 16:43:49,525 WARN  [RpcServer.handler=25,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":11599,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.58:42418","starttimems":1404258217926,"queuetimems":283,"class":"HRegionServer","responsesize":19137,"method":"Multi"}
2014-07-01 16:43:49,526 WARN  [RpcServer.handler=25,port=60020] ipc.RpcServer: RpcServer.respondercallId: 8720 service: ClientService methodName: Multi size: 3.3m connection: 9.1.143.58:42418: output error
2014-07-01 16:43:49,526 WARN  [RpcServer.handler=25,port=60020] ipc.RpcServer: RpcServer.handler=25,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-07-01 16:43:49,526 WARN  [RpcServer.handler=31,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":11606,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.58:42418","starttimems":1404258217920,"queuetimems":352,"class":"HRegionServer","responsesize":19110,"method":"Multi"}
2014-07-01 16:43:49,526 WARN  [RpcServer.handler=12,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":11743,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.58:42418","starttimems":1404258217783,"queuetimems":369,"class":"HRegionServer","responsesize":16876,"method":"Multi"}
2014-07-01 16:43:49,527 WARN  [RpcServer.handler=31,port=60020] ipc.RpcServer: RpcServer.respondercallId: 8714 service: ClientService methodName: Multi size: 3.3m connection: 9.1.143.58:42418: output error
2014-07-01 16:43:49,527 WARN  [RpcServer.handler=31,port=60020] ipc.RpcServer: RpcServer.handler=31,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-07-01 16:43:49,527 WARN  [RpcServer.handler=12,port=60020] ipc.RpcServer: RpcServer.respondercallId: 8706 service: ClientService methodName: Multi size: 3.0m connection: 9.1.143.58:42418: output error
2014-07-01 16:43:49,527 WARN  [RpcServer.handler=12,port=60020] ipc.RpcServer: RpcServer.handler=12,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-07-01 16:43:49,527 WARN  [RpcServer.handler=23,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":11603,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.58:42418","starttimems":1404258217924,"queuetimems":304,"class":"HRegionServer","responsesize":18968,"method":"Multi"}
2014-07-01 16:43:49,528 WARN  [RpcServer.handler=23,port=60020] ipc.RpcServer: RpcServer.respondercallId: 8721 service: ClientService methodName: Multi size: 3.3m connection: 9.1.143.58:42418: output error
2014-07-01 16:43:49,528 WARN  [RpcServer.handler=23,port=60020] ipc.RpcServer: RpcServer.handler=23,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-07-01 16:43:49,528 WARN  [RpcServer.handler=19,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":11692,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"9.1.143.58:42418","starttimems":1404258217836,"queuetimems":337,"class":"HRegionServer","responsesize":19564,"method":"Multi"}
2014-07-01 16:43:49,528 WARN  [RpcServer.handler=19,port=60020] ipc.RpcServer: RpcServer.respondercallId: 8717 service: ClientService methodName: Multi size: 3.4m connection: 9.1.143.58:42418: output error
2014-07-01 16:43:49,529 WARN  [RpcServer.handler=19,port=60020] ipc.RpcServer: RpcServer.handler=19,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-07-01 16:43:49,965 INFO  [MemStoreFlusher.1] regionserver.DefaultStoreFlusher: Flushed, sequenceid=3306, memsize=371.6m, hasBloomFilter=true, into tmp file hdfs://master:54310/hbase/data/default/usertable/c831a0ac8e8f14e82fdaef2ba3a5e1f6/.tmp/840f98efd9e744fba6929e90e8d71936
2014-07-01 16:43:49,976 DEBUG [MemStoreFlusher.1] regionserver.HRegionFileSystem: Committing store file hdfs://master:54310/hbase/data/default/usertable/c831a0ac8e8f14e82fdaef2ba3a5e1f6/.tmp/840f98efd9e744fba6929e90e8d71936 as hdfs://master:54310/hbase/data/default/usertable/c831a0ac8e8f14e82fdaef2ba3a5e1f6/family/840f98efd9e744fba6929e90e8d71936
2014-07-01 16:43:49,984 INFO  [MemStoreFlusher.1] regionserver.HStore: Added hdfs://master:54310/hbase/data/default/usertable/c831a0ac8e8f14e82fdaef2ba3a5e1f6/family/840f98efd9e744fba6929e90e8d71936, entries=1353100, sequenceid=3306, filesize=96.4m
2014-07-01 16:43:49,984 INFO  [MemStoreFlusher.1] regionserver.HRegion: Finished memstore flush of ~731.4m/766964640, currentsize=482.7m/506181600 for region usertable,user4,1404257714774.c831a0ac8e8f14e82fdaef2ba3a5e1f6. in 42402ms, sequenceid=3306, compaction requested=true
2014-07-01 16:43:49,985 DEBUG [MemStoreFlusher.1] regionserver.CompactSplitThread: Small Compaction requested: system; Because: MemStoreFlusher.1; compaction_queue=(0:0), split_queue=0, merge_queue=0
2014-07-01 16:43:49,985 DEBUG [regionserver60020-smallCompactions-1404255751957] compactions.RatioBasedCompactionPolicy: Selecting compaction from 14 store files, 0 compacting, 14 eligible, 20 blocking
2014-07-01 16:43:49,985 DEBUG [regionserver60020-smallCompactions-1404255751957] compactions.RatioBasedCompactionPolicy: Some files are too large. Excluding 14 files from compaction candidates
2014-07-01 16:43:49,985 DEBUG [MemStoreFlusher.1] regionserver.HRegion: Started memstore flush for usertable,user3,1404257714774.a59b89c303508bc329bc1095d0beb9e5., current region memstore size 736.4m
2014-07-01 16:43:49,985 DEBUG [regionserver60020-smallCompactions-1404255751957] compactions.ExploringCompactionPolicy: Exploring compaction algorithm has selected 0 files of size 0 starting at candidate #-1 after considering 0 permutations with 0 in ratio
2014-07-01 16:43:49,985 DEBUG [regionserver60020-smallCompactions-1404255751957] compactions.RatioBasedCompactionPolicy: Not compacting files because we only have 0 files ready for compaction. Need 3 to initiate.
2014-07-01 16:43:49,985 DEBUG [regionserver60020-smallCompactions-1404255751957] regionserver.CompactSplitThread: Not compacting usertable,user4,1404257714774.c831a0ac8e8f14e82fdaef2ba3a5e1f6. because compaction request was cancelled
2014-07-01 16:43:50,080 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-01 16:43:50,098 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 38069 synced till here 38063
2014-07-01 16:43:50,153 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1404255227840/sceplus-vm48.almaden.ibm.com%2C60020%2C1404255227840.1404258227516 with entries=97, filesize=65.4m; new WAL /hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1404255227840/sceplus-vm48.almaden.ibm.com%2C60020%2C1404255227840.1404258230081
2014-07-01 16:43:50,153 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1404255227840/sceplus-vm48.almaden.ibm.com%2C60020%2C1404255227840.1404258135327
2014-07-01 16:43:50,153 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1404255227840/sceplus-vm48.almaden.ibm.com%2C60020%2C1404255227840.1404258137822
2014-07-01 16:43:50,154 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1404255227840/sceplus-vm48.almaden.ibm.com%2C60020%2C1404255227840.1404258140256
2014-07-01 16:43:50,154 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1404255227840/sceplus-vm48.almaden.ibm.com%2C60020%2C1404255227840.1404258142102
2014-07-01 16:43:50,154 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1404255227840/sceplus-vm48.almaden.ibm.com%2C60020%2C1404255227840.1404258144180
2014-07-01 16:43:50,154 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1404255227840/sceplus-vm48.almaden.ibm.com%2C60020%2C1404255227840.1404258146413
2014-07-01 16:43:50,154 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1404255227840/sceplus-vm48.almaden.ibm.com%2C60020%2C1404255227840.1404258148462
2014-07-01 16:43:50,375 DEBUG [RpcServer.handler=20,port=60020] regionserver.HRegion: Flush requested on usertable,user4,1404257714774.c831a0ac8e8f14e82fdaef2ba3a5e1f6.
2014-07-01 16:43:50,646 DEBUG [MemStoreFlusher.1] util.FSUtils: DFS Client does not support most favored nodes create; using default create
2014-07-01 16:43:53,974 INFO  [JvmPauseMonitor] util.JvmPauseMonitor: Detected pause in JVM or host machine (eg GC): pause of approximately 2542ms
GC pool 'ParNew' had collection(s): count=1 time=2990ms
2014-07-01 16:43:54,191 WARN  [RpcServer.handler=35,port=60020] ipc.RpcServer: RpcServer.respondercallId: 8796 service: ClientService methodName: Multi size: 3.4m connection: 9.1.143.58:42422: output error
2014-07-01 16:43:54,193 WARN  [RpcServer.handler=35,port=60020] ipc.RpcServer: RpcServer.handler=35,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-07-01 16:43:54,284 WARN  [RpcServer.handler=6,port=60020] ipc.RpcServer: RpcServer.respondercallId: 8810 service: ClientService methodName: Multi size: 3.4m connection: 9.1.143.58:42422: output error
2014-07-01 16:43:54,284 WARN  [RpcServer.handler=6,port=60020] ipc.RpcServer: RpcServer.handler=6,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-07-01 16:43:54,296 WARN  [RpcServer.handler=13,port=60020] ipc.RpcServer: RpcServer.respondercallId: 8795 service: ClientService methodName: Multi size: 3.3m connection: 9.1.143.58:42422: output error
2014-07-01 16:43:54,296 WARN  [RpcServer.handler=13,port=60020] ipc.RpcServer: RpcServer.handler=13,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-07-01 16:43:54,308 WARN  [RpcServer.handler=30,port=60020] ipc.RpcServer: RpcServer.respondercallId: 8794 service: ClientService methodName: Multi size: 3.0m connection: 9.1.143.58:42422: output error
2014-07-01 16:43:54,308 WARN  [RpcServer.handler=30,port=60020] ipc.RpcServer: RpcServer.handler=30,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-07-01 16:43:54,323 WARN  [RpcServer.handler=10,port=60020] ipc.RpcServer: RpcServer.respondercallId: 8808 service: ClientService methodName: Multi size: 3.4m connection: 9.1.143.58:42422: output error
2014-07-01 16:43:54,323 WARN  [RpcServer.handler=10,port=60020] ipc.RpcServer: RpcServer.handler=10,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-07-01 16:43:54,358 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-01 16:43:54,374 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 38146 synced till here 38143
2014-07-01 16:43:54,391 WARN  [RpcServer.handler=40,port=60020] ipc.RpcServer: RpcServer.respondercallId: 8806 service: ClientService methodName: Multi size: 3.3m connection: 9.1.143.58:42422: output error
2014-07-01 16:43:54,391 WARN  [RpcServer.handler=40,port=60020] ipc.RpcServer: RpcServer.handler=40,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-07-01 16:43:54,551 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1404255227840/sceplus-vm48.almaden.ibm.com%2C60020%2C1404255227840.1404258230081 with entries=77, filesize=63.8m; new WAL /hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1404255227840/sceplus-vm48.almaden.ibm.com%2C60020%2C1404255227840.1404258234358
2014-07-01 16:43:54,570 WARN  [RpcServer.handler=34,port=60020] ipc.RpcServer: RpcServer.respondercallId: 8801 service: ClientService methodName: Multi size: 3.0m connection: 9.1.143.58:42422: output error
2014-07-01 16:43:54,570 WARN  [RpcServer.handler=34,port=60020] ipc.RpcServer: RpcServer.handler=34,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-07-01 16:43:54,571 WARN  [RpcServer.handler=7,port=60020] ipc.RpcServer: RpcServer.respondercallId: 8803 service: ClientService methodName: Multi size: 3.3m connection: 9.1.143.58:42422: output error
2014-07-01 16:43:54,571 WARN  [RpcServer.handler=7,port=60020] ipc.RpcServer: RpcServer.handler=7,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-07-01 16:43:54,571 WARN  [RpcServer.handler=11,port=60020] ipc.RpcServer: RpcServer.respondercallId: 8804 service: ClientService methodName: Multi size: 3.4m connection: 9.1.143.58:42422: output error
2014-07-01 16:43:54,571 WARN  [RpcServer.handler=11,port=60020] ipc.RpcServer: RpcServer.handler=11,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-07-01 16:43:54,572 WARN  [RpcServer.handler=32,port=60020] ipc.RpcServer: RpcServer.respondercallId: 8802 service: ClientService methodName: Multi size: 3.3m connection: 9.1.143.58:42422: output error
2014-07-01 16:43:54,572 WARN  [RpcServer.handler=32,port=60020] ipc.RpcServer: RpcServer.handler=32,port=60020: caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null
2014-07-01 16:43:56,455 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-01 16:43:56,474 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 38224 synced till here 38223
2014-07-01 16:43:56,486 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1404255227840/sceplus-vm48.almaden.ibm.com%2C60020%2C1404255227840.1404258234358 with entries=78, filesize=62.0m; new WAL /hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1404255227840/sceplus-vm48.almaden.ibm.com%2C60020%2C1404255227840.1404258236455
2014-07-01 16:43:56,486 INFO  [regionserver60020.logRoller] wal.FSHLog: Too many hlogs: logs=33, maxlogs=32; forcing flush of 1 regions(s): 84635e5ca79006f09dceafa095992aab
2014-07-01 16:43:58,765 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-01 16:43:58,780 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 38298 synced till here 38297
2014-07-01 16:43:58,807 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1404255227840/sceplus-vm48.almaden.ibm.com%2C60020%2C1404255227840.1404258236455 with entries=74, filesize=61.9m; new WAL /hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1404255227840/sceplus-vm48.almaden.ibm.com%2C60020%2C1404255227840.1404258238765
2014-07-01 16:43:58,808 INFO  [regionserver60020.logRoller] wal.FSHLog: Too many hlogs: logs=34, maxlogs=32; forcing flush of 1 regions(s): 84635e5ca79006f09dceafa095992aab
2014-07-01 16:44:00,045 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-01 16:44:00,070 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 38372 synced till here 38371
2014-07-01 16:44:00,083 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1404255227840/sceplus-vm48.almaden.ibm.com%2C60020%2C1404255227840.1404258238765 with entries=74, filesize=61.8m; new WAL /hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1404255227840/sceplus-vm48.almaden.ibm.com%2C60020%2C1404255227840.1404258240046
2014-07-01 16:44:00,084 INFO  [regionserver60020.logRoller] wal.FSHLog: Too many hlogs: logs=35, maxlogs=32; forcing flush of 1 regions(s): 84635e5ca79006f09dceafa095992aab
2014-07-01 16:44:02,797 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-01 16:44:02,827 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1404255227840/sceplus-vm48.almaden.ibm.com%2C60020%2C1404255227840.1404258240046 with entries=76, filesize=62.5m; new WAL /hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1404255227840/sceplus-vm48.almaden.ibm.com%2C60020%2C1404255227840.1404258242797
2014-07-01 16:44:02,828 INFO  [regionserver60020.logRoller] wal.FSHLog: Too many hlogs: logs=36, maxlogs=32; forcing flush of 1 regions(s): 84635e5ca79006f09dceafa095992aab
2014-07-01 16:44:04,478 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-01 16:44:04,510 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 38533 synced till here 38521
2014-07-01 16:44:04,631 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1404255227840/sceplus-vm48.almaden.ibm.com%2C60020%2C1404255227840.1404258242797 with entries=85, filesize=74.4m; new WAL /hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1404255227840/sceplus-vm48.almaden.ibm.com%2C60020%2C1404255227840.1404258244478
2014-07-01 16:44:04,631 INFO  [regionserver60020.logRoller] wal.FSHLog: Too many hlogs: logs=37, maxlogs=32; forcing flush of 1 regions(s): 84635e5ca79006f09dceafa095992aab
2014-07-01 16:44:06,382 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-01 16:44:06,420 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 38618 synced till here 38609
2014-07-01 16:44:06,477 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1404255227840/sceplus-vm48.almaden.ibm.com%2C60020%2C1404255227840.1404258244478 with entries=85, filesize=70.0m; new WAL /hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1404255227840/sceplus-vm48.almaden.ibm.com%2C60020%2C1404255227840.1404258246382
2014-07-01 16:44:06,477 INFO  [regionserver60020.logRoller] wal.FSHLog: Too many hlogs: logs=38, maxlogs=32; forcing flush of 1 regions(s): 84635e5ca79006f09dceafa095992aab
2014-07-01 16:44:08,199 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-01 16:44:08,226 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 38705 synced till here 38703
2014-07-01 16:44:08,249 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1404255227840/sceplus-vm48.almaden.ibm.com%2C60020%2C1404255227840.1404258246382 with entries=87, filesize=63.9m; new WAL /hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1404255227840/sceplus-vm48.almaden.ibm.com%2C60020%2C1404255227840.1404258248200
2014-07-01 16:44:08,250 INFO  [regionserver60020.logRoller] wal.FSHLog: Too many hlogs: logs=39, maxlogs=32; forcing flush of 1 regions(s): 84635e5ca79006f09dceafa095992aab
2014-07-01 16:44:10,217 INFO  [MemStoreFlusher.0] regionserver.DefaultStoreFlusher: Flushed, sequenceid=3515, memsize=388.0m, hasBloomFilter=true, into tmp file hdfs://master:54310/hbase/data/default/usertable/2ed053f05f5eec791a57de24e16f0043/.tmp/20a26334dd964115a5ebfb22f1c9b049
2014-07-01 16:44:10,230 DEBUG [MemStoreFlusher.0] regionserver.HRegionFileSystem: Committing store file hdfs://master:54310/hbase/data/default/usertable/2ed053f05f5eec791a57de24e16f0043/.tmp/20a26334dd964115a5ebfb22f1c9b049 as hdfs://master:54310/hbase/data/default/usertable/2ed053f05f5eec791a57de24e16f0043/family/20a26334dd964115a5ebfb22f1c9b049
2014-07-01 16:44:10,238 INFO  [MemStoreFlusher.0] regionserver.HStore: Added hdfs://master:54310/hbase/data/default/usertable/2ed053f05f5eec791a57de24e16f0043/family/20a26334dd964115a5ebfb22f1c9b049, entries=1412530, sequenceid=3515, filesize=100.6m
2014-07-01 16:44:10,238 INFO  [MemStoreFlusher.0] regionserver.HRegion: Finished memstore flush of ~741.6m/777594080, currentsize=366.4m/384158800 for region usertable,user2,1404257714774.2ed053f05f5eec791a57de24e16f0043. in 37820ms, sequenceid=3515, compaction requested=true
2014-07-01 16:44:10,238 DEBUG [MemStoreFlusher.0] regionserver.CompactSplitThread: Small Compaction requested: system; Because: MemStoreFlusher.0; compaction_queue=(0:0), split_queue=0, merge_queue=0
2014-07-01 16:44:10,239 DEBUG [regionserver60020-smallCompactions-1404255751957] compactions.RatioBasedCompactionPolicy: Selecting compaction from 15 store files, 0 compacting, 15 eligible, 20 blocking
2014-07-01 16:44:10,239 DEBUG [MemStoreFlusher.0] regionserver.HRegion: Started memstore flush for usertable,user8,1404257714774.b6ae40892bbb2a1c545234ed91d62d1a., current region memstore size 673.8m
2014-07-01 16:44:10,239 DEBUG [regionserver60020-smallCompactions-1404255751957] compactions.RatioBasedCompactionPolicy: Some files are too large. Excluding 15 files from compaction candidates
2014-07-01 16:44:10,239 DEBUG [regionserver60020-smallCompactions-1404255751957] compactions.ExploringCompactionPolicy: Exploring compaction algorithm has selected 0 files of size 0 starting at candidate #-1 after considering 0 permutations with 0 in ratio
2014-07-01 16:44:10,239 DEBUG [regionserver60020-smallCompactions-1404255751957] compactions.RatioBasedCompactionPolicy: Not compacting files because we only have 0 files ready for compaction. Need 3 to initiate.
2014-07-01 16:44:10,239 DEBUG [regionserver60020-smallCompactions-1404255751957] regionserver.CompactSplitThread: Not compacting usertable,user2,1404257714774.2ed053f05f5eec791a57de24e16f0043. because compaction request was cancelled
2014-07-01 16:44:10,265 DEBUG [RpcServer.handler=39,port=60020] regionserver.HRegion: Flush requested on usertable,user2,1404257714774.2ed053f05f5eec791a57de24e16f0043.
2014-07-01 16:44:10,500 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-01 16:44:10,524 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1404255227840/sceplus-vm48.almaden.ibm.com%2C60020%2C1404255227840.1404258248200 with entries=76, filesize=61.5m; new WAL /hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1404255227840/sceplus-vm48.almaden.ibm.com%2C60020%2C1404255227840.1404258250500
2014-07-01 16:44:10,524 INFO  [regionserver60020.logRoller] wal.FSHLog: Too many hlogs: logs=40, maxlogs=32; forcing flush of 1 regions(s): 84635e5ca79006f09dceafa095992aab
2014-07-01 16:44:10,793 DEBUG [MemStoreFlusher.0] util.FSUtils: DFS Client does not support most favored nodes create; using default create
2014-07-01 16:44:13,047 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-01 16:44:13,078 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1404255227840/sceplus-vm48.almaden.ibm.com%2C60020%2C1404255227840.1404258250500 with entries=77, filesize=61.6m; new WAL /hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1404255227840/sceplus-vm48.almaden.ibm.com%2C60020%2C1404255227840.1404258253047
2014-07-01 16:44:13,078 INFO  [regionserver60020.logRoller] wal.FSHLog: Too many hlogs: logs=41, maxlogs=32; forcing flush of 1 regions(s): 84635e5ca79006f09dceafa095992aab
2014-07-01 16:44:14,092 INFO  [MemStoreFlusher.1] regionserver.DefaultStoreFlusher: Flushed, sequenceid=3591, memsize=380.9m, hasBloomFilter=true, into tmp file hdfs://master:54310/hbase/data/default/usertable/a59b89c303508bc329bc1095d0beb9e5/.tmp/c5a2fac26c3e41a4ad14a98e65397b61
2014-07-01 16:44:14,106 DEBUG [MemStoreFlusher.1] regionserver.HRegionFileSystem: Committing store file hdfs://master:54310/hbase/data/default/usertable/a59b89c303508bc329bc1095d0beb9e5/.tmp/c5a2fac26c3e41a4ad14a98e65397b61 as hdfs://master:54310/hbase/data/default/usertable/a59b89c303508bc329bc1095d0beb9e5/family/c5a2fac26c3e41a4ad14a98e65397b61
2014-07-01 16:44:14,124 INFO  [MemStoreFlusher.1] regionserver.HStore: Added hdfs://master:54310/hbase/data/default/usertable/a59b89c303508bc329bc1095d0beb9e5/family/c5a2fac26c3e41a4ad14a98e65397b61, entries=1386700, sequenceid=3591, filesize=98.8m
2014-07-01 16:44:14,124 INFO  [MemStoreFlusher.1] regionserver.HRegion: Finished memstore flush of ~736.4m/772197360, currentsize=289.2m/303218960 for region usertable,user3,1404257714774.a59b89c303508bc329bc1095d0beb9e5. in 24139ms, sequenceid=3591, compaction requested=true
2014-07-01 16:44:14,125 DEBUG [regionserver60020-smallCompactions-1404255751957] compactions.RatioBasedCompactionPolicy: Selecting compaction from 15 store files, 0 compacting, 15 eligible, 20 blocking
2014-07-01 16:44:14,125 DEBUG [regionserver60020-smallCompactions-1404255751957] compactions.RatioBasedCompactionPolicy: Some files are too large. Excluding 15 files from compaction candidates
2014-07-01 16:44:14,125 DEBUG [regionserver60020-smallCompactions-1404255751957] compactions.ExploringCompactionPolicy: Exploring compaction algorithm has selected 0 files of size 0 starting at candidate #-1 after considering 0 permutations with 0 in ratio
2014-07-01 16:44:14,125 DEBUG [regionserver60020-smallCompactions-1404255751957] compactions.RatioBasedCompactionPolicy: Not compacting files because we only have 0 files ready for compaction. Need 3 to initiate.
2014-07-01 16:44:14,125 DEBUG [regionserver60020-smallCompactions-1404255751957] regionserver.CompactSplitThread: Not compacting usertable,user3,1404257714774.a59b89c303508bc329bc1095d0beb9e5. because compaction request was cancelled
2014-07-01 16:44:14,125 DEBUG [MemStoreFlusher.1] regionserver.CompactSplitThread: Small Compaction requested: system; Because: MemStoreFlusher.1; compaction_queue=(0:0), split_queue=0, merge_queue=0
2014-07-01 16:44:14,125 DEBUG [MemStoreFlusher.1] regionserver.HRegion: Started memstore flush for usertable,user9,1404257714774.84635e5ca79006f09dceafa095992aab., current region memstore size 395.2m
2014-07-01 16:44:14,162 DEBUG [RpcServer.handler=28,port=60020] regionserver.HRegion: Flush requested on usertable,user3,1404257714774.a59b89c303508bc329bc1095d0beb9e5.
2014-07-01 16:44:15,344 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-01 16:44:15,369 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1404255227840/sceplus-vm48.almaden.ibm.com%2C60020%2C1404255227840.1404258253047 with entries=81, filesize=62.0m; new WAL /hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1404255227840/sceplus-vm48.almaden.ibm.com%2C60020%2C1404255227840.1404258255344
2014-07-01 16:44:15,488 DEBUG [MemStoreFlusher.1] util.FSUtils: DFS Client does not support most favored nodes create; using default create
2014-07-01 16:44:17,336 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-01 16:44:17,408 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 39025 synced till here 39009
2014-07-01 16:44:17,560 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1404255227840/sceplus-vm48.almaden.ibm.com%2C60020%2C1404255227840.1404258255344 with entries=86, filesize=76.2m; new WAL /hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1404255227840/sceplus-vm48.almaden.ibm.com%2C60020%2C1404255227840.1404258257337
2014-07-01 16:44:18,565 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-01 16:44:18,618 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1404255227840/sceplus-vm48.almaden.ibm.com%2C60020%2C1404255227840.1404258257337 with entries=80, filesize=62.3m; new WAL /hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1404255227840/sceplus-vm48.almaden.ibm.com%2C60020%2C1404255227840.1404258258565
2014-07-01 16:44:20,414 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-01 16:44:20,471 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 39183 synced till here 39179
2014-07-01 16:44:20,531 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1404255227840/sceplus-vm48.almaden.ibm.com%2C60020%2C1404255227840.1404258258565 with entries=78, filesize=65.7m; new WAL /hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1404255227840/sceplus-vm48.almaden.ibm.com%2C60020%2C1404255227840.1404258260415
2014-07-01 16:44:21,962 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-01 16:44:21,994 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 39266 synced till here 39263
2014-07-01 16:44:22,054 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1404255227840/sceplus-vm48.almaden.ibm.com%2C60020%2C1404255227840.1404258260415 with entries=83, filesize=63.9m; new WAL /hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1404255227840/sceplus-vm48.almaden.ibm.com%2C60020%2C1404255227840.1404258261963
2014-07-01 16:44:25,305 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-01 16:44:25,639 INFO  [MemStoreFlusher.1] regionserver.DefaultStoreFlusher: Flushed, sequenceid=3755, memsize=219.4m, hasBloomFilter=true, into tmp file hdfs://master:54310/hbase/data/default/usertable/84635e5ca79006f09dceafa095992aab/.tmp/a8f11c31ee524befb047241df029de89
2014-07-01 16:44:25,695 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1404255227840/sceplus-vm48.almaden.ibm.com%2C60020%2C1404255227840.1404258261963 with entries=89, filesize=69.3m; new WAL /hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1404255227840/sceplus-vm48.almaden.ibm.com%2C60020%2C1404255227840.1404258265371
2014-07-01 16:44:25,705 DEBUG [MemStoreFlusher.1] regionserver.HRegionFileSystem: Committing store file hdfs://master:54310/hbase/data/default/usertable/84635e5ca79006f09dceafa095992aab/.tmp/a8f11c31ee524befb047241df029de89 as hdfs://master:54310/hbase/data/default/usertable/84635e5ca79006f09dceafa095992aab/family/a8f11c31ee524befb047241df029de89
2014-07-01 16:44:25,718 INFO  [MemStoreFlusher.1] regionserver.HStore: Added hdfs://master:54310/hbase/data/default/usertable/84635e5ca79006f09dceafa095992aab/family/a8f11c31ee524befb047241df029de89, entries=798860, sequenceid=3755, filesize=56.9m
2014-07-01 16:44:25,718 INFO  [MemStoreFlusher.1] regionserver.HRegion: Finished memstore flush of ~395.8m/414979520, currentsize=44.7m/46892960 for region usertable,user9,1404257714774.84635e5ca79006f09dceafa095992aab. in 11593ms, sequenceid=3755, compaction requested=true
2014-07-01 16:44:25,727 DEBUG [MemStoreFlusher.1] regionserver.CompactSplitThread: Small Compaction requested: system; Because: MemStoreFlusher.1; compaction_queue=(0:0), split_queue=0, merge_queue=0
2014-07-01 16:44:25,727 DEBUG [regionserver60020-smallCompactions-1404255751957] compactions.RatioBasedCompactionPolicy: Selecting compaction from 6 store files, 0 compacting, 6 eligible, 20 blocking
2014-07-01 16:44:25,727 DEBUG [regionserver60020-smallCompactions-1404255751957] compactions.RatioBasedCompactionPolicy: Some files are too large. Excluding 6 files from compaction candidates
2014-07-01 16:44:25,727 DEBUG [MemStoreFlusher.1] regionserver.HRegion: Started memstore flush for usertable,user4,1404257714774.c831a0ac8e8f14e82fdaef2ba3a5e1f6., current region memstore size 921.4m
2014-07-01 16:44:25,727 DEBUG [regionserver60020-smallCompactions-1404255751957] compactions.ExploringCompactionPolicy: Exploring compaction algorithm has selected 0 files of size 0 starting at candidate #-1 after considering 0 permutations with 0 in ratio
2014-07-01 16:44:25,727 DEBUG [regionserver60020-smallCompactions-1404255751957] compactions.RatioBasedCompactionPolicy: Not compacting files because we only have 0 files ready for compaction. Need 3 to initiate.
2014-07-01 16:44:25,727 DEBUG [regionserver60020-smallCompactions-1404255751957] regionserver.CompactSplitThread: Not compacting usertable,user9,1404257714774.84635e5ca79006f09dceafa095992aab. because compaction request was cancelled
2014-07-01 16:44:27,191 DEBUG [MemStoreFlusher.1] util.FSUtils: DFS Client does not support most favored nodes create; using default create
2014-07-01 16:44:28,211 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-01 16:44:28,235 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 39437 synced till here 39435
2014-07-01 16:44:28,282 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1404255227840/sceplus-vm48.almaden.ibm.com%2C60020%2C1404255227840.1404258265371 with entries=82, filesize=64.6m; new WAL /hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1404255227840/sceplus-vm48.almaden.ibm.com%2C60020%2C1404255227840.1404258268212
2014-07-01 16:44:28,283 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1404255227840/sceplus-vm48.almaden.ibm.com%2C60020%2C1404255227840.1404258151057
2014-07-01 16:44:28,283 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1404255227840/sceplus-vm48.almaden.ibm.com%2C60020%2C1404255227840.1404258152768
2014-07-01 16:44:28,283 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1404255227840/sceplus-vm48.almaden.ibm.com%2C60020%2C1404255227840.1404258154682
2014-07-01 16:44:28,283 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1404255227840/sceplus-vm48.almaden.ibm.com%2C60020%2C1404255227840.1404258156535
2014-07-01 16:44:28,283 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1404255227840/sceplus-vm48.almaden.ibm.com%2C60020%2C1404255227840.1404258157937
2014-07-01 16:44:28,283 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1404255227840/sceplus-vm48.almaden.ibm.com%2C60020%2C1404255227840.1404258161426
2014-07-01 16:44:28,283 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1404255227840/sceplus-vm48.almaden.ibm.com%2C60020%2C1404255227840.1404258163473
2014-07-01 16:44:28,283 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1404255227840/sceplus-vm48.almaden.ibm.com%2C60020%2C1404255227840.1404258165730
2014-07-01 16:44:28,283 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1404255227840/sceplus-vm48.almaden.ibm.com%2C60020%2C1404255227840.1404258168208
2014-07-01 16:44:28,283 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1404255227840/sceplus-vm48.almaden.ibm.com%2C60020%2C1404255227840.1404258171130
2014-07-01 16:44:28,283 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1404255227840/sceplus-vm48.almaden.ibm.com%2C60020%2C1404255227840.1404258173662
2014-07-01 16:44:28,283 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1404255227840/sceplus-vm48.almaden.ibm.com%2C60020%2C1404255227840.1404258177195
2014-07-01 16:44:28,283 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1404255227840/sceplus-vm48.almaden.ibm.com%2C60020%2C1404255227840.1404258179220
2014-07-01 16:44:28,283 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1404255227840/sceplus-vm48.almaden.ibm.com%2C60020%2C1404255227840.1404258180862
2014-07-01 16:44:28,283 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1404255227840/sceplus-vm48.almaden.ibm.com%2C60020%2C1404255227840.1404258182658
2014-07-01 16:44:28,283 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1404255227840/sceplus-vm48.almaden.ibm.com%2C60020%2C1404255227840.1404258185224
2014-07-01 16:44:28,657 INFO  [MemStoreFlusher.0] regionserver.DefaultStoreFlusher: Flushed, sequenceid=3730, memsize=389.1m, hasBloomFilter=true, into tmp file hdfs://master:54310/hbase/data/default/usertable/b6ae40892bbb2a1c545234ed91d62d1a/.tmp/8138734703e34f55b6d6b59884e2bada
2014-07-01 16:44:28,675 DEBUG [MemStoreFlusher.0] regionserver.HRegionFileSystem: Committing store file hdfs://master:54310/hbase/data/default/usertable/b6ae40892bbb2a1c545234ed91d62d1a/.tmp/8138734703e34f55b6d6b59884e2bada as hdfs://master:54310/hbase/data/default/usertable/b6ae40892bbb2a1c545234ed91d62d1a/family/8138734703e34f55b6d6b59884e2bada
2014-07-01 16:44:28,695 INFO  [MemStoreFlusher.0] regionserver.HStore: Added hdfs://master:54310/hbase/data/default/usertable/b6ae40892bbb2a1c545234ed91d62d1a/family/8138734703e34f55b6d6b59884e2bada, entries=1416540, sequenceid=3730, filesize=100.9m
2014-07-01 16:44:28,695 INFO  [MemStoreFlusher.0] regionserver.HRegion: Finished memstore flush of ~673.8m/706582720, currentsize=240.2m/251832800 for region usertable,user8,1404257714774.b6ae40892bbb2a1c545234ed91d62d1a. in 18456ms, sequenceid=3730, compaction requested=true
2014-07-01 16:44:28,696 DEBUG [MemStoreFlusher.0] regionserver.CompactSplitThread: Small Compaction requested: system; Because: MemStoreFlusher.0; compaction_queue=(0:0), split_queue=0, merge_queue=0
2014-07-01 16:44:28,696 DEBUG [regionserver60020-smallCompactions-1404255751957] compactions.RatioBasedCompactionPolicy: Selecting compaction from 15 store files, 0 compacting, 15 eligible, 20 blocking
2014-07-01 16:44:28,696 DEBUG [MemStoreFlusher.0] regionserver.HRegion: Started memstore flush for usertable,user2,1404257714774.2ed053f05f5eec791a57de24e16f0043., current region memstore size 604.2m
2014-07-01 16:44:28,696 DEBUG [regionserver60020-smallCompactions-1404255751957] compactions.RatioBasedCompactionPolicy: Some files are too large. Excluding 15 files from compaction candidates
2014-07-01 16:44:28,696 DEBUG [regionserver60020-smallCompactions-1404255751957] compactions.ExploringCompactionPolicy: Exploring compaction algorithm has selected 0 files of size 0 starting at candidate #-1 after considering 0 permutations with 0 in ratio
2014-07-01 16:44:28,696 DEBUG [regionserver60020-smallCompactions-1404255751957] compactions.RatioBasedCompactionPolicy: Not compacting files because we only have 0 files ready for compaction. Need 3 to initiate.
2014-07-01 16:44:28,696 DEBUG [regionserver60020-smallCompactions-1404255751957] regionserver.CompactSplitThread: Not compacting usertable,user8,1404257714774.b6ae40892bbb2a1c545234ed91d62d1a. because compaction request was cancelled
2014-07-01 16:44:29,942 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-01 16:44:29,942 DEBUG [MemStoreFlusher.0] util.FSUtils: DFS Client does not support most favored nodes create; using default create
2014-07-01 16:44:29,983 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 39515 synced till here 39508
2014-07-01 16:44:30,223 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1404255227840/sceplus-vm48.almaden.ibm.com%2C60020%2C1404255227840.1404258268212 with entries=78, filesize=66.4m; new WAL /hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1404255227840/sceplus-vm48.almaden.ibm.com%2C60020%2C1404255227840.1404258269942
2014-07-01 16:44:30,295 DEBUG [RpcServer.handler=18,port=60020] regionserver.HRegion: Flush requested on usertable,user8,1404257714774.b6ae40892bbb2a1c545234ed91d62d1a.
2014-07-01 16:44:31,775 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-01 16:44:31,803 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 39597 synced till here 39593
2014-07-01 16:44:31,831 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1404255227840/sceplus-vm48.almaden.ibm.com%2C60020%2C1404255227840.1404258269942 with entries=82, filesize=65.0m; new WAL /hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1404255227840/sceplus-vm48.almaden.ibm.com%2C60020%2C1404255227840.1404258271776
2014-07-01 16:44:34,008 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-01 16:44:34,058 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1404255227840/sceplus-vm48.almaden.ibm.com%2C60020%2C1404255227840.1404258271776 with entries=79, filesize=61.9m; new WAL /hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1404255227840/sceplus-vm48.almaden.ibm.com%2C60020%2C1404255227840.1404258274009
2014-07-01 16:44:36,438 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-01 16:44:36,535 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 39761 synced till here 39757
2014-07-01 16:44:36,692 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1404255227840/sceplus-vm48.almaden.ibm.com%2C60020%2C1404255227840.1404258274009 with entries=85, filesize=69.0m; new WAL /hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1404255227840/sceplus-vm48.almaden.ibm.com%2C60020%2C1404255227840.1404258276438
2014-07-01 16:44:38,700 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-01 16:44:38,744 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 39856 synced till here 39851
2014-07-01 16:44:39,210 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1404255227840/sceplus-vm48.almaden.ibm.com%2C60020%2C1404255227840.1404258276438 with entries=95, filesize=68.1m; new WAL /hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1404255227840/sceplus-vm48.almaden.ibm.com%2C60020%2C1404255227840.1404258278700
2014-07-01 16:44:40,650 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-01 16:44:40,675 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 39936 synced till here 39934
2014-07-01 16:44:40,696 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1404255227840/sceplus-vm48.almaden.ibm.com%2C60020%2C1404255227840.1404258278700 with entries=80, filesize=62.9m; new WAL /hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1404255227840/sceplus-vm48.almaden.ibm.com%2C60020%2C1404255227840.1404258280650
2014-07-01 16:44:42,375 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-01 16:44:42,408 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1404255227840/sceplus-vm48.almaden.ibm.com%2C60020%2C1404255227840.1404258280650 with entries=80, filesize=62.7m; new WAL /hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1404255227840/sceplus-vm48.almaden.ibm.com%2C60020%2C1404255227840.1404258282376
2014-07-01 16:44:44,996 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-01 16:44:45,085 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1404255227840/sceplus-vm48.almaden.ibm.com%2C60020%2C1404255227840.1404258282376 with entries=84, filesize=63.6m; new WAL /hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1404255227840/sceplus-vm48.almaden.ibm.com%2C60020%2C1404255227840.1404258284996
2014-07-01 16:44:48,077 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-01 16:44:48,152 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1404255227840/sceplus-vm48.almaden.ibm.com%2C60020%2C1404255227840.1404258284996 with entries=78, filesize=62.8m; new WAL /hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1404255227840/sceplus-vm48.almaden.ibm.com%2C60020%2C1404255227840.1404258288078
2014-07-01 16:44:49,991 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-01 16:44:50,027 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1404255227840/sceplus-vm48.almaden.ibm.com%2C60020%2C1404255227840.1404258288078 with entries=77, filesize=62.2m; new WAL /hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1404255227840/sceplus-vm48.almaden.ibm.com%2C60020%2C1404255227840.1404258289991
2014-07-01 16:44:50,492 INFO  [MemStoreFlusher.0] regionserver.DefaultStoreFlusher: Flushed, sequenceid=3871, memsize=433.4m, hasBloomFilter=true, into tmp file hdfs://master:54310/hbase/data/default/usertable/2ed053f05f5eec791a57de24e16f0043/.tmp/27b5488a878e429a9f17da316444950f
2014-07-01 16:44:50,505 DEBUG [MemStoreFlusher.0] regionserver.HRegionFileSystem: Committing store file hdfs://master:54310/hbase/data/default/usertable/2ed053f05f5eec791a57de24e16f0043/.tmp/27b5488a878e429a9f17da316444950f as hdfs://master:54310/hbase/data/default/usertable/2ed053f05f5eec791a57de24e16f0043/family/27b5488a878e429a9f17da316444950f
2014-07-01 16:44:50,603 INFO  [MemStoreFlusher.0] regionserver.HStore: Added hdfs://master:54310/hbase/data/default/usertable/2ed053f05f5eec791a57de24e16f0043/family/27b5488a878e429a9f17da316444950f, entries=1577910, sequenceid=3871, filesize=112.3m
2014-07-01 16:44:50,603 INFO  [MemStoreFlusher.0] regionserver.HRegion: Finished memstore flush of ~605.9m/635337600, currentsize=262.1m/274878720 for region usertable,user2,1404257714774.2ed053f05f5eec791a57de24e16f0043. in 21907ms, sequenceid=3871, compaction requested=true
2014-07-01 16:44:50,605 DEBUG [MemStoreFlusher.0] regionserver.CompactSplitThread: Small Compaction requested: system; Because: MemStoreFlusher.0; compaction_queue=(0:0), split_queue=0, merge_queue=0
2014-07-01 16:44:50,605 DEBUG [regionserver60020-smallCompactions-1404255751957] compactions.RatioBasedCompactionPolicy: Selecting compaction from 16 store files, 0 compacting, 16 eligible, 20 blocking
2014-07-01 16:44:50,605 DEBUG [MemStoreFlusher.0] regionserver.HRegion: Started memstore flush for usertable,user3,1404257714774.a59b89c303508bc329bc1095d0beb9e5., current region memstore size 742.2m
2014-07-01 16:44:50,605 DEBUG [regionserver60020-smallCompactions-1404255751957] compactions.RatioBasedCompactionPolicy: Some files are too large. Excluding 16 files from compaction candidates
2014-07-01 16:44:50,605 DEBUG [regionserver60020-smallCompactions-1404255751957] compactions.ExploringCompactionPolicy: Exploring compaction algorithm has selected 0 files of size 0 starting at candidate #-1 after considering 0 permutations with 0 in ratio
2014-07-01 16:44:50,606 DEBUG [regionserver60020-smallCompactions-1404255751957] compactions.RatioBasedCompactionPolicy: Not compacting files because we only have 0 files ready for compaction. Need 3 to initiate.
2014-07-01 16:44:50,606 DEBUG [regionserver60020-smallCompactions-1404255751957] regionserver.CompactSplitThread: Not compacting usertable,user2,1404257714774.2ed053f05f5eec791a57de24e16f0043. because compaction request was cancelled
2014-07-01 16:44:51,161 DEBUG [MemStoreFlusher.0] util.FSUtils: DFS Client does not support most favored nodes create; using default create
2014-07-01 16:44:51,470 DEBUG [RpcServer.handler=27,port=60020] regionserver.HRegion: Flush requested on usertable,user2,1404257714774.2ed053f05f5eec791a57de24e16f0043.
2014-07-01 16:44:53,289 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-01 16:44:53,317 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 40343 synced till here 40338
2014-07-01 16:44:53,411 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1404255227840/sceplus-vm48.almaden.ibm.com%2C60020%2C1404255227840.1404258289991 with entries=88, filesize=66.7m; new WAL /hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1404255227840/sceplus-vm48.almaden.ibm.com%2C60020%2C1404255227840.1404258293289
2014-07-01 16:44:55,177 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-01 16:44:55,213 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1404255227840/sceplus-vm48.almaden.ibm.com%2C60020%2C1404255227840.1404258293289 with entries=84, filesize=61.1m; new WAL /hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1404255227840/sceplus-vm48.almaden.ibm.com%2C60020%2C1404255227840.1404258295177
2014-07-01 16:44:57,264 INFO  [MemStoreFlusher.1] regionserver.DefaultStoreFlusher: Flushed, sequenceid=3848, memsize=600.8m, hasBloomFilter=true, into tmp file hdfs://master:54310/hbase/data/default/usertable/c831a0ac8e8f14e82fdaef2ba3a5e1f6/.tmp/be674a32ed964861b612d31a67c5686b
2014-07-01 16:44:57,273 DEBUG [MemStoreFlusher.1] regionserver.HRegionFileSystem: Committing store file hdfs://master:54310/hbase/data/default/usertable/c831a0ac8e8f14e82fdaef2ba3a5e1f6/.tmp/be674a32ed964861b612d31a67c5686b as hdfs://master:54310/hbase/data/default/usertable/c831a0ac8e8f14e82fdaef2ba3a5e1f6/family/be674a32ed964861b612d31a67c5686b
2014-07-01 16:44:57,290 INFO  [MemStoreFlusher.1] regionserver.HStore: Added hdfs://master:54310/hbase/data/default/usertable/c831a0ac8e8f14e82fdaef2ba3a5e1f6/family/be674a32ed964861b612d31a67c5686b, entries=2187330, sequenceid=3848, filesize=155.8m
2014-07-01 16:44:57,290 INFO  [MemStoreFlusher.1] regionserver.HRegion: Finished memstore flush of ~923.1m/967973920, currentsize=362.3m/379852320 for region usertable,user4,1404257714774.c831a0ac8e8f14e82fdaef2ba3a5e1f6. in 31563ms, sequenceid=3848, compaction requested=true
2014-07-01 16:44:57,292 DEBUG [MemStoreFlusher.1] regionserver.CompactSplitThread: Small Compaction requested: system; Because: MemStoreFlusher.1; compaction_queue=(0:0), split_queue=0, merge_queue=0
2014-07-01 16:44:57,292 DEBUG [regionserver60020-smallCompactions-1404255751957] compactions.RatioBasedCompactionPolicy: Selecting compaction from 15 store files, 0 compacting, 15 eligible, 20 blocking
2014-07-01 16:44:57,292 DEBUG [regionserver60020-smallCompactions-1404255751957] compactions.RatioBasedCompactionPolicy: Some files are too large. Excluding 15 files from compaction candidates
2014-07-01 16:44:57,292 DEBUG [regionserver60020-smallCompactions-1404255751957] compactions.ExploringCompactionPolicy: Exploring compaction algorithm has selected 0 files of size 0 starting at candidate #-1 after considering 0 permutations with 0 in ratio
2014-07-01 16:44:57,292 DEBUG [regionserver60020-smallCompactions-1404255751957] compactions.RatioBasedCompactionPolicy: Not compacting files because we only have 0 files ready for compaction. Need 3 to initiate.
2014-07-01 16:44:57,292 DEBUG [regionserver60020-smallCompactions-1404255751957] regionserver.CompactSplitThread: Not compacting usertable,user4,1404257714774.c831a0ac8e8f14e82fdaef2ba3a5e1f6. because compaction request was cancelled
2014-07-01 16:44:57,292 DEBUG [MemStoreFlusher.1] regionserver.HRegion: Started memstore flush for usertable,user8,1404257714774.b6ae40892bbb2a1c545234ed91d62d1a., current region memstore size 567.0m
2014-07-01 16:44:57,662 DEBUG [MemStoreFlusher.1] util.FSUtils: DFS Client does not support most favored nodes create; using default create
2014-07-01 16:45:01,581 DEBUG [RpcServer.handler=2,port=60020] regionserver.HRegion: Flush requested on usertable,user4,1404257714774.c831a0ac8e8f14e82fdaef2ba3a5e1f6.
2014-07-01 16:45:02,310 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-01 16:45:02,379 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 40516 synced till here 40514
2014-07-01 16:45:02,465 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1404255227840/sceplus-vm48.almaden.ibm.com%2C60020%2C1404255227840.1404258295177 with entries=89, filesize=64.6m; new WAL /hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1404255227840/sceplus-vm48.almaden.ibm.com%2C60020%2C1404255227840.1404258302311
2014-07-01 16:45:02,466 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1404255227840/sceplus-vm48.almaden.ibm.com%2C60020%2C1404255227840.1404258187089
2014-07-01 16:45:02,466 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1404255227840/sceplus-vm48.almaden.ibm.com%2C60020%2C1404255227840.1404258189130
2014-07-01 16:45:02,466 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1404255227840/sceplus-vm48.almaden.ibm.com%2C60020%2C1404255227840.1404258190975
2014-07-01 16:45:02,466 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1404255227840/sceplus-vm48.almaden.ibm.com%2C60020%2C1404255227840.1404258193678
2014-07-01 16:45:02,466 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1404255227840/sceplus-vm48.almaden.ibm.com%2C60020%2C1404255227840.1404258195803
2014-07-01 16:45:02,466 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1404255227840/sceplus-vm48.almaden.ibm.com%2C60020%2C1404255227840.1404258198098
2014-07-01 16:45:02,467 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1404255227840/sceplus-vm48.almaden.ibm.com%2C60020%2C1404255227840.1404258200316
2014-07-01 16:45:02,467 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1404255227840/sceplus-vm48.almaden.ibm.com%2C60020%2C1404255227840.1404258201723
2014-07-01 16:45:02,467 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1404255227840/sceplus-vm48.almaden.ibm.com%2C60020%2C1404255227840.1404258204135
2014-07-01 16:45:02,467 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1404255227840/sceplus-vm48.almaden.ibm.com%2C60020%2C1404255227840.1404258207227
2014-07-01 16:45:02,467 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1404255227840/sceplus-vm48.almaden.ibm.com%2C60020%2C1404255227840.1404258209667
2014-07-01 16:45:02,467 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1404255227840/sceplus-vm48.almaden.ibm.com%2C60020%2C1404255227840.1404258213281
2014-07-01 16:45:02,467 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1404255227840/sceplus-vm48.almaden.ibm.com%2C60020%2C1404255227840.1404258222927
2014-07-01 16:45:02,468 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1404255227840/sceplus-vm48.almaden.ibm.com%2C60020%2C1404255227840.1404258226506
2014-07-01 16:45:02,468 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1404255227840/sceplus-vm48.almaden.ibm.com%2C60020%2C1404255227840.1404258227516
2014-07-01 16:45:09,163 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-01 16:45:09,181 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 40601 synced till here 40599
2014-07-01 16:45:09,300 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1404255227840/sceplus-vm48.almaden.ibm.com%2C60020%2C1404255227840.1404258302311 with entries=85, filesize=63.2m; new WAL /hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1404255227840/sceplus-vm48.almaden.ibm.com%2C60020%2C1404255227840.1404258309163
2014-07-01 16:45:12,269 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-01 16:45:12,789 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 40682 synced till here 40679
2014-07-01 16:45:13,019 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1404255227840/sceplus-vm48.almaden.ibm.com%2C60020%2C1404255227840.1404258309163 with entries=81, filesize=65.5m; new WAL /hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1404255227840/sceplus-vm48.almaden.ibm.com%2C60020%2C1404255227840.1404258312269
2014-07-01 16:45:14,506 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-01 16:45:14,538 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 40758 synced till here 40757
2014-07-01 16:45:14,560 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1404255227840/sceplus-vm48.almaden.ibm.com%2C60020%2C1404255227840.1404258312269 with entries=76, filesize=61.9m; new WAL /hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1404255227840/sceplus-vm48.almaden.ibm.com%2C60020%2C1404255227840.1404258314506
2014-07-01 16:45:16,085 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-01 16:45:16,343 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1404255227840/sceplus-vm48.almaden.ibm.com%2C60020%2C1404255227840.1404258314506 with entries=82, filesize=66.1m; new WAL /hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1404255227840/sceplus-vm48.almaden.ibm.com%2C60020%2C1404255227840.1404258316086
2014-07-01 16:45:16,683 INFO  [MemStoreFlusher.0] regionserver.DefaultStoreFlusher: Flushed, sequenceid=4031, memsize=681.9m, hasBloomFilter=true, into tmp file hdfs://master:54310/hbase/data/default/usertable/a59b89c303508bc329bc1095d0beb9e5/.tmp/b69597e7859d414fb10a52b51eabcbc2
2014-07-01 16:45:16,697 DEBUG [MemStoreFlusher.0] regionserver.HRegionFileSystem: Committing store file hdfs://master:54310/hbase/data/default/usertable/a59b89c303508bc329bc1095d0beb9e5/.tmp/b69597e7859d414fb10a52b51eabcbc2 as hdfs://master:54310/hbase/data/default/usertable/a59b89c303508bc329bc1095d0beb9e5/family/b69597e7859d414fb10a52b51eabcbc2
2014-07-01 16:45:16,713 INFO  [MemStoreFlusher.0] regionserver.HStore: Added hdfs://master:54310/hbase/data/default/usertable/a59b89c303508bc329bc1095d0beb9e5/family/b69597e7859d414fb10a52b51eabcbc2, entries=2482810, sequenceid=4031, filesize=176.7m
2014-07-01 16:45:16,713 INFO  [MemStoreFlusher.0] regionserver.HRegion: Finished memstore flush of ~742.2m/778282640, currentsize=188.3m/197397680 for region usertable,user3,1404257714774.a59b89c303508bc329bc1095d0beb9e5. in 26108ms, sequenceid=4031, compaction requested=true
2014-07-01 16:45:16,714 DEBUG [regionserver60020-smallCompactions-1404255751957] compactions.RatioBasedCompactionPolicy: Selecting compaction from 16 store files, 0 compacting, 16 eligible, 20 blocking
2014-07-01 16:45:16,714 DEBUG [regionserver60020-smallCompactions-1404255751957] compactions.RatioBasedCompactionPolicy: Some files are too large. Excluding 16 files from compaction candidates
2014-07-01 16:45:16,714 DEBUG [regionserver60020-smallCompactions-1404255751957] compactions.ExploringCompactionPolicy: Exploring compaction algorithm has selected 0 files of size 0 starting at candidate #-1 after considering 0 permutations with 0 in ratio
2014-07-01 16:45:16,714 DEBUG [regionserver60020-smallCompactions-1404255751957] compactions.RatioBasedCompactionPolicy: Not compacting files because we only have 0 files ready for compaction. Need 3 to initiate.
2014-07-01 16:45:16,714 DEBUG [regionserver60020-smallCompactions-1404255751957] regionserver.CompactSplitThread: Not compacting usertable,user3,1404257714774.a59b89c303508bc329bc1095d0beb9e5. because compaction request was cancelled
2014-07-01 16:45:16,714 DEBUG [MemStoreFlusher.0] regionserver.CompactSplitThread: Small Compaction requested: system; Because: MemStoreFlusher.0; compaction_queue=(0:0), split_queue=0, merge_queue=0
2014-07-01 16:45:16,715 DEBUG [MemStoreFlusher.0] regionserver.HRegion: Started memstore flush for usertable,user2,1404257714774.2ed053f05f5eec791a57de24e16f0043., current region memstore size 449.8m
2014-07-01 16:45:16,769 INFO  [MemStoreFlusher.1] regionserver.DefaultStoreFlusher: Flushed, sequenceid=4074, memsize=548.8m, hasBloomFilter=true, into tmp file hdfs://master:54310/hbase/data/default/usertable/b6ae40892bbb2a1c545234ed91d62d1a/.tmp/00961231f6454f8e8e02f00ba09ff9e6
2014-07-01 16:45:16,782 DEBUG [MemStoreFlusher.1] regionserver.HRegionFileSystem: Committing store file hdfs://master:54310/hbase/data/default/usertable/b6ae40892bbb2a1c545234ed91d62d1a/.tmp/00961231f6454f8e8e02f00ba09ff9e6 as hdfs://master:54310/hbase/data/default/usertable/b6ae40892bbb2a1c545234ed91d62d1a/family/00961231f6454f8e8e02f00ba09ff9e6
2014-07-01 16:45:16,791 INFO  [MemStoreFlusher.1] regionserver.HStore: Added hdfs://master:54310/hbase/data/default/usertable/b6ae40892bbb2a1c545234ed91d62d1a/family/00961231f6454f8e8e02f00ba09ff9e6, entries=1998310, sequenceid=4074, filesize=142.3m
2014-07-01 16:45:16,791 INFO  [MemStoreFlusher.1] regionserver.HRegion: Finished memstore flush of ~567.0m/594535440, currentsize=128.0m/134216720 for region usertable,user8,1404257714774.b6ae40892bbb2a1c545234ed91d62d1a. in 19499ms, sequenceid=4074, compaction requested=true
2014-07-01 16:45:16,792 DEBUG [MemStoreFlusher.1] regionserver.CompactSplitThread: Small Compaction requested: system; Because: MemStoreFlusher.1; compaction_queue=(0:0), split_queue=0, merge_queue=0
2014-07-01 16:45:16,792 DEBUG [regionserver60020-smallCompactions-1404255751957] compactions.RatioBasedCompactionPolicy: Selecting compaction from 16 store files, 0 compacting, 16 eligible, 20 blocking
2014-07-01 16:45:16,792 DEBUG [regionserver60020-smallCompactions-1404255751957] compactions.RatioBasedCompactionPolicy: Some files are too large. Excluding 16 files from compaction candidates
2014-07-01 16:45:16,792 DEBUG [MemStoreFlusher.1] regionserver.HRegion: Started memstore flush for usertable,user4,1404257714774.c831a0ac8e8f14e82fdaef2ba3a5e1f6., current region memstore size 490.1m
2014-07-01 16:45:16,792 DEBUG [regionserver60020-smallCompactions-1404255751957] compactions.ExploringCompactionPolicy: Exploring compaction algorithm has selected 0 files of size 0 starting at candidate #-1 after considering 0 permutations with 0 in ratio
2014-07-01 16:45:16,792 DEBUG [regionserver60020-smallCompactions-1404255751957] compactions.RatioBasedCompactionPolicy: Not compacting files because we only have 0 files ready for compaction. Need 3 to initiate.
2014-07-01 16:45:16,792 DEBUG [regionserver60020-smallCompactions-1404255751957] regionserver.CompactSplitThread: Not compacting usertable,user8,1404257714774.b6ae40892bbb2a1c545234ed91d62d1a. because compaction request was cancelled
2014-07-01 16:45:17,062 DEBUG [MemStoreFlusher.0] util.FSUtils: DFS Client does not support most favored nodes create; using default create
2014-07-01 16:45:17,529 DEBUG [MemStoreFlusher.1] util.FSUtils: DFS Client does not support most favored nodes create; using default create
2014-07-01 16:45:19,086 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-01 16:45:19,131 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 40921 synced till here 40920
2014-07-01 16:45:19,177 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1404255227840/sceplus-vm48.almaden.ibm.com%2C60020%2C1404255227840.1404258316086 with entries=81, filesize=61.8m; new WAL /hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1404255227840/sceplus-vm48.almaden.ibm.com%2C60020%2C1404255227840.1404258319087
2014-07-01 16:45:19,177 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1404255227840/sceplus-vm48.almaden.ibm.com%2C60020%2C1404255227840.1404258230081
2014-07-01 16:45:19,177 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1404255227840/sceplus-vm48.almaden.ibm.com%2C60020%2C1404255227840.1404258234358
2014-07-01 16:45:19,178 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1404255227840/sceplus-vm48.almaden.ibm.com%2C60020%2C1404255227840.1404258236455
2014-07-01 16:45:19,178 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1404255227840/sceplus-vm48.almaden.ibm.com%2C60020%2C1404255227840.1404258238765
2014-07-01 16:45:19,178 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1404255227840/sceplus-vm48.almaden.ibm.com%2C60020%2C1404255227840.1404258240046
2014-07-01 16:45:19,178 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1404255227840/sceplus-vm48.almaden.ibm.com%2C60020%2C1404255227840.1404258242797
2014-07-01 16:45:19,178 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1404255227840/sceplus-vm48.almaden.ibm.com%2C60020%2C1404255227840.1404258244478
2014-07-01 16:45:19,178 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1404255227840/sceplus-vm48.almaden.ibm.com%2C60020%2C1404255227840.1404258246382
2014-07-01 16:45:19,178 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1404255227840/sceplus-vm48.almaden.ibm.com%2C60020%2C1404255227840.1404258248200
2014-07-01 16:45:19,179 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1404255227840/sceplus-vm48.almaden.ibm.com%2C60020%2C1404255227840.1404258250500
2014-07-01 16:45:21,721 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-01 16:45:21,784 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 41002 synced till here 41000
2014-07-01 16:45:21,833 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1404255227840/sceplus-vm48.almaden.ibm.com%2C60020%2C1404255227840.1404258319087 with entries=81, filesize=65.6m; new WAL /hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1404255227840/sceplus-vm48.almaden.ibm.com%2C60020%2C1404255227840.1404258321722
2014-07-01 16:45:23,489 DEBUG [RpcServer.handler=44,port=60020] regionserver.HRegion: Flush requested on usertable,user3,1404257714774.a59b89c303508bc329bc1095d0beb9e5.
2014-07-01 16:45:24,567 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-01 16:45:24,597 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 41080 synced till here 41078
2014-07-01 16:45:24,611 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1404255227840/sceplus-vm48.almaden.ibm.com%2C60020%2C1404255227840.1404258321722 with entries=78, filesize=63.9m; new WAL /hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1404255227840/sceplus-vm48.almaden.ibm.com%2C60020%2C1404255227840.1404258324568
2014-07-01 16:45:27,826 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-01 16:45:27,842 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 41164 synced till here 41163
2014-07-01 16:45:27,861 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1404255227840/sceplus-vm48.almaden.ibm.com%2C60020%2C1404255227840.1404258324568 with entries=84, filesize=62.5m; new WAL /hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1404255227840/sceplus-vm48.almaden.ibm.com%2C60020%2C1404255227840.1404258327827
2014-07-01 16:45:30,876 DEBUG [RpcServer.handler=17,port=60020] regionserver.HRegion: Flush requested on usertable,user8,1404257714774.b6ae40892bbb2a1c545234ed91d62d1a.
2014-07-01 16:45:31,558 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-01 16:45:31,594 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1404255227840/sceplus-vm48.almaden.ibm.com%2C60020%2C1404255227840.1404258327827 with entries=82, filesize=61.5m; new WAL /hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1404255227840/sceplus-vm48.almaden.ibm.com%2C60020%2C1404255227840.1404258331558
2014-07-01 16:45:34,474 INFO  [MemStoreFlusher.0] regionserver.DefaultStoreFlusher: Flushed, sequenceid=4146, memsize=448.0m, hasBloomFilter=true, into tmp file hdfs://master:54310/hbase/data/default/usertable/2ed053f05f5eec791a57de24e16f0043/.tmp/18afcfa833a248058026d42cac1aa2d2
2014-07-01 16:45:34,484 DEBUG [MemStoreFlusher.0] regionserver.HRegionFileSystem: Committing store file hdfs://master:54310/hbase/data/default/usertable/2ed053f05f5eec791a57de24e16f0043/.tmp/18afcfa833a248058026d42cac1aa2d2 as hdfs://master:54310/hbase/data/default/usertable/2ed053f05f5eec791a57de24e16f0043/family/18afcfa833a248058026d42cac1aa2d2
2014-07-01 16:45:34,495 INFO  [MemStoreFlusher.0] regionserver.HStore: Added hdfs://master:54310/hbase/data/default/usertable/2ed053f05f5eec791a57de24e16f0043/family/18afcfa833a248058026d42cac1aa2d2, entries=1631290, sequenceid=4146, filesize=116.2m
2014-07-01 16:45:34,495 INFO  [MemStoreFlusher.0] regionserver.HRegion: Finished memstore flush of ~449.8m/471597840, currentsize=134.1m/140654800 for region usertable,user2,1404257714774.2ed053f05f5eec791a57de24e16f0043. in 17780ms, sequenceid=4146, compaction requested=true
2014-07-01 16:45:34,495 DEBUG [MemStoreFlusher.0] regionserver.CompactSplitThread: Small Compaction requested: system; Because: MemStoreFlusher.0; compaction_queue=(0:0), split_queue=0, merge_queue=0
2014-07-01 16:45:34,496 DEBUG [regionserver60020-smallCompactions-1404255751957] compactions.RatioBasedCompactionPolicy: Selecting compaction from 17 store files, 0 compacting, 17 eligible, 20 blocking
2014-07-01 16:45:34,496 DEBUG [regionserver60020-smallCompactions-1404255751957] compactions.RatioBasedCompactionPolicy: Some files are too large. Excluding 17 files from compaction candidates
2014-07-01 16:45:34,496 DEBUG [regionserver60020-smallCompactions-1404255751957] compactions.ExploringCompactionPolicy: Exploring compaction algorithm has selected 0 files of size 0 starting at candidate #-1 after considering 0 permutations with 0 in ratio
2014-07-01 16:45:34,496 DEBUG [regionserver60020-smallCompactions-1404255751957] compactions.RatioBasedCompactionPolicy: Not compacting files because we only have 0 files ready for compaction. Need 3 to initiate.
2014-07-01 16:45:34,496 DEBUG [regionserver60020-smallCompactions-1404255751957] regionserver.CompactSplitThread: Not compacting usertable,user2,1404257714774.2ed053f05f5eec791a57de24e16f0043. because compaction request was cancelled
2014-07-01 16:45:34,496 DEBUG [MemStoreFlusher.0] regionserver.HRegion: Started memstore flush for usertable,user3,1404257714774.a59b89c303508bc329bc1095d0beb9e5., current region memstore size 321.6m
2014-07-01 16:45:34,716 DEBUG [MemStoreFlusher.0] util.FSUtils: DFS Client does not support most favored nodes create; using default create
2014-07-01 16:45:35,543 INFO  [MemStoreFlusher.1] regionserver.DefaultStoreFlusher: Flushed, sequenceid=4147, memsize=488.5m, hasBloomFilter=true, into tmp file hdfs://master:54310/hbase/data/default/usertable/c831a0ac8e8f14e82fdaef2ba3a5e1f6/.tmp/5bb2ff40635a4e50af14991af1d0d30a
2014-07-01 16:45:35,557 DEBUG [MemStoreFlusher.1] regionserver.HRegionFileSystem: Committing store file hdfs://master:54310/hbase/data/default/usertable/c831a0ac8e8f14e82fdaef2ba3a5e1f6/.tmp/5bb2ff40635a4e50af14991af1d0d30a as hdfs://master:54310/hbase/data/default/usertable/c831a0ac8e8f14e82fdaef2ba3a5e1f6/family/5bb2ff40635a4e50af14991af1d0d30a
2014-07-01 16:45:35,564 INFO  [MemStoreFlusher.1] regionserver.HStore: Added hdfs://master:54310/hbase/data/default/usertable/c831a0ac8e8f14e82fdaef2ba3a5e1f6/family/5bb2ff40635a4e50af14991af1d0d30a, entries=1778500, sequenceid=4147, filesize=126.7m
2014-07-01 16:45:35,565 INFO  [MemStoreFlusher.1] regionserver.HRegion: Finished memstore flush of ~490.1m/513922640, currentsize=133.6m/140073680 for region usertable,user4,1404257714774.c831a0ac8e8f14e82fdaef2ba3a5e1f6. in 18773ms, sequenceid=4147, compaction requested=true
2014-07-01 16:45:35,565 DEBUG [MemStoreFlusher.1] regionserver.CompactSplitThread: Small Compaction requested: system; Because: MemStoreFlusher.1; compaction_queue=(0:0), split_queue=0, merge_queue=0
2014-07-01 16:45:35,565 DEBUG [regionserver60020-smallCompactions-1404255751957] compactions.RatioBasedCompactionPolicy: Selecting compaction from 16 store files, 0 compacting, 16 eligible, 20 blocking
2014-07-01 16:45:35,565 DEBUG [regionserver60020-smallCompactions-1404255751957] compactions.RatioBasedCompactionPolicy: Some files are too large. Excluding 16 files from compaction candidates
2014-07-01 16:45:35,565 DEBUG [MemStoreFlusher.1] regionserver.HRegion: Started memstore flush for usertable,user8,1404257714774.b6ae40892bbb2a1c545234ed91d62d1a., current region memstore size 261.9m
2014-07-01 16:45:35,565 DEBUG [regionserver60020-smallCompactions-1404255751957] compactions.ExploringCompactionPolicy: Exploring compaction algorithm has selected 0 files of size 0 starting at candidate #-1 after considering 0 permutations with 0 in ratio
2014-07-01 16:45:35,565 DEBUG [regionserver60020-smallCompactions-1404255751957] compactions.RatioBasedCompactionPolicy: Not compacting files because we only have 0 files ready for compaction. Need 3 to initiate.
2014-07-01 16:45:35,565 DEBUG [regionserver60020-smallCompactions-1404255751957] regionserver.CompactSplitThread: Not compacting usertable,user4,1404257714774.c831a0ac8e8f14e82fdaef2ba3a5e1f6. because compaction request was cancelled
2014-07-01 16:45:36,098 DEBUG [MemStoreFlusher.1] util.FSUtils: DFS Client does not support most favored nodes create; using default create
2014-07-01 16:45:43,587 INFO  [MemStoreFlusher.1] regionserver.DefaultStoreFlusher: Flushed, sequenceid=4233, memsize=261.9m, hasBloomFilter=true, into tmp file hdfs://master:54310/hbase/data/default/usertable/b6ae40892bbb2a1c545234ed91d62d1a/.tmp/be8d6540ffc648cb8e4179e59d06b559
2014-07-01 16:45:43,596 DEBUG [MemStoreFlusher.1] regionserver.HRegionFileSystem: Committing store file hdfs://master:54310/hbase/data/default/usertable/b6ae40892bbb2a1c545234ed91d62d1a/.tmp/be8d6540ffc648cb8e4179e59d06b559 as hdfs://master:54310/hbase/data/default/usertable/b6ae40892bbb2a1c545234ed91d62d1a/family/be8d6540ffc648cb8e4179e59d06b559
2014-07-01 16:45:43,603 INFO  [MemStoreFlusher.1] regionserver.HStore: Added hdfs://master:54310/hbase/data/default/usertable/b6ae40892bbb2a1c545234ed91d62d1a/family/be8d6540ffc648cb8e4179e59d06b559, entries=953750, sequenceid=4233, filesize=67.9m
2014-07-01 16:45:43,603 INFO  [MemStoreFlusher.1] regionserver.HRegion: Finished memstore flush of ~261.9m/274672720, currentsize=0.0/0 for region usertable,user8,1404257714774.b6ae40892bbb2a1c545234ed91d62d1a. in 8038ms, sequenceid=4233, compaction requested=true
2014-07-01 16:45:43,604 DEBUG [MemStoreFlusher.1] regionserver.CompactSplitThread: Small Compaction requested: system; Because: MemStoreFlusher.1; compaction_queue=(0:0), split_queue=0, merge_queue=0
2014-07-01 16:45:43,604 DEBUG [regionserver60020-smallCompactions-1404255751957] compactions.RatioBasedCompactionPolicy: Selecting compaction from 17 store files, 0 compacting, 17 eligible, 20 blocking
2014-07-01 16:45:43,604 DEBUG [regionserver60020-smallCompactions-1404255751957] compactions.RatioBasedCompactionPolicy: Some files are too large. Excluding 17 files from compaction candidates
2014-07-01 16:45:43,604 DEBUG [regionserver60020-smallCompactions-1404255751957] compactions.ExploringCompactionPolicy: Exploring compaction algorithm has selected 0 files of size 0 starting at candidate #-1 after considering 0 permutations with 0 in ratio
2014-07-01 16:45:43,604 DEBUG [regionserver60020-smallCompactions-1404255751957] compactions.RatioBasedCompactionPolicy: Not compacting files because we only have 0 files ready for compaction. Need 3 to initiate.
2014-07-01 16:45:43,604 DEBUG [regionserver60020-smallCompactions-1404255751957] regionserver.CompactSplitThread: Not compacting usertable,user8,1404257714774.b6ae40892bbb2a1c545234ed91d62d1a. because compaction request was cancelled
2014-07-01 16:45:44,522 INFO  [MemStoreFlusher.0] regionserver.DefaultStoreFlusher: Flushed, sequenceid=4230, memsize=321.6m, hasBloomFilter=true, into tmp file hdfs://master:54310/hbase/data/default/usertable/a59b89c303508bc329bc1095d0beb9e5/.tmp/18b042d528fc47e0a9bbea6645cd6c93
2014-07-01 16:45:44,533 DEBUG [MemStoreFlusher.0] regionserver.HRegionFileSystem: Committing store file hdfs://master:54310/hbase/data/default/usertable/a59b89c303508bc329bc1095d0beb9e5/.tmp/18b042d528fc47e0a9bbea6645cd6c93 as hdfs://master:54310/hbase/data/default/usertable/a59b89c303508bc329bc1095d0beb9e5/family/18b042d528fc47e0a9bbea6645cd6c93
2014-07-01 16:45:44,540 INFO  [MemStoreFlusher.0] regionserver.HStore: Added hdfs://master:54310/hbase/data/default/usertable/a59b89c303508bc329bc1095d0beb9e5/family/18b042d528fc47e0a9bbea6645cd6c93, entries=1171100, sequenceid=4230, filesize=83.4m
2014-07-01 16:45:44,541 INFO  [MemStoreFlusher.0] regionserver.HRegion: Finished memstore flush of ~321.6m/337266960, currentsize=0.0/0 for region usertable,user3,1404257714774.a59b89c303508bc329bc1095d0beb9e5. in 10045ms, sequenceid=4230, compaction requested=true
2014-07-01 16:45:44,541 DEBUG [MemStoreFlusher.0] regionserver.CompactSplitThread: Small Compaction requested: system; Because: MemStoreFlusher.0; compaction_queue=(0:0), split_queue=0, merge_queue=0
2014-07-01 16:45:44,541 DEBUG [regionserver60020-smallCompactions-1404255751957] compactions.RatioBasedCompactionPolicy: Selecting compaction from 17 store files, 0 compacting, 17 eligible, 20 blocking
2014-07-01 16:45:44,541 DEBUG [regionserver60020-smallCompactions-1404255751957] compactions.RatioBasedCompactionPolicy: Some files are too large. Excluding 17 files from compaction candidates
2014-07-01 16:45:44,541 DEBUG [regionserver60020-smallCompactions-1404255751957] compactions.ExploringCompactionPolicy: Exploring compaction algorithm has selected 0 files of size 0 starting at candidate #-1 after considering 0 permutations with 0 in ratio
2014-07-01 16:45:44,541 DEBUG [regionserver60020-smallCompactions-1404255751957] compactions.RatioBasedCompactionPolicy: Not compacting files because we only have 0 files ready for compaction. Need 3 to initiate.
2014-07-01 16:45:44,541 DEBUG [regionserver60020-smallCompactions-1404255751957] regionserver.CompactSplitThread: Not compacting usertable,user3,1404257714774.a59b89c303508bc329bc1095d0beb9e5. because compaction request was cancelled
2014-07-01 16:48:47,928 DEBUG [LruStats #0] hfile.LruBlockCache: Total=1.01 GB, free=2.95 GB, max=3.96 GB, blocks=16228, accesses=17313052, hits=17040328, hitRatio=98.42%, , cachingAccesses=17125114, cachingHits=17029225, cachingHitsRatio=99.44%, evictions=2, evicted=78407, evictedPerRun=39203.5
2014-07-01 16:53:47,928 DEBUG [LruStats #0] hfile.LruBlockCache: Total=1.02 GB, free=2.94 GB, max=3.96 GB, blocks=16356, accesses=20428984, hits=20156132, hitRatio=98.66%, , cachingAccesses=20241045, cachingHits=20145028, cachingHitsRatio=99.52%, evictions=2, evicted=78407, evictedPerRun=39203.5
2014-07-01 16:58:47,928 DEBUG [LruStats #0] hfile.LruBlockCache: Total=1.02 GB, free=2.94 GB, max=3.96 GB, blocks=16385, accesses=23639613, hits=23366732, hitRatio=98.84%, , cachingAccesses=23451674, cachingHits=23355628, cachingHitsRatio=99.59%, evictions=2, evicted=78407, evictedPerRun=39203.5
2014-07-01 17:03:47,928 DEBUG [LruStats #0] hfile.LruBlockCache: Total=1.02 GB, free=2.94 GB, max=3.96 GB, blocks=16393, accesses=26665375, hits=26392486, hitRatio=98.97%, , cachingAccesses=26477436, cachingHits=26381382, cachingHitsRatio=99.63%, evictions=2, evicted=78407, evictedPerRun=39203.5
2014-07-01 17:04:07,141 INFO  [JvmPauseMonitor] util.JvmPauseMonitor: Detected pause in JVM or host machine (eg GC): pause of approximately 4846ms
GC pool 'ParNew' had collection(s): count=1 time=4941ms
2014-07-01 17:04:12,599 INFO  [JvmPauseMonitor] util.JvmPauseMonitor: Detected pause in JVM or host machine (eg GC): pause of approximately 1955ms
GC pool 'ParNew' had collection(s): count=1 time=2068ms
2014-07-01 17:04:17,217 INFO  [JvmPauseMonitor] util.JvmPauseMonitor: Detected pause in JVM or host machine (eg GC): pause of approximately 1115ms
GC pool 'ParNew' had collection(s): count=1 time=1490ms
2014-07-01 17:04:22,308 INFO  [JvmPauseMonitor] util.JvmPauseMonitor: Detected pause in JVM or host machine (eg GC): pause of approximately 1089ms
GC pool 'ParNew' had collection(s): count=1 time=1352ms
2014-07-01 17:04:45,267 WARN  [RpcServer.handler=39,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":14045,"call":"Get(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$GetRequest)","client":"9.1.143.58:58325","starttimems":1404259471222,"queuetimems":0,"class":"HRegionServer","responsesize":1553,"method":"Get"}
2014-07-01 17:04:45,268 WARN  [regionserver60020] util.Sleeper: We slept 16023ms instead of 3000ms, this is likely due to a long garbage collecting pause and it's usually bad, see http://hbase.apache.org/book.html#trouble.rs.runtime.zkexpired
2014-07-01 17:04:45,268 WARN  [JvmPauseMonitor] util.JvmPauseMonitor: Detected pause in JVM or host machine (eg GC): pause of approximately 13549ms
GC pool 'ParNew' had collection(s): count=1 time=659ms
GC pool 'ConcurrentMarkSweep' had collection(s): count=1 time=13382ms
2014-07-01 17:04:45,269 WARN  [RpcServer.handler=35,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":14046,"call":"Get(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$GetRequest)","client":"9.1.143.58:58325","starttimems":1404259471222,"queuetimems":0,"class":"HRegionServer","responsesize":1553,"method":"Get"}
2014-07-01 17:05:24,036 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-01 17:05:24,058 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 41378 synced till here 41375
2014-07-01 17:05:24,096 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1404255227840/sceplus-vm48.almaden.ibm.com%2C60020%2C1404255227840.1404258331558 with entries=132, filesize=65.2m; new WAL /hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1404255227840/sceplus-vm48.almaden.ibm.com%2C60020%2C1404255227840.1404259524039
2014-07-01 17:05:37,366 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-01 17:05:37,393 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1404255227840/sceplus-vm48.almaden.ibm.com%2C60020%2C1404255227840.1404259524039 with entries=86, filesize=61.3m; new WAL /hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1404255227840/sceplus-vm48.almaden.ibm.com%2C60020%2C1404255227840.1404259537366
2014-07-01 17:05:40,696 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-01 17:05:40,742 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1404255227840/sceplus-vm48.almaden.ibm.com%2C60020%2C1404255227840.1404259537366 with entries=84, filesize=61.7m; new WAL /hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1404255227840/sceplus-vm48.almaden.ibm.com%2C60020%2C1404255227840.1404259540697
2014-07-01 17:05:40,990 DEBUG [RpcServer.handler=20,port=60020] regionserver.HRegion: Flush requested on usertable,user9,1404257714774.84635e5ca79006f09dceafa095992aab.
2014-07-01 17:05:40,990 DEBUG [MemStoreFlusher.0] regionserver.HRegion: Started memstore flush for usertable,user9,1404257714774.84635e5ca79006f09dceafa095992aab., current region memstore size 256.4m
2014-07-01 17:05:41,154 DEBUG [MemStoreFlusher.0] util.FSUtils: DFS Client does not support most favored nodes create; using default create
2014-07-01 17:05:48,643 INFO  [MemStoreFlusher.0] regionserver.DefaultStoreFlusher: Flushed, sequenceid=4284, memsize=246.4m, hasBloomFilter=true, into tmp file hdfs://master:54310/hbase/data/default/usertable/84635e5ca79006f09dceafa095992aab/.tmp/53ddb378321442c28f0128532b12027b
2014-07-01 17:05:48,652 DEBUG [MemStoreFlusher.0] regionserver.HRegionFileSystem: Committing store file hdfs://master:54310/hbase/data/default/usertable/84635e5ca79006f09dceafa095992aab/.tmp/53ddb378321442c28f0128532b12027b as hdfs://master:54310/hbase/data/default/usertable/84635e5ca79006f09dceafa095992aab/family/53ddb378321442c28f0128532b12027b
2014-07-01 17:05:48,661 INFO  [MemStoreFlusher.0] regionserver.HStore: Added hdfs://master:54310/hbase/data/default/usertable/84635e5ca79006f09dceafa095992aab/family/53ddb378321442c28f0128532b12027b, entries=897160, sequenceid=4284, filesize=63.9m
2014-07-01 17:05:48,661 INFO  [MemStoreFlusher.0] regionserver.HRegion: Finished memstore flush of ~256.8m/269304000, currentsize=5.2m/5465520 for region usertable,user9,1404257714774.84635e5ca79006f09dceafa095992aab. in 7671ms, sequenceid=4284, compaction requested=true
2014-07-01 17:05:48,661 DEBUG [MemStoreFlusher.0] regionserver.CompactSplitThread: Small Compaction requested: system; Because: MemStoreFlusher.0; compaction_queue=(0:0), split_queue=0, merge_queue=0
2014-07-01 17:05:48,662 DEBUG [regionserver60020-smallCompactions-1404255751957] compactions.RatioBasedCompactionPolicy: Selecting compaction from 7 store files, 0 compacting, 7 eligible, 20 blocking
2014-07-01 17:05:48,662 DEBUG [regionserver60020-smallCompactions-1404255751957] compactions.RatioBasedCompactionPolicy: Some files are too large. Excluding 7 files from compaction candidates
2014-07-01 17:05:48,662 DEBUG [regionserver60020-smallCompactions-1404255751957] compactions.ExploringCompactionPolicy: Exploring compaction algorithm has selected 0 files of size 0 starting at candidate #-1 after considering 0 permutations with 0 in ratio
2014-07-01 17:05:48,662 DEBUG [regionserver60020-smallCompactions-1404255751957] compactions.RatioBasedCompactionPolicy: Not compacting files because we only have 0 files ready for compaction. Need 3 to initiate.
2014-07-01 17:05:48,662 DEBUG [regionserver60020-smallCompactions-1404255751957] regionserver.CompactSplitThread: Not compacting usertable,user9,1404257714774.84635e5ca79006f09dceafa095992aab. because compaction request was cancelled
2014-07-01 17:06:05,918 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-01 17:06:05,958 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 41669 synced till here 41666
2014-07-01 17:06:05,991 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1404255227840/sceplus-vm48.almaden.ibm.com%2C60020%2C1404255227840.1404259540697 with entries=121, filesize=65.5m; new WAL /hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1404255227840/sceplus-vm48.almaden.ibm.com%2C60020%2C1404255227840.1404259565918
2014-07-01 17:06:05,992 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1404255227840/sceplus-vm48.almaden.ibm.com%2C60020%2C1404255227840.1404258253047
2014-07-01 17:06:05,992 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1404255227840/sceplus-vm48.almaden.ibm.com%2C60020%2C1404255227840.1404258255344
2014-07-01 17:06:05,992 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1404255227840/sceplus-vm48.almaden.ibm.com%2C60020%2C1404255227840.1404258257337
2014-07-01 17:06:05,992 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1404255227840/sceplus-vm48.almaden.ibm.com%2C60020%2C1404255227840.1404258258565
2014-07-01 17:06:05,992 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1404255227840/sceplus-vm48.almaden.ibm.com%2C60020%2C1404255227840.1404258260415
2014-07-01 17:06:05,992 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1404255227840/sceplus-vm48.almaden.ibm.com%2C60020%2C1404255227840.1404258261963
2014-07-01 17:06:05,992 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1404255227840/sceplus-vm48.almaden.ibm.com%2C60020%2C1404255227840.1404258265371
2014-07-01 17:06:05,992 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1404255227840/sceplus-vm48.almaden.ibm.com%2C60020%2C1404255227840.1404258268212
2014-07-01 17:06:05,992 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1404255227840/sceplus-vm48.almaden.ibm.com%2C60020%2C1404255227840.1404258269942
2014-07-01 17:06:05,992 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1404255227840/sceplus-vm48.almaden.ibm.com%2C60020%2C1404255227840.1404258271776
2014-07-01 17:06:05,992 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1404255227840/sceplus-vm48.almaden.ibm.com%2C60020%2C1404255227840.1404258274009
2014-07-01 17:06:05,992 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1404255227840/sceplus-vm48.almaden.ibm.com%2C60020%2C1404255227840.1404258276438
2014-07-01 17:06:05,992 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1404255227840/sceplus-vm48.almaden.ibm.com%2C60020%2C1404255227840.1404258278700
2014-07-01 17:06:05,992 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1404255227840/sceplus-vm48.almaden.ibm.com%2C60020%2C1404255227840.1404258280650
2014-07-01 17:06:05,992 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1404255227840/sceplus-vm48.almaden.ibm.com%2C60020%2C1404255227840.1404258282376
2014-07-01 17:06:05,993 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1404255227840/sceplus-vm48.almaden.ibm.com%2C60020%2C1404255227840.1404258284996
2014-07-01 17:06:05,993 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1404255227840/sceplus-vm48.almaden.ibm.com%2C60020%2C1404255227840.1404258288078
2014-07-01 17:06:05,993 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1404255227840/sceplus-vm48.almaden.ibm.com%2C60020%2C1404255227840.1404258289991
2014-07-01 17:06:05,993 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1404255227840/sceplus-vm48.almaden.ibm.com%2C60020%2C1404255227840.1404258293289
2014-07-01 17:06:05,993 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1404255227840/sceplus-vm48.almaden.ibm.com%2C60020%2C1404255227840.1404258295177
2014-07-01 17:06:05,993 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1404255227840/sceplus-vm48.almaden.ibm.com%2C60020%2C1404255227840.1404258302311
2014-07-01 17:06:05,993 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1404255227840/sceplus-vm48.almaden.ibm.com%2C60020%2C1404255227840.1404258309163
2014-07-01 17:06:05,993 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1404255227840/sceplus-vm48.almaden.ibm.com%2C60020%2C1404255227840.1404258312269
2014-07-01 17:06:05,993 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1404255227840/sceplus-vm48.almaden.ibm.com%2C60020%2C1404255227840.1404258314506
2014-07-01 17:06:06,806 DEBUG [RpcServer.handler=21,port=60020] regionserver.HRegion: Flush requested on usertable,user2,1404257714774.2ed053f05f5eec791a57de24e16f0043.
2014-07-01 17:06:06,806 DEBUG [MemStoreFlusher.0] regionserver.HRegion: Started memstore flush for usertable,user2,1404257714774.2ed053f05f5eec791a57de24e16f0043., current region memstore size 256.5m
2014-07-01 17:06:07,088 DEBUG [RpcServer.handler=18,port=60020] regionserver.HRegion: Flush requested on usertable,user4,1404257714774.c831a0ac8e8f14e82fdaef2ba3a5e1f6.
2014-07-01 17:06:07,088 DEBUG [MemStoreFlusher.1] regionserver.HRegion: Started memstore flush for usertable,user4,1404257714774.c831a0ac8e8f14e82fdaef2ba3a5e1f6., current region memstore size 256.1m
2014-07-01 17:06:07,123 DEBUG [MemStoreFlusher.0] util.FSUtils: DFS Client does not support most favored nodes create; using default create
2014-07-01 17:06:07,278 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-01 17:06:07,292 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 41758 synced till here 41756
2014-07-01 17:06:07,318 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1404255227840/sceplus-vm48.almaden.ibm.com%2C60020%2C1404255227840.1404259565918 with entries=89, filesize=64.2m; new WAL /hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1404255227840/sceplus-vm48.almaden.ibm.com%2C60020%2C1404255227840.1404259567278
2014-07-01 17:06:07,378 DEBUG [MemStoreFlusher.1] util.FSUtils: DFS Client does not support most favored nodes create; using default create
2014-07-01 17:06:11,796 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-01 17:06:11,822 DEBUG [regionserver60020.logRoller] wal.FSHLog: cleanupCurrentWriter  waiting for transactions to get synced  total 41842 synced till here 41840
2014-07-01 17:06:11,835 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1404255227840/sceplus-vm48.almaden.ibm.com%2C60020%2C1404255227840.1404259567278 with entries=84, filesize=62.5m; new WAL /hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1404255227840/sceplus-vm48.almaden.ibm.com%2C60020%2C1404255227840.1404259571796
2014-07-01 17:06:13,725 INFO  [MemStoreFlusher.1] regionserver.DefaultStoreFlusher: Flushed, sequenceid=4330, memsize=216.3m, hasBloomFilter=true, into tmp file hdfs://master:54310/hbase/data/default/usertable/c831a0ac8e8f14e82fdaef2ba3a5e1f6/.tmp/f65336fa3a1e4546bed3e4028e66bdc9
2014-07-01 17:06:13,734 DEBUG [MemStoreFlusher.1] regionserver.HRegionFileSystem: Committing store file hdfs://master:54310/hbase/data/default/usertable/c831a0ac8e8f14e82fdaef2ba3a5e1f6/.tmp/f65336fa3a1e4546bed3e4028e66bdc9 as hdfs://master:54310/hbase/data/default/usertable/c831a0ac8e8f14e82fdaef2ba3a5e1f6/family/f65336fa3a1e4546bed3e4028e66bdc9
2014-07-01 17:06:13,741 INFO  [MemStoreFlusher.1] regionserver.HStore: Added hdfs://master:54310/hbase/data/default/usertable/c831a0ac8e8f14e82fdaef2ba3a5e1f6/family/f65336fa3a1e4546bed3e4028e66bdc9, entries=787540, sequenceid=4330, filesize=56.2m
2014-07-01 17:06:13,742 INFO  [MemStoreFlusher.1] regionserver.HRegion: Finished memstore flush of ~264.0m/276843120, currentsize=31.3m/32813040 for region usertable,user4,1404257714774.c831a0ac8e8f14e82fdaef2ba3a5e1f6. in 6654ms, sequenceid=4330, compaction requested=true
2014-07-01 17:06:13,742 DEBUG [MemStoreFlusher.1] regionserver.CompactSplitThread: Small Compaction requested: system; Because: MemStoreFlusher.1; compaction_queue=(0:0), split_queue=0, merge_queue=0
2014-07-01 17:06:13,742 DEBUG [regionserver60020-smallCompactions-1404255751957] compactions.RatioBasedCompactionPolicy: Selecting compaction from 17 store files, 0 compacting, 17 eligible, 20 blocking
2014-07-01 17:06:13,742 DEBUG [regionserver60020-smallCompactions-1404255751957] compactions.RatioBasedCompactionPolicy: Some files are too large. Excluding 17 files from compaction candidates
2014-07-01 17:06:13,742 DEBUG [regionserver60020-smallCompactions-1404255751957] compactions.ExploringCompactionPolicy: Exploring compaction algorithm has selected 0 files of size 0 starting at candidate #-1 after considering 0 permutations with 0 in ratio
2014-07-01 17:06:13,743 DEBUG [regionserver60020-smallCompactions-1404255751957] compactions.RatioBasedCompactionPolicy: Not compacting files because we only have 0 files ready for compaction. Need 3 to initiate.
2014-07-01 17:06:13,743 DEBUG [regionserver60020-smallCompactions-1404255751957] regionserver.CompactSplitThread: Not compacting usertable,user4,1404257714774.c831a0ac8e8f14e82fdaef2ba3a5e1f6. because compaction request was cancelled
2014-07-01 17:06:14,076 INFO  [MemStoreFlusher.0] regionserver.DefaultStoreFlusher: Flushed, sequenceid=4329, memsize=216.7m, hasBloomFilter=true, into tmp file hdfs://master:54310/hbase/data/default/usertable/2ed053f05f5eec791a57de24e16f0043/.tmp/7bf26f5967fb4cbba846a357782748b7
2014-07-01 17:06:14,103 DEBUG [MemStoreFlusher.0] regionserver.HRegionFileSystem: Committing store file hdfs://master:54310/hbase/data/default/usertable/2ed053f05f5eec791a57de24e16f0043/.tmp/7bf26f5967fb4cbba846a357782748b7 as hdfs://master:54310/hbase/data/default/usertable/2ed053f05f5eec791a57de24e16f0043/family/7bf26f5967fb4cbba846a357782748b7
2014-07-01 17:06:14,112 INFO  [MemStoreFlusher.0] regionserver.HStore: Added hdfs://master:54310/hbase/data/default/usertable/2ed053f05f5eec791a57de24e16f0043/family/7bf26f5967fb4cbba846a357782748b7, entries=788840, sequenceid=4329, filesize=56.3m
2014-07-01 17:06:14,112 INFO  [MemStoreFlusher.0] regionserver.HRegion: Finished memstore flush of ~264.4m/277227760, currentsize=37.3m/39063360 for region usertable,user2,1404257714774.2ed053f05f5eec791a57de24e16f0043. in 7306ms, sequenceid=4329, compaction requested=true
2014-07-01 17:06:14,113 DEBUG [MemStoreFlusher.0] regionserver.CompactSplitThread: Small Compaction requested: system; Because: MemStoreFlusher.0; compaction_queue=(0:0), split_queue=0, merge_queue=0
2014-07-01 17:06:14,113 DEBUG [regionserver60020-smallCompactions-1404255751957] compactions.RatioBasedCompactionPolicy: Selecting compaction from 18 store files, 0 compacting, 18 eligible, 20 blocking
2014-07-01 17:06:14,113 DEBUG [regionserver60020-smallCompactions-1404255751957] compactions.RatioBasedCompactionPolicy: Some files are too large. Excluding 18 files from compaction candidates
2014-07-01 17:06:14,113 DEBUG [regionserver60020-smallCompactions-1404255751957] compactions.ExploringCompactionPolicy: Exploring compaction algorithm has selected 0 files of size 0 starting at candidate #-1 after considering 0 permutations with 0 in ratio
2014-07-01 17:06:14,113 DEBUG [regionserver60020-smallCompactions-1404255751957] compactions.RatioBasedCompactionPolicy: Not compacting files because we only have 0 files ready for compaction. Need 3 to initiate.
2014-07-01 17:06:14,113 DEBUG [regionserver60020-smallCompactions-1404255751957] regionserver.CompactSplitThread: Not compacting usertable,user2,1404257714774.2ed053f05f5eec791a57de24e16f0043. because compaction request was cancelled
2014-07-01 17:06:15,511 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-01 17:06:15,776 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1404255227840/sceplus-vm48.almaden.ibm.com%2C60020%2C1404255227840.1404259571796 with entries=85, filesize=62.6m; new WAL /hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1404255227840/sceplus-vm48.almaden.ibm.com%2C60020%2C1404255227840.1404259575511
2014-07-01 17:06:15,777 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1404255227840/sceplus-vm48.almaden.ibm.com%2C60020%2C1404255227840.1404258316086
2014-07-01 17:06:15,777 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1404255227840/sceplus-vm48.almaden.ibm.com%2C60020%2C1404255227840.1404258319087
2014-07-01 17:06:15,777 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1404255227840/sceplus-vm48.almaden.ibm.com%2C60020%2C1404255227840.1404258321722
2014-07-01 17:06:15,777 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1404255227840/sceplus-vm48.almaden.ibm.com%2C60020%2C1404255227840.1404258324568
2014-07-01 17:06:15,777 DEBUG [regionserver60020.logRoller] wal.FSHLog: log file is ready for archiving hdfs://master:54310/hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1404255227840/sceplus-vm48.almaden.ibm.com%2C60020%2C1404255227840.1404258327827
2014-07-01 17:06:20,873 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-01 17:06:21,158 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1404255227840/sceplus-vm48.almaden.ibm.com%2C60020%2C1404255227840.1404259575511 with entries=101, filesize=73.8m; new WAL /hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1404255227840/sceplus-vm48.almaden.ibm.com%2C60020%2C1404255227840.1404259580874
2014-07-01 17:06:24,406 DEBUG [regionserver60020.logRoller] regionserver.LogRoller: HLog roll requested
2014-07-01 17:06:24,758 INFO  [regionserver60020.logRoller] wal.FSHLog: Rolled WAL /hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1404255227840/sceplus-vm48.almaden.ibm.com%2C60020%2C1404255227840.1404259580874 with entries=110, filesize=69.2m; new WAL /hbase/WALs/sceplus-vm48.almaden.ibm.com,60020,1404255227840/sceplus-vm48.almaden.ibm.com%2C60020%2C1404255227840.1404259584407
2014-07-01 17:08:47,928 DEBUG [LruStats #0] hfile.LruBlockCache: Total=2.86 GB, free=1.1 GB, max=3.96 GB, blocks=46221, accesses=34834685, hits=34531782, hitRatio=99.13%, , cachingAccesses=34646746, cachingHits=34520678, cachingHitsRatio=99.63%, evictions=2, evicted=78407, evictedPerRun=39203.5
2014-07-01 17:13:47,928 DEBUG [LruStats #0] hfile.LruBlockCache: Total=2.86 GB, free=1.1 GB, max=3.96 GB, blocks=46221, accesses=34834685, hits=34531782, hitRatio=99.13%, , cachingAccesses=34646746, cachingHits=34520678, cachingHitsRatio=99.63%, evictions=2, evicted=78407, evictedPerRun=39203.5
2014-07-01 17:18:47,928 DEBUG [LruStats #0] hfile.LruBlockCache: Total=2.86 GB, free=1.1 GB, max=3.96 GB, blocks=46221, accesses=34834685, hits=34531782, hitRatio=99.13%, , cachingAccesses=34646746, cachingHits=34520678, cachingHitsRatio=99.63%, evictions=2, evicted=78407, evictedPerRun=39203.5
